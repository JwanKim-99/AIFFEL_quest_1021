{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef8bdc78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 주의! ray를 tensorflow보다 먼저 import하면 오류가 발생할 수 있습니다\n",
    "import io, json, os, math , sys\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Add, Concatenate, Lambda\n",
    "from tensorflow.keras.layers import Input, Conv2D, ReLU, MaxPool2D\n",
    "from tensorflow.keras.layers import UpSampling2D, ZeroPadding2D\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "import ray\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "PROJECT_PATH = os.getenv('HOME') + '/aiffel/mpii'\n",
    "IMAGE_PATH = os.path.join(PROJECT_PATH, 'images')\n",
    "MODEL_PATH = os.path.join(PROJECT_PATH, 'models')\n",
    "TFRECORD_PATH = os.path.join(PROJECT_PATH, 'tfrecords_mpii')\n",
    "TRAIN_JSON = os.path.join(PROJECT_PATH, 'mpii_human_pose_v1_u12_2', 'train.json')\n",
    "VALID_JSON = os.path.join(PROJECT_PATH, 'mpii_human_pose_v1_u12_2', 'validation.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f99ffcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"joints_vis\": [\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1\n",
      "  ],\n",
      "  \"joints\": [\n",
      "    [\n",
      "      620.0,\n",
      "      394.0\n",
      "    ],\n",
      "    [\n",
      "      616.0,\n",
      "      269.0\n",
      "    ],\n",
      "    [\n",
      "      573.0,\n",
      "      185.0\n",
      "    ],\n",
      "    [\n",
      "      647.0,\n",
      "      188.0\n",
      "    ],\n",
      "    [\n",
      "      661.0,\n",
      "      221.0\n",
      "    ],\n",
      "    [\n",
      "      656.0,\n",
      "      231.0\n",
      "    ],\n",
      "    [\n",
      "      610.0,\n",
      "      187.0\n",
      "    ],\n",
      "    [\n",
      "      647.0,\n",
      "      176.0\n",
      "    ],\n",
      "    [\n",
      "      637.0201,\n",
      "      189.8183\n",
      "    ],\n",
      "    [\n",
      "      695.9799,\n",
      "      108.1817\n",
      "    ],\n",
      "    [\n",
      "      606.0,\n",
      "      217.0\n",
      "    ],\n",
      "    [\n",
      "      553.0,\n",
      "      161.0\n",
      "    ],\n",
      "    [\n",
      "      601.0,\n",
      "      167.0\n",
      "    ],\n",
      "    [\n",
      "      692.0,\n",
      "      185.0\n",
      "    ],\n",
      "    [\n",
      "      693.0,\n",
      "      240.0\n",
      "    ],\n",
      "    [\n",
      "      688.0,\n",
      "      313.0\n",
      "    ]\n",
      "  ],\n",
      "  \"image\": \"015601864.jpg\",\n",
      "  \"scale\": 3.021046,\n",
      "  \"center\": [\n",
      "    594.0,\n",
      "    257.0\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "with open(TRAIN_JSON) as train_json:\n",
    "    train_annos = json.load(train_json)\n",
    "    json_formatted_str = json.dumps(train_annos[0], indent=2)\n",
    "    print(json_formatted_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63d5bdcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_one_annotation(anno, image_dir):\n",
    "    filename = anno['image']\n",
    "    joints = anno['joints']\n",
    "    joints_visibility = anno['joints_vis']\n",
    "    annotation = {\n",
    "        'filename': filename,\n",
    "        'filepath': os.path.join(image_dir, filename),\n",
    "        'joints_visibility': joints_visibility,\n",
    "        'joints': joints,\n",
    "        'center': anno['center'],\n",
    "        'scale' : anno['scale']\n",
    "    }\n",
    "    return annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4359625e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'filename': '015601864.jpg', 'filepath': '/aiffel/aiffel/mpii/images/015601864.jpg', 'joints_visibility': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'joints': [[620.0, 394.0], [616.0, 269.0], [573.0, 185.0], [647.0, 188.0], [661.0, 221.0], [656.0, 231.0], [610.0, 187.0], [647.0, 176.0], [637.0201, 189.8183], [695.9799, 108.1817], [606.0, 217.0], [553.0, 161.0], [601.0, 167.0], [692.0, 185.0], [693.0, 240.0], [688.0, 313.0]], 'center': [594.0, 257.0], 'scale': 3.021046}\n"
     ]
    }
   ],
   "source": [
    "with open(TRAIN_JSON) as train_json:\n",
    "    train_annos = json.load(train_json)\n",
    "    test = parse_one_annotation(train_annos[0], IMAGE_PATH)\n",
    "    print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea8f2b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_tfexample(anno):\n",
    "\n",
    "    # byte 인코딩을 위한 함수\n",
    "    def _bytes_feature(value):\n",
    "        if isinstance(value, type(tf.constant(0))):\n",
    "            value = value.numpy()\n",
    "        return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "    filename = anno['filename']\n",
    "    filepath = anno['filepath']\n",
    "    with open(filepath, 'rb') as image_file:\n",
    "        content = image_file.read()\n",
    "\n",
    "    image = Image.open(filepath)\n",
    "    if image.format != 'JPEG' or image.mode != 'RGB':\n",
    "        image_rgb = image.convert('RGB')\n",
    "        with io.BytesIO() as output:\n",
    "            image_rgb.save(output, format=\"JPEG\", quality=95)\n",
    "            content = output.getvalue()\n",
    "\n",
    "    width, height = image.size\n",
    "    depth = 3\n",
    "\n",
    "    c_x = int(anno['center'][0])\n",
    "    c_y = int(anno['center'][1])\n",
    "    scale = anno['scale']\n",
    "\n",
    "    x = [\n",
    "        int(joint[0]) if joint[0] >= 0 else int(joint[0]) \n",
    "        for joint in anno['joints']\n",
    "    ]\n",
    "    y = [\n",
    "        int(joint[1]) if joint[1] >= 0 else int(joint[0]) \n",
    "        for joint in anno['joints']\n",
    "    ]\n",
    "\n",
    "    v = [0 if joint_v == 0 else 2 for joint_v in anno['joints_visibility']]\n",
    "\n",
    "    feature = {\n",
    "        'image/height':\n",
    "        tf.train.Feature(int64_list=tf.train.Int64List(value=[height])),\n",
    "        'image/width':\n",
    "        tf.train.Feature(int64_list=tf.train.Int64List(value=[width])),\n",
    "        'image/depth':\n",
    "        tf.train.Feature(int64_list=tf.train.Int64List(value=[depth])),\n",
    "        'image/object/parts/x':\n",
    "        tf.train.Feature(int64_list=tf.train.Int64List(value=x)),\n",
    "        'image/object/parts/y':\n",
    "        tf.train.Feature(int64_list=tf.train.Int64List(value=y)),\n",
    "        'image/object/center/x': \n",
    "        tf.train.Feature(int64_list=tf.train.Int64List(value=[c_x])),\n",
    "        'image/object/center/y': \n",
    "        tf.train.Feature(int64_list=tf.train.Int64List(value=[c_y])),\n",
    "        'image/object/scale':\n",
    "        tf.train.Feature(float_list=tf.train.FloatList(value=[scale])),\n",
    "        'image/object/parts/v':\n",
    "        tf.train.Feature(int64_list=tf.train.Int64List(value=v)),\n",
    "        'image/encoded':\n",
    "        _bytes_feature(content),\n",
    "        'image/filename':\n",
    "        _bytes_feature(filename.encode())\n",
    "    }\n",
    "\n",
    "    return tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8a0e5368",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunkify(l, n):\n",
    "    size = len(l) // n\n",
    "    start = 0\n",
    "    results = []\n",
    "    for i in range(n):\n",
    "        results.append(l[start:start + size])\n",
    "        start += size\n",
    "    return results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f5f8f1f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
      "64\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "test_chunks = chunkify([0] * 1000, 64)\n",
    "print(test_chunks)\n",
    "print(len(test_chunks))\n",
    "print(len(test_chunks[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "98810da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def build_single_tfrecord(chunk, path):\n",
    "    print('start to build tf records for ' + path)\n",
    "\n",
    "    with tf.io.TFRecordWriter(path) as writer:\n",
    "        for anno in chunk:\n",
    "            tf_example = generate_tfexample(anno)\n",
    "            writer.write(tf_example.SerializeToString())\n",
    "\n",
    "    print('finished building tf records for ' + path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ce6d0156",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_tf_records(annotations, total_shards, split):\n",
    "    chunks = chunkify(annotations, total_shards)\n",
    "    futures = [\n",
    "        build_single_tfrecord.remote(\n",
    "            chunk, '{}/{}_{}_of_{}.tfrecords'.format(\n",
    "                TFRECORD_PATH,\n",
    "                split,\n",
    "                str(i + 1).zfill(4),\n",
    "                str(total_shards).zfill(4),\n",
    "            )) for i, chunk in enumerate(chunks)\n",
    "    ]\n",
    "    ray.get(futures)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7b58706e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-31 21:10:50,825\tWARNING services.py:1729 -- WARNING: The object store is using /tmp instead of /dev/shm because /dev/shm has only 67108864 bytes available. This will harm performance! You may be able to free up space by deleting files in /dev/shm. If you are inside a Docker container, you can increase /dev/shm size by passing '--shm-size=3.92gb' to 'docker run' (or add it to the run_options list in a Ray cluster config). Make sure to set this to more than 30% of available RAM.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start to parse annotations.\n",
      "First train annotation:  {'filename': '015601864.jpg', 'filepath': '/aiffel/aiffel/mpii/images/015601864.jpg', 'joints_visibility': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'joints': [[620.0, 394.0], [616.0, 269.0], [573.0, 185.0], [647.0, 188.0], [661.0, 221.0], [656.0, 231.0], [610.0, 187.0], [647.0, 176.0], [637.0201, 189.8183], [695.9799, 108.1817], [606.0, 217.0], [553.0, 161.0], [601.0, 167.0], [692.0, 185.0], [693.0, 240.0], [688.0, 313.0]], 'center': [594.0, 257.0], 'scale': 3.021046}\n",
      "First val annotation:  {'filename': '005808361.jpg', 'filepath': '/aiffel/aiffel/mpii/images/005808361.jpg', 'joints_visibility': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'joints': [[804.0, 711.0], [816.0, 510.0], [908.0, 438.0], [1040.0, 454.0], [906.0, 528.0], [883.0, 707.0], [974.0, 446.0], [985.0, 253.0], [982.7591, 235.9694], [962.2409, 80.0306], [869.0, 214.0], [798.0, 340.0], [902.0, 253.0], [1067.0, 253.0], [1167.0, 353.0], [1142.0, 478.0]], 'center': [966.0, 340.0], 'scale': 4.718488}\n",
      "Start to build TF Records.\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=136)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0002_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=135)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0003_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=134)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0001_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=135)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0003_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=135)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0004_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=134)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0001_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=134)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0005_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=136)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0002_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=136)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0006_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=136)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0006_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=136)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0007_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=135)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0004_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=135)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0008_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=134)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0005_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=134)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0009_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=135)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0008_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=135)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0010_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=136)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0007_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=136)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0011_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=134)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0009_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=134)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0012_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=135)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0010_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=135)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0013_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=136)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0011_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=136)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0014_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=134)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0012_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=134)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0015_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=135)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0013_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=135)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0016_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=136)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0014_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=136)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0017_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=134)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0015_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=134)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0018_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=135)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0016_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=135)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0019_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=136)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0017_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=136)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0020_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=134)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0018_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=134)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0021_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=135)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0019_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=135)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0022_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=136)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0020_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=136)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0023_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=135)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0022_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=135)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0024_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=134)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0021_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=134)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0025_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=136)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0023_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=136)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0026_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=135)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0024_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=135)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0027_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=134)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0025_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=134)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0028_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=136)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0026_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=136)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0029_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=135)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0027_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=135)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0030_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=134)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0028_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=134)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0031_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=136)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0029_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=136)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0032_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=135)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0030_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=135)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0033_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=136)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0032_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=136)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0034_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=134)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0031_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=134)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0035_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=135)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0033_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=135)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0036_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=136)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0034_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=136)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0037_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=134)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0035_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=134)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0038_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=135)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0036_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=135)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0039_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=136)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0037_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=136)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0040_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=134)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0038_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=134)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0041_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=136)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0040_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=136)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0042_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=135)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0039_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=135)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0043_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=134)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0041_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=134)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0044_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=136)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0042_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=136)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0045_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=135)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0043_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=135)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0046_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=134)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0044_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=134)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0047_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=136)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0045_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=136)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0048_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=135)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0046_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=135)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0049_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=134)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0047_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=134)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0050_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=136)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0048_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=136)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0051_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=135)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0049_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=135)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0052_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=134)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0050_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=134)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0053_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=136)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0051_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=136)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0054_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=135)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0052_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=135)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0055_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=134)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0053_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=134)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0056_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=136)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0054_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=136)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0057_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=135)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0055_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=135)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0058_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=134)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0056_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=134)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0059_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=136)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0057_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=136)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0060_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=135)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0058_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=135)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0061_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=134)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0059_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=134)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0062_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=136)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0060_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=136)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0063_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=135)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0061_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=135)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0064_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=134)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0062_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=136)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0063_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=136)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/val_0002_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=135)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0064_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=135)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/val_0001_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=134)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/val_0003_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=135)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/val_0001_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=135)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/val_0004_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=136)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/val_0002_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=136)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/val_0005_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=134)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/val_0003_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=134)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/val_0006_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=135)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/val_0004_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=135)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/val_0007_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=136)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/val_0005_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=136)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/val_0008_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=134)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/val_0006_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=135)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/val_0007_of_0008.tfrecords\n",
      "Successfully wrote 25204 annotations to TF Records.\n"
     ]
    }
   ],
   "source": [
    "num_train_shards = 64\n",
    "num_val_shards = 8\n",
    "\n",
    "ray.init()\n",
    "\n",
    "print('Start to parse annotations.')\n",
    "if not os.path.exists(TFRECORD_PATH):\n",
    "    os.makedirs(TFRECORD_PATH)\n",
    "\n",
    "with open(TRAIN_JSON) as train_json:\n",
    "    train_annos = json.load(train_json)\n",
    "    train_annotations = [\n",
    "        parse_one_annotation(anno, IMAGE_PATH)\n",
    "        for anno in train_annos\n",
    "    ]\n",
    "    print('First train annotation: ', train_annotations[0])\n",
    "\n",
    "with open(VALID_JSON) as val_json:\n",
    "    val_annos = json.load(val_json)\n",
    "    val_annotations = [\n",
    "        parse_one_annotation(anno, IMAGE_PATH) \n",
    "        for anno in val_annos\n",
    "    ]\n",
    "    print('First val annotation: ', val_annotations[0])\n",
    "    \n",
    "print('Start to build TF Records.')\n",
    "build_tf_records(train_annotations, num_train_shards, 'train')\n",
    "build_tf_records(val_annotations, num_val_shards, 'val')\n",
    "\n",
    "print('Successfully wrote {} annotations to TF Records.'.format(\n",
    "    len(train_annotations) + len(val_annotations)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b37b2f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## %%writefile processing.py\n",
    "class Preprocessor(object):\n",
    "    def __init__(self,\n",
    "                 image_shape=(256, 256, 3),\n",
    "                 heatmap_shape=(64, 64, 16),\n",
    "                 is_train=False):\n",
    "        self.is_train = is_train\n",
    "        self.image_shape = image_shape\n",
    "        self.heatmap_shape = heatmap_shape\n",
    "\n",
    "    def __call__(self, example):\n",
    "        features = self.parse_tfexample(example)\n",
    "        image = tf.io.decode_jpeg(features['image/encoded'])\n",
    "\n",
    "        if self.is_train:\n",
    "            random_margin = tf.random.uniform([1], 0.1, 0.3)[0]\n",
    "            image, keypoint_x, keypoint_y = self.crop_roi(image, features, margin=random_margin)\n",
    "            image = tf.image.resize(image, self.image_shape[0:2])\n",
    "        else:\n",
    "            image, keypoint_x, keypoint_y = self.crop_roi(image, features)\n",
    "            image = tf.image.resize(image, self.image_shape[0:2])\n",
    "\n",
    "        image = tf.cast(image, tf.float32) / 127.5 - 1\n",
    "        heatmaps = self.make_heatmaps(features, keypoint_x, keypoint_y, self.heatmap_shape)\n",
    "\n",
    "        return image, heatmaps\n",
    "\n",
    "        \n",
    "    def crop_roi(self, image, features, margin=0.2):\n",
    "        img_shape = tf.shape(image)\n",
    "        img_height = img_shape[0]\n",
    "        img_width = img_shape[1]\n",
    "        img_depth = img_shape[2]\n",
    "\n",
    "        keypoint_x = tf.cast(tf.sparse.to_dense(features['image/object/parts/x']), dtype=tf.int32)\n",
    "        keypoint_y = tf.cast(tf.sparse.to_dense(features['image/object/parts/y']), dtype=tf.int32)\n",
    "        center_x = features['image/object/center/x']\n",
    "        center_y = features['image/object/center/y']\n",
    "        body_height = features['image/object/scale'] * 200.0\n",
    "        \n",
    "        masked_keypoint_x = tf.boolean_mask(keypoint_x, keypoint_x > 0)\n",
    "        masked_keypoint_y = tf.boolean_mask(keypoint_y, keypoint_y > 0)\n",
    "        \n",
    "        keypoint_xmin = tf.reduce_min(masked_keypoint_x)\n",
    "        keypoint_xmax = tf.reduce_max(masked_keypoint_x)\n",
    "        keypoint_ymin = tf.reduce_min(masked_keypoint_y)\n",
    "        keypoint_ymax = tf.reduce_max(masked_keypoint_y)\n",
    "        \n",
    "        xmin = keypoint_xmin - tf.cast(body_height * margin, dtype=tf.int32)\n",
    "        xmax = keypoint_xmax + tf.cast(body_height * margin, dtype=tf.int32)\n",
    "        ymin = keypoint_ymin - tf.cast(body_height * margin, dtype=tf.int32)\n",
    "        ymax = keypoint_ymax + tf.cast(body_height * margin, dtype=tf.int32)\n",
    "        \n",
    "        effective_xmin = xmin if xmin > 0 else 0\n",
    "        effective_ymin = ymin if ymin > 0 else 0\n",
    "        effective_xmax = xmax if xmax < img_width else img_width\n",
    "        effective_ymax = ymax if ymax < img_height else img_height\n",
    "        effective_height = effective_ymax - effective_ymin\n",
    "        effective_width = effective_xmax - effective_xmin\n",
    "\n",
    "        image = image[effective_ymin:effective_ymax, effective_xmin:effective_xmax, :]\n",
    "        new_shape = tf.shape(image)\n",
    "        new_height = new_shape[0]\n",
    "        new_width = new_shape[1]\n",
    "        \n",
    "        effective_keypoint_x = (keypoint_x - effective_xmin) / new_width\n",
    "        effective_keypoint_y = (keypoint_y - effective_ymin) / new_height\n",
    "        \n",
    "        return image, effective_keypoint_x, effective_keypoint_y\n",
    "        \n",
    "    \n",
    "    def generate_2d_guassian(self, height, width, y0, x0, visibility=2, sigma=1, scale=12):\n",
    "        \n",
    "        heatmap = tf.zeros((height, width))\n",
    "\n",
    "        xmin = x0 - 3 * sigma\n",
    "        ymin = y0 - 3 * sigma\n",
    "        xmax = x0 + 3 * sigma\n",
    "        ymax = y0 + 3 * sigma\n",
    "\n",
    "        if xmin >= width or ymin >= height or xmax < 0 or ymax <0 or visibility == 0:\n",
    "            return heatmap\n",
    "\n",
    "        size = 6 * sigma + 1\n",
    "        x, y = tf.meshgrid(tf.range(0, 6*sigma+1, 1), tf.range(0, 6*sigma+1, 1), indexing='xy')\n",
    "\n",
    "        center_x = size // 2\n",
    "        center_y = size // 2\n",
    "\n",
    "        gaussian_patch = tf.cast(tf.math.exp(-(tf.square(x - center_x) + tf.math.square(y - center_y)) / (tf.math.square(sigma) * 2)) * scale, dtype=tf.float32)\n",
    "\n",
    "        patch_xmin = tf.math.maximum(0, -xmin)\n",
    "        patch_ymin = tf.math.maximum(0, -ymin)\n",
    "        patch_xmax = tf.math.minimum(xmax, width) - xmin\n",
    "        patch_ymax = tf.math.minimum(ymax, height) - ymin\n",
    "\n",
    "        heatmap_xmin = tf.math.maximum(0, xmin)\n",
    "        heatmap_ymin = tf.math.maximum(0, ymin)\n",
    "        heatmap_xmax = tf.math.minimum(xmax, width)\n",
    "        heatmap_ymax = tf.math.minimum(ymax, height)\n",
    "\n",
    "        indices = tf.TensorArray(tf.int32, 1, dynamic_size=True)\n",
    "        updates = tf.TensorArray(tf.float32, 1, dynamic_size=True)\n",
    "\n",
    "        count = 0\n",
    "\n",
    "        for j in tf.range(patch_ymin, patch_ymax):\n",
    "            for i in tf.range(patch_xmin, patch_xmax):\n",
    "                indices = indices.write(count, [heatmap_ymin+j, heatmap_xmin+i])\n",
    "                updates = updates.write(count, gaussian_patch[j][i])\n",
    "                count += 1\n",
    "                \n",
    "        heatmap = tf.tensor_scatter_nd_update(heatmap, indices.stack(), updates.stack())\n",
    "\n",
    "        return heatmap\n",
    "\n",
    "\n",
    "    def make_heatmaps(self, features, keypoint_x, keypoint_y, heatmap_shape):\n",
    "        v = tf.cast(tf.sparse.to_dense(features['image/object/parts/v']), dtype=tf.float32)\n",
    "        x = tf.cast(tf.math.round(keypoint_x * heatmap_shape[0]), dtype=tf.int32)\n",
    "        y = tf.cast(tf.math.round(keypoint_y * heatmap_shape[1]), dtype=tf.int32)\n",
    "        \n",
    "        num_heatmap = heatmap_shape[2]\n",
    "        heatmap_array = tf.TensorArray(tf.float32, 16)\n",
    "\n",
    "        for i in range(num_heatmap):\n",
    "            gaussian = self.generate_2d_guassian(heatmap_shape[1], heatmap_shape[0], y[i], x[i], v[i])\n",
    "            heatmap_array = heatmap_array.write(i, gaussian)\n",
    "        \n",
    "        heatmaps = heatmap_array.stack()\n",
    "        heatmaps = tf.transpose(heatmaps, perm=[1, 2, 0]) # change to (64, 64, 16)\n",
    "        \n",
    "        return heatmaps\n",
    "\n",
    "    def parse_tfexample(self, example):\n",
    "        image_feature_description = {\n",
    "            'image/height': tf.io.FixedLenFeature([], tf.int64),\n",
    "            'image/width': tf.io.FixedLenFeature([], tf.int64),\n",
    "            'image/depth': tf.io.FixedLenFeature([], tf.int64),\n",
    "            'image/object/parts/x': tf.io.VarLenFeature(tf.int64),\n",
    "            'image/object/parts/y': tf.io.VarLenFeature(tf.int64),\n",
    "            'image/object/parts/v': tf.io.VarLenFeature(tf.int64),\n",
    "            'image/object/center/x': tf.io.FixedLenFeature([], tf.int64),\n",
    "            'image/object/center/y': tf.io.FixedLenFeature([], tf.int64),\n",
    "            'image/object/scale': tf.io.FixedLenFeature([], tf.float32),\n",
    "            'image/encoded': tf.io.FixedLenFeature([], tf.string),\n",
    "            'image/filename': tf.io.FixedLenFeature([], tf.string),\n",
    "        }\n",
    "        return tf.io.parse_single_example(example,\n",
    "                                          image_feature_description)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5363392d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=136)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/val_0008_of_0008.tfrecords\n"
     ]
    }
   ],
   "source": [
    "IMAGE_SHAPE = (256, 256, 3)\n",
    "HEATMAP_SIZE = (64, 64)\n",
    "\n",
    "def create_dataset(tfrecords, batch_size, num_heatmap, is_train):\n",
    "    preprocess = Preprocessor(\n",
    "        IMAGE_SHAPE, (HEATMAP_SIZE[0], HEATMAP_SIZE[1], num_heatmap), is_train)\n",
    "\n",
    "    dataset = tf.data.Dataset.list_files(tfrecords)\n",
    "    dataset = tf.data.TFRecordDataset(dataset)\n",
    "    dataset = dataset.map(\n",
    "        preprocess, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "    if is_train:\n",
    "        dataset = dataset.shuffle(batch_size)\n",
    "\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "    return dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c376f95",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6719bd3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def BottleneckBlock(inputs, filters, strides=1, downsample=False, name=None):\n",
    "    identity = inputs\n",
    "    if downsample:\n",
    "        identity = Conv2D(\n",
    "            filters=filters,\n",
    "            kernel_size=1,\n",
    "            strides=strides,\n",
    "            padding='same',\n",
    "            kernel_initializer='he_normal')(inputs)\n",
    "\n",
    "    x = BatchNormalization(momentum=0.9)(inputs)\n",
    "    x = ReLU()(x)\n",
    "    x = Conv2D(\n",
    "        filters=filters // 2,\n",
    "        kernel_size=1,\n",
    "        strides=1,\n",
    "        padding='same',\n",
    "        kernel_initializer='he_normal')(x)\n",
    "\n",
    "    x = BatchNormalization(momentum=0.9)(x)\n",
    "    x = ReLU()(x)\n",
    "    x = Conv2D(\n",
    "        filters=filters // 2,\n",
    "        kernel_size=3,\n",
    "        strides=strides,\n",
    "        padding='same',\n",
    "        kernel_initializer='he_normal')(x)\n",
    "\n",
    "    x = BatchNormalization(momentum=0.9)(x)\n",
    "    x = ReLU()(x)\n",
    "    x = Conv2D(\n",
    "        filters=filters,\n",
    "        kernel_size=1,\n",
    "        strides=1,\n",
    "        padding='same',\n",
    "        kernel_initializer='he_normal')(x)\n",
    "\n",
    "    x = Add()([identity, x])\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6cc07850",
   "metadata": {},
   "outputs": [],
   "source": [
    "def HourglassModule(inputs, order, filters, num_residual):\n",
    "    \n",
    "    up1 = BottleneckBlock(inputs, filters, downsample=False)\n",
    "    for i in range(num_residual):\n",
    "        up1 = BottleneckBlock(up1, filters, downsample=False)\n",
    "\n",
    "    low1 = MaxPool2D(pool_size=2, strides=2)(inputs)\n",
    "    for i in range(num_residual):\n",
    "        low1 = BottleneckBlock(low1, filters, downsample=False)\n",
    "\n",
    "    low2 = low1\n",
    "    if order > 1:\n",
    "        low2 = HourglassModule(low1, order - 1, filters, num_residual)\n",
    "    else:\n",
    "        for i in range(num_residual):\n",
    "            low2 = BottleneckBlock(low2, filters, downsample=False)\n",
    "\n",
    "    low3 = low2\n",
    "    for i in range(num_residual):\n",
    "        low3 = BottleneckBlock(low3, filters, downsample=False)\n",
    "\n",
    "    up2 = UpSampling2D(size=2)(low3)\n",
    "\n",
    "    return up2 + up1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "413bcaed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LinearLayer(inputs, filters):\n",
    "    x = Conv2D(\n",
    "        filters=filters,\n",
    "        kernel_size=1,\n",
    "        strides=1,\n",
    "        padding='same',\n",
    "        kernel_initializer='he_normal')(inputs)\n",
    "    x = BatchNormalization(momentum=0.9)(x)\n",
    "    x = ReLU()(x)\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "61ab59e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def StackedHourglassNetwork(\n",
    "        input_shape=(256, 256, 3), \n",
    "        num_stack=4, \n",
    "        num_residual=1,\n",
    "        num_heatmap=16):\n",
    "    \n",
    "    inputs = Input(shape=input_shape)\n",
    "\n",
    "    x = Conv2D(\n",
    "        filters=64,\n",
    "        kernel_size=7,\n",
    "        strides=2,\n",
    "        padding='same',\n",
    "        kernel_initializer='he_normal')(inputs)\n",
    "    x = BatchNormalization(momentum=0.9)(x)\n",
    "    x = ReLU()(x)\n",
    "    x = BottleneckBlock(x, 128, downsample=True)\n",
    "    x = MaxPool2D(pool_size=2, strides=2)(x)\n",
    "    x = BottleneckBlock(x, 128, downsample=False)\n",
    "    x = BottleneckBlock(x, 256, downsample=True)\n",
    "\n",
    "    ys = []\n",
    "    for i in range(num_stack):\n",
    "        x = HourglassModule(x, order=4, filters=256, num_residual=num_residual)\n",
    "        for i in range(num_residual):\n",
    "            x = BottleneckBlock(x, 256, downsample=False)\n",
    "\n",
    "        x = LinearLayer(x, 256)\n",
    "\n",
    "        y = Conv2D(\n",
    "            filters=num_heatmap,\n",
    "            kernel_size=1,\n",
    "            strides=1,\n",
    "            padding='same',\n",
    "            kernel_initializer='he_normal')(x)\n",
    "        ys.append(y)\n",
    "\n",
    "        if i < num_stack - 1:\n",
    "            y_intermediate_1 = Conv2D(filters=256, kernel_size=1, strides=1)(x)\n",
    "            y_intermediate_2 = Conv2D(filters=256, kernel_size=1, strides=1)(y)\n",
    "            x = Add()([y_intermediate_1, y_intermediate_2])\n",
    "\n",
    "    return tf.keras.Model(inputs, ys, name='stacked_hourglass')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3af2fb74",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer(object):\n",
    "    def __init__(self,\n",
    "                 model,\n",
    "                 epochs,\n",
    "                 global_batch_size,\n",
    "                 strategy,\n",
    "                 initial_learning_rate):\n",
    "        self.model = model\n",
    "        self.epochs = epochs\n",
    "        self.strategy = strategy\n",
    "        self.global_batch_size = global_batch_size\n",
    "        self.loss_object = tf.keras.losses.MeanSquaredError(\n",
    "            reduction=tf.keras.losses.Reduction.NONE)\n",
    "        self.optimizer = tf.keras.optimizers.Adam(\n",
    "            learning_rate=initial_learning_rate)\n",
    "        self.model = model\n",
    "\n",
    "        self.current_learning_rate = initial_learning_rate\n",
    "        self.last_val_loss = math.inf\n",
    "        self.lowest_val_loss = math.inf\n",
    "        self.patience_count = 0\n",
    "        self.max_patience = 10\n",
    "        self.best_model = None\n",
    "\n",
    "    def lr_decay(self):\n",
    "        if self.patience_count >= self.max_patience:\n",
    "            self.current_learning_rate /= 10.0\n",
    "            self.patience_count = 0\n",
    "        elif self.last_val_loss == self.lowest_val_loss:\n",
    "            self.patience_count = 0\n",
    "        self.patience_count += 1\n",
    "\n",
    "        self.optimizer.learning_rate = self.current_learning_rate\n",
    "\n",
    "    def lr_decay_step(self, epoch):\n",
    "        if epoch == 3 or epoch == 5 or epoch == 7:\n",
    "            self.current_learning_rate /= 10.0\n",
    "        self.optimizer.learning_rate = self.current_learning_rate\n",
    "\n",
    "    def compute_loss(self, labels, outputs):\n",
    "        loss = 0\n",
    "        for output in outputs:\n",
    "            weights = tf.cast(labels > 0, dtype=tf.float32) * 81 + 1\n",
    "            loss += tf.math.reduce_mean(\n",
    "                tf.math.square(labels - output) * weights) * (\n",
    "                    1. / self.global_batch_size)\n",
    "        return loss\n",
    "\n",
    "    def train_step(self, inputs):\n",
    "        images, labels = inputs\n",
    "        with tf.GradientTape() as tape:\n",
    "            outputs = self.model(images, training=True)\n",
    "            loss = self.compute_loss(labels, outputs)\n",
    "\n",
    "        grads = tape.gradient(\n",
    "            target=loss, sources=self.model.trainable_variables)\n",
    "        self.optimizer.apply_gradients(\n",
    "            zip(grads, self.model.trainable_variables))\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def val_step(self, inputs):\n",
    "        images, labels = inputs\n",
    "        outputs = self.model(images, training=False)\n",
    "        loss = self.compute_loss(labels, outputs)\n",
    "        return loss\n",
    "\n",
    "    def run(self, train_dist_dataset, val_dist_dataset):\n",
    "        @tf.function\n",
    "        def distributed_train_epoch(dataset):\n",
    "            tf.print('Start distributed traininng...')\n",
    "            total_loss = 0.0\n",
    "            num_train_batches = 0.0\n",
    "            for one_batch in dataset:\n",
    "                per_replica_loss = self.strategy.run(\n",
    "                    self.train_step, args=(one_batch, ))\n",
    "                batch_loss = self.strategy.reduce(\n",
    "                    tf.distribute.ReduceOp.SUM, per_replica_loss, axis=None)\n",
    "                total_loss += batch_loss\n",
    "                num_train_batches += 1\n",
    "                tf.print('Trained batch', num_train_batches, 'batch loss',\n",
    "                         batch_loss, 'epoch total loss', total_loss / num_train_batches)\n",
    "            return total_loss, num_train_batches\n",
    "\n",
    "        @tf.function\n",
    "        def distributed_val_epoch(dataset):\n",
    "            total_loss = 0.0\n",
    "            num_val_batches = 0.0\n",
    "            for one_batch in dataset:\n",
    "                per_replica_loss = self.strategy.run(\n",
    "                    self.val_step, args=(one_batch, ))\n",
    "                num_val_batches += 1\n",
    "                batch_loss = self.strategy.reduce(\n",
    "                    tf.distribute.ReduceOp.SUM, per_replica_loss, axis=None)\n",
    "                tf.print('Validated batch', num_val_batches, 'batch loss',\n",
    "                         batch_loss)\n",
    "                if not tf.math.is_nan(batch_loss):\n",
    "                    # TODO: Find out why the last validation batch loss become NaN\n",
    "                    total_loss += batch_loss\n",
    "                else:\n",
    "                    num_val_batches -= 1\n",
    "\n",
    "            return total_loss, num_val_batches\n",
    "\n",
    "        for epoch in range(1, self.epochs + 1):\n",
    "            self.lr_decay()\n",
    "            print('Start epoch {} with learning rate {}'.format(\n",
    "                epoch, self.current_learning_rate))\n",
    "\n",
    "            train_total_loss, num_train_batches = distributed_train_epoch(\n",
    "                train_dist_dataset)\n",
    "            train_loss = train_total_loss / num_train_batches\n",
    "            print('Epoch {} train loss {}'.format(epoch, train_loss))\n",
    "\n",
    "            val_total_loss, num_val_batches = distributed_val_epoch(\n",
    "                val_dist_dataset)\n",
    "            val_loss = val_total_loss / num_val_batches\n",
    "            print('Epoch {} val loss {}'.format(epoch, val_loss))\n",
    "\n",
    "            # save model when reach a new lowest validation loss\n",
    "            if val_loss < self.lowest_val_loss:\n",
    "                self.save_model(epoch, val_loss)\n",
    "                self.lowest_val_loss = val_loss\n",
    "            self.last_val_loss = val_loss\n",
    "\n",
    "        return self.best_model\n",
    "\n",
    "    def save_model(self, epoch, loss):\n",
    "        model_name = './model_hourglass-epoch-{}-loss-{:.4f}.h5'.format(epoch, loss)\n",
    "        self.model.save_weights(model_name)\n",
    "        self.best_model = model_name\n",
    "        print(\"Model {} saved.\".format(model_name))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1b7469f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs, learning_rate, num_heatmap, batch_size, train_tfrecords, val_tfrecords):\n",
    "    strategy = tf.distribute.MirroredStrategy()\n",
    "    global_batch_size = strategy.num_replicas_in_sync * batch_size\n",
    "    train_dataset = create_dataset(\n",
    "        train_tfrecords, global_batch_size, num_heatmap, is_train=True)\n",
    "    val_dataset = create_dataset(\n",
    "        val_tfrecords, global_batch_size, num_heatmap, is_train=False)\n",
    "\n",
    "    if not os.path.exists(MODEL_PATH):\n",
    "        os.makedirs(MODEL_PATH)\n",
    "\n",
    "    with strategy.scope():\n",
    "        train_dist_dataset = strategy.experimental_distribute_dataset(\n",
    "            train_dataset)\n",
    "        val_dist_dataset = strategy.experimental_distribute_dataset(\n",
    "            val_dataset)\n",
    "\n",
    "        model = StackedHourglassNetwork(IMAGE_SHAPE, 4, 1, num_heatmap)\n",
    "        \n",
    "        trainer = Trainer(\n",
    "            model,\n",
    "            epochs,\n",
    "            global_batch_size,\n",
    "            strategy,\n",
    "            initial_learning_rate=learning_rate)\n",
    "\n",
    "        print('Start training...')\n",
    "        return trainer.run(train_dist_dataset, val_dist_dataset)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "335b50be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"stacked_hourglass\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 256, 256, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 128, 128, 64) 9472        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 128, 128, 64) 256         conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "re_lu (ReLU)                    (None, 128, 128, 64) 0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 128, 128, 64) 256         re_lu[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_1 (ReLU)                  (None, 128, 128, 64) 0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 128, 128, 64) 4160        re_lu_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 128, 128, 64) 256         conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_2 (ReLU)                  (None, 128, 128, 64) 0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 128, 128, 64) 36928       re_lu_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 128, 128, 64) 256         conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_3 (ReLU)                  (None, 128, 128, 64) 0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 128, 128, 128 8320        re_lu[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 128, 128, 128 8320        re_lu_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 128, 128, 128 0           conv2d_1[0][0]                   \n",
      "                                                                 conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D)    (None, 64, 64, 128)  0           add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 64, 64, 128)  512         max_pooling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_4 (ReLU)                  (None, 64, 64, 128)  0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 64, 64, 64)   8256        re_lu_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 64, 64, 64)   256         conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_5 (ReLU)                  (None, 64, 64, 64)   0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 64, 64, 64)   36928       re_lu_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 64, 64, 64)   256         conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_6 (ReLU)                  (None, 64, 64, 64)   0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 64, 64, 128)  8320        re_lu_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 64, 64, 128)  0           max_pooling2d[0][0]              \n",
      "                                                                 conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 64, 64, 128)  512         add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_7 (ReLU)                  (None, 64, 64, 128)  0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 64, 64, 128)  16512       re_lu_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 64, 64, 128)  512         conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_8 (ReLU)                  (None, 64, 64, 128)  0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 64, 64, 128)  147584      re_lu_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 64, 64, 128)  512         conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_9 (ReLU)                  (None, 64, 64, 128)  0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 64, 64, 256)  33024       add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 64, 64, 256)  33024       re_lu_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 64, 64, 256)  0           conv2d_8[0][0]                   \n",
      "                                                                 conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 32, 32, 256)  0           add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 32, 32, 256)  1024        max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_16 (ReLU)                 (None, 32, 32, 256)  0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 32, 32, 128)  32896       re_lu_16[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 32, 32, 128)  512         conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_17 (ReLU)                 (None, 32, 32, 128)  0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 32, 32, 128)  147584      re_lu_17[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 32, 32, 128)  512         conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_18 (ReLU)                 (None, 32, 32, 128)  0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 32, 32, 256)  33024       re_lu_18[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 32, 32, 256)  0           max_pooling2d_1[0][0]            \n",
      "                                                                 conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 16, 16, 256)  0           add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_25 (BatchNo (None, 16, 16, 256)  1024        max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_25 (ReLU)                 (None, 16, 16, 256)  0           batch_normalization_25[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_27 (Conv2D)              (None, 16, 16, 128)  32896       re_lu_25[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_26 (BatchNo (None, 16, 16, 128)  512         conv2d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_26 (ReLU)                 (None, 16, 16, 128)  0           batch_normalization_26[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_28 (Conv2D)              (None, 16, 16, 128)  147584      re_lu_26[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_27 (BatchNo (None, 16, 16, 128)  512         conv2d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_27 (ReLU)                 (None, 16, 16, 128)  0           batch_normalization_27[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_29 (Conv2D)              (None, 16, 16, 256)  33024       re_lu_27[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_8 (Add)                     (None, 16, 16, 256)  0           max_pooling2d_2[0][0]            \n",
      "                                                                 conv2d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 8, 8, 256)    0           add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_34 (BatchNo (None, 8, 8, 256)    1024        max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_34 (ReLU)                 (None, 8, 8, 256)    0           batch_normalization_34[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_36 (Conv2D)              (None, 8, 8, 128)    32896       re_lu_34[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_35 (BatchNo (None, 8, 8, 128)    512         conv2d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_35 (ReLU)                 (None, 8, 8, 128)    0           batch_normalization_35[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_37 (Conv2D)              (None, 8, 8, 128)    147584      re_lu_35[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_36 (BatchNo (None, 8, 8, 128)    512         conv2d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_36 (ReLU)                 (None, 8, 8, 128)    0           batch_normalization_36[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_38 (Conv2D)              (None, 8, 8, 256)    33024       re_lu_36[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_11 (Add)                    (None, 8, 8, 256)    0           max_pooling2d_3[0][0]            \n",
      "                                                                 conv2d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2D)  (None, 4, 4, 256)    0           add_11[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_43 (BatchNo (None, 4, 4, 256)    1024        max_pooling2d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_43 (ReLU)                 (None, 4, 4, 256)    0           batch_normalization_43[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_45 (Conv2D)              (None, 4, 4, 128)    32896       re_lu_43[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_44 (BatchNo (None, 4, 4, 128)    512         conv2d_45[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_44 (ReLU)                 (None, 4, 4, 128)    0           batch_normalization_44[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_46 (Conv2D)              (None, 4, 4, 128)    147584      re_lu_44[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_45 (BatchNo (None, 4, 4, 128)    512         conv2d_46[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_45 (ReLU)                 (None, 4, 4, 128)    0           batch_normalization_45[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_47 (Conv2D)              (None, 4, 4, 256)    33024       re_lu_45[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_14 (Add)                    (None, 4, 4, 256)    0           max_pooling2d_4[0][0]            \n",
      "                                                                 conv2d_47[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_46 (BatchNo (None, 4, 4, 256)    1024        add_14[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_46 (ReLU)                 (None, 4, 4, 256)    0           batch_normalization_46[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_37 (BatchNo (None, 8, 8, 256)    1024        add_11[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_48 (Conv2D)              (None, 4, 4, 128)    32896       re_lu_46[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_37 (ReLU)                 (None, 8, 8, 256)    0           batch_normalization_37[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_47 (BatchNo (None, 4, 4, 128)    512         conv2d_48[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_39 (Conv2D)              (None, 8, 8, 128)    32896       re_lu_37[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_47 (ReLU)                 (None, 4, 4, 128)    0           batch_normalization_47[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_38 (BatchNo (None, 8, 8, 128)    512         conv2d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_49 (Conv2D)              (None, 4, 4, 128)    147584      re_lu_47[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_38 (ReLU)                 (None, 8, 8, 128)    0           batch_normalization_38[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_48 (BatchNo (None, 4, 4, 128)    512         conv2d_49[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_40 (Conv2D)              (None, 8, 8, 128)    147584      re_lu_38[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_48 (ReLU)                 (None, 4, 4, 128)    0           batch_normalization_48[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_39 (BatchNo (None, 8, 8, 128)    512         conv2d_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_50 (Conv2D)              (None, 4, 4, 256)    33024       re_lu_48[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_39 (ReLU)                 (None, 8, 8, 128)    0           batch_normalization_39[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_15 (Add)                    (None, 4, 4, 256)    0           add_14[0][0]                     \n",
      "                                                                 conv2d_50[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_41 (Conv2D)              (None, 8, 8, 256)    33024       re_lu_39[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_49 (BatchNo (None, 4, 4, 256)    1024        add_15[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "add_12 (Add)                    (None, 8, 8, 256)    0           add_11[0][0]                     \n",
      "                                                                 conv2d_41[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_49 (ReLU)                 (None, 4, 4, 256)    0           batch_normalization_49[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_40 (BatchNo (None, 8, 8, 256)    1024        add_12[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_51 (Conv2D)              (None, 4, 4, 128)    32896       re_lu_49[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_40 (ReLU)                 (None, 8, 8, 256)    0           batch_normalization_40[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_50 (BatchNo (None, 4, 4, 128)    512         conv2d_51[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_42 (Conv2D)              (None, 8, 8, 128)    32896       re_lu_40[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_28 (BatchNo (None, 16, 16, 256)  1024        add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_50 (ReLU)                 (None, 4, 4, 128)    0           batch_normalization_50[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_41 (BatchNo (None, 8, 8, 128)    512         conv2d_42[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_28 (ReLU)                 (None, 16, 16, 256)  0           batch_normalization_28[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_52 (Conv2D)              (None, 4, 4, 128)    147584      re_lu_50[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_41 (ReLU)                 (None, 8, 8, 128)    0           batch_normalization_41[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_30 (Conv2D)              (None, 16, 16, 128)  32896       re_lu_28[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_51 (BatchNo (None, 4, 4, 128)    512         conv2d_52[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_43 (Conv2D)              (None, 8, 8, 128)    147584      re_lu_41[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_29 (BatchNo (None, 16, 16, 128)  512         conv2d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_51 (ReLU)                 (None, 4, 4, 128)    0           batch_normalization_51[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_42 (BatchNo (None, 8, 8, 128)    512         conv2d_43[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_29 (ReLU)                 (None, 16, 16, 128)  0           batch_normalization_29[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_53 (Conv2D)              (None, 4, 4, 256)    33024       re_lu_51[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_42 (ReLU)                 (None, 8, 8, 128)    0           batch_normalization_42[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_31 (Conv2D)              (None, 16, 16, 128)  147584      re_lu_29[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_16 (Add)                    (None, 4, 4, 256)    0           add_15[0][0]                     \n",
      "                                                                 conv2d_53[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_44 (Conv2D)              (None, 8, 8, 256)    33024       re_lu_42[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_30 (BatchNo (None, 16, 16, 128)  512         conv2d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d (UpSampling2D)    (None, 8, 8, 256)    0           add_16[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "add_13 (Add)                    (None, 8, 8, 256)    0           add_12[0][0]                     \n",
      "                                                                 conv2d_44[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_30 (ReLU)                 (None, 16, 16, 128)  0           batch_normalization_30[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add (TFOpLambd (None, 8, 8, 256)    0           up_sampling2d[0][0]              \n",
      "                                                                 add_13[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_32 (Conv2D)              (None, 16, 16, 256)  33024       re_lu_30[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_52 (BatchNo (None, 8, 8, 256)    1024        tf.__operators__.add[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "add_9 (Add)                     (None, 16, 16, 256)  0           add_8[0][0]                      \n",
      "                                                                 conv2d_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_52 (ReLU)                 (None, 8, 8, 256)    0           batch_normalization_52[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_31 (BatchNo (None, 16, 16, 256)  1024        add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_54 (Conv2D)              (None, 8, 8, 128)    32896       re_lu_52[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_31 (ReLU)                 (None, 16, 16, 256)  0           batch_normalization_31[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_53 (BatchNo (None, 8, 8, 128)    512         conv2d_54[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_33 (Conv2D)              (None, 16, 16, 128)  32896       re_lu_31[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 32, 32, 256)  1024        add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_53 (ReLU)                 (None, 8, 8, 128)    0           batch_normalization_53[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_32 (BatchNo (None, 16, 16, 128)  512         conv2d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_19 (ReLU)                 (None, 32, 32, 256)  0           batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_55 (Conv2D)              (None, 8, 8, 128)    147584      re_lu_53[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_32 (ReLU)                 (None, 16, 16, 128)  0           batch_normalization_32[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 32, 32, 128)  32896       re_lu_19[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_54 (BatchNo (None, 8, 8, 128)    512         conv2d_55[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_34 (Conv2D)              (None, 16, 16, 128)  147584      re_lu_32[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, 32, 32, 128)  512         conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_54 (ReLU)                 (None, 8, 8, 128)    0           batch_normalization_54[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_33 (BatchNo (None, 16, 16, 128)  512         conv2d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_20 (ReLU)                 (None, 32, 32, 128)  0           batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_56 (Conv2D)              (None, 8, 8, 256)    33024       re_lu_54[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_33 (ReLU)                 (None, 16, 16, 128)  0           batch_normalization_33[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_22 (Conv2D)              (None, 32, 32, 128)  147584      re_lu_20[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_17 (Add)                    (None, 8, 8, 256)    0           tf.__operators__.add[0][0]       \n",
      "                                                                 conv2d_56[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_35 (Conv2D)              (None, 16, 16, 256)  33024       re_lu_33[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, 32, 32, 128)  512         conv2d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2D)  (None, 16, 16, 256)  0           add_17[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "add_10 (Add)                    (None, 16, 16, 256)  0           add_9[0][0]                      \n",
      "                                                                 conv2d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_21 (ReLU)                 (None, 32, 32, 128)  0           batch_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_1 (TFOpLam (None, 16, 16, 256)  0           up_sampling2d_1[0][0]            \n",
      "                                                                 add_10[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_23 (Conv2D)              (None, 32, 32, 256)  33024       re_lu_21[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_55 (BatchNo (None, 16, 16, 256)  1024        tf.__operators__.add_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_6 (Add)                     (None, 32, 32, 256)  0           add_5[0][0]                      \n",
      "                                                                 conv2d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_55 (ReLU)                 (None, 16, 16, 256)  0           batch_normalization_55[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, 32, 32, 256)  1024        add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_57 (Conv2D)              (None, 16, 16, 128)  32896       re_lu_55[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_22 (ReLU)                 (None, 32, 32, 256)  0           batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_56 (BatchNo (None, 16, 16, 128)  512         conv2d_57[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_24 (Conv2D)              (None, 32, 32, 128)  32896       re_lu_22[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 64, 64, 256)  1024        add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_56 (ReLU)                 (None, 16, 16, 128)  0           batch_normalization_56[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNo (None, 32, 32, 128)  512         conv2d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_10 (ReLU)                 (None, 64, 64, 256)  0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_58 (Conv2D)              (None, 16, 16, 128)  147584      re_lu_56[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_23 (ReLU)                 (None, 32, 32, 128)  0           batch_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 64, 64, 128)  32896       re_lu_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_57 (BatchNo (None, 16, 16, 128)  512         conv2d_58[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_25 (Conv2D)              (None, 32, 32, 128)  147584      re_lu_23[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 64, 64, 128)  512         conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_57 (ReLU)                 (None, 16, 16, 128)  0           batch_normalization_57[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_24 (BatchNo (None, 32, 32, 128)  512         conv2d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_11 (ReLU)                 (None, 64, 64, 128)  0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_59 (Conv2D)              (None, 16, 16, 256)  33024       re_lu_57[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_24 (ReLU)                 (None, 32, 32, 128)  0           batch_normalization_24[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 64, 64, 128)  147584      re_lu_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_18 (Add)                    (None, 16, 16, 256)  0           tf.__operators__.add_1[0][0]     \n",
      "                                                                 conv2d_59[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_26 (Conv2D)              (None, 32, 32, 256)  33024       re_lu_24[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 64, 64, 128)  512         conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_2 (UpSampling2D)  (None, 32, 32, 256)  0           add_18[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "add_7 (Add)                     (None, 32, 32, 256)  0           add_6[0][0]                      \n",
      "                                                                 conv2d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_12 (ReLU)                 (None, 64, 64, 128)  0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_2 (TFOpLam (None, 32, 32, 256)  0           up_sampling2d_2[0][0]            \n",
      "                                                                 add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 64, 64, 256)  33024       re_lu_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_58 (BatchNo (None, 32, 32, 256)  1024        tf.__operators__.add_2[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 64, 64, 256)  0           add_2[0][0]                      \n",
      "                                                                 conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_58 (ReLU)                 (None, 32, 32, 256)  0           batch_normalization_58[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 64, 64, 256)  1024        add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_60 (Conv2D)              (None, 32, 32, 128)  32896       re_lu_58[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_13 (ReLU)                 (None, 64, 64, 256)  0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_59 (BatchNo (None, 32, 32, 128)  512         conv2d_60[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 64, 64, 128)  32896       re_lu_13[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_59 (ReLU)                 (None, 32, 32, 128)  0           batch_normalization_59[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 64, 64, 128)  512         conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_61 (Conv2D)              (None, 32, 32, 128)  147584      re_lu_59[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_14 (ReLU)                 (None, 64, 64, 128)  0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_60 (BatchNo (None, 32, 32, 128)  512         conv2d_61[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 64, 64, 128)  147584      re_lu_14[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_60 (ReLU)                 (None, 32, 32, 128)  0           batch_normalization_60[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 64, 64, 128)  512         conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_62 (Conv2D)              (None, 32, 32, 256)  33024       re_lu_60[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_15 (ReLU)                 (None, 64, 64, 128)  0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_19 (Add)                    (None, 32, 32, 256)  0           tf.__operators__.add_2[0][0]     \n",
      "                                                                 conv2d_62[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 64, 64, 256)  33024       re_lu_15[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_3 (UpSampling2D)  (None, 64, 64, 256)  0           add_19[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 64, 64, 256)  0           add_3[0][0]                      \n",
      "                                                                 conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_3 (TFOpLam (None, 64, 64, 256)  0           up_sampling2d_3[0][0]            \n",
      "                                                                 add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_61 (BatchNo (None, 64, 64, 256)  1024        tf.__operators__.add_3[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_61 (ReLU)                 (None, 64, 64, 256)  0           batch_normalization_61[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_63 (Conv2D)              (None, 64, 64, 128)  32896       re_lu_61[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_62 (BatchNo (None, 64, 64, 128)  512         conv2d_63[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_62 (ReLU)                 (None, 64, 64, 128)  0           batch_normalization_62[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_64 (Conv2D)              (None, 64, 64, 128)  147584      re_lu_62[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_63 (BatchNo (None, 64, 64, 128)  512         conv2d_64[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_63 (ReLU)                 (None, 64, 64, 128)  0           batch_normalization_63[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_65 (Conv2D)              (None, 64, 64, 256)  33024       re_lu_63[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_20 (Add)                    (None, 64, 64, 256)  0           tf.__operators__.add_3[0][0]     \n",
      "                                                                 conv2d_65[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_66 (Conv2D)              (None, 64, 64, 256)  65792       add_20[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_64 (BatchNo (None, 64, 64, 256)  1024        conv2d_66[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_64 (ReLU)                 (None, 64, 64, 256)  0           batch_normalization_64[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_67 (Conv2D)              (None, 64, 64, 16)   4112        re_lu_64[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_68 (Conv2D)              (None, 64, 64, 256)  65792       re_lu_64[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_69 (Conv2D)              (None, 64, 64, 256)  4352        conv2d_67[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_21 (Add)                    (None, 64, 64, 256)  0           conv2d_68[0][0]                  \n",
      "                                                                 conv2d_69[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2D)  (None, 32, 32, 256)  0           add_21[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_71 (BatchNo (None, 32, 32, 256)  1024        max_pooling2d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_71 (ReLU)                 (None, 32, 32, 256)  0           batch_normalization_71[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_76 (Conv2D)              (None, 32, 32, 128)  32896       re_lu_71[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_72 (BatchNo (None, 32, 32, 128)  512         conv2d_76[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_72 (ReLU)                 (None, 32, 32, 128)  0           batch_normalization_72[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_77 (Conv2D)              (None, 32, 32, 128)  147584      re_lu_72[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_73 (BatchNo (None, 32, 32, 128)  512         conv2d_77[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_73 (ReLU)                 (None, 32, 32, 128)  0           batch_normalization_73[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_78 (Conv2D)              (None, 32, 32, 256)  33024       re_lu_73[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_24 (Add)                    (None, 32, 32, 256)  0           max_pooling2d_5[0][0]            \n",
      "                                                                 conv2d_78[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2D)  (None, 16, 16, 256)  0           add_24[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_80 (BatchNo (None, 16, 16, 256)  1024        max_pooling2d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_80 (ReLU)                 (None, 16, 16, 256)  0           batch_normalization_80[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_85 (Conv2D)              (None, 16, 16, 128)  32896       re_lu_80[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_81 (BatchNo (None, 16, 16, 128)  512         conv2d_85[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_81 (ReLU)                 (None, 16, 16, 128)  0           batch_normalization_81[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_86 (Conv2D)              (None, 16, 16, 128)  147584      re_lu_81[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_82 (BatchNo (None, 16, 16, 128)  512         conv2d_86[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_82 (ReLU)                 (None, 16, 16, 128)  0           batch_normalization_82[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_87 (Conv2D)              (None, 16, 16, 256)  33024       re_lu_82[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_27 (Add)                    (None, 16, 16, 256)  0           max_pooling2d_6[0][0]            \n",
      "                                                                 conv2d_87[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2D)  (None, 8, 8, 256)    0           add_27[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_89 (BatchNo (None, 8, 8, 256)    1024        max_pooling2d_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_89 (ReLU)                 (None, 8, 8, 256)    0           batch_normalization_89[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_94 (Conv2D)              (None, 8, 8, 128)    32896       re_lu_89[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_90 (BatchNo (None, 8, 8, 128)    512         conv2d_94[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_90 (ReLU)                 (None, 8, 8, 128)    0           batch_normalization_90[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_95 (Conv2D)              (None, 8, 8, 128)    147584      re_lu_90[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_91 (BatchNo (None, 8, 8, 128)    512         conv2d_95[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_91 (ReLU)                 (None, 8, 8, 128)    0           batch_normalization_91[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_96 (Conv2D)              (None, 8, 8, 256)    33024       re_lu_91[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_30 (Add)                    (None, 8, 8, 256)    0           max_pooling2d_7[0][0]            \n",
      "                                                                 conv2d_96[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2D)  (None, 4, 4, 256)    0           add_30[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_98 (BatchNo (None, 4, 4, 256)    1024        max_pooling2d_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_98 (ReLU)                 (None, 4, 4, 256)    0           batch_normalization_98[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_103 (Conv2D)             (None, 4, 4, 128)    32896       re_lu_98[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_99 (BatchNo (None, 4, 4, 128)    512         conv2d_103[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_99 (ReLU)                 (None, 4, 4, 128)    0           batch_normalization_99[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_104 (Conv2D)             (None, 4, 4, 128)    147584      re_lu_99[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_100 (BatchN (None, 4, 4, 128)    512         conv2d_104[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_100 (ReLU)                (None, 4, 4, 128)    0           batch_normalization_100[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_105 (Conv2D)             (None, 4, 4, 256)    33024       re_lu_100[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_33 (Add)                    (None, 4, 4, 256)    0           max_pooling2d_8[0][0]            \n",
      "                                                                 conv2d_105[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_101 (BatchN (None, 4, 4, 256)    1024        add_33[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_101 (ReLU)                (None, 4, 4, 256)    0           batch_normalization_101[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_92 (BatchNo (None, 8, 8, 256)    1024        add_30[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_106 (Conv2D)             (None, 4, 4, 128)    32896       re_lu_101[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_92 (ReLU)                 (None, 8, 8, 256)    0           batch_normalization_92[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_102 (BatchN (None, 4, 4, 128)    512         conv2d_106[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_97 (Conv2D)              (None, 8, 8, 128)    32896       re_lu_92[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_102 (ReLU)                (None, 4, 4, 128)    0           batch_normalization_102[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_93 (BatchNo (None, 8, 8, 128)    512         conv2d_97[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_107 (Conv2D)             (None, 4, 4, 128)    147584      re_lu_102[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_93 (ReLU)                 (None, 8, 8, 128)    0           batch_normalization_93[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_103 (BatchN (None, 4, 4, 128)    512         conv2d_107[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_98 (Conv2D)              (None, 8, 8, 128)    147584      re_lu_93[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_103 (ReLU)                (None, 4, 4, 128)    0           batch_normalization_103[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_94 (BatchNo (None, 8, 8, 128)    512         conv2d_98[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_108 (Conv2D)             (None, 4, 4, 256)    33024       re_lu_103[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_94 (ReLU)                 (None, 8, 8, 128)    0           batch_normalization_94[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_34 (Add)                    (None, 4, 4, 256)    0           add_33[0][0]                     \n",
      "                                                                 conv2d_108[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_99 (Conv2D)              (None, 8, 8, 256)    33024       re_lu_94[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_104 (BatchN (None, 4, 4, 256)    1024        add_34[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "add_31 (Add)                    (None, 8, 8, 256)    0           add_30[0][0]                     \n",
      "                                                                 conv2d_99[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_104 (ReLU)                (None, 4, 4, 256)    0           batch_normalization_104[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_95 (BatchNo (None, 8, 8, 256)    1024        add_31[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_109 (Conv2D)             (None, 4, 4, 128)    32896       re_lu_104[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_95 (ReLU)                 (None, 8, 8, 256)    0           batch_normalization_95[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_105 (BatchN (None, 4, 4, 128)    512         conv2d_109[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_100 (Conv2D)             (None, 8, 8, 128)    32896       re_lu_95[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_83 (BatchNo (None, 16, 16, 256)  1024        add_27[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_105 (ReLU)                (None, 4, 4, 128)    0           batch_normalization_105[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_96 (BatchNo (None, 8, 8, 128)    512         conv2d_100[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_83 (ReLU)                 (None, 16, 16, 256)  0           batch_normalization_83[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_110 (Conv2D)             (None, 4, 4, 128)    147584      re_lu_105[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_96 (ReLU)                 (None, 8, 8, 128)    0           batch_normalization_96[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_88 (Conv2D)              (None, 16, 16, 128)  32896       re_lu_83[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_106 (BatchN (None, 4, 4, 128)    512         conv2d_110[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_101 (Conv2D)             (None, 8, 8, 128)    147584      re_lu_96[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_84 (BatchNo (None, 16, 16, 128)  512         conv2d_88[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_106 (ReLU)                (None, 4, 4, 128)    0           batch_normalization_106[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_97 (BatchNo (None, 8, 8, 128)    512         conv2d_101[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_84 (ReLU)                 (None, 16, 16, 128)  0           batch_normalization_84[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_111 (Conv2D)             (None, 4, 4, 256)    33024       re_lu_106[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_97 (ReLU)                 (None, 8, 8, 128)    0           batch_normalization_97[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_89 (Conv2D)              (None, 16, 16, 128)  147584      re_lu_84[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_35 (Add)                    (None, 4, 4, 256)    0           add_34[0][0]                     \n",
      "                                                                 conv2d_111[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_102 (Conv2D)             (None, 8, 8, 256)    33024       re_lu_97[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_85 (BatchNo (None, 16, 16, 128)  512         conv2d_89[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_4 (UpSampling2D)  (None, 8, 8, 256)    0           add_35[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "add_32 (Add)                    (None, 8, 8, 256)    0           add_31[0][0]                     \n",
      "                                                                 conv2d_102[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_85 (ReLU)                 (None, 16, 16, 128)  0           batch_normalization_85[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_4 (TFOpLam (None, 8, 8, 256)    0           up_sampling2d_4[0][0]            \n",
      "                                                                 add_32[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_90 (Conv2D)              (None, 16, 16, 256)  33024       re_lu_85[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_107 (BatchN (None, 8, 8, 256)    1024        tf.__operators__.add_4[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_28 (Add)                    (None, 16, 16, 256)  0           add_27[0][0]                     \n",
      "                                                                 conv2d_90[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_107 (ReLU)                (None, 8, 8, 256)    0           batch_normalization_107[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_86 (BatchNo (None, 16, 16, 256)  1024        add_28[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_112 (Conv2D)             (None, 8, 8, 128)    32896       re_lu_107[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_86 (ReLU)                 (None, 16, 16, 256)  0           batch_normalization_86[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_108 (BatchN (None, 8, 8, 128)    512         conv2d_112[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_91 (Conv2D)              (None, 16, 16, 128)  32896       re_lu_86[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_74 (BatchNo (None, 32, 32, 256)  1024        add_24[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_108 (ReLU)                (None, 8, 8, 128)    0           batch_normalization_108[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_87 (BatchNo (None, 16, 16, 128)  512         conv2d_91[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_74 (ReLU)                 (None, 32, 32, 256)  0           batch_normalization_74[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_113 (Conv2D)             (None, 8, 8, 128)    147584      re_lu_108[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_87 (ReLU)                 (None, 16, 16, 128)  0           batch_normalization_87[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_79 (Conv2D)              (None, 32, 32, 128)  32896       re_lu_74[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_109 (BatchN (None, 8, 8, 128)    512         conv2d_113[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_92 (Conv2D)              (None, 16, 16, 128)  147584      re_lu_87[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_75 (BatchNo (None, 32, 32, 128)  512         conv2d_79[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_109 (ReLU)                (None, 8, 8, 128)    0           batch_normalization_109[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_88 (BatchNo (None, 16, 16, 128)  512         conv2d_92[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_75 (ReLU)                 (None, 32, 32, 128)  0           batch_normalization_75[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_114 (Conv2D)             (None, 8, 8, 256)    33024       re_lu_109[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_88 (ReLU)                 (None, 16, 16, 128)  0           batch_normalization_88[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_80 (Conv2D)              (None, 32, 32, 128)  147584      re_lu_75[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_36 (Add)                    (None, 8, 8, 256)    0           tf.__operators__.add_4[0][0]     \n",
      "                                                                 conv2d_114[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_93 (Conv2D)              (None, 16, 16, 256)  33024       re_lu_88[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_76 (BatchNo (None, 32, 32, 128)  512         conv2d_80[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_5 (UpSampling2D)  (None, 16, 16, 256)  0           add_36[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "add_29 (Add)                    (None, 16, 16, 256)  0           add_28[0][0]                     \n",
      "                                                                 conv2d_93[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_76 (ReLU)                 (None, 32, 32, 128)  0           batch_normalization_76[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_5 (TFOpLam (None, 16, 16, 256)  0           up_sampling2d_5[0][0]            \n",
      "                                                                 add_29[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_81 (Conv2D)              (None, 32, 32, 256)  33024       re_lu_76[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_110 (BatchN (None, 16, 16, 256)  1024        tf.__operators__.add_5[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_25 (Add)                    (None, 32, 32, 256)  0           add_24[0][0]                     \n",
      "                                                                 conv2d_81[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_110 (ReLU)                (None, 16, 16, 256)  0           batch_normalization_110[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_77 (BatchNo (None, 32, 32, 256)  1024        add_25[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_115 (Conv2D)             (None, 16, 16, 128)  32896       re_lu_110[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_77 (ReLU)                 (None, 32, 32, 256)  0           batch_normalization_77[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_111 (BatchN (None, 16, 16, 128)  512         conv2d_115[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_82 (Conv2D)              (None, 32, 32, 128)  32896       re_lu_77[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_65 (BatchNo (None, 64, 64, 256)  1024        add_21[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_111 (ReLU)                (None, 16, 16, 128)  0           batch_normalization_111[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_78 (BatchNo (None, 32, 32, 128)  512         conv2d_82[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_65 (ReLU)                 (None, 64, 64, 256)  0           batch_normalization_65[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_116 (Conv2D)             (None, 16, 16, 128)  147584      re_lu_111[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_78 (ReLU)                 (None, 32, 32, 128)  0           batch_normalization_78[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_70 (Conv2D)              (None, 64, 64, 128)  32896       re_lu_65[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_112 (BatchN (None, 16, 16, 128)  512         conv2d_116[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_83 (Conv2D)              (None, 32, 32, 128)  147584      re_lu_78[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_66 (BatchNo (None, 64, 64, 128)  512         conv2d_70[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_112 (ReLU)                (None, 16, 16, 128)  0           batch_normalization_112[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_79 (BatchNo (None, 32, 32, 128)  512         conv2d_83[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_66 (ReLU)                 (None, 64, 64, 128)  0           batch_normalization_66[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_117 (Conv2D)             (None, 16, 16, 256)  33024       re_lu_112[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_79 (ReLU)                 (None, 32, 32, 128)  0           batch_normalization_79[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_71 (Conv2D)              (None, 64, 64, 128)  147584      re_lu_66[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_37 (Add)                    (None, 16, 16, 256)  0           tf.__operators__.add_5[0][0]     \n",
      "                                                                 conv2d_117[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_84 (Conv2D)              (None, 32, 32, 256)  33024       re_lu_79[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_67 (BatchNo (None, 64, 64, 128)  512         conv2d_71[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_6 (UpSampling2D)  (None, 32, 32, 256)  0           add_37[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "add_26 (Add)                    (None, 32, 32, 256)  0           add_25[0][0]                     \n",
      "                                                                 conv2d_84[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_67 (ReLU)                 (None, 64, 64, 128)  0           batch_normalization_67[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_6 (TFOpLam (None, 32, 32, 256)  0           up_sampling2d_6[0][0]            \n",
      "                                                                 add_26[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_72 (Conv2D)              (None, 64, 64, 256)  33024       re_lu_67[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_113 (BatchN (None, 32, 32, 256)  1024        tf.__operators__.add_6[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_22 (Add)                    (None, 64, 64, 256)  0           add_21[0][0]                     \n",
      "                                                                 conv2d_72[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_113 (ReLU)                (None, 32, 32, 256)  0           batch_normalization_113[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_68 (BatchNo (None, 64, 64, 256)  1024        add_22[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_118 (Conv2D)             (None, 32, 32, 128)  32896       re_lu_113[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_68 (ReLU)                 (None, 64, 64, 256)  0           batch_normalization_68[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_114 (BatchN (None, 32, 32, 128)  512         conv2d_118[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_73 (Conv2D)              (None, 64, 64, 128)  32896       re_lu_68[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_114 (ReLU)                (None, 32, 32, 128)  0           batch_normalization_114[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_69 (BatchNo (None, 64, 64, 128)  512         conv2d_73[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_119 (Conv2D)             (None, 32, 32, 128)  147584      re_lu_114[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_69 (ReLU)                 (None, 64, 64, 128)  0           batch_normalization_69[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_115 (BatchN (None, 32, 32, 128)  512         conv2d_119[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_74 (Conv2D)              (None, 64, 64, 128)  147584      re_lu_69[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_115 (ReLU)                (None, 32, 32, 128)  0           batch_normalization_115[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_70 (BatchNo (None, 64, 64, 128)  512         conv2d_74[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_120 (Conv2D)             (None, 32, 32, 256)  33024       re_lu_115[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_70 (ReLU)                 (None, 64, 64, 128)  0           batch_normalization_70[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_38 (Add)                    (None, 32, 32, 256)  0           tf.__operators__.add_6[0][0]     \n",
      "                                                                 conv2d_120[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_75 (Conv2D)              (None, 64, 64, 256)  33024       re_lu_70[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_7 (UpSampling2D)  (None, 64, 64, 256)  0           add_38[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "add_23 (Add)                    (None, 64, 64, 256)  0           add_22[0][0]                     \n",
      "                                                                 conv2d_75[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_7 (TFOpLam (None, 64, 64, 256)  0           up_sampling2d_7[0][0]            \n",
      "                                                                 add_23[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_116 (BatchN (None, 64, 64, 256)  1024        tf.__operators__.add_7[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_116 (ReLU)                (None, 64, 64, 256)  0           batch_normalization_116[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_121 (Conv2D)             (None, 64, 64, 128)  32896       re_lu_116[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_117 (BatchN (None, 64, 64, 128)  512         conv2d_121[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_117 (ReLU)                (None, 64, 64, 128)  0           batch_normalization_117[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_122 (Conv2D)             (None, 64, 64, 128)  147584      re_lu_117[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_118 (BatchN (None, 64, 64, 128)  512         conv2d_122[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_118 (ReLU)                (None, 64, 64, 128)  0           batch_normalization_118[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_123 (Conv2D)             (None, 64, 64, 256)  33024       re_lu_118[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_39 (Add)                    (None, 64, 64, 256)  0           tf.__operators__.add_7[0][0]     \n",
      "                                                                 conv2d_123[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_124 (Conv2D)             (None, 64, 64, 256)  65792       add_39[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_119 (BatchN (None, 64, 64, 256)  1024        conv2d_124[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_119 (ReLU)                (None, 64, 64, 256)  0           batch_normalization_119[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_125 (Conv2D)             (None, 64, 64, 16)   4112        re_lu_119[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_126 (Conv2D)             (None, 64, 64, 256)  65792       re_lu_119[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_127 (Conv2D)             (None, 64, 64, 256)  4352        conv2d_125[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_40 (Add)                    (None, 64, 64, 256)  0           conv2d_126[0][0]                 \n",
      "                                                                 conv2d_127[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2D)  (None, 32, 32, 256)  0           add_40[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_126 (BatchN (None, 32, 32, 256)  1024        max_pooling2d_9[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_126 (ReLU)                (None, 32, 32, 256)  0           batch_normalization_126[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_134 (Conv2D)             (None, 32, 32, 128)  32896       re_lu_126[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_127 (BatchN (None, 32, 32, 128)  512         conv2d_134[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_127 (ReLU)                (None, 32, 32, 128)  0           batch_normalization_127[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_135 (Conv2D)             (None, 32, 32, 128)  147584      re_lu_127[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_128 (BatchN (None, 32, 32, 128)  512         conv2d_135[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_128 (ReLU)                (None, 32, 32, 128)  0           batch_normalization_128[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_136 (Conv2D)             (None, 32, 32, 256)  33024       re_lu_128[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_43 (Add)                    (None, 32, 32, 256)  0           max_pooling2d_9[0][0]            \n",
      "                                                                 conv2d_136[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_10 (MaxPooling2D) (None, 16, 16, 256)  0           add_43[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_135 (BatchN (None, 16, 16, 256)  1024        max_pooling2d_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_135 (ReLU)                (None, 16, 16, 256)  0           batch_normalization_135[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_143 (Conv2D)             (None, 16, 16, 128)  32896       re_lu_135[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_136 (BatchN (None, 16, 16, 128)  512         conv2d_143[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_136 (ReLU)                (None, 16, 16, 128)  0           batch_normalization_136[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_144 (Conv2D)             (None, 16, 16, 128)  147584      re_lu_136[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_137 (BatchN (None, 16, 16, 128)  512         conv2d_144[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_137 (ReLU)                (None, 16, 16, 128)  0           batch_normalization_137[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_145 (Conv2D)             (None, 16, 16, 256)  33024       re_lu_137[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_46 (Add)                    (None, 16, 16, 256)  0           max_pooling2d_10[0][0]           \n",
      "                                                                 conv2d_145[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_11 (MaxPooling2D) (None, 8, 8, 256)    0           add_46[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_144 (BatchN (None, 8, 8, 256)    1024        max_pooling2d_11[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_144 (ReLU)                (None, 8, 8, 256)    0           batch_normalization_144[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_152 (Conv2D)             (None, 8, 8, 128)    32896       re_lu_144[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_145 (BatchN (None, 8, 8, 128)    512         conv2d_152[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_145 (ReLU)                (None, 8, 8, 128)    0           batch_normalization_145[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_153 (Conv2D)             (None, 8, 8, 128)    147584      re_lu_145[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_146 (BatchN (None, 8, 8, 128)    512         conv2d_153[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_146 (ReLU)                (None, 8, 8, 128)    0           batch_normalization_146[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_154 (Conv2D)             (None, 8, 8, 256)    33024       re_lu_146[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_49 (Add)                    (None, 8, 8, 256)    0           max_pooling2d_11[0][0]           \n",
      "                                                                 conv2d_154[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_12 (MaxPooling2D) (None, 4, 4, 256)    0           add_49[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_153 (BatchN (None, 4, 4, 256)    1024        max_pooling2d_12[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_153 (ReLU)                (None, 4, 4, 256)    0           batch_normalization_153[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_161 (Conv2D)             (None, 4, 4, 128)    32896       re_lu_153[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_154 (BatchN (None, 4, 4, 128)    512         conv2d_161[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_154 (ReLU)                (None, 4, 4, 128)    0           batch_normalization_154[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_162 (Conv2D)             (None, 4, 4, 128)    147584      re_lu_154[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_155 (BatchN (None, 4, 4, 128)    512         conv2d_162[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_155 (ReLU)                (None, 4, 4, 128)    0           batch_normalization_155[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_163 (Conv2D)             (None, 4, 4, 256)    33024       re_lu_155[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_52 (Add)                    (None, 4, 4, 256)    0           max_pooling2d_12[0][0]           \n",
      "                                                                 conv2d_163[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_156 (BatchN (None, 4, 4, 256)    1024        add_52[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_156 (ReLU)                (None, 4, 4, 256)    0           batch_normalization_156[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_147 (BatchN (None, 8, 8, 256)    1024        add_49[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_164 (Conv2D)             (None, 4, 4, 128)    32896       re_lu_156[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_147 (ReLU)                (None, 8, 8, 256)    0           batch_normalization_147[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_157 (BatchN (None, 4, 4, 128)    512         conv2d_164[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_155 (Conv2D)             (None, 8, 8, 128)    32896       re_lu_147[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_157 (ReLU)                (None, 4, 4, 128)    0           batch_normalization_157[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_148 (BatchN (None, 8, 8, 128)    512         conv2d_155[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_165 (Conv2D)             (None, 4, 4, 128)    147584      re_lu_157[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_148 (ReLU)                (None, 8, 8, 128)    0           batch_normalization_148[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_158 (BatchN (None, 4, 4, 128)    512         conv2d_165[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_156 (Conv2D)             (None, 8, 8, 128)    147584      re_lu_148[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_158 (ReLU)                (None, 4, 4, 128)    0           batch_normalization_158[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_149 (BatchN (None, 8, 8, 128)    512         conv2d_156[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_166 (Conv2D)             (None, 4, 4, 256)    33024       re_lu_158[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_149 (ReLU)                (None, 8, 8, 128)    0           batch_normalization_149[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "add_53 (Add)                    (None, 4, 4, 256)    0           add_52[0][0]                     \n",
      "                                                                 conv2d_166[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_157 (Conv2D)             (None, 8, 8, 256)    33024       re_lu_149[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_159 (BatchN (None, 4, 4, 256)    1024        add_53[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "add_50 (Add)                    (None, 8, 8, 256)    0           add_49[0][0]                     \n",
      "                                                                 conv2d_157[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_159 (ReLU)                (None, 4, 4, 256)    0           batch_normalization_159[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_150 (BatchN (None, 8, 8, 256)    1024        add_50[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_167 (Conv2D)             (None, 4, 4, 128)    32896       re_lu_159[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_150 (ReLU)                (None, 8, 8, 256)    0           batch_normalization_150[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_160 (BatchN (None, 4, 4, 128)    512         conv2d_167[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_158 (Conv2D)             (None, 8, 8, 128)    32896       re_lu_150[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_138 (BatchN (None, 16, 16, 256)  1024        add_46[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_160 (ReLU)                (None, 4, 4, 128)    0           batch_normalization_160[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_151 (BatchN (None, 8, 8, 128)    512         conv2d_158[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_138 (ReLU)                (None, 16, 16, 256)  0           batch_normalization_138[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_168 (Conv2D)             (None, 4, 4, 128)    147584      re_lu_160[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_151 (ReLU)                (None, 8, 8, 128)    0           batch_normalization_151[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_146 (Conv2D)             (None, 16, 16, 128)  32896       re_lu_138[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_161 (BatchN (None, 4, 4, 128)    512         conv2d_168[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_159 (Conv2D)             (None, 8, 8, 128)    147584      re_lu_151[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_139 (BatchN (None, 16, 16, 128)  512         conv2d_146[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_161 (ReLU)                (None, 4, 4, 128)    0           batch_normalization_161[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_152 (BatchN (None, 8, 8, 128)    512         conv2d_159[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_139 (ReLU)                (None, 16, 16, 128)  0           batch_normalization_139[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_169 (Conv2D)             (None, 4, 4, 256)    33024       re_lu_161[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_152 (ReLU)                (None, 8, 8, 128)    0           batch_normalization_152[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_147 (Conv2D)             (None, 16, 16, 128)  147584      re_lu_139[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_54 (Add)                    (None, 4, 4, 256)    0           add_53[0][0]                     \n",
      "                                                                 conv2d_169[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_160 (Conv2D)             (None, 8, 8, 256)    33024       re_lu_152[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_140 (BatchN (None, 16, 16, 128)  512         conv2d_147[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_8 (UpSampling2D)  (None, 8, 8, 256)    0           add_54[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "add_51 (Add)                    (None, 8, 8, 256)    0           add_50[0][0]                     \n",
      "                                                                 conv2d_160[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_140 (ReLU)                (None, 16, 16, 128)  0           batch_normalization_140[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_8 (TFOpLam (None, 8, 8, 256)    0           up_sampling2d_8[0][0]            \n",
      "                                                                 add_51[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_148 (Conv2D)             (None, 16, 16, 256)  33024       re_lu_140[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_162 (BatchN (None, 8, 8, 256)    1024        tf.__operators__.add_8[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_47 (Add)                    (None, 16, 16, 256)  0           add_46[0][0]                     \n",
      "                                                                 conv2d_148[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_162 (ReLU)                (None, 8, 8, 256)    0           batch_normalization_162[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_141 (BatchN (None, 16, 16, 256)  1024        add_47[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_170 (Conv2D)             (None, 8, 8, 128)    32896       re_lu_162[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_141 (ReLU)                (None, 16, 16, 256)  0           batch_normalization_141[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_163 (BatchN (None, 8, 8, 128)    512         conv2d_170[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_149 (Conv2D)             (None, 16, 16, 128)  32896       re_lu_141[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_129 (BatchN (None, 32, 32, 256)  1024        add_43[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_163 (ReLU)                (None, 8, 8, 128)    0           batch_normalization_163[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_142 (BatchN (None, 16, 16, 128)  512         conv2d_149[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_129 (ReLU)                (None, 32, 32, 256)  0           batch_normalization_129[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_171 (Conv2D)             (None, 8, 8, 128)    147584      re_lu_163[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_142 (ReLU)                (None, 16, 16, 128)  0           batch_normalization_142[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_137 (Conv2D)             (None, 32, 32, 128)  32896       re_lu_129[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_164 (BatchN (None, 8, 8, 128)    512         conv2d_171[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_150 (Conv2D)             (None, 16, 16, 128)  147584      re_lu_142[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_130 (BatchN (None, 32, 32, 128)  512         conv2d_137[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_164 (ReLU)                (None, 8, 8, 128)    0           batch_normalization_164[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_143 (BatchN (None, 16, 16, 128)  512         conv2d_150[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_130 (ReLU)                (None, 32, 32, 128)  0           batch_normalization_130[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_172 (Conv2D)             (None, 8, 8, 256)    33024       re_lu_164[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_143 (ReLU)                (None, 16, 16, 128)  0           batch_normalization_143[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_138 (Conv2D)             (None, 32, 32, 128)  147584      re_lu_130[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_55 (Add)                    (None, 8, 8, 256)    0           tf.__operators__.add_8[0][0]     \n",
      "                                                                 conv2d_172[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_151 (Conv2D)             (None, 16, 16, 256)  33024       re_lu_143[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_131 (BatchN (None, 32, 32, 128)  512         conv2d_138[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_9 (UpSampling2D)  (None, 16, 16, 256)  0           add_55[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "add_48 (Add)                    (None, 16, 16, 256)  0           add_47[0][0]                     \n",
      "                                                                 conv2d_151[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_131 (ReLU)                (None, 32, 32, 128)  0           batch_normalization_131[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_9 (TFOpLam (None, 16, 16, 256)  0           up_sampling2d_9[0][0]            \n",
      "                                                                 add_48[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_139 (Conv2D)             (None, 32, 32, 256)  33024       re_lu_131[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_165 (BatchN (None, 16, 16, 256)  1024        tf.__operators__.add_9[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_44 (Add)                    (None, 32, 32, 256)  0           add_43[0][0]                     \n",
      "                                                                 conv2d_139[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_165 (ReLU)                (None, 16, 16, 256)  0           batch_normalization_165[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_132 (BatchN (None, 32, 32, 256)  1024        add_44[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_173 (Conv2D)             (None, 16, 16, 128)  32896       re_lu_165[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_132 (ReLU)                (None, 32, 32, 256)  0           batch_normalization_132[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_166 (BatchN (None, 16, 16, 128)  512         conv2d_173[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_140 (Conv2D)             (None, 32, 32, 128)  32896       re_lu_132[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_120 (BatchN (None, 64, 64, 256)  1024        add_40[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_166 (ReLU)                (None, 16, 16, 128)  0           batch_normalization_166[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_133 (BatchN (None, 32, 32, 128)  512         conv2d_140[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_120 (ReLU)                (None, 64, 64, 256)  0           batch_normalization_120[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_174 (Conv2D)             (None, 16, 16, 128)  147584      re_lu_166[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_133 (ReLU)                (None, 32, 32, 128)  0           batch_normalization_133[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_128 (Conv2D)             (None, 64, 64, 128)  32896       re_lu_120[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_167 (BatchN (None, 16, 16, 128)  512         conv2d_174[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_141 (Conv2D)             (None, 32, 32, 128)  147584      re_lu_133[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_121 (BatchN (None, 64, 64, 128)  512         conv2d_128[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_167 (ReLU)                (None, 16, 16, 128)  0           batch_normalization_167[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_134 (BatchN (None, 32, 32, 128)  512         conv2d_141[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_121 (ReLU)                (None, 64, 64, 128)  0           batch_normalization_121[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_175 (Conv2D)             (None, 16, 16, 256)  33024       re_lu_167[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_134 (ReLU)                (None, 32, 32, 128)  0           batch_normalization_134[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_129 (Conv2D)             (None, 64, 64, 128)  147584      re_lu_121[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_56 (Add)                    (None, 16, 16, 256)  0           tf.__operators__.add_9[0][0]     \n",
      "                                                                 conv2d_175[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_142 (Conv2D)             (None, 32, 32, 256)  33024       re_lu_134[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_122 (BatchN (None, 64, 64, 128)  512         conv2d_129[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_10 (UpSampling2D) (None, 32, 32, 256)  0           add_56[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "add_45 (Add)                    (None, 32, 32, 256)  0           add_44[0][0]                     \n",
      "                                                                 conv2d_142[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_122 (ReLU)                (None, 64, 64, 128)  0           batch_normalization_122[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_10 (TFOpLa (None, 32, 32, 256)  0           up_sampling2d_10[0][0]           \n",
      "                                                                 add_45[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_130 (Conv2D)             (None, 64, 64, 256)  33024       re_lu_122[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_168 (BatchN (None, 32, 32, 256)  1024        tf.__operators__.add_10[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "add_41 (Add)                    (None, 64, 64, 256)  0           add_40[0][0]                     \n",
      "                                                                 conv2d_130[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_168 (ReLU)                (None, 32, 32, 256)  0           batch_normalization_168[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_123 (BatchN (None, 64, 64, 256)  1024        add_41[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_176 (Conv2D)             (None, 32, 32, 128)  32896       re_lu_168[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_123 (ReLU)                (None, 64, 64, 256)  0           batch_normalization_123[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_169 (BatchN (None, 32, 32, 128)  512         conv2d_176[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_131 (Conv2D)             (None, 64, 64, 128)  32896       re_lu_123[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_169 (ReLU)                (None, 32, 32, 128)  0           batch_normalization_169[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_124 (BatchN (None, 64, 64, 128)  512         conv2d_131[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_177 (Conv2D)             (None, 32, 32, 128)  147584      re_lu_169[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_124 (ReLU)                (None, 64, 64, 128)  0           batch_normalization_124[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_170 (BatchN (None, 32, 32, 128)  512         conv2d_177[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_132 (Conv2D)             (None, 64, 64, 128)  147584      re_lu_124[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_170 (ReLU)                (None, 32, 32, 128)  0           batch_normalization_170[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_125 (BatchN (None, 64, 64, 128)  512         conv2d_132[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_178 (Conv2D)             (None, 32, 32, 256)  33024       re_lu_170[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_125 (ReLU)                (None, 64, 64, 128)  0           batch_normalization_125[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "add_57 (Add)                    (None, 32, 32, 256)  0           tf.__operators__.add_10[0][0]    \n",
      "                                                                 conv2d_178[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_133 (Conv2D)             (None, 64, 64, 256)  33024       re_lu_125[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_11 (UpSampling2D) (None, 64, 64, 256)  0           add_57[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "add_42 (Add)                    (None, 64, 64, 256)  0           add_41[0][0]                     \n",
      "                                                                 conv2d_133[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_11 (TFOpLa (None, 64, 64, 256)  0           up_sampling2d_11[0][0]           \n",
      "                                                                 add_42[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_171 (BatchN (None, 64, 64, 256)  1024        tf.__operators__.add_11[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_171 (ReLU)                (None, 64, 64, 256)  0           batch_normalization_171[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_179 (Conv2D)             (None, 64, 64, 128)  32896       re_lu_171[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_172 (BatchN (None, 64, 64, 128)  512         conv2d_179[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_172 (ReLU)                (None, 64, 64, 128)  0           batch_normalization_172[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_180 (Conv2D)             (None, 64, 64, 128)  147584      re_lu_172[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_173 (BatchN (None, 64, 64, 128)  512         conv2d_180[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_173 (ReLU)                (None, 64, 64, 128)  0           batch_normalization_173[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_181 (Conv2D)             (None, 64, 64, 256)  33024       re_lu_173[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_58 (Add)                    (None, 64, 64, 256)  0           tf.__operators__.add_11[0][0]    \n",
      "                                                                 conv2d_181[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_182 (Conv2D)             (None, 64, 64, 256)  65792       add_58[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_174 (BatchN (None, 64, 64, 256)  1024        conv2d_182[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_174 (ReLU)                (None, 64, 64, 256)  0           batch_normalization_174[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_183 (Conv2D)             (None, 64, 64, 16)   4112        re_lu_174[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_184 (Conv2D)             (None, 64, 64, 256)  65792       re_lu_174[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_185 (Conv2D)             (None, 64, 64, 256)  4352        conv2d_183[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_59 (Add)                    (None, 64, 64, 256)  0           conv2d_184[0][0]                 \n",
      "                                                                 conv2d_185[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_13 (MaxPooling2D) (None, 32, 32, 256)  0           add_59[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_181 (BatchN (None, 32, 32, 256)  1024        max_pooling2d_13[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_181 (ReLU)                (None, 32, 32, 256)  0           batch_normalization_181[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_192 (Conv2D)             (None, 32, 32, 128)  32896       re_lu_181[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_182 (BatchN (None, 32, 32, 128)  512         conv2d_192[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_182 (ReLU)                (None, 32, 32, 128)  0           batch_normalization_182[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_193 (Conv2D)             (None, 32, 32, 128)  147584      re_lu_182[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_183 (BatchN (None, 32, 32, 128)  512         conv2d_193[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_183 (ReLU)                (None, 32, 32, 128)  0           batch_normalization_183[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_194 (Conv2D)             (None, 32, 32, 256)  33024       re_lu_183[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_62 (Add)                    (None, 32, 32, 256)  0           max_pooling2d_13[0][0]           \n",
      "                                                                 conv2d_194[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_14 (MaxPooling2D) (None, 16, 16, 256)  0           add_62[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_190 (BatchN (None, 16, 16, 256)  1024        max_pooling2d_14[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_190 (ReLU)                (None, 16, 16, 256)  0           batch_normalization_190[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_201 (Conv2D)             (None, 16, 16, 128)  32896       re_lu_190[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_191 (BatchN (None, 16, 16, 128)  512         conv2d_201[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_191 (ReLU)                (None, 16, 16, 128)  0           batch_normalization_191[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_202 (Conv2D)             (None, 16, 16, 128)  147584      re_lu_191[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_192 (BatchN (None, 16, 16, 128)  512         conv2d_202[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_192 (ReLU)                (None, 16, 16, 128)  0           batch_normalization_192[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_203 (Conv2D)             (None, 16, 16, 256)  33024       re_lu_192[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_65 (Add)                    (None, 16, 16, 256)  0           max_pooling2d_14[0][0]           \n",
      "                                                                 conv2d_203[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_15 (MaxPooling2D) (None, 8, 8, 256)    0           add_65[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_199 (BatchN (None, 8, 8, 256)    1024        max_pooling2d_15[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_199 (ReLU)                (None, 8, 8, 256)    0           batch_normalization_199[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_210 (Conv2D)             (None, 8, 8, 128)    32896       re_lu_199[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_200 (BatchN (None, 8, 8, 128)    512         conv2d_210[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_200 (ReLU)                (None, 8, 8, 128)    0           batch_normalization_200[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_211 (Conv2D)             (None, 8, 8, 128)    147584      re_lu_200[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_201 (BatchN (None, 8, 8, 128)    512         conv2d_211[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_201 (ReLU)                (None, 8, 8, 128)    0           batch_normalization_201[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_212 (Conv2D)             (None, 8, 8, 256)    33024       re_lu_201[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_68 (Add)                    (None, 8, 8, 256)    0           max_pooling2d_15[0][0]           \n",
      "                                                                 conv2d_212[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_16 (MaxPooling2D) (None, 4, 4, 256)    0           add_68[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_208 (BatchN (None, 4, 4, 256)    1024        max_pooling2d_16[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_208 (ReLU)                (None, 4, 4, 256)    0           batch_normalization_208[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_219 (Conv2D)             (None, 4, 4, 128)    32896       re_lu_208[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_209 (BatchN (None, 4, 4, 128)    512         conv2d_219[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_209 (ReLU)                (None, 4, 4, 128)    0           batch_normalization_209[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_220 (Conv2D)             (None, 4, 4, 128)    147584      re_lu_209[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_210 (BatchN (None, 4, 4, 128)    512         conv2d_220[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_210 (ReLU)                (None, 4, 4, 128)    0           batch_normalization_210[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_221 (Conv2D)             (None, 4, 4, 256)    33024       re_lu_210[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_71 (Add)                    (None, 4, 4, 256)    0           max_pooling2d_16[0][0]           \n",
      "                                                                 conv2d_221[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_211 (BatchN (None, 4, 4, 256)    1024        add_71[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_211 (ReLU)                (None, 4, 4, 256)    0           batch_normalization_211[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_202 (BatchN (None, 8, 8, 256)    1024        add_68[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_222 (Conv2D)             (None, 4, 4, 128)    32896       re_lu_211[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_202 (ReLU)                (None, 8, 8, 256)    0           batch_normalization_202[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_212 (BatchN (None, 4, 4, 128)    512         conv2d_222[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_213 (Conv2D)             (None, 8, 8, 128)    32896       re_lu_202[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_212 (ReLU)                (None, 4, 4, 128)    0           batch_normalization_212[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_203 (BatchN (None, 8, 8, 128)    512         conv2d_213[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_223 (Conv2D)             (None, 4, 4, 128)    147584      re_lu_212[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_203 (ReLU)                (None, 8, 8, 128)    0           batch_normalization_203[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_213 (BatchN (None, 4, 4, 128)    512         conv2d_223[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_214 (Conv2D)             (None, 8, 8, 128)    147584      re_lu_203[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_213 (ReLU)                (None, 4, 4, 128)    0           batch_normalization_213[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_204 (BatchN (None, 8, 8, 128)    512         conv2d_214[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_224 (Conv2D)             (None, 4, 4, 256)    33024       re_lu_213[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_204 (ReLU)                (None, 8, 8, 128)    0           batch_normalization_204[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "add_72 (Add)                    (None, 4, 4, 256)    0           add_71[0][0]                     \n",
      "                                                                 conv2d_224[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_215 (Conv2D)             (None, 8, 8, 256)    33024       re_lu_204[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_214 (BatchN (None, 4, 4, 256)    1024        add_72[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "add_69 (Add)                    (None, 8, 8, 256)    0           add_68[0][0]                     \n",
      "                                                                 conv2d_215[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_214 (ReLU)                (None, 4, 4, 256)    0           batch_normalization_214[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_205 (BatchN (None, 8, 8, 256)    1024        add_69[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_225 (Conv2D)             (None, 4, 4, 128)    32896       re_lu_214[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_205 (ReLU)                (None, 8, 8, 256)    0           batch_normalization_205[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_215 (BatchN (None, 4, 4, 128)    512         conv2d_225[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_216 (Conv2D)             (None, 8, 8, 128)    32896       re_lu_205[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_193 (BatchN (None, 16, 16, 256)  1024        add_65[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_215 (ReLU)                (None, 4, 4, 128)    0           batch_normalization_215[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_206 (BatchN (None, 8, 8, 128)    512         conv2d_216[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_193 (ReLU)                (None, 16, 16, 256)  0           batch_normalization_193[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_226 (Conv2D)             (None, 4, 4, 128)    147584      re_lu_215[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_206 (ReLU)                (None, 8, 8, 128)    0           batch_normalization_206[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_204 (Conv2D)             (None, 16, 16, 128)  32896       re_lu_193[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_216 (BatchN (None, 4, 4, 128)    512         conv2d_226[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_217 (Conv2D)             (None, 8, 8, 128)    147584      re_lu_206[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_194 (BatchN (None, 16, 16, 128)  512         conv2d_204[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_216 (ReLU)                (None, 4, 4, 128)    0           batch_normalization_216[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_207 (BatchN (None, 8, 8, 128)    512         conv2d_217[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_194 (ReLU)                (None, 16, 16, 128)  0           batch_normalization_194[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_227 (Conv2D)             (None, 4, 4, 256)    33024       re_lu_216[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_207 (ReLU)                (None, 8, 8, 128)    0           batch_normalization_207[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_205 (Conv2D)             (None, 16, 16, 128)  147584      re_lu_194[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_73 (Add)                    (None, 4, 4, 256)    0           add_72[0][0]                     \n",
      "                                                                 conv2d_227[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_218 (Conv2D)             (None, 8, 8, 256)    33024       re_lu_207[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_195 (BatchN (None, 16, 16, 128)  512         conv2d_205[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_12 (UpSampling2D) (None, 8, 8, 256)    0           add_73[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "add_70 (Add)                    (None, 8, 8, 256)    0           add_69[0][0]                     \n",
      "                                                                 conv2d_218[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_195 (ReLU)                (None, 16, 16, 128)  0           batch_normalization_195[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_12 (TFOpLa (None, 8, 8, 256)    0           up_sampling2d_12[0][0]           \n",
      "                                                                 add_70[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_206 (Conv2D)             (None, 16, 16, 256)  33024       re_lu_195[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_217 (BatchN (None, 8, 8, 256)    1024        tf.__operators__.add_12[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "add_66 (Add)                    (None, 16, 16, 256)  0           add_65[0][0]                     \n",
      "                                                                 conv2d_206[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_217 (ReLU)                (None, 8, 8, 256)    0           batch_normalization_217[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_196 (BatchN (None, 16, 16, 256)  1024        add_66[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_228 (Conv2D)             (None, 8, 8, 128)    32896       re_lu_217[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_196 (ReLU)                (None, 16, 16, 256)  0           batch_normalization_196[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_218 (BatchN (None, 8, 8, 128)    512         conv2d_228[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_207 (Conv2D)             (None, 16, 16, 128)  32896       re_lu_196[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_184 (BatchN (None, 32, 32, 256)  1024        add_62[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_218 (ReLU)                (None, 8, 8, 128)    0           batch_normalization_218[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_197 (BatchN (None, 16, 16, 128)  512         conv2d_207[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_184 (ReLU)                (None, 32, 32, 256)  0           batch_normalization_184[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_229 (Conv2D)             (None, 8, 8, 128)    147584      re_lu_218[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_197 (ReLU)                (None, 16, 16, 128)  0           batch_normalization_197[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_195 (Conv2D)             (None, 32, 32, 128)  32896       re_lu_184[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_219 (BatchN (None, 8, 8, 128)    512         conv2d_229[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_208 (Conv2D)             (None, 16, 16, 128)  147584      re_lu_197[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_185 (BatchN (None, 32, 32, 128)  512         conv2d_195[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_219 (ReLU)                (None, 8, 8, 128)    0           batch_normalization_219[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_198 (BatchN (None, 16, 16, 128)  512         conv2d_208[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_185 (ReLU)                (None, 32, 32, 128)  0           batch_normalization_185[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_230 (Conv2D)             (None, 8, 8, 256)    33024       re_lu_219[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_198 (ReLU)                (None, 16, 16, 128)  0           batch_normalization_198[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_196 (Conv2D)             (None, 32, 32, 128)  147584      re_lu_185[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_74 (Add)                    (None, 8, 8, 256)    0           tf.__operators__.add_12[0][0]    \n",
      "                                                                 conv2d_230[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_209 (Conv2D)             (None, 16, 16, 256)  33024       re_lu_198[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_186 (BatchN (None, 32, 32, 128)  512         conv2d_196[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_13 (UpSampling2D) (None, 16, 16, 256)  0           add_74[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "add_67 (Add)                    (None, 16, 16, 256)  0           add_66[0][0]                     \n",
      "                                                                 conv2d_209[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_186 (ReLU)                (None, 32, 32, 128)  0           batch_normalization_186[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_13 (TFOpLa (None, 16, 16, 256)  0           up_sampling2d_13[0][0]           \n",
      "                                                                 add_67[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_197 (Conv2D)             (None, 32, 32, 256)  33024       re_lu_186[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_220 (BatchN (None, 16, 16, 256)  1024        tf.__operators__.add_13[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "add_63 (Add)                    (None, 32, 32, 256)  0           add_62[0][0]                     \n",
      "                                                                 conv2d_197[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_220 (ReLU)                (None, 16, 16, 256)  0           batch_normalization_220[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_187 (BatchN (None, 32, 32, 256)  1024        add_63[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_231 (Conv2D)             (None, 16, 16, 128)  32896       re_lu_220[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_187 (ReLU)                (None, 32, 32, 256)  0           batch_normalization_187[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_221 (BatchN (None, 16, 16, 128)  512         conv2d_231[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_198 (Conv2D)             (None, 32, 32, 128)  32896       re_lu_187[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_175 (BatchN (None, 64, 64, 256)  1024        add_59[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_221 (ReLU)                (None, 16, 16, 128)  0           batch_normalization_221[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_188 (BatchN (None, 32, 32, 128)  512         conv2d_198[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_175 (ReLU)                (None, 64, 64, 256)  0           batch_normalization_175[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_232 (Conv2D)             (None, 16, 16, 128)  147584      re_lu_221[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_188 (ReLU)                (None, 32, 32, 128)  0           batch_normalization_188[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_186 (Conv2D)             (None, 64, 64, 128)  32896       re_lu_175[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_222 (BatchN (None, 16, 16, 128)  512         conv2d_232[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_199 (Conv2D)             (None, 32, 32, 128)  147584      re_lu_188[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_176 (BatchN (None, 64, 64, 128)  512         conv2d_186[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_222 (ReLU)                (None, 16, 16, 128)  0           batch_normalization_222[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_189 (BatchN (None, 32, 32, 128)  512         conv2d_199[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_176 (ReLU)                (None, 64, 64, 128)  0           batch_normalization_176[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_233 (Conv2D)             (None, 16, 16, 256)  33024       re_lu_222[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_189 (ReLU)                (None, 32, 32, 128)  0           batch_normalization_189[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_187 (Conv2D)             (None, 64, 64, 128)  147584      re_lu_176[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_75 (Add)                    (None, 16, 16, 256)  0           tf.__operators__.add_13[0][0]    \n",
      "                                                                 conv2d_233[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_200 (Conv2D)             (None, 32, 32, 256)  33024       re_lu_189[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_177 (BatchN (None, 64, 64, 128)  512         conv2d_187[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_14 (UpSampling2D) (None, 32, 32, 256)  0           add_75[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "add_64 (Add)                    (None, 32, 32, 256)  0           add_63[0][0]                     \n",
      "                                                                 conv2d_200[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_177 (ReLU)                (None, 64, 64, 128)  0           batch_normalization_177[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_14 (TFOpLa (None, 32, 32, 256)  0           up_sampling2d_14[0][0]           \n",
      "                                                                 add_64[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_188 (Conv2D)             (None, 64, 64, 256)  33024       re_lu_177[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_223 (BatchN (None, 32, 32, 256)  1024        tf.__operators__.add_14[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "add_60 (Add)                    (None, 64, 64, 256)  0           add_59[0][0]                     \n",
      "                                                                 conv2d_188[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_223 (ReLU)                (None, 32, 32, 256)  0           batch_normalization_223[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_178 (BatchN (None, 64, 64, 256)  1024        add_60[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_234 (Conv2D)             (None, 32, 32, 128)  32896       re_lu_223[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_178 (ReLU)                (None, 64, 64, 256)  0           batch_normalization_178[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_224 (BatchN (None, 32, 32, 128)  512         conv2d_234[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_189 (Conv2D)             (None, 64, 64, 128)  32896       re_lu_178[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_224 (ReLU)                (None, 32, 32, 128)  0           batch_normalization_224[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_179 (BatchN (None, 64, 64, 128)  512         conv2d_189[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_235 (Conv2D)             (None, 32, 32, 128)  147584      re_lu_224[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_179 (ReLU)                (None, 64, 64, 128)  0           batch_normalization_179[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_225 (BatchN (None, 32, 32, 128)  512         conv2d_235[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_190 (Conv2D)             (None, 64, 64, 128)  147584      re_lu_179[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_225 (ReLU)                (None, 32, 32, 128)  0           batch_normalization_225[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_180 (BatchN (None, 64, 64, 128)  512         conv2d_190[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_236 (Conv2D)             (None, 32, 32, 256)  33024       re_lu_225[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_180 (ReLU)                (None, 64, 64, 128)  0           batch_normalization_180[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "add_76 (Add)                    (None, 32, 32, 256)  0           tf.__operators__.add_14[0][0]    \n",
      "                                                                 conv2d_236[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_191 (Conv2D)             (None, 64, 64, 256)  33024       re_lu_180[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_15 (UpSampling2D) (None, 64, 64, 256)  0           add_76[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "add_61 (Add)                    (None, 64, 64, 256)  0           add_60[0][0]                     \n",
      "                                                                 conv2d_191[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_15 (TFOpLa (None, 64, 64, 256)  0           up_sampling2d_15[0][0]           \n",
      "                                                                 add_61[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_226 (BatchN (None, 64, 64, 256)  1024        tf.__operators__.add_15[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_226 (ReLU)                (None, 64, 64, 256)  0           batch_normalization_226[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_237 (Conv2D)             (None, 64, 64, 128)  32896       re_lu_226[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_227 (BatchN (None, 64, 64, 128)  512         conv2d_237[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_227 (ReLU)                (None, 64, 64, 128)  0           batch_normalization_227[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_238 (Conv2D)             (None, 64, 64, 128)  147584      re_lu_227[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_228 (BatchN (None, 64, 64, 128)  512         conv2d_238[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_228 (ReLU)                (None, 64, 64, 128)  0           batch_normalization_228[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_239 (Conv2D)             (None, 64, 64, 256)  33024       re_lu_228[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_77 (Add)                    (None, 64, 64, 256)  0           tf.__operators__.add_15[0][0]    \n",
      "                                                                 conv2d_239[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_240 (Conv2D)             (None, 64, 64, 256)  65792       add_77[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_229 (BatchN (None, 64, 64, 256)  1024        conv2d_240[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_229 (ReLU)                (None, 64, 64, 256)  0           batch_normalization_229[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_241 (Conv2D)             (None, 64, 64, 16)   4112        re_lu_229[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 16,368,320\n",
      "Trainable params: 16,290,752\n",
      "Non-trainable params: 77,568\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "num_heatmap = 16\n",
    "model = StackedHourglassNetwork(IMAGE_SHAPE, 4, 1, num_heatmap)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "980cbd55",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n",
      "Start training...\n",
      "Start epoch 1 with learning rate 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/tensorflow/python/data/ops/dataset_ops.py:374: UserWarning: To make it possible to preserve tf.data options across serialization boundaries, their implementation has moved to be part of the TensorFlow graph. As a consequence, the options value is in general no longer known at graph construction time. Invoking this method in graph mode retains the legacy behavior of the original implementation, but note that the returned value might not reflect the actual value of the options.\n",
      "  warnings.warn(\"To make it possible to preserve tf.data options across \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start distributed traininng...\n",
      "Trained batch 1 batch loss 2.36763191 epoch total loss 2.36763191\n",
      "Trained batch 2 batch loss 741.39679 epoch total loss 371.882202\n",
      "Trained batch 3 batch loss 175.798355 epoch total loss 306.520905\n",
      "Trained batch 4 batch loss 52.269989 epoch total loss 242.958191\n",
      "Trained batch 5 batch loss 75.305809 epoch total loss 209.427704\n",
      "Trained batch 6 batch loss 73.2239 epoch total loss 186.727066\n",
      "Trained batch 7 batch loss 34.8950043 epoch total loss 165.036774\n",
      "Trained batch 8 batch loss 97.6139908 epoch total loss 156.608932\n",
      "Trained batch 9 batch loss 186.018707 epoch total loss 159.876678\n",
      "Trained batch 10 batch loss 200.894455 epoch total loss 163.978455\n",
      "Trained batch 11 batch loss 84.6429062 epoch total loss 156.766129\n",
      "Trained batch 12 batch loss 69.5646439 epoch total loss 149.499344\n",
      "Trained batch 13 batch loss 45.0307388 epoch total loss 141.463303\n",
      "Trained batch 14 batch loss 37.9454918 epoch total loss 134.069168\n",
      "Trained batch 15 batch loss 38.8007812 epoch total loss 127.717941\n",
      "Trained batch 16 batch loss 28.4178333 epoch total loss 121.511688\n",
      "Trained batch 17 batch loss 18.4694901 epoch total loss 115.450378\n",
      "Trained batch 18 batch loss 13.422123 epoch total loss 109.782143\n",
      "Trained batch 19 batch loss 13.5613947 epoch total loss 104.717896\n",
      "Trained batch 20 batch loss 7.67227364 epoch total loss 99.8656158\n",
      "Trained batch 21 batch loss 6.21534109 epoch total loss 95.4060745\n",
      "Trained batch 22 batch loss 5.35895157 epoch total loss 91.3130264\n",
      "Trained batch 23 batch loss 6.29839706 epoch total loss 87.6167374\n",
      "Trained batch 24 batch loss 5.87614298 epoch total loss 84.2108765\n",
      "Trained batch 25 batch loss 5.02352381 epoch total loss 81.0433807\n",
      "Trained batch 26 batch loss 4.42711926 epoch total loss 78.0966\n",
      "Trained batch 27 batch loss 15.3659554 epoch total loss 75.7732468\n",
      "Trained batch 28 batch loss 6.25097 epoch total loss 73.2903061\n",
      "Trained batch 29 batch loss 10.2410498 epoch total loss 71.1161957\n",
      "Trained batch 30 batch loss 14.0824757 epoch total loss 69.2150726\n",
      "Trained batch 31 batch loss 16.1097126 epoch total loss 67.5019913\n",
      "Trained batch 32 batch loss 16.5670738 epoch total loss 65.9102783\n",
      "Trained batch 33 batch loss 13.7217932 epoch total loss 64.328804\n",
      "Trained batch 34 batch loss 13.7504559 epoch total loss 62.8412094\n",
      "Trained batch 35 batch loss 9.43502903 epoch total loss 61.3153191\n",
      "Trained batch 36 batch loss 7.06851387 epoch total loss 59.8084641\n",
      "Trained batch 37 batch loss 6.58689547 epoch total loss 58.3700447\n",
      "Trained batch 38 batch loss 6.68328381 epoch total loss 57.0098686\n",
      "Trained batch 39 batch loss 6.29372168 epoch total loss 55.7094536\n",
      "Trained batch 40 batch loss 6.21269798 epoch total loss 54.4720345\n",
      "Trained batch 41 batch loss 5.8534565 epoch total loss 53.2862167\n",
      "Trained batch 42 batch loss 4.720119 epoch total loss 52.1298828\n",
      "Trained batch 43 batch loss 4.3018589 epoch total loss 51.0176\n",
      "Trained batch 44 batch loss 4.24913836 epoch total loss 49.9546776\n",
      "Trained batch 45 batch loss 3.48878431 epoch total loss 48.9221039\n",
      "Trained batch 46 batch loss 3.22581196 epoch total loss 47.9287071\n",
      "Trained batch 47 batch loss 3.31052685 epoch total loss 46.9793816\n",
      "Trained batch 48 batch loss 3.46951723 epoch total loss 46.0729256\n",
      "Trained batch 49 batch loss 3.3219018 epoch total loss 45.2004585\n",
      "Trained batch 50 batch loss 2.8646698 epoch total loss 44.3537445\n",
      "Trained batch 51 batch loss 2.80882668 epoch total loss 43.5391388\n",
      "Trained batch 52 batch loss 2.81608176 epoch total loss 42.7560043\n",
      "Trained batch 53 batch loss 2.55541849 epoch total loss 41.9975052\n",
      "Trained batch 54 batch loss 2.58134627 epoch total loss 41.2675743\n",
      "Trained batch 55 batch loss 2.49324179 epoch total loss 40.5625839\n",
      "Trained batch 56 batch loss 2.36053038 epoch total loss 39.8804054\n",
      "Trained batch 57 batch loss 2.34234905 epoch total loss 39.2218437\n",
      "Trained batch 58 batch loss 2.2929244 epoch total loss 38.5851364\n",
      "Trained batch 59 batch loss 2.12121391 epoch total loss 37.9671021\n",
      "Trained batch 60 batch loss 2.14274311 epoch total loss 37.3700333\n",
      "Trained batch 61 batch loss 2.11909 epoch total loss 36.7921486\n",
      "Trained batch 62 batch loss 2.2301507 epoch total loss 36.2347\n",
      "Trained batch 63 batch loss 2.22611427 epoch total loss 35.6948776\n",
      "Trained batch 64 batch loss 2.28499675 epoch total loss 35.1728477\n",
      "Trained batch 65 batch loss 2.19750023 epoch total loss 34.665535\n",
      "Trained batch 66 batch loss 2.04613137 epoch total loss 34.1713028\n",
      "Trained batch 67 batch loss 2.12693858 epoch total loss 33.6930275\n",
      "Trained batch 68 batch loss 2.11912012 epoch total loss 33.2287064\n",
      "Trained batch 69 batch loss 2.11604881 epoch total loss 32.7777977\n",
      "Trained batch 70 batch loss 2.03056169 epoch total loss 32.3385506\n",
      "Trained batch 71 batch loss 2.02551055 epoch total loss 31.9116039\n",
      "Trained batch 72 batch loss 2.15161014 epoch total loss 31.49827\n",
      "Trained batch 73 batch loss 2.08680439 epoch total loss 31.0953751\n",
      "Trained batch 74 batch loss 2.08934522 epoch total loss 30.7034016\n",
      "Trained batch 75 batch loss 2.07533717 epoch total loss 30.3216953\n",
      "Trained batch 76 batch loss 2.02071023 epoch total loss 29.949316\n",
      "Trained batch 77 batch loss 1.79137564 epoch total loss 29.5836258\n",
      "Trained batch 78 batch loss 1.68129134 epoch total loss 29.2259045\n",
      "Trained batch 79 batch loss 1.65257454 epoch total loss 28.8768749\n",
      "Trained batch 80 batch loss 1.79249358 epoch total loss 28.5383205\n",
      "Trained batch 81 batch loss 1.81906366 epoch total loss 28.2084541\n",
      "Trained batch 82 batch loss 1.82814538 epoch total loss 27.8867416\n",
      "Trained batch 83 batch loss 1.85244954 epoch total loss 27.5730782\n",
      "Trained batch 84 batch loss 1.95246673 epoch total loss 27.2680702\n",
      "Trained batch 85 batch loss 1.87449908 epoch total loss 26.9693222\n",
      "Trained batch 86 batch loss 1.68506241 epoch total loss 26.6753178\n",
      "Trained batch 87 batch loss 1.94124711 epoch total loss 26.3910179\n",
      "Trained batch 88 batch loss 1.8542583 epoch total loss 26.1121902\n",
      "Trained batch 89 batch loss 1.9687562 epoch total loss 25.8409157\n",
      "Trained batch 90 batch loss 2.00394988 epoch total loss 25.5760612\n",
      "Trained batch 91 batch loss 2.17466497 epoch total loss 25.3189011\n",
      "Trained batch 92 batch loss 2.05530453 epoch total loss 25.0660381\n",
      "Trained batch 93 batch loss 2.08216715 epoch total loss 24.8189011\n",
      "Trained batch 94 batch loss 1.95983601 epoch total loss 24.5757179\n",
      "Trained batch 95 batch loss 1.95743656 epoch total loss 24.3376312\n",
      "Trained batch 96 batch loss 1.91718626 epoch total loss 24.1040859\n",
      "Trained batch 97 batch loss 2.0352459 epoch total loss 23.8765697\n",
      "Trained batch 98 batch loss 2.02152181 epoch total loss 23.6535587\n",
      "Trained batch 99 batch loss 1.96647167 epoch total loss 23.4345\n",
      "Trained batch 100 batch loss 1.96638203 epoch total loss 23.2198162\n",
      "Trained batch 101 batch loss 2.04469156 epoch total loss 23.0101624\n",
      "Trained batch 102 batch loss 1.86010444 epoch total loss 22.8028088\n",
      "Trained batch 103 batch loss 1.97675967 epoch total loss 22.6006145\n",
      "Trained batch 104 batch loss 1.77571285 epoch total loss 22.4003735\n",
      "Trained batch 105 batch loss 1.8929553 epoch total loss 22.2050667\n",
      "Trained batch 106 batch loss 1.84185278 epoch total loss 22.0129604\n",
      "Trained batch 107 batch loss 1.6630609 epoch total loss 21.8227749\n",
      "Trained batch 108 batch loss 1.62902415 epoch total loss 21.6357937\n",
      "Trained batch 109 batch loss 1.82804716 epoch total loss 21.454073\n",
      "Trained batch 110 batch loss 1.90295863 epoch total loss 21.2763367\n",
      "Trained batch 111 batch loss 1.88881803 epoch total loss 21.101675\n",
      "Trained batch 112 batch loss 1.95909286 epoch total loss 20.9307575\n",
      "Trained batch 113 batch loss 1.96346438 epoch total loss 20.7629051\n",
      "Trained batch 114 batch loss 1.99729848 epoch total loss 20.5982952\n",
      "Trained batch 115 batch loss 1.87482893 epoch total loss 20.4354801\n",
      "Trained batch 116 batch loss 1.98319292 epoch total loss 20.2764091\n",
      "Trained batch 117 batch loss 1.93488717 epoch total loss 20.1196442\n",
      "Trained batch 118 batch loss 1.9453187 epoch total loss 19.9656239\n",
      "Trained batch 119 batch loss 1.90247893 epoch total loss 19.8138332\n",
      "Trained batch 120 batch loss 1.74551558 epoch total loss 19.6632652\n",
      "Trained batch 121 batch loss 1.81358659 epoch total loss 19.5157452\n",
      "Trained batch 122 batch loss 1.92933166 epoch total loss 19.3715954\n",
      "Trained batch 123 batch loss 1.85488379 epoch total loss 19.2291851\n",
      "Trained batch 124 batch loss 1.85070229 epoch total loss 19.089035\n",
      "Trained batch 125 batch loss 1.88163161 epoch total loss 18.9513741\n",
      "Trained batch 126 batch loss 1.96118164 epoch total loss 18.8165321\n",
      "Trained batch 127 batch loss 1.84126592 epoch total loss 18.682869\n",
      "Trained batch 128 batch loss 1.93078 epoch total loss 18.5519924\n",
      "Trained batch 129 batch loss 2.06359148 epoch total loss 18.4241753\n",
      "Trained batch 130 batch loss 2.0930109 epoch total loss 18.2985497\n",
      "Trained batch 131 batch loss 2.3678236 epoch total loss 18.1769428\n",
      "Trained batch 132 batch loss 2.09706736 epoch total loss 18.0551262\n",
      "Trained batch 133 batch loss 2.0847435 epoch total loss 17.9350471\n",
      "Trained batch 134 batch loss 2.02573323 epoch total loss 17.8163204\n",
      "Trained batch 135 batch loss 1.96395361 epoch total loss 17.6988945\n",
      "Trained batch 136 batch loss 1.98446989 epoch total loss 17.5833473\n",
      "Trained batch 137 batch loss 1.90439653 epoch total loss 17.4689\n",
      "Trained batch 138 batch loss 2.05445981 epoch total loss 17.3572025\n",
      "Trained batch 139 batch loss 1.99131048 epoch total loss 17.2466564\n",
      "Trained batch 140 batch loss 1.99565578 epoch total loss 17.1377201\n",
      "Trained batch 141 batch loss 2.00009441 epoch total loss 17.0303593\n",
      "Trained batch 142 batch loss 1.96173131 epoch total loss 16.924242\n",
      "Trained batch 143 batch loss 1.95420837 epoch total loss 16.8195553\n",
      "Trained batch 144 batch loss 1.99921668 epoch total loss 16.7166367\n",
      "Trained batch 145 batch loss 1.96668291 epoch total loss 16.6149139\n",
      "Trained batch 146 batch loss 1.91663313 epoch total loss 16.5142422\n",
      "Trained batch 147 batch loss 1.8332305 epoch total loss 16.4143715\n",
      "Trained batch 148 batch loss 1.82038271 epoch total loss 16.3157635\n",
      "Trained batch 149 batch loss 1.76343906 epoch total loss 16.2180958\n",
      "Trained batch 150 batch loss 1.73502588 epoch total loss 16.1215439\n",
      "Trained batch 151 batch loss 1.70690441 epoch total loss 16.0260811\n",
      "Trained batch 152 batch loss 1.78041291 epoch total loss 15.9323606\n",
      "Trained batch 153 batch loss 1.7287488 epoch total loss 15.8395262\n",
      "Trained batch 154 batch loss 1.93373179 epoch total loss 15.7492294\n",
      "Trained batch 155 batch loss 1.85796654 epoch total loss 15.6596079\n",
      "Trained batch 156 batch loss 1.91944945 epoch total loss 15.5715303\n",
      "Trained batch 157 batch loss 1.92555702 epoch total loss 15.4846125\n",
      "Trained batch 158 batch loss 1.98452616 epoch total loss 15.3991699\n",
      "Trained batch 159 batch loss 1.97960389 epoch total loss 15.3147697\n",
      "Trained batch 160 batch loss 1.91895735 epoch total loss 15.2310457\n",
      "Trained batch 161 batch loss 1.84381831 epoch total loss 15.1478949\n",
      "Trained batch 162 batch loss 1.99432564 epoch total loss 15.0667\n",
      "Trained batch 163 batch loss 1.90349746 epoch total loss 14.9859447\n",
      "Trained batch 164 batch loss 1.81271219 epoch total loss 14.9056206\n",
      "Trained batch 165 batch loss 1.82688618 epoch total loss 14.826355\n",
      "Trained batch 166 batch loss 1.83452606 epoch total loss 14.7480907\n",
      "Trained batch 167 batch loss 1.9254694 epoch total loss 14.6713095\n",
      "Trained batch 168 batch loss 1.95639992 epoch total loss 14.5956249\n",
      "Trained batch 169 batch loss 1.95303583 epoch total loss 14.5208168\n",
      "Trained batch 170 batch loss 1.93932581 epoch total loss 14.4468079\n",
      "Trained batch 171 batch loss 1.93095446 epoch total loss 14.3736153\n",
      "Trained batch 172 batch loss 1.91826665 epoch total loss 14.3012\n",
      "Trained batch 173 batch loss 1.77743506 epoch total loss 14.2288074\n",
      "Trained batch 174 batch loss 1.80289519 epoch total loss 14.1573954\n",
      "Trained batch 175 batch loss 1.89967585 epoch total loss 14.0873508\n",
      "Trained batch 176 batch loss 1.86971736 epoch total loss 14.0179319\n",
      "Trained batch 177 batch loss 1.81863594 epoch total loss 13.9490089\n",
      "Trained batch 178 batch loss 1.75773394 epoch total loss 13.8805189\n",
      "Trained batch 179 batch loss 1.51550961 epoch total loss 13.8114414\n",
      "Trained batch 180 batch loss 1.74703598 epoch total loss 13.7444172\n",
      "Trained batch 181 batch loss 1.870592 epoch total loss 13.6788158\n",
      "Trained batch 182 batch loss 1.90811563 epoch total loss 13.6141424\n",
      "Trained batch 183 batch loss 1.93983209 epoch total loss 13.5503492\n",
      "Trained batch 184 batch loss 1.94881034 epoch total loss 13.4872971\n",
      "Trained batch 185 batch loss 1.92166674 epoch total loss 13.4247799\n",
      "Trained batch 186 batch loss 1.93244529 epoch total loss 13.3629923\n",
      "Trained batch 187 batch loss 1.94643962 epoch total loss 13.3019419\n",
      "Trained batch 188 batch loss 1.8462956 epoch total loss 13.2410069\n",
      "Trained batch 189 batch loss 1.88913894 epoch total loss 13.1809444\n",
      "Trained batch 190 batch loss 1.85048115 epoch total loss 13.1213112\n",
      "Trained batch 191 batch loss 1.87087941 epoch total loss 13.0624075\n",
      "Trained batch 192 batch loss 1.86799479 epoch total loss 13.0041037\n",
      "Trained batch 193 batch loss 1.9317044 epoch total loss 12.9467335\n",
      "Trained batch 194 batch loss 1.87699389 epoch total loss 12.8896723\n",
      "Trained batch 195 batch loss 1.67103481 epoch total loss 12.8321419\n",
      "Trained batch 196 batch loss 1.5498867 epoch total loss 12.7745781\n",
      "Trained batch 197 batch loss 1.75367534 epoch total loss 12.7186346\n",
      "Trained batch 198 batch loss 1.78957975 epoch total loss 12.6634369\n",
      "Trained batch 199 batch loss 1.90580463 epoch total loss 12.6093788\n",
      "Trained batch 200 batch loss 1.87842989 epoch total loss 12.5557241\n",
      "Trained batch 201 batch loss 1.84702134 epoch total loss 12.5024462\n",
      "Trained batch 202 batch loss 1.85315287 epoch total loss 12.449728\n",
      "Trained batch 203 batch loss 1.91745281 epoch total loss 12.3978443\n",
      "Trained batch 204 batch loss 1.92850506 epoch total loss 12.3465242\n",
      "Trained batch 205 batch loss 1.92816663 epoch total loss 12.2957029\n",
      "Trained batch 206 batch loss 1.81617808 epoch total loss 12.244832\n",
      "Trained batch 207 batch loss 1.87925124 epoch total loss 12.1947556\n",
      "Trained batch 208 batch loss 1.88366508 epoch total loss 12.1451826\n",
      "Trained batch 209 batch loss 1.71795988 epoch total loss 12.0952921\n",
      "Trained batch 210 batch loss 1.8196249 epoch total loss 12.04636\n",
      "Trained batch 211 batch loss 1.77875042 epoch total loss 11.9976988\n",
      "Trained batch 212 batch loss 1.92833769 epoch total loss 11.950201\n",
      "Trained batch 213 batch loss 1.81155336 epoch total loss 11.9026012\n",
      "Trained batch 214 batch loss 1.60091507 epoch total loss 11.8544626\n",
      "Trained batch 215 batch loss 1.72293448 epoch total loss 11.8073387\n",
      "Trained batch 216 batch loss 1.71306026 epoch total loss 11.7606068\n",
      "Trained batch 217 batch loss 1.67509663 epoch total loss 11.7141294\n",
      "Trained batch 218 batch loss 1.6240716 epoch total loss 11.6678448\n",
      "Trained batch 219 batch loss 1.77366829 epoch total loss 11.6226654\n",
      "Trained batch 220 batch loss 1.89729202 epoch total loss 11.5784588\n",
      "Trained batch 221 batch loss 1.88753581 epoch total loss 11.5346079\n",
      "Trained batch 222 batch loss 1.78437793 epoch total loss 11.4906883\n",
      "Trained batch 223 batch loss 1.76117849 epoch total loss 11.4470587\n",
      "Trained batch 224 batch loss 1.71906006 epoch total loss 11.4036303\n",
      "Trained batch 225 batch loss 1.7895124 epoch total loss 11.3609009\n",
      "Trained batch 226 batch loss 1.78210878 epoch total loss 11.3185167\n",
      "Trained batch 227 batch loss 1.79174948 epoch total loss 11.2765493\n",
      "Trained batch 228 batch loss 1.78500891 epoch total loss 11.2349186\n",
      "Trained batch 229 batch loss 1.59298038 epoch total loss 11.1928148\n",
      "Trained batch 230 batch loss 1.79055548 epoch total loss 11.1519346\n",
      "Trained batch 231 batch loss 1.63528824 epoch total loss 11.1107378\n",
      "Trained batch 232 batch loss 1.64216828 epoch total loss 11.0699244\n",
      "Trained batch 233 batch loss 1.79577279 epoch total loss 11.0301208\n",
      "Trained batch 234 batch loss 1.72762036 epoch total loss 10.990366\n",
      "Trained batch 235 batch loss 1.79192758 epoch total loss 10.9512234\n",
      "Trained batch 236 batch loss 1.87505376 epoch total loss 10.9127655\n",
      "Trained batch 237 batch loss 1.91881514 epoch total loss 10.8748159\n",
      "Trained batch 238 batch loss 1.86180782 epoch total loss 10.8369455\n",
      "Trained batch 239 batch loss 1.8398236 epoch total loss 10.7993011\n",
      "Trained batch 240 batch loss 1.77729511 epoch total loss 10.7617092\n",
      "Trained batch 241 batch loss 1.84497237 epoch total loss 10.7247105\n",
      "Trained batch 242 batch loss 1.70871425 epoch total loss 10.6874542\n",
      "Trained batch 243 batch loss 1.64218473 epoch total loss 10.6502304\n",
      "Trained batch 244 batch loss 1.77661347 epoch total loss 10.6138639\n",
      "Trained batch 245 batch loss 1.84474313 epoch total loss 10.5780716\n",
      "Trained batch 246 batch loss 1.61042023 epoch total loss 10.5416174\n",
      "Trained batch 247 batch loss 1.52505684 epoch total loss 10.5051126\n",
      "Trained batch 248 batch loss 1.53557 epoch total loss 10.4689455\n",
      "Trained batch 249 batch loss 1.65990567 epoch total loss 10.433568\n",
      "Trained batch 250 batch loss 1.65257549 epoch total loss 10.3984442\n",
      "Trained batch 251 batch loss 1.65617263 epoch total loss 10.363615\n",
      "Trained batch 252 batch loss 1.77968884 epoch total loss 10.3295517\n",
      "Trained batch 253 batch loss 1.79408252 epoch total loss 10.2958155\n",
      "Trained batch 254 batch loss 1.73131323 epoch total loss 10.2620964\n",
      "Trained batch 255 batch loss 1.70102024 epoch total loss 10.2285233\n",
      "Trained batch 256 batch loss 1.69967389 epoch total loss 10.1952076\n",
      "Trained batch 257 batch loss 1.77235091 epoch total loss 10.1624346\n",
      "Trained batch 258 batch loss 1.91560459 epoch total loss 10.1304693\n",
      "Trained batch 259 batch loss 1.86416125 epoch total loss 10.0985537\n",
      "Trained batch 260 batch loss 1.83241129 epoch total loss 10.066761\n",
      "Trained batch 261 batch loss 1.71682811 epoch total loss 10.0347691\n",
      "Trained batch 262 batch loss 1.7983793 epoch total loss 10.0033321\n",
      "Trained batch 263 batch loss 1.83623731 epoch total loss 9.97227859\n",
      "Trained batch 264 batch loss 1.87381876 epoch total loss 9.94160271\n",
      "Trained batch 265 batch loss 1.73882449 epoch total loss 9.91064835\n",
      "Trained batch 266 batch loss 1.84032178 epoch total loss 9.8803091\n",
      "Trained batch 267 batch loss 1.87744975 epoch total loss 9.85033512\n",
      "Trained batch 268 batch loss 1.74876118 epoch total loss 9.82010555\n",
      "Trained batch 269 batch loss 1.83292437 epoch total loss 9.79041386\n",
      "Trained batch 270 batch loss 1.87906015 epoch total loss 9.76111317\n",
      "Trained batch 271 batch loss 1.80522442 epoch total loss 9.73175526\n",
      "Trained batch 272 batch loss 1.90600777 epoch total loss 9.70298386\n",
      "Trained batch 273 batch loss 1.8163079 epoch total loss 9.67409515\n",
      "Trained batch 274 batch loss 1.63915396 epoch total loss 9.64477062\n",
      "Trained batch 275 batch loss 1.7382319 epoch total loss 9.61602\n",
      "Trained batch 276 batch loss 1.62350738 epoch total loss 9.58706188\n",
      "Trained batch 277 batch loss 1.65962648 epoch total loss 9.55844307\n",
      "Trained batch 278 batch loss 1.7091347 epoch total loss 9.53020859\n",
      "Trained batch 279 batch loss 1.85441399 epoch total loss 9.50269699\n",
      "Trained batch 280 batch loss 1.78967154 epoch total loss 9.47515\n",
      "Trained batch 281 batch loss 1.88562667 epoch total loss 9.4481411\n",
      "Trained batch 282 batch loss 1.83501887 epoch total loss 9.42114449\n",
      "Trained batch 283 batch loss 1.91674709 epoch total loss 9.39462662\n",
      "Trained batch 284 batch loss 1.93869126 epoch total loss 9.36837387\n",
      "Trained batch 285 batch loss 1.80581188 epoch total loss 9.34183884\n",
      "Trained batch 286 batch loss 1.84206915 epoch total loss 9.31561565\n",
      "Trained batch 287 batch loss 1.89550757 epoch total loss 9.28976154\n",
      "Trained batch 288 batch loss 1.88113129 epoch total loss 9.26403713\n",
      "Trained batch 289 batch loss 1.79734027 epoch total loss 9.23820114\n",
      "Trained batch 290 batch loss 1.83820605 epoch total loss 9.21268368\n",
      "Trained batch 291 batch loss 1.88489604 epoch total loss 9.18750286\n",
      "Trained batch 292 batch loss 1.73940849 epoch total loss 9.16199589\n",
      "Trained batch 293 batch loss 1.80849528 epoch total loss 9.13689899\n",
      "Trained batch 294 batch loss 1.89956641 epoch total loss 9.1122818\n",
      "Trained batch 295 batch loss 1.91086125 epoch total loss 9.0878706\n",
      "Trained batch 296 batch loss 1.94250441 epoch total loss 9.06373\n",
      "Trained batch 297 batch loss 1.83167779 epoch total loss 9.03938103\n",
      "Trained batch 298 batch loss 1.71163845 epoch total loss 9.01479053\n",
      "Trained batch 299 batch loss 1.61924374 epoch total loss 8.99005604\n",
      "Trained batch 300 batch loss 1.527035 epoch total loss 8.96517944\n",
      "Trained batch 301 batch loss 1.90087807 epoch total loss 8.94171\n",
      "Trained batch 302 batch loss 1.8815372 epoch total loss 8.9183321\n",
      "Trained batch 303 batch loss 1.8224088 epoch total loss 8.89491367\n",
      "Trained batch 304 batch loss 1.68913662 epoch total loss 8.87121105\n",
      "Trained batch 305 batch loss 1.56516993 epoch total loss 8.84725666\n",
      "Trained batch 306 batch loss 1.69933033 epoch total loss 8.82389736\n",
      "Trained batch 307 batch loss 1.85553861 epoch total loss 8.80119896\n",
      "Trained batch 308 batch loss 1.8879118 epoch total loss 8.77875328\n",
      "Trained batch 309 batch loss 1.80544019 epoch total loss 8.75618553\n",
      "Trained batch 310 batch loss 1.92455125 epoch total loss 8.73414803\n",
      "Trained batch 311 batch loss 1.92792797 epoch total loss 8.71226311\n",
      "Trained batch 312 batch loss 1.86270523 epoch total loss 8.69031\n",
      "Trained batch 313 batch loss 1.89263582 epoch total loss 8.6685915\n",
      "Trained batch 314 batch loss 1.90189874 epoch total loss 8.64704227\n",
      "Trained batch 315 batch loss 1.91883087 epoch total loss 8.62568283\n",
      "Trained batch 316 batch loss 1.87999034 epoch total loss 8.60433578\n",
      "Trained batch 317 batch loss 1.84700131 epoch total loss 8.5830183\n",
      "Trained batch 318 batch loss 1.71115541 epoch total loss 8.561409\n",
      "Trained batch 319 batch loss 1.58140182 epoch total loss 8.53952789\n",
      "Trained batch 320 batch loss 1.77114427 epoch total loss 8.5183773\n",
      "Trained batch 321 batch loss 1.86455846 epoch total loss 8.49764824\n",
      "Trained batch 322 batch loss 1.7881211 epoch total loss 8.47681141\n",
      "Trained batch 323 batch loss 1.73292589 epoch total loss 8.45593262\n",
      "Trained batch 324 batch loss 1.70940137 epoch total loss 8.43511\n",
      "Trained batch 325 batch loss 1.66210365 epoch total loss 8.41426945\n",
      "Trained batch 326 batch loss 1.67600393 epoch total loss 8.3936\n",
      "Trained batch 327 batch loss 1.50177979 epoch total loss 8.37252426\n",
      "Trained batch 328 batch loss 1.75925648 epoch total loss 8.35236168\n",
      "Trained batch 329 batch loss 1.7993685 epoch total loss 8.33244419\n",
      "Trained batch 330 batch loss 1.90378463 epoch total loss 8.31296349\n",
      "Trained batch 331 batch loss 1.78880501 epoch total loss 8.29325294\n",
      "Trained batch 332 batch loss 1.74292135 epoch total loss 8.27352238\n",
      "Trained batch 333 batch loss 1.69662988 epoch total loss 8.25377178\n",
      "Trained batch 334 batch loss 1.79129899 epoch total loss 8.23442364\n",
      "Trained batch 335 batch loss 1.88107717 epoch total loss 8.21545792\n",
      "Trained batch 336 batch loss 1.86712193 epoch total loss 8.19656467\n",
      "Trained batch 337 batch loss 1.86303091 epoch total loss 8.17777061\n",
      "Trained batch 338 batch loss 1.77411401 epoch total loss 8.15882492\n",
      "Trained batch 339 batch loss 1.73499727 epoch total loss 8.13987637\n",
      "Trained batch 340 batch loss 1.76449752 epoch total loss 8.12112427\n",
      "Trained batch 341 batch loss 1.81964993 epoch total loss 8.10264492\n",
      "Trained batch 342 batch loss 1.80312061 epoch total loss 8.08422565\n",
      "Trained batch 343 batch loss 1.8675766 epoch total loss 8.06610203\n",
      "Trained batch 344 batch loss 1.83946323 epoch total loss 8.048\n",
      "Trained batch 345 batch loss 1.87085104 epoch total loss 8.03009605\n",
      "Trained batch 346 batch loss 1.86241221 epoch total loss 8.01227\n",
      "Trained batch 347 batch loss 1.94516325 epoch total loss 7.99478531\n",
      "Trained batch 348 batch loss 1.91548479 epoch total loss 7.9773159\n",
      "Trained batch 349 batch loss 1.91065621 epoch total loss 7.9599328\n",
      "Trained batch 350 batch loss 1.90809357 epoch total loss 7.94264221\n",
      "Trained batch 351 batch loss 1.8665688 epoch total loss 7.92533112\n",
      "Trained batch 352 batch loss 1.85483515 epoch total loss 7.90808535\n",
      "Trained batch 353 batch loss 1.7606777 epoch total loss 7.89067078\n",
      "Trained batch 354 batch loss 1.71598113 epoch total loss 7.87322807\n",
      "Trained batch 355 batch loss 1.80619836 epoch total loss 7.85613775\n",
      "Trained batch 356 batch loss 1.74001491 epoch total loss 7.83895779\n",
      "Trained batch 357 batch loss 1.77114 epoch total loss 7.8219614\n",
      "Trained batch 358 batch loss 1.84415615 epoch total loss 7.80526352\n",
      "Trained batch 359 batch loss 1.74142146 epoch total loss 7.78837299\n",
      "Trained batch 360 batch loss 1.80088592 epoch total loss 7.77174091\n",
      "Trained batch 361 batch loss 1.75457239 epoch total loss 7.75507307\n",
      "Trained batch 362 batch loss 1.80016959 epoch total loss 7.73862267\n",
      "Trained batch 363 batch loss 1.80605 epoch total loss 7.72227955\n",
      "Trained batch 364 batch loss 1.75559831 epoch total loss 7.70588779\n",
      "Trained batch 365 batch loss 1.75819623 epoch total loss 7.68959284\n",
      "Trained batch 366 batch loss 1.76707089 epoch total loss 7.67341137\n",
      "Trained batch 367 batch loss 1.68210387 epoch total loss 7.65708637\n",
      "Trained batch 368 batch loss 1.74555564 epoch total loss 7.64102221\n",
      "Trained batch 369 batch loss 1.67017078 epoch total loss 7.62484121\n",
      "Trained batch 370 batch loss 1.72446942 epoch total loss 7.60889387\n",
      "Trained batch 371 batch loss 1.81308079 epoch total loss 7.59327173\n",
      "Trained batch 372 batch loss 1.74625218 epoch total loss 7.57755423\n",
      "Trained batch 373 batch loss 1.89247131 epoch total loss 7.5623126\n",
      "Trained batch 374 batch loss 1.57210445 epoch total loss 7.54629612\n",
      "Trained batch 375 batch loss 1.71269917 epoch total loss 7.53074\n",
      "Trained batch 376 batch loss 1.73642051 epoch total loss 7.51532888\n",
      "Trained batch 377 batch loss 1.92574739 epoch total loss 7.50050259\n",
      "Trained batch 378 batch loss 1.91439366 epoch total loss 7.48572445\n",
      "Trained batch 379 batch loss 1.91632605 epoch total loss 7.47102928\n",
      "Trained batch 380 batch loss 1.91223454 epoch total loss 7.45640087\n",
      "Trained batch 381 batch loss 1.91918421 epoch total loss 7.44186783\n",
      "Trained batch 382 batch loss 1.88548088 epoch total loss 7.42732191\n",
      "Trained batch 383 batch loss 1.82801747 epoch total loss 7.41270304\n",
      "Trained batch 384 batch loss 1.84739113 epoch total loss 7.39821\n",
      "Trained batch 385 batch loss 1.81616485 epoch total loss 7.38371086\n",
      "Trained batch 386 batch loss 1.75018597 epoch total loss 7.36911678\n",
      "Trained batch 387 batch loss 1.81945968 epoch total loss 7.35477686\n",
      "Trained batch 388 batch loss 1.77046466 epoch total loss 7.34038448\n",
      "Trained batch 389 batch loss 1.73730385 epoch total loss 7.32598066\n",
      "Trained batch 390 batch loss 1.75853968 epoch total loss 7.31170511\n",
      "Trained batch 391 batch loss 1.75459802 epoch total loss 7.2974925\n",
      "Trained batch 392 batch loss 1.75797069 epoch total loss 7.28336143\n",
      "Trained batch 393 batch loss 1.84680617 epoch total loss 7.26952791\n",
      "Trained batch 394 batch loss 1.85210824 epoch total loss 7.25577831\n",
      "Trained batch 395 batch loss 1.68020701 epoch total loss 7.24166298\n",
      "Trained batch 396 batch loss 1.66944027 epoch total loss 7.22759151\n",
      "Trained batch 397 batch loss 1.62143624 epoch total loss 7.21347\n",
      "Trained batch 398 batch loss 1.70491803 epoch total loss 7.19962931\n",
      "Trained batch 399 batch loss 1.82158446 epoch total loss 7.18615\n",
      "Trained batch 400 batch loss 1.68116081 epoch total loss 7.1723876\n",
      "Trained batch 401 batch loss 1.58403933 epoch total loss 7.15845156\n",
      "Trained batch 402 batch loss 1.74795318 epoch total loss 7.14499283\n",
      "Trained batch 403 batch loss 1.53431153 epoch total loss 7.13107061\n",
      "Trained batch 404 batch loss 1.76513624 epoch total loss 7.11778879\n",
      "Trained batch 405 batch loss 1.83946621 epoch total loss 7.1047554\n",
      "Trained batch 406 batch loss 1.75491095 epoch total loss 7.09157848\n",
      "Trained batch 407 batch loss 1.85059917 epoch total loss 7.0787015\n",
      "Trained batch 408 batch loss 1.89160538 epoch total loss 7.06598806\n",
      "Trained batch 409 batch loss 1.89699531 epoch total loss 7.05335\n",
      "Trained batch 410 batch loss 1.89511025 epoch total loss 7.04076862\n",
      "Trained batch 411 batch loss 1.89211011 epoch total loss 7.02824116\n",
      "Trained batch 412 batch loss 1.91644955 epoch total loss 7.01583433\n",
      "Trained batch 413 batch loss 1.79524732 epoch total loss 7.00319338\n",
      "Trained batch 414 batch loss 1.79273438 epoch total loss 6.99060774\n",
      "Trained batch 415 batch loss 1.82259297 epoch total loss 6.97815418\n",
      "Trained batch 416 batch loss 1.76392198 epoch total loss 6.96562\n",
      "Trained batch 417 batch loss 1.77874863 epoch total loss 6.95318174\n",
      "Trained batch 418 batch loss 1.75294793 epoch total loss 6.94074106\n",
      "Trained batch 419 batch loss 1.81998849 epoch total loss 6.92851973\n",
      "Trained batch 420 batch loss 1.71692252 epoch total loss 6.91611147\n",
      "Trained batch 421 batch loss 1.75732 epoch total loss 6.90385771\n",
      "Trained batch 422 batch loss 1.71971619 epoch total loss 6.89157343\n",
      "Trained batch 423 batch loss 1.84086347 epoch total loss 6.87963295\n",
      "Trained batch 424 batch loss 1.7562964 epoch total loss 6.86755\n",
      "Trained batch 425 batch loss 1.84037888 epoch total loss 6.855721\n",
      "Trained batch 426 batch loss 1.8446666 epoch total loss 6.8439579\n",
      "Trained batch 427 batch loss 1.87236929 epoch total loss 6.83231497\n",
      "Trained batch 428 batch loss 1.86872029 epoch total loss 6.82071733\n",
      "Trained batch 429 batch loss 1.8484863 epoch total loss 6.80912685\n",
      "Trained batch 430 batch loss 1.78087378 epoch total loss 6.7974329\n",
      "Trained batch 431 batch loss 1.81678295 epoch total loss 6.78587723\n",
      "Trained batch 432 batch loss 1.77433765 epoch total loss 6.77427673\n",
      "Trained batch 433 batch loss 1.66046596 epoch total loss 6.76246643\n",
      "Trained batch 434 batch loss 1.83208168 epoch total loss 6.75110579\n",
      "Trained batch 435 batch loss 1.82602918 epoch total loss 6.73978376\n",
      "Trained batch 436 batch loss 1.74057603 epoch total loss 6.72831726\n",
      "Trained batch 437 batch loss 1.88457179 epoch total loss 6.71723318\n",
      "Trained batch 438 batch loss 1.80963969 epoch total loss 6.70602846\n",
      "Trained batch 439 batch loss 1.83806276 epoch total loss 6.69494\n",
      "Trained batch 440 batch loss 1.8135947 epoch total loss 6.68384552\n",
      "Trained batch 441 batch loss 1.83489716 epoch total loss 6.67285061\n",
      "Trained batch 442 batch loss 1.88328505 epoch total loss 6.66201448\n",
      "Trained batch 443 batch loss 1.73584414 epoch total loss 6.65089417\n",
      "Trained batch 444 batch loss 1.86827147 epoch total loss 6.64012241\n",
      "Trained batch 445 batch loss 1.82673216 epoch total loss 6.62930584\n",
      "Trained batch 446 batch loss 1.78233981 epoch total loss 6.61843777\n",
      "Trained batch 447 batch loss 1.77987731 epoch total loss 6.60761309\n",
      "Trained batch 448 batch loss 1.88002276 epoch total loss 6.59706068\n",
      "Trained batch 449 batch loss 1.91902375 epoch total loss 6.58664179\n",
      "Trained batch 450 batch loss 1.89845562 epoch total loss 6.57622337\n",
      "Trained batch 451 batch loss 1.88269889 epoch total loss 6.56581688\n",
      "Trained batch 452 batch loss 1.8483057 epoch total loss 6.55538\n",
      "Trained batch 453 batch loss 2.31970119 epoch total loss 6.54602957\n",
      "Trained batch 454 batch loss 3.54584384 epoch total loss 6.53942108\n",
      "Trained batch 455 batch loss 3.9791913 epoch total loss 6.5337944\n",
      "Trained batch 456 batch loss 5.1936965 epoch total loss 6.53085518\n",
      "Trained batch 457 batch loss 3.74284482 epoch total loss 6.524755\n",
      "Trained batch 458 batch loss 3.15712786 epoch total loss 6.51740217\n",
      "Trained batch 459 batch loss 2.84217262 epoch total loss 6.5093956\n",
      "Trained batch 460 batch loss 2.50643492 epoch total loss 6.50069332\n",
      "Trained batch 461 batch loss 2.45671344 epoch total loss 6.49192095\n",
      "Trained batch 462 batch loss 2.36228466 epoch total loss 6.48298264\n",
      "Trained batch 463 batch loss 2.59450388 epoch total loss 6.4745841\n",
      "Trained batch 464 batch loss 2.41504288 epoch total loss 6.46583509\n",
      "Trained batch 465 batch loss 2.41181898 epoch total loss 6.4571166\n",
      "Trained batch 466 batch loss 2.39952517 epoch total loss 6.44840908\n",
      "Trained batch 467 batch loss 2.35414028 epoch total loss 6.43964243\n",
      "Trained batch 468 batch loss 2.23079967 epoch total loss 6.4306488\n",
      "Trained batch 469 batch loss 2.20805788 epoch total loss 6.42164564\n",
      "Trained batch 470 batch loss 2.09695339 epoch total loss 6.41244411\n",
      "Trained batch 471 batch loss 2.10514593 epoch total loss 6.40329885\n",
      "Trained batch 472 batch loss 2.18284559 epoch total loss 6.39435768\n",
      "Trained batch 473 batch loss 2.09989595 epoch total loss 6.38527822\n",
      "Trained batch 474 batch loss 1.97143602 epoch total loss 6.37596607\n",
      "Trained batch 475 batch loss 2.09635878 epoch total loss 6.36695671\n",
      "Trained batch 476 batch loss 2.00921512 epoch total loss 6.35780191\n",
      "Trained batch 477 batch loss 2.08608484 epoch total loss 6.34884691\n",
      "Trained batch 478 batch loss 2.05845809 epoch total loss 6.33987093\n",
      "Trained batch 479 batch loss 2.06308603 epoch total loss 6.33094215\n",
      "Trained batch 480 batch loss 2.06495118 epoch total loss 6.32205439\n",
      "Trained batch 481 batch loss 2.02849078 epoch total loss 6.31312847\n",
      "Trained batch 482 batch loss 2.02920508 epoch total loss 6.3042407\n",
      "Trained batch 483 batch loss 1.94407403 epoch total loss 6.2952137\n",
      "Trained batch 484 batch loss 2.01273084 epoch total loss 6.28636551\n",
      "Trained batch 485 batch loss 1.92917347 epoch total loss 6.27738142\n",
      "Trained batch 486 batch loss 1.92185688 epoch total loss 6.26841974\n",
      "Trained batch 487 batch loss 1.88859797 epoch total loss 6.25942612\n",
      "Trained batch 488 batch loss 1.94349957 epoch total loss 6.25058222\n",
      "Trained batch 489 batch loss 1.87019885 epoch total loss 6.24162436\n",
      "Trained batch 490 batch loss 1.88691127 epoch total loss 6.23273706\n",
      "Trained batch 491 batch loss 1.90708816 epoch total loss 6.22392702\n",
      "Trained batch 492 batch loss 1.93497443 epoch total loss 6.21521\n",
      "Trained batch 493 batch loss 1.81971395 epoch total loss 6.20629454\n",
      "Trained batch 494 batch loss 1.87623882 epoch total loss 6.19752884\n",
      "Trained batch 495 batch loss 1.81093693 epoch total loss 6.1886673\n",
      "Trained batch 496 batch loss 1.76897132 epoch total loss 6.17975712\n",
      "Trained batch 497 batch loss 1.794379 epoch total loss 6.17093325\n",
      "Trained batch 498 batch loss 1.90034509 epoch total loss 6.16235781\n",
      "Trained batch 499 batch loss 1.90401161 epoch total loss 6.15382433\n",
      "Trained batch 500 batch loss 1.81703568 epoch total loss 6.14515066\n",
      "Trained batch 501 batch loss 1.8713479 epoch total loss 6.13662052\n",
      "Trained batch 502 batch loss 1.75514734 epoch total loss 6.12789202\n",
      "Trained batch 503 batch loss 1.73669672 epoch total loss 6.11916256\n",
      "Trained batch 504 batch loss 1.71037531 epoch total loss 6.11041498\n",
      "Trained batch 505 batch loss 1.823982 epoch total loss 6.1019268\n",
      "Trained batch 506 batch loss 1.88017249 epoch total loss 6.09358358\n",
      "Trained batch 507 batch loss 1.90519857 epoch total loss 6.08532238\n",
      "Trained batch 508 batch loss 1.8900671 epoch total loss 6.07706451\n",
      "Trained batch 509 batch loss 1.84581387 epoch total loss 6.06875134\n",
      "Trained batch 510 batch loss 1.70070767 epoch total loss 6.06018639\n",
      "Trained batch 511 batch loss 1.8289814 epoch total loss 6.05190659\n",
      "Trained batch 512 batch loss 1.71070576 epoch total loss 6.04342747\n",
      "Trained batch 513 batch loss 1.85127616 epoch total loss 6.03525591\n",
      "Trained batch 514 batch loss 1.84513879 epoch total loss 6.0271039\n",
      "Trained batch 515 batch loss 1.80768204 epoch total loss 6.01891088\n",
      "Trained batch 516 batch loss 1.92499495 epoch total loss 6.01097679\n",
      "Trained batch 517 batch loss 1.95899951 epoch total loss 6.0031395\n",
      "Trained batch 518 batch loss 2.02695632 epoch total loss 5.99546289\n",
      "Trained batch 519 batch loss 1.93453562 epoch total loss 5.98763847\n",
      "Trained batch 520 batch loss 1.87490273 epoch total loss 5.97972965\n",
      "Trained batch 521 batch loss 1.46772981 epoch total loss 5.97106934\n",
      "Trained batch 522 batch loss 1.42954648 epoch total loss 5.96236897\n",
      "Trained batch 523 batch loss 1.88066244 epoch total loss 5.95456457\n",
      "Trained batch 524 batch loss 1.69464016 epoch total loss 5.94643497\n",
      "Trained batch 525 batch loss 1.79806733 epoch total loss 5.93853331\n",
      "Trained batch 526 batch loss 1.86795938 epoch total loss 5.93079472\n",
      "Trained batch 527 batch loss 1.9341383 epoch total loss 5.92321062\n",
      "Trained batch 528 batch loss 1.83145893 epoch total loss 5.91546106\n",
      "Trained batch 529 batch loss 1.84960914 epoch total loss 5.9077754\n",
      "Trained batch 530 batch loss 1.81171989 epoch total loss 5.90004683\n",
      "Trained batch 531 batch loss 1.88737702 epoch total loss 5.89249039\n",
      "Trained batch 532 batch loss 1.71889925 epoch total loss 5.88464546\n",
      "Trained batch 533 batch loss 1.75684333 epoch total loss 5.87690067\n",
      "Trained batch 534 batch loss 1.69572306 epoch total loss 5.86907101\n",
      "Trained batch 535 batch loss 1.75345778 epoch total loss 5.86137819\n",
      "Trained batch 536 batch loss 1.80101633 epoch total loss 5.85380316\n",
      "Trained batch 537 batch loss 1.7647264 epoch total loss 5.84618807\n",
      "Trained batch 538 batch loss 1.80757022 epoch total loss 5.8386817\n",
      "Trained batch 539 batch loss 1.74205887 epoch total loss 5.83108091\n",
      "Trained batch 540 batch loss 1.71065605 epoch total loss 5.82345057\n",
      "Trained batch 541 batch loss 1.60165048 epoch total loss 5.81564665\n",
      "Trained batch 542 batch loss 1.66121817 epoch total loss 5.80798149\n",
      "Trained batch 543 batch loss 1.79395556 epoch total loss 5.80058908\n",
      "Trained batch 544 batch loss 1.79019856 epoch total loss 5.79321718\n",
      "Trained batch 545 batch loss 1.87091756 epoch total loss 5.78602028\n",
      "Trained batch 546 batch loss 1.89058518 epoch total loss 5.77888584\n",
      "Trained batch 547 batch loss 1.92583644 epoch total loss 5.771842\n",
      "Trained batch 548 batch loss 1.90421724 epoch total loss 5.76478434\n",
      "Trained batch 549 batch loss 1.91274202 epoch total loss 5.75776815\n",
      "Trained batch 550 batch loss 1.91117477 epoch total loss 5.75077438\n",
      "Trained batch 551 batch loss 1.90166807 epoch total loss 5.74378824\n",
      "Trained batch 552 batch loss 1.90188599 epoch total loss 5.73682833\n",
      "Trained batch 553 batch loss 1.91862416 epoch total loss 5.72992373\n",
      "Trained batch 554 batch loss 1.88612568 epoch total loss 5.72298574\n",
      "Trained batch 555 batch loss 1.86574149 epoch total loss 5.71603584\n",
      "Trained batch 556 batch loss 1.81834126 epoch total loss 5.70902586\n",
      "Trained batch 557 batch loss 1.90221739 epoch total loss 5.70219088\n",
      "Trained batch 558 batch loss 1.8116672 epoch total loss 5.69521904\n",
      "Trained batch 559 batch loss 1.83349872 epoch total loss 5.68831062\n",
      "Trained batch 560 batch loss 1.81908286 epoch total loss 5.68140125\n",
      "Trained batch 561 batch loss 1.83502197 epoch total loss 5.67454481\n",
      "Trained batch 562 batch loss 1.79360127 epoch total loss 5.66763926\n",
      "Trained batch 563 batch loss 1.73722053 epoch total loss 5.66065836\n",
      "Trained batch 564 batch loss 1.6602577 epoch total loss 5.65356541\n",
      "Trained batch 565 batch loss 1.66564536 epoch total loss 5.64650679\n",
      "Trained batch 566 batch loss 1.84211922 epoch total loss 5.63978529\n",
      "Trained batch 567 batch loss 1.72552371 epoch total loss 5.63288164\n",
      "Trained batch 568 batch loss 1.83553076 epoch total loss 5.62619638\n",
      "Trained batch 569 batch loss 1.83719766 epoch total loss 5.61953688\n",
      "Trained batch 570 batch loss 1.89643967 epoch total loss 5.61300564\n",
      "Trained batch 571 batch loss 1.87694371 epoch total loss 5.60646248\n",
      "Trained batch 572 batch loss 1.86163247 epoch total loss 5.5999155\n",
      "Trained batch 573 batch loss 1.85580885 epoch total loss 5.59338093\n",
      "Trained batch 574 batch loss 1.79650521 epoch total loss 5.58676624\n",
      "Trained batch 575 batch loss 1.63473499 epoch total loss 5.57989311\n",
      "Trained batch 576 batch loss 1.84671259 epoch total loss 5.57341194\n",
      "Trained batch 577 batch loss 1.80329323 epoch total loss 5.56687784\n",
      "Trained batch 578 batch loss 1.77968895 epoch total loss 5.56032562\n",
      "Trained batch 579 batch loss 1.82300413 epoch total loss 5.55387068\n",
      "Trained batch 580 batch loss 1.87432706 epoch total loss 5.54752684\n",
      "Trained batch 581 batch loss 1.74292636 epoch total loss 5.54097843\n",
      "Trained batch 582 batch loss 1.74812233 epoch total loss 5.53446102\n",
      "Trained batch 583 batch loss 1.75395358 epoch total loss 5.52797651\n",
      "Trained batch 584 batch loss 1.89747405 epoch total loss 5.52176\n",
      "Trained batch 585 batch loss 1.71860266 epoch total loss 5.51525879\n",
      "Trained batch 586 batch loss 1.8052032 epoch total loss 5.50892735\n",
      "Trained batch 587 batch loss 1.76386094 epoch total loss 5.50254726\n",
      "Trained batch 588 batch loss 1.74445868 epoch total loss 5.49615622\n",
      "Trained batch 589 batch loss 1.80343461 epoch total loss 5.48988676\n",
      "Trained batch 590 batch loss 1.79301178 epoch total loss 5.48362064\n",
      "Trained batch 591 batch loss 1.87045157 epoch total loss 5.47750664\n",
      "Trained batch 592 batch loss 1.84350133 epoch total loss 5.47136831\n",
      "Trained batch 593 batch loss 1.82400966 epoch total loss 5.46521759\n",
      "Trained batch 594 batch loss 1.7991035 epoch total loss 5.45904541\n",
      "Trained batch 595 batch loss 1.6339767 epoch total loss 5.45261717\n",
      "Trained batch 596 batch loss 1.74888778 epoch total loss 5.44640255\n",
      "Trained batch 597 batch loss 1.7768476 epoch total loss 5.44025612\n",
      "Trained batch 598 batch loss 1.74928379 epoch total loss 5.43408346\n",
      "Trained batch 599 batch loss 1.78327274 epoch total loss 5.42798853\n",
      "Trained batch 600 batch loss 1.91270888 epoch total loss 5.42212963\n",
      "Trained batch 601 batch loss 1.9236598 epoch total loss 5.4163084\n",
      "Trained batch 602 batch loss 1.7527504 epoch total loss 5.41022253\n",
      "Trained batch 603 batch loss 1.68597 epoch total loss 5.40404654\n",
      "Trained batch 604 batch loss 1.75299382 epoch total loss 5.39800167\n",
      "Trained batch 605 batch loss 1.87616694 epoch total loss 5.39218044\n",
      "Trained batch 606 batch loss 1.94084835 epoch total loss 5.38648558\n",
      "Trained batch 607 batch loss 1.91863346 epoch total loss 5.38077259\n",
      "Trained batch 608 batch loss 1.80747759 epoch total loss 5.3748951\n",
      "Trained batch 609 batch loss 1.87096381 epoch total loss 5.36914158\n",
      "Trained batch 610 batch loss 1.73557448 epoch total loss 5.36318493\n",
      "Trained batch 611 batch loss 1.76825452 epoch total loss 5.35730124\n",
      "Trained batch 612 batch loss 1.85687459 epoch total loss 5.35158157\n",
      "Trained batch 613 batch loss 1.65964651 epoch total loss 5.34555912\n",
      "Trained batch 614 batch loss 1.78548133 epoch total loss 5.33976078\n",
      "Trained batch 615 batch loss 1.701 epoch total loss 5.33384371\n",
      "Trained batch 616 batch loss 1.65816784 epoch total loss 5.32787704\n",
      "Trained batch 617 batch loss 1.76429117 epoch total loss 5.32210159\n",
      "Trained batch 618 batch loss 1.79052234 epoch total loss 5.3163867\n",
      "Trained batch 619 batch loss 1.83797109 epoch total loss 5.31076717\n",
      "Trained batch 620 batch loss 1.87502265 epoch total loss 5.30522585\n",
      "Trained batch 621 batch loss 1.90590072 epoch total loss 5.29975224\n",
      "Trained batch 622 batch loss 1.87456679 epoch total loss 5.29424524\n",
      "Trained batch 623 batch loss 1.88374829 epoch total loss 5.28877115\n",
      "Trained batch 624 batch loss 1.81671238 epoch total loss 5.28320646\n",
      "Trained batch 625 batch loss 1.62271345 epoch total loss 5.27735\n",
      "Trained batch 626 batch loss 1.54132462 epoch total loss 5.27138186\n",
      "Trained batch 627 batch loss 1.47622228 epoch total loss 5.26532888\n",
      "Trained batch 628 batch loss 1.57158458 epoch total loss 5.2594471\n",
      "Trained batch 629 batch loss 1.67623377 epoch total loss 5.2537508\n",
      "Trained batch 630 batch loss 2.00649047 epoch total loss 5.24859619\n",
      "Trained batch 631 batch loss 2.59102345 epoch total loss 5.24438477\n",
      "Trained batch 632 batch loss 3.06077504 epoch total loss 5.2409296\n",
      "Trained batch 633 batch loss 3.29226208 epoch total loss 5.23785114\n",
      "Trained batch 634 batch loss 3.19998932 epoch total loss 5.23463678\n",
      "Trained batch 635 batch loss 3.19391251 epoch total loss 5.2314229\n",
      "Trained batch 636 batch loss 3.1046195 epoch total loss 5.22807932\n",
      "Trained batch 637 batch loss 2.96088791 epoch total loss 5.22452\n",
      "Trained batch 638 batch loss 2.91715574 epoch total loss 5.22090387\n",
      "Trained batch 639 batch loss 3.04319358 epoch total loss 5.21749544\n",
      "Trained batch 640 batch loss 3.46901917 epoch total loss 5.21476364\n",
      "Trained batch 641 batch loss 2.97663379 epoch total loss 5.21127176\n",
      "Trained batch 642 batch loss 2.90110087 epoch total loss 5.20767355\n",
      "Trained batch 643 batch loss 3.31463885 epoch total loss 5.20472956\n",
      "Trained batch 644 batch loss 3.54499936 epoch total loss 5.20215225\n",
      "Trained batch 645 batch loss 3.07217121 epoch total loss 5.19885\n",
      "Trained batch 646 batch loss 2.78936243 epoch total loss 5.19512033\n",
      "Trained batch 647 batch loss 3.09898329 epoch total loss 5.19188\n",
      "Trained batch 648 batch loss 2.81258249 epoch total loss 5.1882081\n",
      "Trained batch 649 batch loss 2.63681 epoch total loss 5.18427706\n",
      "Trained batch 650 batch loss 2.62992239 epoch total loss 5.18034697\n",
      "Trained batch 651 batch loss 2.55610085 epoch total loss 5.17631626\n",
      "Trained batch 652 batch loss 2.32668805 epoch total loss 5.17194557\n",
      "Trained batch 653 batch loss 2.25759554 epoch total loss 5.16748238\n",
      "Trained batch 654 batch loss 2.06437063 epoch total loss 5.16273785\n",
      "Trained batch 655 batch loss 1.96288729 epoch total loss 5.15785217\n",
      "Trained batch 656 batch loss 2.06107116 epoch total loss 5.15313148\n",
      "Trained batch 657 batch loss 2.14909863 epoch total loss 5.14855957\n",
      "Trained batch 658 batch loss 2.23596621 epoch total loss 5.14413309\n",
      "Trained batch 659 batch loss 2.13351965 epoch total loss 5.13956451\n",
      "Trained batch 660 batch loss 2.11369252 epoch total loss 5.13498\n",
      "Trained batch 661 batch loss 2.07390356 epoch total loss 5.13034916\n",
      "Trained batch 662 batch loss 1.97084153 epoch total loss 5.12557697\n",
      "Trained batch 663 batch loss 1.92437828 epoch total loss 5.12074852\n",
      "Trained batch 664 batch loss 2.00269985 epoch total loss 5.11605263\n",
      "Trained batch 665 batch loss 2.07855916 epoch total loss 5.111485\n",
      "Trained batch 666 batch loss 2.00584936 epoch total loss 5.10682201\n",
      "Trained batch 667 batch loss 1.96441078 epoch total loss 5.10211039\n",
      "Trained batch 668 batch loss 1.9954797 epoch total loss 5.09746\n",
      "Trained batch 669 batch loss 1.99748349 epoch total loss 5.09282589\n",
      "Trained batch 670 batch loss 1.95116138 epoch total loss 5.08813715\n",
      "Trained batch 671 batch loss 1.9106034 epoch total loss 5.08340168\n",
      "Trained batch 672 batch loss 1.92425561 epoch total loss 5.07870054\n",
      "Trained batch 673 batch loss 1.86751652 epoch total loss 5.07392883\n",
      "Trained batch 674 batch loss 1.77288604 epoch total loss 5.06903124\n",
      "Trained batch 675 batch loss 1.78722477 epoch total loss 5.06416941\n",
      "Trained batch 676 batch loss 1.61690092 epoch total loss 5.05906963\n",
      "Trained batch 677 batch loss 1.67654967 epoch total loss 5.05407333\n",
      "Trained batch 678 batch loss 1.60242283 epoch total loss 5.04898262\n",
      "Trained batch 679 batch loss 1.61246705 epoch total loss 5.04392147\n",
      "Trained batch 680 batch loss 1.77257276 epoch total loss 5.03911066\n",
      "Trained batch 681 batch loss 1.92200816 epoch total loss 5.0345335\n",
      "Trained batch 682 batch loss 1.94315481 epoch total loss 5.03000069\n",
      "Trained batch 683 batch loss 1.91443896 epoch total loss 5.02543926\n",
      "Trained batch 684 batch loss 1.90586543 epoch total loss 5.02087831\n",
      "Trained batch 685 batch loss 1.93123031 epoch total loss 5.01636791\n",
      "Trained batch 686 batch loss 1.87317336 epoch total loss 5.01178598\n",
      "Trained batch 687 batch loss 1.63470912 epoch total loss 5.00687027\n",
      "Trained batch 688 batch loss 1.66360486 epoch total loss 5.00201082\n",
      "Trained batch 689 batch loss 1.81325138 epoch total loss 4.99738264\n",
      "Trained batch 690 batch loss 1.87070322 epoch total loss 4.99285126\n",
      "Trained batch 691 batch loss 1.85342634 epoch total loss 4.98830795\n",
      "Trained batch 692 batch loss 1.87700415 epoch total loss 4.98381186\n",
      "Trained batch 693 batch loss 1.85850978 epoch total loss 4.97930193\n",
      "Trained batch 694 batch loss 1.82959259 epoch total loss 4.97476339\n",
      "Trained batch 695 batch loss 1.82442951 epoch total loss 4.97023058\n",
      "Trained batch 696 batch loss 1.66559708 epoch total loss 4.96548271\n",
      "Trained batch 697 batch loss 1.55995846 epoch total loss 4.96059656\n",
      "Trained batch 698 batch loss 1.51697695 epoch total loss 4.9556632\n",
      "Trained batch 699 batch loss 1.57287312 epoch total loss 4.95082378\n",
      "Trained batch 700 batch loss 1.72054648 epoch total loss 4.94620895\n",
      "Trained batch 701 batch loss 1.72962534 epoch total loss 4.94162035\n",
      "Trained batch 702 batch loss 1.85599387 epoch total loss 4.93722486\n",
      "Trained batch 703 batch loss 1.77589834 epoch total loss 4.93272781\n",
      "Trained batch 704 batch loss 1.90370464 epoch total loss 4.92842531\n",
      "Trained batch 705 batch loss 1.87249529 epoch total loss 4.92409086\n",
      "Trained batch 706 batch loss 1.88799715 epoch total loss 4.91979027\n",
      "Trained batch 707 batch loss 1.90239596 epoch total loss 4.91552258\n",
      "Trained batch 708 batch loss 1.83912396 epoch total loss 4.91117716\n",
      "Trained batch 709 batch loss 1.92015946 epoch total loss 4.90695858\n",
      "Trained batch 710 batch loss 1.89770699 epoch total loss 4.90272045\n",
      "Trained batch 711 batch loss 1.77808 epoch total loss 4.89832544\n",
      "Trained batch 712 batch loss 1.78129697 epoch total loss 4.8939476\n",
      "Trained batch 713 batch loss 1.83640742 epoch total loss 4.8896594\n",
      "Trained batch 714 batch loss 1.63381863 epoch total loss 4.88509941\n",
      "Trained batch 715 batch loss 1.66108096 epoch total loss 4.88059044\n",
      "Trained batch 716 batch loss 1.7313242 epoch total loss 4.87619209\n",
      "Trained batch 717 batch loss 1.84814298 epoch total loss 4.87196875\n",
      "Trained batch 718 batch loss 1.82281923 epoch total loss 4.86772203\n",
      "Trained batch 719 batch loss 1.62951422 epoch total loss 4.86321831\n",
      "Trained batch 720 batch loss 1.5347625 epoch total loss 4.85859489\n",
      "Trained batch 721 batch loss 1.62691355 epoch total loss 4.8541131\n",
      "Trained batch 722 batch loss 1.65393424 epoch total loss 4.8496809\n",
      "Trained batch 723 batch loss 1.71365893 epoch total loss 4.84534311\n",
      "Trained batch 724 batch loss 1.77735639 epoch total loss 4.84110546\n",
      "Trained batch 725 batch loss 1.86401117 epoch total loss 4.83699942\n",
      "Trained batch 726 batch loss 1.8285625 epoch total loss 4.8328557\n",
      "Trained batch 727 batch loss 1.85383666 epoch total loss 4.82875776\n",
      "Trained batch 728 batch loss 1.87979424 epoch total loss 4.82470703\n",
      "Trained batch 729 batch loss 1.81846011 epoch total loss 4.82058287\n",
      "Trained batch 730 batch loss 1.77403617 epoch total loss 4.81640959\n",
      "Trained batch 731 batch loss 1.7416662 epoch total loss 4.81220341\n",
      "Trained batch 732 batch loss 1.81599796 epoch total loss 4.80811\n",
      "Trained batch 733 batch loss 1.69625521 epoch total loss 4.80386496\n",
      "Trained batch 734 batch loss 1.66927803 epoch total loss 4.79959393\n",
      "Trained batch 735 batch loss 1.79836082 epoch total loss 4.79551077\n",
      "Trained batch 736 batch loss 1.68756676 epoch total loss 4.7912879\n",
      "Trained batch 737 batch loss 1.66182554 epoch total loss 4.78704166\n",
      "Trained batch 738 batch loss 1.46293902 epoch total loss 4.78253746\n",
      "Trained batch 739 batch loss 1.40844 epoch total loss 4.77797174\n",
      "Trained batch 740 batch loss 1.557989 epoch total loss 4.77362061\n",
      "Trained batch 741 batch loss 1.75596845 epoch total loss 4.76954794\n",
      "Trained batch 742 batch loss 1.92101169 epoch total loss 4.76570892\n",
      "Trained batch 743 batch loss 1.80372882 epoch total loss 4.76172256\n",
      "Trained batch 744 batch loss 1.66401744 epoch total loss 4.75755882\n",
      "Trained batch 745 batch loss 1.60628569 epoch total loss 4.7533288\n",
      "Trained batch 746 batch loss 1.8180232 epoch total loss 4.74939442\n",
      "Trained batch 747 batch loss 1.84582019 epoch total loss 4.74550724\n",
      "Trained batch 748 batch loss 1.90240407 epoch total loss 4.74170589\n",
      "Trained batch 749 batch loss 1.87476373 epoch total loss 4.73787832\n",
      "Trained batch 750 batch loss 1.86772227 epoch total loss 4.73405123\n",
      "Trained batch 751 batch loss 1.87352824 epoch total loss 4.73024225\n",
      "Trained batch 752 batch loss 1.84433234 epoch total loss 4.72640467\n",
      "Trained batch 753 batch loss 1.84517229 epoch total loss 4.72257853\n",
      "Trained batch 754 batch loss 1.82117188 epoch total loss 4.71873045\n",
      "Trained batch 755 batch loss 1.76109982 epoch total loss 4.71481323\n",
      "Trained batch 756 batch loss 1.85428059 epoch total loss 4.71102905\n",
      "Trained batch 757 batch loss 1.8185401 epoch total loss 4.70720816\n",
      "Trained batch 758 batch loss 1.65468347 epoch total loss 4.70318127\n",
      "Trained batch 759 batch loss 1.64812 epoch total loss 4.69915628\n",
      "Trained batch 760 batch loss 1.70204449 epoch total loss 4.69521284\n",
      "Trained batch 761 batch loss 1.72408533 epoch total loss 4.6913085\n",
      "Trained batch 762 batch loss 1.76533973 epoch total loss 4.68746901\n",
      "Trained batch 763 batch loss 1.82402897 epoch total loss 4.68371582\n",
      "Trained batch 764 batch loss 1.90206981 epoch total loss 4.68007517\n",
      "Trained batch 765 batch loss 1.90167403 epoch total loss 4.6764431\n",
      "Trained batch 766 batch loss 1.85192299 epoch total loss 4.67275572\n",
      "Trained batch 767 batch loss 1.55854023 epoch total loss 4.66869545\n",
      "Trained batch 768 batch loss 1.64649975 epoch total loss 4.66476\n",
      "Trained batch 769 batch loss 1.69480717 epoch total loss 4.66089821\n",
      "Trained batch 770 batch loss 1.83286369 epoch total loss 4.65722513\n",
      "Trained batch 771 batch loss 1.81472969 epoch total loss 4.6535387\n",
      "Trained batch 772 batch loss 1.84221601 epoch total loss 4.6498971\n",
      "Trained batch 773 batch loss 1.87083137 epoch total loss 4.64630175\n",
      "Trained batch 774 batch loss 1.87363362 epoch total loss 4.64271927\n",
      "Trained batch 775 batch loss 1.88841748 epoch total loss 4.6391654\n",
      "Trained batch 776 batch loss 1.8734802 epoch total loss 4.63560152\n",
      "Trained batch 777 batch loss 1.87619972 epoch total loss 4.63205\n",
      "Trained batch 778 batch loss 1.87486041 epoch total loss 4.62850618\n",
      "Trained batch 779 batch loss 1.86325836 epoch total loss 4.62495661\n",
      "Trained batch 780 batch loss 1.86419415 epoch total loss 4.62141705\n",
      "Trained batch 781 batch loss 1.79426944 epoch total loss 4.6177969\n",
      "Trained batch 782 batch loss 1.86260831 epoch total loss 4.61427355\n",
      "Trained batch 783 batch loss 1.84845901 epoch total loss 4.61074114\n",
      "Trained batch 784 batch loss 1.86674333 epoch total loss 4.60724115\n",
      "Trained batch 785 batch loss 1.81910276 epoch total loss 4.60368967\n",
      "Trained batch 786 batch loss 1.8426044 epoch total loss 4.60017633\n",
      "Trained batch 787 batch loss 1.82632375 epoch total loss 4.59665203\n",
      "Trained batch 788 batch loss 1.78125858 epoch total loss 4.59307909\n",
      "Trained batch 789 batch loss 1.7011106 epoch total loss 4.58941412\n",
      "Trained batch 790 batch loss 1.69976425 epoch total loss 4.5857563\n",
      "Trained batch 791 batch loss 1.62660038 epoch total loss 4.58201504\n",
      "Trained batch 792 batch loss 1.93192458 epoch total loss 4.57866907\n",
      "Trained batch 793 batch loss 1.95377636 epoch total loss 4.57535887\n",
      "Trained batch 794 batch loss 2.00444102 epoch total loss 4.57212114\n",
      "Trained batch 795 batch loss 2.0109446 epoch total loss 4.56889963\n",
      "Trained batch 796 batch loss 1.92454958 epoch total loss 4.56557751\n",
      "Trained batch 797 batch loss 1.82179308 epoch total loss 4.56213474\n",
      "Trained batch 798 batch loss 1.65432811 epoch total loss 4.55849075\n",
      "Trained batch 799 batch loss 1.83076227 epoch total loss 4.55507708\n",
      "Trained batch 800 batch loss 1.75597191 epoch total loss 4.55157804\n",
      "Trained batch 801 batch loss 1.80968499 epoch total loss 4.54815483\n",
      "Trained batch 802 batch loss 1.8284024 epoch total loss 4.54476357\n",
      "Trained batch 803 batch loss 1.88050365 epoch total loss 4.54144573\n",
      "Trained batch 804 batch loss 1.94434202 epoch total loss 4.53821564\n",
      "Trained batch 805 batch loss 1.939183 epoch total loss 4.53498697\n",
      "Trained batch 806 batch loss 1.8998208 epoch total loss 4.53171778\n",
      "Trained batch 807 batch loss 1.83817589 epoch total loss 4.52838\n",
      "Trained batch 808 batch loss 1.83615685 epoch total loss 4.52504778\n",
      "Trained batch 809 batch loss 1.83150792 epoch total loss 4.5217185\n",
      "Trained batch 810 batch loss 1.86878371 epoch total loss 4.51844358\n",
      "Trained batch 811 batch loss 1.90228724 epoch total loss 4.51521778\n",
      "Trained batch 812 batch loss 1.87630141 epoch total loss 4.51196766\n",
      "Trained batch 813 batch loss 1.85147 epoch total loss 4.5086956\n",
      "Trained batch 814 batch loss 1.91020739 epoch total loss 4.50550318\n",
      "Trained batch 815 batch loss 1.89041305 epoch total loss 4.50229454\n",
      "Trained batch 816 batch loss 1.87564874 epoch total loss 4.49907541\n",
      "Trained batch 817 batch loss 1.82295299 epoch total loss 4.4958\n",
      "Trained batch 818 batch loss 1.73558712 epoch total loss 4.49242544\n",
      "Trained batch 819 batch loss 1.80622017 epoch total loss 4.48914576\n",
      "Trained batch 820 batch loss 1.82879543 epoch total loss 4.48590136\n",
      "Trained batch 821 batch loss 1.84849072 epoch total loss 4.4826889\n",
      "Trained batch 822 batch loss 1.86836505 epoch total loss 4.4795084\n",
      "Trained batch 823 batch loss 1.80758262 epoch total loss 4.47626209\n",
      "Trained batch 824 batch loss 1.74799311 epoch total loss 4.47295094\n",
      "Trained batch 825 batch loss 1.86743033 epoch total loss 4.46979284\n",
      "Trained batch 826 batch loss 1.74470198 epoch total loss 4.46649361\n",
      "Trained batch 827 batch loss 1.61864281 epoch total loss 4.46305\n",
      "Trained batch 828 batch loss 1.46850133 epoch total loss 4.45943356\n",
      "Trained batch 829 batch loss 1.62921071 epoch total loss 4.4560194\n",
      "Trained batch 830 batch loss 1.85676193 epoch total loss 4.45288754\n",
      "Trained batch 831 batch loss 1.78754973 epoch total loss 4.44968033\n",
      "Trained batch 832 batch loss 1.78926706 epoch total loss 4.44648266\n",
      "Trained batch 833 batch loss 1.72863626 epoch total loss 4.44321966\n",
      "Trained batch 834 batch loss 1.84870017 epoch total loss 4.44010878\n",
      "Trained batch 835 batch loss 1.82557654 epoch total loss 4.43697786\n",
      "Trained batch 836 batch loss 1.66688967 epoch total loss 4.43366432\n",
      "Trained batch 837 batch loss 1.74056339 epoch total loss 4.43044662\n",
      "Trained batch 838 batch loss 1.68420935 epoch total loss 4.42717\n",
      "Trained batch 839 batch loss 1.73670578 epoch total loss 4.42396307\n",
      "Trained batch 840 batch loss 1.75369942 epoch total loss 4.420784\n",
      "Trained batch 841 batch loss 1.79919124 epoch total loss 4.41766691\n",
      "Trained batch 842 batch loss 1.83053112 epoch total loss 4.41459417\n",
      "Trained batch 843 batch loss 1.84998429 epoch total loss 4.41155195\n",
      "Trained batch 844 batch loss 2.11711264 epoch total loss 4.4088335\n",
      "Trained batch 845 batch loss 2.87735081 epoch total loss 4.40702152\n",
      "Trained batch 846 batch loss 2.90910292 epoch total loss 4.40525103\n",
      "Trained batch 847 batch loss 3.33709502 epoch total loss 4.40399\n",
      "Trained batch 848 batch loss 3.59405494 epoch total loss 4.40303469\n",
      "Trained batch 849 batch loss 3.69859028 epoch total loss 4.40220499\n",
      "Trained batch 850 batch loss 4.04803228 epoch total loss 4.40178823\n",
      "Trained batch 851 batch loss 3.69005394 epoch total loss 4.40095186\n",
      "Trained batch 852 batch loss 3.81737518 epoch total loss 4.40026665\n",
      "Trained batch 853 batch loss 3.60154271 epoch total loss 4.39933062\n",
      "Trained batch 854 batch loss 3.47192478 epoch total loss 4.39824438\n",
      "Trained batch 855 batch loss 3.39357042 epoch total loss 4.39706945\n",
      "Trained batch 856 batch loss 3.0185256 epoch total loss 4.39545918\n",
      "Trained batch 857 batch loss 3.03483939 epoch total loss 4.39387131\n",
      "Trained batch 858 batch loss 3.81102467 epoch total loss 4.39319229\n",
      "Trained batch 859 batch loss 3.27484918 epoch total loss 4.39189053\n",
      "Trained batch 860 batch loss 3.58930898 epoch total loss 4.39095736\n",
      "Trained batch 861 batch loss 3.22788 epoch total loss 4.389606\n",
      "Trained batch 862 batch loss 3.80372238 epoch total loss 4.38892651\n",
      "Trained batch 863 batch loss 3.12068367 epoch total loss 4.38745689\n",
      "Trained batch 864 batch loss 3.48557758 epoch total loss 4.3864131\n",
      "Trained batch 865 batch loss 3.12889504 epoch total loss 4.38495922\n",
      "Trained batch 866 batch loss 3.11584401 epoch total loss 4.38349342\n",
      "Trained batch 867 batch loss 3.01719832 epoch total loss 4.38191748\n",
      "Trained batch 868 batch loss 3.20365334 epoch total loss 4.38056\n",
      "Trained batch 869 batch loss 2.72185922 epoch total loss 4.37865162\n",
      "Trained batch 870 batch loss 2.81169939 epoch total loss 4.37685061\n",
      "Trained batch 871 batch loss 3.39249611 epoch total loss 4.3757205\n",
      "Trained batch 872 batch loss 2.84201837 epoch total loss 4.37396145\n",
      "Trained batch 873 batch loss 2.60063362 epoch total loss 4.37193\n",
      "Trained batch 874 batch loss 2.75282836 epoch total loss 4.37007761\n",
      "Trained batch 875 batch loss 2.56587267 epoch total loss 4.36801577\n",
      "Trained batch 876 batch loss 2.59075642 epoch total loss 4.3659873\n",
      "Trained batch 877 batch loss 2.37353277 epoch total loss 4.36371517\n",
      "Trained batch 878 batch loss 2.57383037 epoch total loss 4.36167669\n",
      "Trained batch 879 batch loss 2.45541596 epoch total loss 4.35950756\n",
      "Trained batch 880 batch loss 2.6007967 epoch total loss 4.35750914\n",
      "Trained batch 881 batch loss 2.37349081 epoch total loss 4.35525751\n",
      "Trained batch 882 batch loss 2.22576165 epoch total loss 4.35284281\n",
      "Trained batch 883 batch loss 2.20768952 epoch total loss 4.3504138\n",
      "Trained batch 884 batch loss 2.12287593 epoch total loss 4.34789371\n",
      "Trained batch 885 batch loss 2.20787 epoch total loss 4.34547567\n",
      "Trained batch 886 batch loss 2.27840567 epoch total loss 4.34314251\n",
      "Trained batch 887 batch loss 2.11270142 epoch total loss 4.34062815\n",
      "Trained batch 888 batch loss 2.17490339 epoch total loss 4.33818913\n",
      "Trained batch 889 batch loss 2.1888454 epoch total loss 4.33577156\n",
      "Trained batch 890 batch loss 2.44962358 epoch total loss 4.33365202\n",
      "Trained batch 891 batch loss 2.17772222 epoch total loss 4.33123255\n",
      "Trained batch 892 batch loss 2.32516646 epoch total loss 4.32898378\n",
      "Trained batch 893 batch loss 2.13931417 epoch total loss 4.32653189\n",
      "Trained batch 894 batch loss 2.30558681 epoch total loss 4.3242712\n",
      "Trained batch 895 batch loss 2.01670027 epoch total loss 4.32169294\n",
      "Trained batch 896 batch loss 2.18126416 epoch total loss 4.31930399\n",
      "Trained batch 897 batch loss 1.91417062 epoch total loss 4.31662226\n",
      "Trained batch 898 batch loss 2.09783196 epoch total loss 4.31415176\n",
      "Trained batch 899 batch loss 2.01727247 epoch total loss 4.31159687\n",
      "Trained batch 900 batch loss 2.04412556 epoch total loss 4.30907726\n",
      "Trained batch 901 batch loss 2.09888148 epoch total loss 4.30662441\n",
      "Trained batch 902 batch loss 2.09178114 epoch total loss 4.3041687\n",
      "Trained batch 903 batch loss 2.05255795 epoch total loss 4.30167532\n",
      "Trained batch 904 batch loss 2.05586362 epoch total loss 4.299191\n",
      "Trained batch 905 batch loss 2.16255236 epoch total loss 4.29683\n",
      "Trained batch 906 batch loss 2.03098178 epoch total loss 4.29432917\n",
      "Trained batch 907 batch loss 1.96923327 epoch total loss 4.29176569\n",
      "Trained batch 908 batch loss 1.94328761 epoch total loss 4.28917933\n",
      "Trained batch 909 batch loss 1.90671122 epoch total loss 4.28655863\n",
      "Trained batch 910 batch loss 1.99474669 epoch total loss 4.28404\n",
      "Trained batch 911 batch loss 2.03584194 epoch total loss 4.28157234\n",
      "Trained batch 912 batch loss 1.96953177 epoch total loss 4.279037\n",
      "Trained batch 913 batch loss 1.95013285 epoch total loss 4.2764864\n",
      "Trained batch 914 batch loss 1.95745945 epoch total loss 4.27394915\n",
      "Trained batch 915 batch loss 2.04344392 epoch total loss 4.27151155\n",
      "Trained batch 916 batch loss 1.97638774 epoch total loss 4.26900578\n",
      "Trained batch 917 batch loss 1.95299447 epoch total loss 4.26648\n",
      "Trained batch 918 batch loss 1.82781971 epoch total loss 4.26382351\n",
      "Trained batch 919 batch loss 1.78336513 epoch total loss 4.26112461\n",
      "Trained batch 920 batch loss 1.9988575 epoch total loss 4.25866556\n",
      "Trained batch 921 batch loss 1.90450811 epoch total loss 4.25610924\n",
      "Trained batch 922 batch loss 1.92927754 epoch total loss 4.25358582\n",
      "Trained batch 923 batch loss 1.81413794 epoch total loss 4.25094271\n",
      "Trained batch 924 batch loss 1.72608578 epoch total loss 4.24821\n",
      "Trained batch 925 batch loss 1.85833776 epoch total loss 4.24562645\n",
      "Trained batch 926 batch loss 1.90907526 epoch total loss 4.2431035\n",
      "Trained batch 927 batch loss 1.83211124 epoch total loss 4.24050236\n",
      "Trained batch 928 batch loss 1.88551223 epoch total loss 4.23796463\n",
      "Trained batch 929 batch loss 1.76032507 epoch total loss 4.23529768\n",
      "Trained batch 930 batch loss 1.88127768 epoch total loss 4.23276663\n",
      "Trained batch 931 batch loss 1.88520408 epoch total loss 4.23024511\n",
      "Trained batch 932 batch loss 1.93606734 epoch total loss 4.22778368\n",
      "Trained batch 933 batch loss 1.70199955 epoch total loss 4.2250762\n",
      "Trained batch 934 batch loss 1.82317853 epoch total loss 4.22250462\n",
      "Trained batch 935 batch loss 1.89412761 epoch total loss 4.2200141\n",
      "Trained batch 936 batch loss 1.89058137 epoch total loss 4.21752548\n",
      "Trained batch 937 batch loss 1.98966777 epoch total loss 4.21514797\n",
      "Trained batch 938 batch loss 1.87068105 epoch total loss 4.21264839\n",
      "Trained batch 939 batch loss 1.84651589 epoch total loss 4.21012878\n",
      "Trained batch 940 batch loss 1.92032564 epoch total loss 4.20769262\n",
      "Trained batch 941 batch loss 1.80358124 epoch total loss 4.20513773\n",
      "Trained batch 942 batch loss 1.70582891 epoch total loss 4.20248461\n",
      "Trained batch 943 batch loss 1.75993204 epoch total loss 4.19989443\n",
      "Trained batch 944 batch loss 1.88659906 epoch total loss 4.19744396\n",
      "Trained batch 945 batch loss 1.85046148 epoch total loss 4.19496059\n",
      "Trained batch 946 batch loss 1.77077961 epoch total loss 4.19239759\n",
      "Trained batch 947 batch loss 1.73578751 epoch total loss 4.1898036\n",
      "Trained batch 948 batch loss 1.72296631 epoch total loss 4.1872015\n",
      "Trained batch 949 batch loss 1.69519162 epoch total loss 4.18457556\n",
      "Trained batch 950 batch loss 1.75603759 epoch total loss 4.18201923\n",
      "Trained batch 951 batch loss 1.7280314 epoch total loss 4.17943907\n",
      "Trained batch 952 batch loss 1.79839313 epoch total loss 4.17693806\n",
      "Trained batch 953 batch loss 1.80148315 epoch total loss 4.17444515\n",
      "Trained batch 954 batch loss 1.8736589 epoch total loss 4.17203379\n",
      "Trained batch 955 batch loss 1.86865425 epoch total loss 4.16962194\n",
      "Trained batch 956 batch loss 1.8131417 epoch total loss 4.1671567\n",
      "Trained batch 957 batch loss 1.92689514 epoch total loss 4.1648159\n",
      "Trained batch 958 batch loss 1.94334984 epoch total loss 4.16249704\n",
      "Trained batch 959 batch loss 1.94609284 epoch total loss 4.16018581\n",
      "Trained batch 960 batch loss 2.04750562 epoch total loss 4.15798521\n",
      "Trained batch 961 batch loss 1.86570621 epoch total loss 4.1556\n",
      "Trained batch 962 batch loss 1.87154377 epoch total loss 4.1532259\n",
      "Trained batch 963 batch loss 1.81169474 epoch total loss 4.15079451\n",
      "Trained batch 964 batch loss 1.9375881 epoch total loss 4.14849854\n",
      "Trained batch 965 batch loss 1.85189533 epoch total loss 4.14611864\n",
      "Trained batch 966 batch loss 1.80518353 epoch total loss 4.14369535\n",
      "Trained batch 967 batch loss 1.90256262 epoch total loss 4.14137745\n",
      "Trained batch 968 batch loss 1.80713856 epoch total loss 4.13896608\n",
      "Trained batch 969 batch loss 1.70315218 epoch total loss 4.13645267\n",
      "Trained batch 970 batch loss 1.81309187 epoch total loss 4.13405704\n",
      "Trained batch 971 batch loss 1.85462594 epoch total loss 4.13170958\n",
      "Trained batch 972 batch loss 2.05707574 epoch total loss 4.12957525\n",
      "Trained batch 973 batch loss 1.93443644 epoch total loss 4.12731934\n",
      "Trained batch 974 batch loss 1.94128096 epoch total loss 4.12507486\n",
      "Trained batch 975 batch loss 1.98817492 epoch total loss 4.12288332\n",
      "Trained batch 976 batch loss 1.99747264 epoch total loss 4.1207056\n",
      "Trained batch 977 batch loss 1.94757438 epoch total loss 4.11848116\n",
      "Trained batch 978 batch loss 1.94504189 epoch total loss 4.1162591\n",
      "Trained batch 979 batch loss 1.83746576 epoch total loss 4.11393118\n",
      "Trained batch 980 batch loss 1.92704797 epoch total loss 4.11169958\n",
      "Trained batch 981 batch loss 1.86818886 epoch total loss 4.10941267\n",
      "Trained batch 982 batch loss 1.76081371 epoch total loss 4.10702085\n",
      "Trained batch 983 batch loss 1.95568955 epoch total loss 4.10483265\n",
      "Trained batch 984 batch loss 1.83766949 epoch total loss 4.10252857\n",
      "Trained batch 985 batch loss 1.8162204 epoch total loss 4.10020733\n",
      "Trained batch 986 batch loss 1.73210311 epoch total loss 4.0978055\n",
      "Trained batch 987 batch loss 1.91163254 epoch total loss 4.09559059\n",
      "Trained batch 988 batch loss 1.88297796 epoch total loss 4.09335136\n",
      "Trained batch 989 batch loss 1.88036227 epoch total loss 4.09111357\n",
      "Trained batch 990 batch loss 1.87183094 epoch total loss 4.08887196\n",
      "Trained batch 991 batch loss 1.82177448 epoch total loss 4.08658409\n",
      "Trained batch 992 batch loss 1.91874695 epoch total loss 4.08439875\n",
      "Trained batch 993 batch loss 1.8240298 epoch total loss 4.08212233\n",
      "Trained batch 994 batch loss 1.67142534 epoch total loss 4.07969713\n",
      "Trained batch 995 batch loss 1.69598258 epoch total loss 4.0773015\n",
      "Trained batch 996 batch loss 1.71653628 epoch total loss 4.07493114\n",
      "Trained batch 997 batch loss 1.76075745 epoch total loss 4.07261038\n",
      "Trained batch 998 batch loss 1.8465662 epoch total loss 4.07037973\n",
      "Trained batch 999 batch loss 1.72717047 epoch total loss 4.06803417\n",
      "Trained batch 1000 batch loss 1.6959362 epoch total loss 4.06566191\n",
      "Trained batch 1001 batch loss 1.67263258 epoch total loss 4.06327152\n",
      "Trained batch 1002 batch loss 1.6048454 epoch total loss 4.06081772\n",
      "Trained batch 1003 batch loss 1.55831873 epoch total loss 4.05832291\n",
      "Trained batch 1004 batch loss 1.70853245 epoch total loss 4.05598259\n",
      "Trained batch 1005 batch loss 1.52771592 epoch total loss 4.0534668\n",
      "Trained batch 1006 batch loss 1.64655495 epoch total loss 4.05107403\n",
      "Trained batch 1007 batch loss 1.7709775 epoch total loss 4.04881\n",
      "Trained batch 1008 batch loss 1.82212377 epoch total loss 4.04660082\n",
      "Trained batch 1009 batch loss 1.82196856 epoch total loss 4.04439592\n",
      "Trained batch 1010 batch loss 1.71786535 epoch total loss 4.04209232\n",
      "Trained batch 1011 batch loss 1.69591582 epoch total loss 4.03977156\n",
      "Trained batch 1012 batch loss 1.70596457 epoch total loss 4.03746557\n",
      "Trained batch 1013 batch loss 1.79273129 epoch total loss 4.03524971\n",
      "Trained batch 1014 batch loss 1.72085547 epoch total loss 4.03296757\n",
      "Trained batch 1015 batch loss 1.78437221 epoch total loss 4.03075218\n",
      "Trained batch 1016 batch loss 1.73269129 epoch total loss 4.02849\n",
      "Trained batch 1017 batch loss 1.77198052 epoch total loss 4.02627134\n",
      "Trained batch 1018 batch loss 1.84674191 epoch total loss 4.02413082\n",
      "Trained batch 1019 batch loss 1.77748799 epoch total loss 4.02192593\n",
      "Trained batch 1020 batch loss 1.74097228 epoch total loss 4.01968956\n",
      "Trained batch 1021 batch loss 1.81518483 epoch total loss 4.01753044\n",
      "Trained batch 1022 batch loss 1.82778931 epoch total loss 4.01538754\n",
      "Trained batch 1023 batch loss 1.85861766 epoch total loss 4.01327896\n",
      "Trained batch 1024 batch loss 1.88950932 epoch total loss 4.0112052\n",
      "Trained batch 1025 batch loss 1.86462057 epoch total loss 4.00911093\n",
      "Trained batch 1026 batch loss 1.85185611 epoch total loss 4.00700855\n",
      "Trained batch 1027 batch loss 1.80521369 epoch total loss 4.00486469\n",
      "Trained batch 1028 batch loss 1.81043267 epoch total loss 4.00273037\n",
      "Trained batch 1029 batch loss 1.85493422 epoch total loss 4.00064278\n",
      "Trained batch 1030 batch loss 1.87153268 epoch total loss 3.99857593\n",
      "Trained batch 1031 batch loss 1.77475095 epoch total loss 3.99641919\n",
      "Trained batch 1032 batch loss 1.78755021 epoch total loss 3.99427867\n",
      "Trained batch 1033 batch loss 1.75257087 epoch total loss 3.99210858\n",
      "Trained batch 1034 batch loss 1.64973152 epoch total loss 3.98984337\n",
      "Trained batch 1035 batch loss 1.68954754 epoch total loss 3.98762083\n",
      "Trained batch 1036 batch loss 1.72995257 epoch total loss 3.98544168\n",
      "Trained batch 1037 batch loss 1.78735888 epoch total loss 3.98332214\n",
      "Trained batch 1038 batch loss 1.93007267 epoch total loss 3.98134422\n",
      "Trained batch 1039 batch loss 1.82200539 epoch total loss 3.97926569\n",
      "Trained batch 1040 batch loss 1.65478706 epoch total loss 3.97703052\n",
      "Trained batch 1041 batch loss 1.74566543 epoch total loss 3.97488713\n",
      "Trained batch 1042 batch loss 1.77620769 epoch total loss 3.97277713\n",
      "Trained batch 1043 batch loss 1.76365674 epoch total loss 3.97065902\n",
      "Trained batch 1044 batch loss 1.69708991 epoch total loss 3.96848154\n",
      "Trained batch 1045 batch loss 1.54638314 epoch total loss 3.96616364\n",
      "Trained batch 1046 batch loss 1.76324582 epoch total loss 3.96405768\n",
      "Trained batch 1047 batch loss 1.59739339 epoch total loss 3.961797\n",
      "Trained batch 1048 batch loss 1.70192575 epoch total loss 3.95964074\n",
      "Trained batch 1049 batch loss 1.69200253 epoch total loss 3.957479\n",
      "Trained batch 1050 batch loss 1.69810987 epoch total loss 3.95532727\n",
      "Trained batch 1051 batch loss 1.77011395 epoch total loss 3.95324802\n",
      "Trained batch 1052 batch loss 1.8131789 epoch total loss 3.9512136\n",
      "Trained batch 1053 batch loss 1.87377822 epoch total loss 3.94924045\n",
      "Trained batch 1054 batch loss 1.8909651 epoch total loss 3.9472878\n",
      "Trained batch 1055 batch loss 1.82872796 epoch total loss 3.9452796\n",
      "Trained batch 1056 batch loss 1.62565374 epoch total loss 3.94308281\n",
      "Trained batch 1057 batch loss 1.38103437 epoch total loss 3.94065881\n",
      "Trained batch 1058 batch loss 1.47760642 epoch total loss 3.93833065\n",
      "Trained batch 1059 batch loss 1.78652644 epoch total loss 3.93629885\n",
      "Trained batch 1060 batch loss 1.80814719 epoch total loss 3.93429112\n",
      "Trained batch 1061 batch loss 1.93417 epoch total loss 3.93240595\n",
      "Trained batch 1062 batch loss 1.93222439 epoch total loss 3.93052244\n",
      "Trained batch 1063 batch loss 1.84831738 epoch total loss 3.92856359\n",
      "Trained batch 1064 batch loss 1.84807479 epoch total loss 3.92660832\n",
      "Trained batch 1065 batch loss 1.82397246 epoch total loss 3.92463374\n",
      "Trained batch 1066 batch loss 1.84132099 epoch total loss 3.92267942\n",
      "Trained batch 1067 batch loss 1.83802938 epoch total loss 3.92072535\n",
      "Trained batch 1068 batch loss 1.89860809 epoch total loss 3.91883183\n",
      "Trained batch 1069 batch loss 1.78267336 epoch total loss 3.91683364\n",
      "Trained batch 1070 batch loss 1.70666623 epoch total loss 3.91476798\n",
      "Trained batch 1071 batch loss 1.57703173 epoch total loss 3.91258526\n",
      "Trained batch 1072 batch loss 1.39732206 epoch total loss 3.91023922\n",
      "Trained batch 1073 batch loss 1.43652987 epoch total loss 3.90793371\n",
      "Trained batch 1074 batch loss 1.78934348 epoch total loss 3.90596128\n",
      "Trained batch 1075 batch loss 1.73567235 epoch total loss 3.90394258\n",
      "Trained batch 1076 batch loss 1.87128079 epoch total loss 3.90205336\n",
      "Trained batch 1077 batch loss 1.7367363 epoch total loss 3.90004277\n",
      "Trained batch 1078 batch loss 1.75056386 epoch total loss 3.89804888\n",
      "Trained batch 1079 batch loss 1.74336469 epoch total loss 3.89605165\n",
      "Trained batch 1080 batch loss 1.62782359 epoch total loss 3.89395165\n",
      "Trained batch 1081 batch loss 1.65875351 epoch total loss 3.89188385\n",
      "Trained batch 1082 batch loss 1.67387676 epoch total loss 3.88983393\n",
      "Trained batch 1083 batch loss 1.71048594 epoch total loss 3.88782144\n",
      "Trained batch 1084 batch loss 1.64430952 epoch total loss 3.8857522\n",
      "Trained batch 1085 batch loss 1.79802799 epoch total loss 3.88382769\n",
      "Trained batch 1086 batch loss 1.68360686 epoch total loss 3.88180184\n",
      "Trained batch 1087 batch loss 1.82527673 epoch total loss 3.87990975\n",
      "Trained batch 1088 batch loss 1.81977534 epoch total loss 3.87801623\n",
      "Trained batch 1089 batch loss 1.83200073 epoch total loss 3.8761375\n",
      "Trained batch 1090 batch loss 1.79935896 epoch total loss 3.87423229\n",
      "Trained batch 1091 batch loss 1.73332381 epoch total loss 3.87226987\n",
      "Trained batch 1092 batch loss 1.73659253 epoch total loss 3.87031436\n",
      "Trained batch 1093 batch loss 1.72174263 epoch total loss 3.8683486\n",
      "Trained batch 1094 batch loss 1.69421411 epoch total loss 3.86636138\n",
      "Trained batch 1095 batch loss 1.75842965 epoch total loss 3.86443615\n",
      "Trained batch 1096 batch loss 1.77393842 epoch total loss 3.8625288\n",
      "Trained batch 1097 batch loss 1.78571033 epoch total loss 3.86063552\n",
      "Trained batch 1098 batch loss 1.83880186 epoch total loss 3.85879421\n",
      "Trained batch 1099 batch loss 1.84042978 epoch total loss 3.85695767\n",
      "Trained batch 1100 batch loss 1.73807347 epoch total loss 3.85503149\n",
      "Trained batch 1101 batch loss 1.72527361 epoch total loss 3.85309696\n",
      "Trained batch 1102 batch loss 1.83822763 epoch total loss 3.85126877\n",
      "Trained batch 1103 batch loss 1.78249288 epoch total loss 3.84939337\n",
      "Trained batch 1104 batch loss 1.72173178 epoch total loss 3.84746599\n",
      "Trained batch 1105 batch loss 1.68342829 epoch total loss 3.84550786\n",
      "Trained batch 1106 batch loss 1.73281741 epoch total loss 3.84359765\n",
      "Trained batch 1107 batch loss 1.64510453 epoch total loss 3.84161162\n",
      "Trained batch 1108 batch loss 1.78541017 epoch total loss 3.83975601\n",
      "Trained batch 1109 batch loss 1.76327515 epoch total loss 3.83788347\n",
      "Trained batch 1110 batch loss 1.72840893 epoch total loss 3.83598328\n",
      "Trained batch 1111 batch loss 1.82640553 epoch total loss 3.83417416\n",
      "Trained batch 1112 batch loss 1.85886574 epoch total loss 3.83239794\n",
      "Trained batch 1113 batch loss 1.74253583 epoch total loss 3.83052039\n",
      "Trained batch 1114 batch loss 1.66591847 epoch total loss 3.82857728\n",
      "Trained batch 1115 batch loss 1.8051002 epoch total loss 3.82676268\n",
      "Trained batch 1116 batch loss 1.66230321 epoch total loss 3.8248229\n",
      "Trained batch 1117 batch loss 1.84417725 epoch total loss 3.82304978\n",
      "Trained batch 1118 batch loss 1.85603857 epoch total loss 3.82129025\n",
      "Trained batch 1119 batch loss 1.83817172 epoch total loss 3.81951833\n",
      "Trained batch 1120 batch loss 1.84762192 epoch total loss 3.81775784\n",
      "Trained batch 1121 batch loss 1.84708047 epoch total loss 3.81599975\n",
      "Trained batch 1122 batch loss 1.77350175 epoch total loss 3.81417942\n",
      "Trained batch 1123 batch loss 1.85241783 epoch total loss 3.81243253\n",
      "Trained batch 1124 batch loss 1.76298261 epoch total loss 3.81060934\n",
      "Trained batch 1125 batch loss 1.78188515 epoch total loss 3.80880594\n",
      "Trained batch 1126 batch loss 1.79494143 epoch total loss 3.80701756\n",
      "Trained batch 1127 batch loss 1.76512039 epoch total loss 3.80520558\n",
      "Trained batch 1128 batch loss 1.85730553 epoch total loss 3.80347896\n",
      "Trained batch 1129 batch loss 1.87043738 epoch total loss 3.80176687\n",
      "Trained batch 1130 batch loss 1.83973408 epoch total loss 3.80003071\n",
      "Trained batch 1131 batch loss 1.79410422 epoch total loss 3.79825687\n",
      "Trained batch 1132 batch loss 1.87479687 epoch total loss 3.7965579\n",
      "Trained batch 1133 batch loss 1.8699162 epoch total loss 3.79485774\n",
      "Trained batch 1134 batch loss 1.74745369 epoch total loss 3.7930522\n",
      "Trained batch 1135 batch loss 1.70098853 epoch total loss 3.79120922\n",
      "Trained batch 1136 batch loss 1.66059101 epoch total loss 3.78933382\n",
      "Trained batch 1137 batch loss 1.5758059 epoch total loss 3.78738689\n",
      "Trained batch 1138 batch loss 1.53362167 epoch total loss 3.78540635\n",
      "Trained batch 1139 batch loss 1.64773357 epoch total loss 3.78352976\n",
      "Trained batch 1140 batch loss 1.51831818 epoch total loss 3.78154302\n",
      "Trained batch 1141 batch loss 1.41691089 epoch total loss 3.77947068\n",
      "Trained batch 1142 batch loss 1.45623147 epoch total loss 3.77743602\n",
      "Trained batch 1143 batch loss 1.54840469 epoch total loss 3.77548599\n",
      "Trained batch 1144 batch loss 1.544052 epoch total loss 3.77353525\n",
      "Trained batch 1145 batch loss 1.83991456 epoch total loss 3.77184653\n",
      "Trained batch 1146 batch loss 1.83132708 epoch total loss 3.77015328\n",
      "Trained batch 1147 batch loss 1.86805475 epoch total loss 3.76849508\n",
      "Trained batch 1148 batch loss 1.87632847 epoch total loss 3.7668469\n",
      "Trained batch 1149 batch loss 1.87127674 epoch total loss 3.76519704\n",
      "Trained batch 1150 batch loss 1.79742622 epoch total loss 3.76348591\n",
      "Trained batch 1151 batch loss 1.69498682 epoch total loss 3.76168871\n",
      "Trained batch 1152 batch loss 1.69387221 epoch total loss 3.75989366\n",
      "Trained batch 1153 batch loss 1.58084822 epoch total loss 3.75800395\n",
      "Trained batch 1154 batch loss 1.65342367 epoch total loss 3.75618\n",
      "Trained batch 1155 batch loss 1.75120556 epoch total loss 3.75444388\n",
      "Trained batch 1156 batch loss 1.8564229 epoch total loss 3.75280213\n",
      "Trained batch 1157 batch loss 1.89043868 epoch total loss 3.75119257\n",
      "Trained batch 1158 batch loss 1.86452258 epoch total loss 3.74956369\n",
      "Trained batch 1159 batch loss 1.90265036 epoch total loss 3.74797\n",
      "Trained batch 1160 batch loss 1.90803111 epoch total loss 3.74638414\n",
      "Trained batch 1161 batch loss 1.7884165 epoch total loss 3.74469781\n",
      "Trained batch 1162 batch loss 1.83307445 epoch total loss 3.74305272\n",
      "Trained batch 1163 batch loss 1.76800609 epoch total loss 3.74135447\n",
      "Trained batch 1164 batch loss 1.82896972 epoch total loss 3.73971176\n",
      "Trained batch 1165 batch loss 1.80714679 epoch total loss 3.73805285\n",
      "Trained batch 1166 batch loss 1.87238824 epoch total loss 3.73645282\n",
      "Trained batch 1167 batch loss 1.81394291 epoch total loss 3.73480558\n",
      "Trained batch 1168 batch loss 1.84338212 epoch total loss 3.73318601\n",
      "Trained batch 1169 batch loss 1.79314053 epoch total loss 3.73152637\n",
      "Trained batch 1170 batch loss 1.78834319 epoch total loss 3.72986579\n",
      "Trained batch 1171 batch loss 1.86305022 epoch total loss 3.72827172\n",
      "Trained batch 1172 batch loss 1.79894555 epoch total loss 3.72662544\n",
      "Trained batch 1173 batch loss 1.80403376 epoch total loss 3.72498655\n",
      "Trained batch 1174 batch loss 1.79508448 epoch total loss 3.72334242\n",
      "Trained batch 1175 batch loss 1.77017546 epoch total loss 3.72168016\n",
      "Trained batch 1176 batch loss 1.81581771 epoch total loss 3.72005963\n",
      "Trained batch 1177 batch loss 1.79523599 epoch total loss 3.71842432\n",
      "Trained batch 1178 batch loss 1.7759335 epoch total loss 3.71677542\n",
      "Trained batch 1179 batch loss 1.81439841 epoch total loss 3.7151618\n",
      "Trained batch 1180 batch loss 1.81101501 epoch total loss 3.71354818\n",
      "Trained batch 1181 batch loss 1.84765911 epoch total loss 3.71196818\n",
      "Trained batch 1182 batch loss 1.84433198 epoch total loss 3.71038818\n",
      "Trained batch 1183 batch loss 1.87260807 epoch total loss 3.70883465\n",
      "Trained batch 1184 batch loss 1.88203931 epoch total loss 3.70729136\n",
      "Trained batch 1185 batch loss 1.86426604 epoch total loss 3.70573616\n",
      "Trained batch 1186 batch loss 1.86410475 epoch total loss 3.70418358\n",
      "Trained batch 1187 batch loss 1.83624911 epoch total loss 3.70261\n",
      "Trained batch 1188 batch loss 1.82241535 epoch total loss 3.70102715\n",
      "Trained batch 1189 batch loss 1.77903378 epoch total loss 3.69941044\n",
      "Trained batch 1190 batch loss 1.77567351 epoch total loss 3.6977942\n",
      "Trained batch 1191 batch loss 1.82935381 epoch total loss 3.6962254\n",
      "Trained batch 1192 batch loss 1.77931046 epoch total loss 3.69461727\n",
      "Trained batch 1193 batch loss 1.77210021 epoch total loss 3.6930058\n",
      "Trained batch 1194 batch loss 1.76531243 epoch total loss 3.69139123\n",
      "Trained batch 1195 batch loss 1.75316036 epoch total loss 3.68976903\n",
      "Trained batch 1196 batch loss 1.74425209 epoch total loss 3.6881423\n",
      "Trained batch 1197 batch loss 1.69030237 epoch total loss 3.68647337\n",
      "Trained batch 1198 batch loss 1.83456945 epoch total loss 3.68492746\n",
      "Trained batch 1199 batch loss 1.77362442 epoch total loss 3.68333316\n",
      "Trained batch 1200 batch loss 1.82734525 epoch total loss 3.6817863\n",
      "Trained batch 1201 batch loss 1.83964682 epoch total loss 3.68025255\n",
      "Trained batch 1202 batch loss 1.84549892 epoch total loss 3.67872643\n",
      "Trained batch 1203 batch loss 1.72973061 epoch total loss 3.67710614\n",
      "Trained batch 1204 batch loss 1.72593796 epoch total loss 3.67548561\n",
      "Trained batch 1205 batch loss 1.80753255 epoch total loss 3.67393541\n",
      "Trained batch 1206 batch loss 1.74726379 epoch total loss 3.67233777\n",
      "Trained batch 1207 batch loss 1.79983604 epoch total loss 3.67078638\n",
      "Trained batch 1208 batch loss 1.82784605 epoch total loss 3.6692605\n",
      "Trained batch 1209 batch loss 1.80519724 epoch total loss 3.66771865\n",
      "Trained batch 1210 batch loss 1.89023268 epoch total loss 3.66624975\n",
      "Trained batch 1211 batch loss 1.88094079 epoch total loss 3.66477537\n",
      "Trained batch 1212 batch loss 1.7278527 epoch total loss 3.66317749\n",
      "Trained batch 1213 batch loss 1.74120855 epoch total loss 3.66159296\n",
      "Trained batch 1214 batch loss 1.75317597 epoch total loss 3.66002107\n",
      "Trained batch 1215 batch loss 1.74384832 epoch total loss 3.65844393\n",
      "Trained batch 1216 batch loss 1.72728872 epoch total loss 3.65685558\n",
      "Trained batch 1217 batch loss 1.76831377 epoch total loss 3.65530396\n",
      "Trained batch 1218 batch loss 1.74055302 epoch total loss 3.65373206\n",
      "Trained batch 1219 batch loss 1.8319298 epoch total loss 3.65223765\n",
      "Trained batch 1220 batch loss 1.84366941 epoch total loss 3.65075517\n",
      "Trained batch 1221 batch loss 1.80488586 epoch total loss 3.64924335\n",
      "Trained batch 1222 batch loss 1.81042278 epoch total loss 3.6477387\n",
      "Trained batch 1223 batch loss 1.8066653 epoch total loss 3.64623332\n",
      "Trained batch 1224 batch loss 1.7417028 epoch total loss 3.64467716\n",
      "Trained batch 1225 batch loss 1.79991448 epoch total loss 3.64317131\n",
      "Trained batch 1226 batch loss 1.72873712 epoch total loss 3.64160943\n",
      "Trained batch 1227 batch loss 1.55921757 epoch total loss 3.63991237\n",
      "Trained batch 1228 batch loss 1.62594068 epoch total loss 3.63827229\n",
      "Trained batch 1229 batch loss 1.55960751 epoch total loss 3.63658094\n",
      "Trained batch 1230 batch loss 1.6761167 epoch total loss 3.63498712\n",
      "Trained batch 1231 batch loss 1.74734807 epoch total loss 3.63345385\n",
      "Trained batch 1232 batch loss 1.83342981 epoch total loss 3.63199282\n",
      "Trained batch 1233 batch loss 1.80266249 epoch total loss 3.63050938\n",
      "Trained batch 1234 batch loss 1.60874295 epoch total loss 3.62887096\n",
      "Trained batch 1235 batch loss 1.63842165 epoch total loss 3.62725925\n",
      "Trained batch 1236 batch loss 1.7211386 epoch total loss 3.62571692\n",
      "Trained batch 1237 batch loss 1.72586727 epoch total loss 3.62418127\n",
      "Trained batch 1238 batch loss 1.77441669 epoch total loss 3.6226871\n",
      "Trained batch 1239 batch loss 1.80668294 epoch total loss 3.62122154\n",
      "Trained batch 1240 batch loss 1.79922009 epoch total loss 3.61975217\n",
      "Trained batch 1241 batch loss 1.82548797 epoch total loss 3.6183064\n",
      "Trained batch 1242 batch loss 1.84098363 epoch total loss 3.61687541\n",
      "Trained batch 1243 batch loss 1.82579327 epoch total loss 3.61543441\n",
      "Trained batch 1244 batch loss 1.71671343 epoch total loss 3.61390805\n",
      "Trained batch 1245 batch loss 1.77772856 epoch total loss 3.61243343\n",
      "Trained batch 1246 batch loss 1.76012623 epoch total loss 3.61094689\n",
      "Trained batch 1247 batch loss 1.79091346 epoch total loss 3.6094873\n",
      "Trained batch 1248 batch loss 1.82510066 epoch total loss 3.60805774\n",
      "Trained batch 1249 batch loss 1.85079479 epoch total loss 3.60665059\n",
      "Trained batch 1250 batch loss 1.83451533 epoch total loss 3.60523272\n",
      "Trained batch 1251 batch loss 1.88184 epoch total loss 3.60385513\n",
      "Trained batch 1252 batch loss 1.86846292 epoch total loss 3.60246921\n",
      "Trained batch 1253 batch loss 1.87694287 epoch total loss 3.6010921\n",
      "Trained batch 1254 batch loss 1.90363646 epoch total loss 3.5997386\n",
      "Trained batch 1255 batch loss 1.8703928 epoch total loss 3.59836078\n",
      "Trained batch 1256 batch loss 1.85709047 epoch total loss 3.59697437\n",
      "Trained batch 1257 batch loss 1.79074931 epoch total loss 3.59553719\n",
      "Trained batch 1258 batch loss 1.81038642 epoch total loss 3.59411836\n",
      "Trained batch 1259 batch loss 1.78425837 epoch total loss 3.59268069\n",
      "Trained batch 1260 batch loss 1.78858185 epoch total loss 3.59124899\n",
      "Trained batch 1261 batch loss 1.77189314 epoch total loss 3.58980608\n",
      "Trained batch 1262 batch loss 1.77381539 epoch total loss 3.58836722\n",
      "Trained batch 1263 batch loss 1.71475565 epoch total loss 3.58688402\n",
      "Trained batch 1264 batch loss 1.63438308 epoch total loss 3.58533907\n",
      "Trained batch 1265 batch loss 1.71005273 epoch total loss 3.58385658\n",
      "Trained batch 1266 batch loss 1.74828529 epoch total loss 3.58240652\n",
      "Trained batch 1267 batch loss 1.74780822 epoch total loss 3.58095884\n",
      "Trained batch 1268 batch loss 1.76455176 epoch total loss 3.57952642\n",
      "Trained batch 1269 batch loss 1.78918052 epoch total loss 3.57811546\n",
      "Trained batch 1270 batch loss 1.76749945 epoch total loss 3.57668972\n",
      "Trained batch 1271 batch loss 1.77717423 epoch total loss 3.57527399\n",
      "Trained batch 1272 batch loss 1.81121564 epoch total loss 3.57388711\n",
      "Trained batch 1273 batch loss 1.70477724 epoch total loss 3.57241869\n",
      "Trained batch 1274 batch loss 1.5735749 epoch total loss 3.57085\n",
      "Trained batch 1275 batch loss 1.70804548 epoch total loss 3.56938887\n",
      "Trained batch 1276 batch loss 1.67990136 epoch total loss 3.56790781\n",
      "Trained batch 1277 batch loss 1.80922437 epoch total loss 3.56653047\n",
      "Trained batch 1278 batch loss 1.80873609 epoch total loss 3.56515503\n",
      "Trained batch 1279 batch loss 1.81731832 epoch total loss 3.56378841\n",
      "Trained batch 1280 batch loss 1.73429108 epoch total loss 3.56235933\n",
      "Trained batch 1281 batch loss 1.70045114 epoch total loss 3.56090593\n",
      "Trained batch 1282 batch loss 1.7497648 epoch total loss 3.5594933\n",
      "Trained batch 1283 batch loss 1.72009242 epoch total loss 3.55805969\n",
      "Trained batch 1284 batch loss 1.68478286 epoch total loss 3.55660081\n",
      "Trained batch 1285 batch loss 1.69971275 epoch total loss 3.55515575\n",
      "Trained batch 1286 batch loss 1.79486609 epoch total loss 3.55378699\n",
      "Trained batch 1287 batch loss 1.80459595 epoch total loss 3.55242777\n",
      "Trained batch 1288 batch loss 1.6143775 epoch total loss 3.55092311\n",
      "Trained batch 1289 batch loss 1.81012058 epoch total loss 3.54957247\n",
      "Trained batch 1290 batch loss 1.72950649 epoch total loss 3.54816151\n",
      "Trained batch 1291 batch loss 1.6582489 epoch total loss 3.54669762\n",
      "Trained batch 1292 batch loss 1.72940958 epoch total loss 3.54529119\n",
      "Trained batch 1293 batch loss 1.65555704 epoch total loss 3.54382968\n",
      "Trained batch 1294 batch loss 1.64004159 epoch total loss 3.54235864\n",
      "Trained batch 1295 batch loss 1.81222439 epoch total loss 3.5410223\n",
      "Trained batch 1296 batch loss 1.63857293 epoch total loss 3.5395546\n",
      "Trained batch 1297 batch loss 1.77452064 epoch total loss 3.5381937\n",
      "Trained batch 1298 batch loss 1.7512064 epoch total loss 3.5368166\n",
      "Trained batch 1299 batch loss 1.68823564 epoch total loss 3.53539371\n",
      "Trained batch 1300 batch loss 1.69045603 epoch total loss 3.53397465\n",
      "Trained batch 1301 batch loss 1.54860842 epoch total loss 3.53244877\n",
      "Trained batch 1302 batch loss 1.7091918 epoch total loss 3.5310483\n",
      "Trained batch 1303 batch loss 1.68062615 epoch total loss 3.52962804\n",
      "Trained batch 1304 batch loss 1.63557267 epoch total loss 3.52817583\n",
      "Trained batch 1305 batch loss 1.73102665 epoch total loss 3.52679849\n",
      "Trained batch 1306 batch loss 1.59570742 epoch total loss 3.52532\n",
      "Trained batch 1307 batch loss 1.6265713 epoch total loss 3.52386713\n",
      "Trained batch 1308 batch loss 1.74771345 epoch total loss 3.5225091\n",
      "Trained batch 1309 batch loss 1.71209049 epoch total loss 3.52112579\n",
      "Trained batch 1310 batch loss 1.76613569 epoch total loss 3.51978612\n",
      "Trained batch 1311 batch loss 1.7855221 epoch total loss 3.51846337\n",
      "Trained batch 1312 batch loss 1.78481841 epoch total loss 3.51714206\n",
      "Trained batch 1313 batch loss 1.73851311 epoch total loss 3.51578712\n",
      "Trained batch 1314 batch loss 1.70166039 epoch total loss 3.51440644\n",
      "Trained batch 1315 batch loss 1.7075367 epoch total loss 3.51303244\n",
      "Trained batch 1316 batch loss 1.80463314 epoch total loss 3.51173425\n",
      "Trained batch 1317 batch loss 1.71457231 epoch total loss 3.51036954\n",
      "Trained batch 1318 batch loss 1.78911519 epoch total loss 3.50906348\n",
      "Trained batch 1319 batch loss 1.80839181 epoch total loss 3.50777435\n",
      "Trained batch 1320 batch loss 1.7632215 epoch total loss 3.5064528\n",
      "Trained batch 1321 batch loss 1.549348 epoch total loss 3.50497127\n",
      "Trained batch 1322 batch loss 1.67340279 epoch total loss 3.50358558\n",
      "Trained batch 1323 batch loss 1.80362475 epoch total loss 3.50230074\n",
      "Trained batch 1324 batch loss 1.83542633 epoch total loss 3.50104189\n",
      "Trained batch 1325 batch loss 1.87374735 epoch total loss 3.49981356\n",
      "Trained batch 1326 batch loss 1.83943462 epoch total loss 3.49856138\n",
      "Trained batch 1327 batch loss 1.6701591 epoch total loss 3.49718332\n",
      "Trained batch 1328 batch loss 1.7294178 epoch total loss 3.49585223\n",
      "Trained batch 1329 batch loss 1.74179864 epoch total loss 3.49453235\n",
      "Trained batch 1330 batch loss 1.72401643 epoch total loss 3.49320126\n",
      "Trained batch 1331 batch loss 1.82095146 epoch total loss 3.49194455\n",
      "Trained batch 1332 batch loss 1.75147128 epoch total loss 3.49063802\n",
      "Trained batch 1333 batch loss 1.76336884 epoch total loss 3.48934197\n",
      "Trained batch 1334 batch loss 1.66519952 epoch total loss 3.48797441\n",
      "Trained batch 1335 batch loss 1.74252605 epoch total loss 3.48666716\n",
      "Trained batch 1336 batch loss 1.87320316 epoch total loss 3.48545933\n",
      "Trained batch 1337 batch loss 1.84659326 epoch total loss 3.48423362\n",
      "Trained batch 1338 batch loss 1.83632135 epoch total loss 3.48300219\n",
      "Trained batch 1339 batch loss 1.84832788 epoch total loss 3.48178124\n",
      "Trained batch 1340 batch loss 1.83835876 epoch total loss 3.48055482\n",
      "Trained batch 1341 batch loss 1.7709409 epoch total loss 3.47928\n",
      "Trained batch 1342 batch loss 1.82980049 epoch total loss 3.47805071\n",
      "Trained batch 1343 batch loss 1.82379389 epoch total loss 3.4768188\n",
      "Trained batch 1344 batch loss 1.85729706 epoch total loss 3.47561383\n",
      "Trained batch 1345 batch loss 1.83767879 epoch total loss 3.47439623\n",
      "Trained batch 1346 batch loss 1.82416642 epoch total loss 3.47317028\n",
      "Trained batch 1347 batch loss 1.74962449 epoch total loss 3.47189069\n",
      "Trained batch 1348 batch loss 1.75934589 epoch total loss 3.47062016\n",
      "Trained batch 1349 batch loss 1.82751131 epoch total loss 3.46940231\n",
      "Trained batch 1350 batch loss 1.82714307 epoch total loss 3.46818566\n",
      "Trained batch 1351 batch loss 1.8324585 epoch total loss 3.46697497\n",
      "Trained batch 1352 batch loss 1.811903 epoch total loss 3.46575093\n",
      "Trained batch 1353 batch loss 1.71435463 epoch total loss 3.46445656\n",
      "Trained batch 1354 batch loss 1.51752126 epoch total loss 3.46301866\n",
      "Trained batch 1355 batch loss 1.58170164 epoch total loss 3.46163\n",
      "Trained batch 1356 batch loss 1.56771123 epoch total loss 3.46023345\n",
      "Trained batch 1357 batch loss 1.60711122 epoch total loss 3.45886779\n",
      "Trained batch 1358 batch loss 1.51526868 epoch total loss 3.45743656\n",
      "Trained batch 1359 batch loss 1.4179666 epoch total loss 3.45593572\n",
      "Trained batch 1360 batch loss 1.43428981 epoch total loss 3.45444918\n",
      "Trained batch 1361 batch loss 1.55240333 epoch total loss 3.45305157\n",
      "Trained batch 1362 batch loss 1.78397477 epoch total loss 3.4518261\n",
      "Trained batch 1363 batch loss 1.84600616 epoch total loss 3.45064807\n",
      "Trained batch 1364 batch loss 1.76905346 epoch total loss 3.44941521\n",
      "Trained batch 1365 batch loss 1.71430266 epoch total loss 3.4481442\n",
      "Trained batch 1366 batch loss 1.78806388 epoch total loss 3.44692898\n",
      "Trained batch 1367 batch loss 1.82818341 epoch total loss 3.44574475\n",
      "Trained batch 1368 batch loss 1.85246062 epoch total loss 3.44458\n",
      "Trained batch 1369 batch loss 1.80188346 epoch total loss 3.44338\n",
      "Trained batch 1370 batch loss 1.71814132 epoch total loss 3.44212079\n",
      "Trained batch 1371 batch loss 1.84312665 epoch total loss 3.44095469\n",
      "Trained batch 1372 batch loss 1.79500866 epoch total loss 3.43975496\n",
      "Trained batch 1373 batch loss 1.71883738 epoch total loss 3.43850136\n",
      "Trained batch 1374 batch loss 1.68334317 epoch total loss 3.43722391\n",
      "Trained batch 1375 batch loss 1.58776891 epoch total loss 3.43587899\n",
      "Trained batch 1376 batch loss 1.68119657 epoch total loss 3.43460369\n",
      "Trained batch 1377 batch loss 1.74227643 epoch total loss 3.43337464\n",
      "Trained batch 1378 batch loss 1.76338172 epoch total loss 3.43216252\n",
      "Trained batch 1379 batch loss 1.78301072 epoch total loss 3.43096685\n",
      "Trained batch 1380 batch loss 1.76600218 epoch total loss 3.42976046\n",
      "Trained batch 1381 batch loss 1.82939219 epoch total loss 3.42860174\n",
      "Trained batch 1382 batch loss 1.81011128 epoch total loss 3.42743063\n",
      "Trained batch 1383 batch loss 1.72786283 epoch total loss 3.42620182\n",
      "Trained batch 1384 batch loss 1.74500465 epoch total loss 3.42498708\n",
      "Trained batch 1385 batch loss 1.76552546 epoch total loss 3.42378902\n",
      "Trained batch 1386 batch loss 1.72428751 epoch total loss 3.4225626\n",
      "Trained batch 1387 batch loss 1.80461311 epoch total loss 3.42139626\n",
      "Trained batch 1388 batch loss 1.82938516 epoch total loss 3.42024946\n",
      "Epoch 1 train loss 3.4202494621276855\n",
      "Validated batch 1 batch loss 1.91104901\n",
      "Validated batch 2 batch loss 1.82485175\n",
      "Validated batch 3 batch loss 1.83254707\n",
      "Validated batch 4 batch loss 1.8193742\n",
      "Validated batch 5 batch loss 1.8588891\n",
      "Validated batch 6 batch loss 1.84828842\n",
      "Validated batch 7 batch loss 1.83120823\n",
      "Validated batch 8 batch loss 1.69637048\n",
      "Validated batch 9 batch loss 1.81805611\n",
      "Validated batch 10 batch loss 1.77374554\n",
      "Validated batch 11 batch loss 1.81491721\n",
      "Validated batch 12 batch loss 1.72347176\n",
      "Validated batch 13 batch loss 1.7561419\n",
      "Validated batch 14 batch loss 1.72211015\n",
      "Validated batch 15 batch loss 1.78596115\n",
      "Validated batch 16 batch loss 1.86715126\n",
      "Validated batch 17 batch loss 1.83370638\n",
      "Validated batch 18 batch loss 1.87591136\n",
      "Validated batch 19 batch loss 1.93669713\n",
      "Validated batch 20 batch loss 1.93591189\n",
      "Validated batch 21 batch loss 1.88399\n",
      "Validated batch 22 batch loss 1.75620353\n",
      "Validated batch 23 batch loss 1.72505534\n",
      "Validated batch 24 batch loss 1.81839943\n",
      "Validated batch 25 batch loss 1.81296611\n",
      "Validated batch 26 batch loss 1.76890635\n",
      "Validated batch 27 batch loss 1.74021983\n",
      "Validated batch 28 batch loss 1.69109952\n",
      "Validated batch 29 batch loss 1.81547749\n",
      "Validated batch 30 batch loss 1.7641536\n",
      "Validated batch 31 batch loss 1.83535361\n",
      "Validated batch 32 batch loss 1.87977886\n",
      "Validated batch 33 batch loss 1.79184937\n",
      "Validated batch 34 batch loss 1.79313993\n",
      "Validated batch 35 batch loss 1.68285203\n",
      "Validated batch 36 batch loss 1.82610035\n",
      "Validated batch 37 batch loss 1.75220013\n",
      "Validated batch 38 batch loss 1.85231447\n",
      "Validated batch 39 batch loss 1.80251575\n",
      "Validated batch 40 batch loss 1.77087379\n",
      "Validated batch 41 batch loss 1.63619661\n",
      "Validated batch 42 batch loss 1.72140348\n",
      "Validated batch 43 batch loss 1.85765362\n",
      "Validated batch 44 batch loss 1.72247076\n",
      "Validated batch 45 batch loss 1.74409127\n",
      "Validated batch 46 batch loss 1.76086569\n",
      "Validated batch 47 batch loss 1.78528035\n",
      "Validated batch 48 batch loss 1.78777218\n",
      "Validated batch 49 batch loss 1.66855192\n",
      "Validated batch 50 batch loss 1.66469693\n",
      "Validated batch 51 batch loss 1.72139049\n",
      "Validated batch 52 batch loss 1.82044315\n",
      "Validated batch 53 batch loss 1.75989556\n",
      "Validated batch 54 batch loss 1.82271838\n",
      "Validated batch 55 batch loss 1.88164628\n",
      "Validated batch 56 batch loss 1.84654689\n",
      "Validated batch 57 batch loss 1.86039746\n",
      "Validated batch 58 batch loss 1.70758545\n",
      "Validated batch 59 batch loss 1.80946398\n",
      "Validated batch 60 batch loss 1.79244268\n",
      "Validated batch 61 batch loss 1.83602035\n",
      "Validated batch 62 batch loss 1.87955046\n",
      "Validated batch 63 batch loss 1.76961029\n",
      "Validated batch 64 batch loss 1.85414577\n",
      "Validated batch 65 batch loss 1.83909273\n",
      "Validated batch 66 batch loss 1.83013082\n",
      "Validated batch 67 batch loss 1.80864418\n",
      "Validated batch 68 batch loss 1.86093378\n",
      "Validated batch 69 batch loss 1.86759818\n",
      "Validated batch 70 batch loss 1.77358878\n",
      "Validated batch 71 batch loss 1.80129039\n",
      "Validated batch 72 batch loss 1.75375032\n",
      "Validated batch 73 batch loss 1.8717463\n",
      "Validated batch 74 batch loss 1.91006148\n",
      "Validated batch 75 batch loss 1.89153314\n",
      "Validated batch 76 batch loss 1.82898009\n",
      "Validated batch 77 batch loss 1.79755676\n",
      "Validated batch 78 batch loss 1.84694135\n",
      "Validated batch 79 batch loss 1.88489449\n",
      "Validated batch 80 batch loss 1.90763092\n",
      "Validated batch 81 batch loss 1.82757068\n",
      "Validated batch 82 batch loss 1.72730982\n",
      "Validated batch 83 batch loss 1.84962237\n",
      "Validated batch 84 batch loss 1.77670956\n",
      "Validated batch 85 batch loss 1.79499829\n",
      "Validated batch 86 batch loss 1.83355546\n",
      "Validated batch 87 batch loss 1.63198686\n",
      "Validated batch 88 batch loss 1.71655798\n",
      "Validated batch 89 batch loss 1.89400578\n",
      "Validated batch 90 batch loss 1.76382113\n",
      "Validated batch 91 batch loss 1.89558387\n",
      "Validated batch 92 batch loss 1.81671321\n",
      "Validated batch 93 batch loss 1.84650612\n",
      "Validated batch 94 batch loss 1.90475309\n",
      "Validated batch 95 batch loss 1.78737271\n",
      "Validated batch 96 batch loss 1.648628\n",
      "Validated batch 97 batch loss 1.80474949\n",
      "Validated batch 98 batch loss 1.74064052\n",
      "Validated batch 99 batch loss 1.65510058\n",
      "Validated batch 100 batch loss 1.78939509\n",
      "Validated batch 101 batch loss 1.6524272\n",
      "Validated batch 102 batch loss 1.8526262\n",
      "Validated batch 103 batch loss 1.73422587\n",
      "Validated batch 104 batch loss 1.7326231\n",
      "Validated batch 105 batch loss 1.6612258\n",
      "Validated batch 106 batch loss 1.78905761\n",
      "Validated batch 107 batch loss 1.74290574\n",
      "Validated batch 108 batch loss 1.7779305\n",
      "Validated batch 109 batch loss 1.77060986\n",
      "Validated batch 110 batch loss 1.83762908\n",
      "Validated batch 111 batch loss 1.77833807\n",
      "Validated batch 112 batch loss 1.74650812\n",
      "Validated batch 113 batch loss 1.80772805\n",
      "Validated batch 114 batch loss 1.5879029\n",
      "Validated batch 115 batch loss 1.85850012\n",
      "Validated batch 116 batch loss 1.73118925\n",
      "Validated batch 117 batch loss 1.85057032\n",
      "Validated batch 118 batch loss 1.79910564\n",
      "Validated batch 119 batch loss 1.86147881\n",
      "Validated batch 120 batch loss 1.80292106\n",
      "Validated batch 121 batch loss 1.86819339\n",
      "Validated batch 122 batch loss 1.75650454\n",
      "Validated batch 123 batch loss 1.89678049\n",
      "Validated batch 124 batch loss 1.7740804\n",
      "Validated batch 125 batch loss 1.7459662\n",
      "Validated batch 126 batch loss 1.79683495\n",
      "Validated batch 127 batch loss 1.73985696\n",
      "Validated batch 128 batch loss 1.57902491\n",
      "Validated batch 129 batch loss 1.86178923\n",
      "Validated batch 130 batch loss 1.86976695\n",
      "Validated batch 131 batch loss 1.79013181\n",
      "Validated batch 132 batch loss 1.80302715\n",
      "Validated batch 133 batch loss 1.85723031\n",
      "Validated batch 134 batch loss 1.88292825\n",
      "Validated batch 135 batch loss 1.87441278\n",
      "Validated batch 136 batch loss 1.88541651\n",
      "Validated batch 137 batch loss 1.85472405\n",
      "Validated batch 138 batch loss 1.66688514\n",
      "Validated batch 139 batch loss 1.69561148\n",
      "Validated batch 140 batch loss 1.74972296\n",
      "Validated batch 141 batch loss 1.80384648\n",
      "Validated batch 142 batch loss 1.86796832\n",
      "Validated batch 143 batch loss 1.70529354\n",
      "Validated batch 144 batch loss 1.82714474\n",
      "Validated batch 145 batch loss 1.81368923\n",
      "Validated batch 146 batch loss 1.81806922\n",
      "Validated batch 147 batch loss 1.79562831\n",
      "Validated batch 148 batch loss 1.82591712\n",
      "Validated batch 149 batch loss 1.79546547\n",
      "Validated batch 150 batch loss 1.75551069\n",
      "Validated batch 151 batch loss 1.79456627\n",
      "Validated batch 152 batch loss 1.79549861\n",
      "Validated batch 153 batch loss 1.86253715\n",
      "Validated batch 154 batch loss 1.82382131\n",
      "Validated batch 155 batch loss 1.75288939\n",
      "Validated batch 156 batch loss 1.69287753\n",
      "Validated batch 157 batch loss 1.84220672\n",
      "Validated batch 158 batch loss 1.89039326\n",
      "Validated batch 159 batch loss 1.82268512\n",
      "Validated batch 160 batch loss 1.87349176\n",
      "Validated batch 161 batch loss 1.78493881\n",
      "Validated batch 162 batch loss 1.83801699\n",
      "Validated batch 163 batch loss 1.77081549\n",
      "Validated batch 164 batch loss 1.72862232\n",
      "Validated batch 165 batch loss 1.71415281\n",
      "Validated batch 166 batch loss 1.75701618\n",
      "Validated batch 167 batch loss 1.82160819\n",
      "Validated batch 168 batch loss 1.7517885\n",
      "Validated batch 169 batch loss 1.85824883\n",
      "Validated batch 170 batch loss 1.81522512\n",
      "Validated batch 171 batch loss 1.81255031\n",
      "Validated batch 172 batch loss 1.88460934\n",
      "Validated batch 173 batch loss 1.88578498\n",
      "Validated batch 174 batch loss 1.73370874\n",
      "Validated batch 175 batch loss 1.78248978\n",
      "Validated batch 176 batch loss 1.88236475\n",
      "Validated batch 177 batch loss 1.75501752\n",
      "Validated batch 178 batch loss 1.82685339\n",
      "Validated batch 179 batch loss 1.72842205\n",
      "Validated batch 180 batch loss 1.87347555\n",
      "Validated batch 181 batch loss 1.70836139\n",
      "Validated batch 182 batch loss 1.79780531\n",
      "Validated batch 183 batch loss 1.78104687\n",
      "Validated batch 184 batch loss 1.81034875\n",
      "Validated batch 185 batch loss 1.89687991\n",
      "Epoch 1 val loss 1.7978627681732178\n",
      "Model ./model_hourglass-epoch-1-loss-1.7979.h5 saved.\n",
      "Start epoch 2 with learning rate 0.5\n",
      "Start distributed traininng...\n",
      "Trained batch 1 batch loss 1.78345418 epoch total loss 1.78345418\n",
      "Trained batch 2 batch loss 1.72684479 epoch total loss 1.75514948\n",
      "Trained batch 3 batch loss 1.80578458 epoch total loss 1.77202785\n",
      "Trained batch 4 batch loss 1.91384757 epoch total loss 1.80748272\n",
      "Trained batch 5 batch loss 1.87203455 epoch total loss 1.82039297\n",
      "Trained batch 6 batch loss 1.87581015 epoch total loss 1.8296293\n",
      "Trained batch 7 batch loss 1.77381361 epoch total loss 1.82165551\n",
      "Trained batch 8 batch loss 1.85091555 epoch total loss 1.82531309\n",
      "Trained batch 9 batch loss 1.59954262 epoch total loss 1.80022752\n",
      "Trained batch 10 batch loss 1.6950146 epoch total loss 1.78970623\n",
      "Trained batch 11 batch loss 1.84619284 epoch total loss 1.79484141\n",
      "Trained batch 12 batch loss 1.80811799 epoch total loss 1.79594791\n",
      "Trained batch 13 batch loss 1.65364 epoch total loss 1.78500116\n",
      "Trained batch 14 batch loss 1.69700515 epoch total loss 1.77871573\n",
      "Trained batch 15 batch loss 1.68732202 epoch total loss 1.77262282\n",
      "Trained batch 16 batch loss 1.7579422 epoch total loss 1.77170527\n",
      "Trained batch 17 batch loss 1.72709954 epoch total loss 1.76908147\n",
      "Trained batch 18 batch loss 1.6833539 epoch total loss 1.76431882\n",
      "Trained batch 19 batch loss 1.62714958 epoch total loss 1.75709939\n",
      "Trained batch 20 batch loss 1.74592197 epoch total loss 1.75654054\n",
      "Trained batch 21 batch loss 1.72172523 epoch total loss 1.75488257\n",
      "Trained batch 22 batch loss 1.86363447 epoch total loss 1.75982594\n",
      "Trained batch 23 batch loss 1.85532928 epoch total loss 1.76397824\n",
      "Trained batch 24 batch loss 1.81519508 epoch total loss 1.76611233\n",
      "Trained batch 25 batch loss 1.73880732 epoch total loss 1.76502013\n",
      "Trained batch 26 batch loss 1.7894733 epoch total loss 1.76596069\n",
      "Trained batch 27 batch loss 1.81496263 epoch total loss 1.76777565\n",
      "Trained batch 28 batch loss 1.81113613 epoch total loss 1.76932418\n",
      "Trained batch 29 batch loss 1.77058983 epoch total loss 1.76936793\n",
      "Trained batch 30 batch loss 1.74657917 epoch total loss 1.76860821\n",
      "Trained batch 31 batch loss 1.69142938 epoch total loss 1.76611853\n",
      "Trained batch 32 batch loss 1.72529268 epoch total loss 1.76484275\n",
      "Trained batch 33 batch loss 1.83377552 epoch total loss 1.76693153\n",
      "Trained batch 34 batch loss 1.80913794 epoch total loss 1.76817298\n",
      "Trained batch 35 batch loss 1.83442461 epoch total loss 1.7700659\n",
      "Trained batch 36 batch loss 1.86469603 epoch total loss 1.77269447\n",
      "Trained batch 37 batch loss 1.76562405 epoch total loss 1.77250338\n",
      "Trained batch 38 batch loss 1.64914179 epoch total loss 1.76925695\n",
      "Trained batch 39 batch loss 1.5104686 epoch total loss 1.7626214\n",
      "Trained batch 40 batch loss 1.5871644 epoch total loss 1.75823498\n",
      "Trained batch 41 batch loss 1.75993431 epoch total loss 1.75827646\n",
      "Trained batch 42 batch loss 1.82710993 epoch total loss 1.75991535\n",
      "Trained batch 43 batch loss 1.83366752 epoch total loss 1.76163042\n",
      "Trained batch 44 batch loss 1.79617608 epoch total loss 1.76241541\n",
      "Trained batch 45 batch loss 1.7269702 epoch total loss 1.76162767\n",
      "Trained batch 46 batch loss 1.82263851 epoch total loss 1.762954\n",
      "Trained batch 47 batch loss 1.77221751 epoch total loss 1.76315117\n",
      "Trained batch 48 batch loss 1.82151484 epoch total loss 1.7643671\n",
      "Trained batch 49 batch loss 1.83585048 epoch total loss 1.76582599\n",
      "Trained batch 50 batch loss 1.85802531 epoch total loss 1.76767\n",
      "Trained batch 51 batch loss 1.71165347 epoch total loss 1.76657164\n",
      "Trained batch 52 batch loss 1.77341604 epoch total loss 1.76670325\n",
      "Trained batch 53 batch loss 1.81573832 epoch total loss 1.76762831\n",
      "Trained batch 54 batch loss 1.83080637 epoch total loss 1.76879823\n",
      "Trained batch 55 batch loss 1.8416785 epoch total loss 1.77012324\n",
      "Trained batch 56 batch loss 1.75813091 epoch total loss 1.76990914\n",
      "Trained batch 57 batch loss 1.84047484 epoch total loss 1.77114725\n",
      "Trained batch 58 batch loss 1.79330873 epoch total loss 1.77152932\n",
      "Trained batch 59 batch loss 1.74522161 epoch total loss 1.77108347\n",
      "Trained batch 60 batch loss 1.82667422 epoch total loss 1.77201009\n",
      "Trained batch 61 batch loss 1.67511344 epoch total loss 1.7704215\n",
      "Trained batch 62 batch loss 1.77720773 epoch total loss 1.77053094\n",
      "Trained batch 63 batch loss 1.62776923 epoch total loss 1.76826489\n",
      "Trained batch 64 batch loss 1.62717843 epoch total loss 1.76606047\n",
      "Trained batch 65 batch loss 1.69322073 epoch total loss 1.7649399\n",
      "Trained batch 66 batch loss 1.82819307 epoch total loss 1.76589823\n",
      "Trained batch 67 batch loss 1.91183257 epoch total loss 1.76807642\n",
      "Trained batch 68 batch loss 1.8094399 epoch total loss 1.76868474\n",
      "Trained batch 69 batch loss 1.77404559 epoch total loss 1.76876247\n",
      "Trained batch 70 batch loss 1.76384056 epoch total loss 1.76869214\n",
      "Trained batch 71 batch loss 1.80993593 epoch total loss 1.76927304\n",
      "Trained batch 72 batch loss 1.72673154 epoch total loss 1.76868212\n",
      "Trained batch 73 batch loss 1.83905506 epoch total loss 1.76964617\n",
      "Trained batch 74 batch loss 1.80990696 epoch total loss 1.77019024\n",
      "Trained batch 75 batch loss 1.77712214 epoch total loss 1.77028263\n",
      "Trained batch 76 batch loss 1.80295444 epoch total loss 1.77071238\n",
      "Trained batch 77 batch loss 1.84235644 epoch total loss 1.77164292\n",
      "Trained batch 78 batch loss 1.80233979 epoch total loss 1.77203643\n",
      "Trained batch 79 batch loss 1.82137775 epoch total loss 1.77266109\n",
      "Trained batch 80 batch loss 1.7977097 epoch total loss 1.77297425\n",
      "Trained batch 81 batch loss 1.75394905 epoch total loss 1.77273941\n",
      "Trained batch 82 batch loss 1.7659508 epoch total loss 1.77265656\n",
      "Trained batch 83 batch loss 1.69981694 epoch total loss 1.77177894\n",
      "Trained batch 84 batch loss 1.67122233 epoch total loss 1.77058172\n",
      "Trained batch 85 batch loss 1.5883733 epoch total loss 1.76843822\n",
      "Trained batch 86 batch loss 1.56322742 epoch total loss 1.76605213\n",
      "Trained batch 87 batch loss 1.74687755 epoch total loss 1.76583159\n",
      "Trained batch 88 batch loss 1.66744959 epoch total loss 1.76471364\n",
      "Trained batch 89 batch loss 1.71017087 epoch total loss 1.76410079\n",
      "Trained batch 90 batch loss 1.70976388 epoch total loss 1.76349711\n",
      "Trained batch 91 batch loss 1.61312783 epoch total loss 1.76184464\n",
      "Trained batch 92 batch loss 1.7218883 epoch total loss 1.76141047\n",
      "Trained batch 93 batch loss 1.91482794 epoch total loss 1.76306009\n",
      "Trained batch 94 batch loss 1.85714066 epoch total loss 1.76406097\n",
      "Trained batch 95 batch loss 1.73076034 epoch total loss 1.76371038\n",
      "Trained batch 96 batch loss 1.82086921 epoch total loss 1.76430571\n",
      "Trained batch 97 batch loss 1.71764481 epoch total loss 1.76382482\n",
      "Trained batch 98 batch loss 1.71228671 epoch total loss 1.76329887\n",
      "Trained batch 99 batch loss 1.83734417 epoch total loss 1.76404667\n",
      "Trained batch 100 batch loss 1.78532147 epoch total loss 1.76425946\n",
      "Trained batch 101 batch loss 1.80876732 epoch total loss 1.7647\n",
      "Trained batch 102 batch loss 1.81660092 epoch total loss 1.76520896\n",
      "Trained batch 103 batch loss 1.86566854 epoch total loss 1.76618421\n",
      "Trained batch 104 batch loss 1.89398956 epoch total loss 1.76741326\n",
      "Trained batch 105 batch loss 1.84183311 epoch total loss 1.76812196\n",
      "Trained batch 106 batch loss 1.65493488 epoch total loss 1.7670542\n",
      "Trained batch 107 batch loss 1.8281579 epoch total loss 1.76762521\n",
      "Trained batch 108 batch loss 1.74246836 epoch total loss 1.76739216\n",
      "Trained batch 109 batch loss 1.6382004 epoch total loss 1.76620698\n",
      "Trained batch 110 batch loss 1.73544466 epoch total loss 1.76592731\n",
      "Trained batch 111 batch loss 1.6823771 epoch total loss 1.76517451\n",
      "Trained batch 112 batch loss 1.76521993 epoch total loss 1.76517487\n",
      "Trained batch 113 batch loss 1.65592623 epoch total loss 1.76420808\n",
      "Trained batch 114 batch loss 1.67761195 epoch total loss 1.76344848\n",
      "Trained batch 115 batch loss 1.72485793 epoch total loss 1.7631129\n",
      "Trained batch 116 batch loss 1.73541737 epoch total loss 1.76287413\n",
      "Trained batch 117 batch loss 1.72711897 epoch total loss 1.76256847\n",
      "Trained batch 118 batch loss 1.64767885 epoch total loss 1.76159477\n",
      "Trained batch 119 batch loss 1.82913947 epoch total loss 1.76216233\n",
      "Trained batch 120 batch loss 1.66588783 epoch total loss 1.76136\n",
      "Trained batch 121 batch loss 1.71673417 epoch total loss 1.76099122\n",
      "Trained batch 122 batch loss 1.74459016 epoch total loss 1.76085675\n",
      "Trained batch 123 batch loss 1.8677969 epoch total loss 1.76172614\n",
      "Trained batch 124 batch loss 1.83388507 epoch total loss 1.762308\n",
      "Trained batch 125 batch loss 1.85028934 epoch total loss 1.76301193\n",
      "Trained batch 126 batch loss 1.74434566 epoch total loss 1.76286376\n",
      "Trained batch 127 batch loss 1.71324885 epoch total loss 1.76247299\n",
      "Trained batch 128 batch loss 1.712538 epoch total loss 1.76208293\n",
      "Trained batch 129 batch loss 1.79558623 epoch total loss 1.76234269\n",
      "Trained batch 130 batch loss 1.56327403 epoch total loss 1.76081145\n",
      "Trained batch 131 batch loss 1.62006164 epoch total loss 1.75973701\n",
      "Trained batch 132 batch loss 1.72668195 epoch total loss 1.75948656\n",
      "Trained batch 133 batch loss 1.66614544 epoch total loss 1.75878477\n",
      "Trained batch 134 batch loss 1.67962277 epoch total loss 1.75819409\n",
      "Trained batch 135 batch loss 1.67087936 epoch total loss 1.75754738\n",
      "Trained batch 136 batch loss 1.6992805 epoch total loss 1.75711894\n",
      "Trained batch 137 batch loss 1.68749583 epoch total loss 1.75661075\n",
      "Trained batch 138 batch loss 1.77530086 epoch total loss 1.75674617\n",
      "Trained batch 139 batch loss 1.79278457 epoch total loss 1.75700545\n",
      "Trained batch 140 batch loss 1.5258888 epoch total loss 1.75535464\n",
      "Trained batch 141 batch loss 1.54187107 epoch total loss 1.75384057\n",
      "Trained batch 142 batch loss 1.52545285 epoch total loss 1.75223219\n",
      "Trained batch 143 batch loss 1.66107297 epoch total loss 1.75159466\n",
      "Trained batch 144 batch loss 1.7658335 epoch total loss 1.75169361\n",
      "Trained batch 145 batch loss 1.68246651 epoch total loss 1.75121617\n",
      "Trained batch 146 batch loss 1.75128734 epoch total loss 1.75121665\n",
      "Trained batch 147 batch loss 1.68747461 epoch total loss 1.75078309\n",
      "Trained batch 148 batch loss 1.77850449 epoch total loss 1.75097036\n",
      "Trained batch 149 batch loss 1.71069813 epoch total loss 1.75070012\n",
      "Trained batch 150 batch loss 1.80377209 epoch total loss 1.75105393\n",
      "Trained batch 151 batch loss 1.87781656 epoch total loss 1.75189328\n",
      "Trained batch 152 batch loss 1.66509461 epoch total loss 1.75132227\n",
      "Trained batch 153 batch loss 1.70410657 epoch total loss 1.75101364\n",
      "Trained batch 154 batch loss 1.61682343 epoch total loss 1.75014234\n",
      "Trained batch 155 batch loss 1.79342055 epoch total loss 1.75042152\n",
      "Trained batch 156 batch loss 1.78468394 epoch total loss 1.75064123\n",
      "Trained batch 157 batch loss 1.79898119 epoch total loss 1.75094914\n",
      "Trained batch 158 batch loss 1.74383855 epoch total loss 1.75090408\n",
      "Trained batch 159 batch loss 1.72405601 epoch total loss 1.75073528\n",
      "Trained batch 160 batch loss 1.75321853 epoch total loss 1.75075078\n",
      "Trained batch 161 batch loss 1.80907381 epoch total loss 1.75111306\n",
      "Trained batch 162 batch loss 1.74162626 epoch total loss 1.75105453\n",
      "Trained batch 163 batch loss 1.80454707 epoch total loss 1.75138271\n",
      "Trained batch 164 batch loss 1.82020783 epoch total loss 1.75180244\n",
      "Trained batch 165 batch loss 1.64989579 epoch total loss 1.75118482\n",
      "Trained batch 166 batch loss 1.66098177 epoch total loss 1.75064147\n",
      "Trained batch 167 batch loss 1.59326673 epoch total loss 1.749699\n",
      "Trained batch 168 batch loss 1.58822632 epoch total loss 1.74873793\n",
      "Trained batch 169 batch loss 1.58971632 epoch total loss 1.74779701\n",
      "Trained batch 170 batch loss 1.68304181 epoch total loss 1.74741602\n",
      "Trained batch 171 batch loss 1.74903083 epoch total loss 1.74742544\n",
      "Trained batch 172 batch loss 1.82932889 epoch total loss 1.74790156\n",
      "Trained batch 173 batch loss 1.77056134 epoch total loss 1.74803257\n",
      "Trained batch 174 batch loss 1.79839766 epoch total loss 1.74832201\n",
      "Trained batch 175 batch loss 1.74451542 epoch total loss 1.74830031\n",
      "Trained batch 176 batch loss 1.72682869 epoch total loss 1.74817836\n",
      "Trained batch 177 batch loss 1.8324976 epoch total loss 1.7486546\n",
      "Trained batch 178 batch loss 1.60037565 epoch total loss 1.74782157\n",
      "Trained batch 179 batch loss 1.61191106 epoch total loss 1.74706233\n",
      "Trained batch 180 batch loss 1.70623386 epoch total loss 1.74683547\n",
      "Trained batch 181 batch loss 1.8016628 epoch total loss 1.74713838\n",
      "Trained batch 182 batch loss 1.81637108 epoch total loss 1.7475189\n",
      "Trained batch 183 batch loss 1.81611776 epoch total loss 1.74789381\n",
      "Trained batch 184 batch loss 1.79158187 epoch total loss 1.74813128\n",
      "Trained batch 185 batch loss 1.76824808 epoch total loss 1.74824\n",
      "Trained batch 186 batch loss 1.65998077 epoch total loss 1.74776554\n",
      "Trained batch 187 batch loss 1.74000263 epoch total loss 1.74772394\n",
      "Trained batch 188 batch loss 1.67027581 epoch total loss 1.74731207\n",
      "Trained batch 189 batch loss 1.78163898 epoch total loss 1.74749362\n",
      "Trained batch 190 batch loss 1.83629322 epoch total loss 1.74796116\n",
      "Trained batch 191 batch loss 1.85960102 epoch total loss 1.74854553\n",
      "Trained batch 192 batch loss 1.87764 epoch total loss 1.74921799\n",
      "Trained batch 193 batch loss 1.88373876 epoch total loss 1.74991488\n",
      "Trained batch 194 batch loss 1.81768322 epoch total loss 1.75026429\n",
      "Trained batch 195 batch loss 1.80502164 epoch total loss 1.75054514\n",
      "Trained batch 196 batch loss 1.65470433 epoch total loss 1.75005603\n",
      "Trained batch 197 batch loss 1.55776191 epoch total loss 1.74908\n",
      "Trained batch 198 batch loss 1.79009318 epoch total loss 1.74928713\n",
      "Trained batch 199 batch loss 1.83456123 epoch total loss 1.74971569\n",
      "Trained batch 200 batch loss 1.62362242 epoch total loss 1.74908519\n",
      "Trained batch 201 batch loss 1.60642731 epoch total loss 1.74837542\n",
      "Trained batch 202 batch loss 1.72662723 epoch total loss 1.74826777\n",
      "Trained batch 203 batch loss 1.82274544 epoch total loss 1.7486347\n",
      "Trained batch 204 batch loss 1.83355939 epoch total loss 1.74905097\n",
      "Trained batch 205 batch loss 1.83943307 epoch total loss 1.74949193\n",
      "Trained batch 206 batch loss 1.82820582 epoch total loss 1.74987411\n",
      "Trained batch 207 batch loss 1.86177981 epoch total loss 1.75041473\n",
      "Trained batch 208 batch loss 1.84265637 epoch total loss 1.75085819\n",
      "Trained batch 209 batch loss 1.83331525 epoch total loss 1.75125265\n",
      "Trained batch 210 batch loss 1.76970053 epoch total loss 1.75134063\n",
      "Trained batch 211 batch loss 1.70472145 epoch total loss 1.75111961\n",
      "Trained batch 212 batch loss 1.70858622 epoch total loss 1.75091898\n",
      "Trained batch 213 batch loss 1.82265759 epoch total loss 1.75125575\n",
      "Trained batch 214 batch loss 1.80525124 epoch total loss 1.751508\n",
      "Trained batch 215 batch loss 1.67680871 epoch total loss 1.75116062\n",
      "Trained batch 216 batch loss 1.72554195 epoch total loss 1.75104213\n",
      "Trained batch 217 batch loss 1.67657077 epoch total loss 1.75069892\n",
      "Trained batch 218 batch loss 1.7422365 epoch total loss 1.75066018\n",
      "Trained batch 219 batch loss 1.78644872 epoch total loss 1.7508235\n",
      "Trained batch 220 batch loss 1.83570981 epoch total loss 1.7512095\n",
      "Trained batch 221 batch loss 1.83672667 epoch total loss 1.75159645\n",
      "Trained batch 222 batch loss 1.83415198 epoch total loss 1.75196826\n",
      "Trained batch 223 batch loss 1.81459546 epoch total loss 1.75224912\n",
      "Trained batch 224 batch loss 1.64507282 epoch total loss 1.75177073\n",
      "Trained batch 225 batch loss 1.68526626 epoch total loss 1.7514751\n",
      "Trained batch 226 batch loss 1.85351658 epoch total loss 1.75192666\n",
      "Trained batch 227 batch loss 1.79550588 epoch total loss 1.75211859\n",
      "Trained batch 228 batch loss 1.76277637 epoch total loss 1.75216544\n",
      "Trained batch 229 batch loss 1.7139802 epoch total loss 1.75199866\n",
      "Trained batch 230 batch loss 1.68146861 epoch total loss 1.75169206\n",
      "Trained batch 231 batch loss 1.75156355 epoch total loss 1.75169146\n",
      "Trained batch 232 batch loss 1.79719651 epoch total loss 1.75188756\n",
      "Trained batch 233 batch loss 1.76627445 epoch total loss 1.75194931\n",
      "Trained batch 234 batch loss 1.74458241 epoch total loss 1.75191784\n",
      "Trained batch 235 batch loss 1.66082335 epoch total loss 1.75153017\n",
      "Trained batch 236 batch loss 1.77915657 epoch total loss 1.75164711\n",
      "Trained batch 237 batch loss 1.79551625 epoch total loss 1.75183225\n",
      "Trained batch 238 batch loss 1.61056137 epoch total loss 1.7512387\n",
      "Trained batch 239 batch loss 1.74343133 epoch total loss 1.75120604\n",
      "Trained batch 240 batch loss 1.65778959 epoch total loss 1.7508167\n",
      "Trained batch 241 batch loss 1.5338012 epoch total loss 1.74991632\n",
      "Trained batch 242 batch loss 1.49999857 epoch total loss 1.74888361\n",
      "Trained batch 243 batch loss 1.47813392 epoch total loss 1.74776936\n",
      "Trained batch 244 batch loss 1.57905662 epoch total loss 1.74707794\n",
      "Trained batch 245 batch loss 1.6865859 epoch total loss 1.74683106\n",
      "Trained batch 246 batch loss 1.75212073 epoch total loss 1.74685252\n",
      "Trained batch 247 batch loss 1.87636697 epoch total loss 1.7473768\n",
      "Trained batch 248 batch loss 1.7743144 epoch total loss 1.74748552\n",
      "Trained batch 249 batch loss 1.85437381 epoch total loss 1.74791479\n",
      "Trained batch 250 batch loss 1.83893323 epoch total loss 1.74827886\n",
      "Trained batch 251 batch loss 1.85571837 epoch total loss 1.74870682\n",
      "Trained batch 252 batch loss 1.89171135 epoch total loss 1.74927437\n",
      "Trained batch 253 batch loss 1.9871037 epoch total loss 1.75021434\n",
      "Trained batch 254 batch loss 1.87445521 epoch total loss 1.75070345\n",
      "Trained batch 255 batch loss 1.92340851 epoch total loss 1.75138068\n",
      "Trained batch 256 batch loss 1.77964246 epoch total loss 1.75149107\n",
      "Trained batch 257 batch loss 1.80145109 epoch total loss 1.7516855\n",
      "Trained batch 258 batch loss 1.85318398 epoch total loss 1.75207889\n",
      "Trained batch 259 batch loss 1.61439157 epoch total loss 1.75154722\n",
      "Trained batch 260 batch loss 1.64759076 epoch total loss 1.75114739\n",
      "Trained batch 261 batch loss 1.75011611 epoch total loss 1.75114346\n",
      "Trained batch 262 batch loss 1.63272166 epoch total loss 1.75069141\n",
      "Trained batch 263 batch loss 1.81584477 epoch total loss 1.75093925\n",
      "Trained batch 264 batch loss 1.80201244 epoch total loss 1.75113261\n",
      "Trained batch 265 batch loss 1.86228275 epoch total loss 1.75155199\n",
      "Trained batch 266 batch loss 1.8648622 epoch total loss 1.75197804\n",
      "Trained batch 267 batch loss 1.80496311 epoch total loss 1.7521764\n",
      "Trained batch 268 batch loss 1.83153582 epoch total loss 1.75247264\n",
      "Trained batch 269 batch loss 1.60986483 epoch total loss 1.75194252\n",
      "Trained batch 270 batch loss 1.75697029 epoch total loss 1.75196099\n",
      "Trained batch 271 batch loss 1.8117367 epoch total loss 1.75218165\n",
      "Trained batch 272 batch loss 1.70484102 epoch total loss 1.75200748\n",
      "Trained batch 273 batch loss 1.74157691 epoch total loss 1.75196934\n",
      "Trained batch 274 batch loss 1.52980304 epoch total loss 1.7511586\n",
      "Trained batch 275 batch loss 1.77527094 epoch total loss 1.75124621\n",
      "Trained batch 276 batch loss 1.67881012 epoch total loss 1.75098372\n",
      "Trained batch 277 batch loss 1.83038354 epoch total loss 1.75127041\n",
      "Trained batch 278 batch loss 1.81234193 epoch total loss 1.75149012\n",
      "Trained batch 279 batch loss 1.8389895 epoch total loss 1.75180376\n",
      "Trained batch 280 batch loss 1.79227614 epoch total loss 1.75194824\n",
      "Trained batch 281 batch loss 1.60911465 epoch total loss 1.75143981\n",
      "Trained batch 282 batch loss 1.71756971 epoch total loss 1.75131977\n",
      "Trained batch 283 batch loss 1.6839931 epoch total loss 1.75108182\n",
      "Trained batch 284 batch loss 1.65709686 epoch total loss 1.7507509\n",
      "Trained batch 285 batch loss 1.74542212 epoch total loss 1.75073218\n",
      "Trained batch 286 batch loss 1.70579457 epoch total loss 1.75057507\n",
      "Trained batch 287 batch loss 1.81260085 epoch total loss 1.75079107\n",
      "Trained batch 288 batch loss 1.69328916 epoch total loss 1.75059152\n",
      "Trained batch 289 batch loss 1.63457239 epoch total loss 1.75019\n",
      "Trained batch 290 batch loss 1.80751073 epoch total loss 1.75038779\n",
      "Trained batch 291 batch loss 1.82898474 epoch total loss 1.75065792\n",
      "Trained batch 292 batch loss 1.83031011 epoch total loss 1.75093067\n",
      "Trained batch 293 batch loss 1.72982788 epoch total loss 1.75085866\n",
      "Trained batch 294 batch loss 1.70588946 epoch total loss 1.7507056\n",
      "Trained batch 295 batch loss 1.74335098 epoch total loss 1.75068069\n",
      "Trained batch 296 batch loss 1.59216332 epoch total loss 1.7501452\n",
      "Trained batch 297 batch loss 1.76094472 epoch total loss 1.75018144\n",
      "Trained batch 298 batch loss 1.66832972 epoch total loss 1.74990678\n",
      "Trained batch 299 batch loss 1.70646691 epoch total loss 1.74976158\n",
      "Trained batch 300 batch loss 1.66605568 epoch total loss 1.74948263\n",
      "Trained batch 301 batch loss 1.77817631 epoch total loss 1.749578\n",
      "Trained batch 302 batch loss 1.75060368 epoch total loss 1.74958146\n",
      "Trained batch 303 batch loss 1.72659981 epoch total loss 1.74950564\n",
      "Trained batch 304 batch loss 1.70561528 epoch total loss 1.7493614\n",
      "Trained batch 305 batch loss 1.75732684 epoch total loss 1.7493875\n",
      "Trained batch 306 batch loss 1.78568268 epoch total loss 1.74950612\n",
      "Trained batch 307 batch loss 1.83467066 epoch total loss 1.74978352\n",
      "Trained batch 308 batch loss 1.83759129 epoch total loss 1.75006855\n",
      "Trained batch 309 batch loss 1.83850408 epoch total loss 1.75035477\n",
      "Trained batch 310 batch loss 1.81389427 epoch total loss 1.75055981\n",
      "Trained batch 311 batch loss 1.78298616 epoch total loss 1.75066388\n",
      "Trained batch 312 batch loss 1.82309282 epoch total loss 1.75089622\n",
      "Trained batch 313 batch loss 1.75083351 epoch total loss 1.75089598\n",
      "Trained batch 314 batch loss 1.83878827 epoch total loss 1.751176\n",
      "Trained batch 315 batch loss 1.79146647 epoch total loss 1.75130379\n",
      "Trained batch 316 batch loss 1.75182879 epoch total loss 1.75130546\n",
      "Trained batch 317 batch loss 1.71113145 epoch total loss 1.75117874\n",
      "Trained batch 318 batch loss 1.6604135 epoch total loss 1.75089324\n",
      "Trained batch 319 batch loss 1.6257112 epoch total loss 1.75050092\n",
      "Trained batch 320 batch loss 1.66581964 epoch total loss 1.75023627\n",
      "Trained batch 321 batch loss 1.8115977 epoch total loss 1.75042748\n",
      "Trained batch 322 batch loss 1.78949153 epoch total loss 1.75054872\n",
      "Trained batch 323 batch loss 1.74559903 epoch total loss 1.75053346\n",
      "Trained batch 324 batch loss 1.70318866 epoch total loss 1.75038731\n",
      "Trained batch 325 batch loss 1.63448775 epoch total loss 1.75003064\n",
      "Trained batch 326 batch loss 1.68403924 epoch total loss 1.7498281\n",
      "Trained batch 327 batch loss 1.72035968 epoch total loss 1.74973798\n",
      "Trained batch 328 batch loss 1.79984915 epoch total loss 1.7498908\n",
      "Trained batch 329 batch loss 1.80240917 epoch total loss 1.75005043\n",
      "Trained batch 330 batch loss 1.81325269 epoch total loss 1.75024188\n",
      "Trained batch 331 batch loss 1.82234502 epoch total loss 1.75045967\n",
      "Trained batch 332 batch loss 1.8114028 epoch total loss 1.75064325\n",
      "Trained batch 333 batch loss 1.6476593 epoch total loss 1.75033391\n",
      "Trained batch 334 batch loss 1.66125703 epoch total loss 1.75006723\n",
      "Trained batch 335 batch loss 1.77053761 epoch total loss 1.75012827\n",
      "Trained batch 336 batch loss 1.74899733 epoch total loss 1.75012493\n",
      "Trained batch 337 batch loss 1.81269884 epoch total loss 1.75031066\n",
      "Trained batch 338 batch loss 1.8398447 epoch total loss 1.75057554\n",
      "Trained batch 339 batch loss 1.85318029 epoch total loss 1.75087821\n",
      "Trained batch 340 batch loss 1.82238352 epoch total loss 1.75108862\n",
      "Trained batch 341 batch loss 1.84775913 epoch total loss 1.7513721\n",
      "Trained batch 342 batch loss 1.86000121 epoch total loss 1.75168967\n",
      "Trained batch 343 batch loss 1.8868103 epoch total loss 1.75208378\n",
      "Trained batch 344 batch loss 1.83695936 epoch total loss 1.75233054\n",
      "Trained batch 345 batch loss 1.81437993 epoch total loss 1.75251043\n",
      "Trained batch 346 batch loss 1.80100715 epoch total loss 1.75265062\n",
      "Trained batch 347 batch loss 1.76719034 epoch total loss 1.75269258\n",
      "Trained batch 348 batch loss 1.8125608 epoch total loss 1.7528646\n",
      "Trained batch 349 batch loss 1.73736632 epoch total loss 1.75282025\n",
      "Trained batch 350 batch loss 1.7713356 epoch total loss 1.75287318\n",
      "Trained batch 351 batch loss 1.80504131 epoch total loss 1.75302184\n",
      "Trained batch 352 batch loss 1.78213644 epoch total loss 1.75310469\n",
      "Trained batch 353 batch loss 1.79173064 epoch total loss 1.75321412\n",
      "Trained batch 354 batch loss 1.74756849 epoch total loss 1.75319815\n",
      "Trained batch 355 batch loss 1.77486539 epoch total loss 1.75325906\n",
      "Trained batch 356 batch loss 1.78572917 epoch total loss 1.75335026\n",
      "Trained batch 357 batch loss 1.83123446 epoch total loss 1.75356841\n",
      "Trained batch 358 batch loss 1.83611321 epoch total loss 1.75379896\n",
      "Trained batch 359 batch loss 1.85016966 epoch total loss 1.75406742\n",
      "Trained batch 360 batch loss 1.84727907 epoch total loss 1.75432634\n",
      "Trained batch 361 batch loss 1.84334767 epoch total loss 1.75457287\n",
      "Trained batch 362 batch loss 1.84801805 epoch total loss 1.75483108\n",
      "Trained batch 363 batch loss 1.78781283 epoch total loss 1.75492203\n",
      "Trained batch 364 batch loss 1.82372057 epoch total loss 1.75511098\n",
      "Trained batch 365 batch loss 1.80155134 epoch total loss 1.75523829\n",
      "Trained batch 366 batch loss 1.75564408 epoch total loss 1.75523937\n",
      "Trained batch 367 batch loss 1.80596101 epoch total loss 1.75537753\n",
      "Trained batch 368 batch loss 1.70746541 epoch total loss 1.75524735\n",
      "Trained batch 369 batch loss 1.76666403 epoch total loss 1.75527835\n",
      "Trained batch 370 batch loss 1.8298862 epoch total loss 1.75547993\n",
      "Trained batch 371 batch loss 1.84175479 epoch total loss 1.75571251\n",
      "Trained batch 372 batch loss 1.86994863 epoch total loss 1.75601947\n",
      "Trained batch 373 batch loss 1.83307719 epoch total loss 1.75622606\n",
      "Trained batch 374 batch loss 1.8187865 epoch total loss 1.75639331\n",
      "Trained batch 375 batch loss 1.7409662 epoch total loss 1.75635219\n",
      "Trained batch 376 batch loss 1.71263814 epoch total loss 1.75623596\n",
      "Trained batch 377 batch loss 1.70083499 epoch total loss 1.75608897\n",
      "Trained batch 378 batch loss 1.81386054 epoch total loss 1.75624168\n",
      "Trained batch 379 batch loss 1.88024735 epoch total loss 1.75656891\n",
      "Trained batch 380 batch loss 1.80008185 epoch total loss 1.75668347\n",
      "Trained batch 381 batch loss 1.73823667 epoch total loss 1.75663507\n",
      "Trained batch 382 batch loss 1.81159639 epoch total loss 1.75677896\n",
      "Trained batch 383 batch loss 1.82741868 epoch total loss 1.75696325\n",
      "Trained batch 384 batch loss 1.81845737 epoch total loss 1.75712347\n",
      "Trained batch 385 batch loss 1.8232193 epoch total loss 1.75729525\n",
      "Trained batch 386 batch loss 1.77583313 epoch total loss 1.75734317\n",
      "Trained batch 387 batch loss 1.80702424 epoch total loss 1.75747156\n",
      "Trained batch 388 batch loss 1.76885927 epoch total loss 1.75750089\n",
      "Trained batch 389 batch loss 1.77753592 epoch total loss 1.75755239\n",
      "Trained batch 390 batch loss 1.80389917 epoch total loss 1.75767124\n",
      "Trained batch 391 batch loss 1.77246785 epoch total loss 1.75770903\n",
      "Trained batch 392 batch loss 1.65521693 epoch total loss 1.75744748\n",
      "Trained batch 393 batch loss 1.70011461 epoch total loss 1.75730169\n",
      "Trained batch 394 batch loss 1.68226242 epoch total loss 1.75711119\n",
      "Trained batch 395 batch loss 1.60324955 epoch total loss 1.75672174\n",
      "Trained batch 396 batch loss 1.52722108 epoch total loss 1.75614226\n",
      "Trained batch 397 batch loss 1.68075204 epoch total loss 1.75595224\n",
      "Trained batch 398 batch loss 1.51700437 epoch total loss 1.7553519\n",
      "Trained batch 399 batch loss 1.67122269 epoch total loss 1.75514102\n",
      "Trained batch 400 batch loss 1.65372241 epoch total loss 1.75488758\n",
      "Trained batch 401 batch loss 1.78539073 epoch total loss 1.75496364\n",
      "Trained batch 402 batch loss 1.88161218 epoch total loss 1.75527859\n",
      "Trained batch 403 batch loss 1.63549626 epoch total loss 1.7549814\n",
      "Trained batch 404 batch loss 1.66537094 epoch total loss 1.75475955\n",
      "Trained batch 405 batch loss 1.71153367 epoch total loss 1.75465286\n",
      "Trained batch 406 batch loss 1.77902746 epoch total loss 1.75471294\n",
      "Trained batch 407 batch loss 1.71189058 epoch total loss 1.7546078\n",
      "Trained batch 408 batch loss 1.62848 epoch total loss 1.75429869\n",
      "Trained batch 409 batch loss 1.75887394 epoch total loss 1.75430977\n",
      "Trained batch 410 batch loss 1.74888277 epoch total loss 1.75429654\n",
      "Trained batch 411 batch loss 1.81118631 epoch total loss 1.75443494\n",
      "Trained batch 412 batch loss 1.80772865 epoch total loss 1.75456429\n",
      "Trained batch 413 batch loss 1.77114713 epoch total loss 1.75460434\n",
      "Trained batch 414 batch loss 1.72669065 epoch total loss 1.75453699\n",
      "Trained batch 415 batch loss 1.70156801 epoch total loss 1.75440919\n",
      "Trained batch 416 batch loss 1.74819791 epoch total loss 1.75439429\n",
      "Trained batch 417 batch loss 1.82318878 epoch total loss 1.75455916\n",
      "Trained batch 418 batch loss 1.81705236 epoch total loss 1.75470877\n",
      "Trained batch 419 batch loss 1.6689043 epoch total loss 1.75450397\n",
      "Trained batch 420 batch loss 1.70007396 epoch total loss 1.75437438\n",
      "Trained batch 421 batch loss 1.75043893 epoch total loss 1.75436497\n",
      "Trained batch 422 batch loss 1.79580784 epoch total loss 1.7544632\n",
      "Trained batch 423 batch loss 1.80257535 epoch total loss 1.75457692\n",
      "Trained batch 424 batch loss 1.74418485 epoch total loss 1.75455248\n",
      "Trained batch 425 batch loss 1.74739027 epoch total loss 1.75453556\n",
      "Trained batch 426 batch loss 1.70794666 epoch total loss 1.75442624\n",
      "Trained batch 427 batch loss 1.82155836 epoch total loss 1.75458336\n",
      "Trained batch 428 batch loss 1.83224094 epoch total loss 1.75476468\n",
      "Trained batch 429 batch loss 1.79668641 epoch total loss 1.75486243\n",
      "Trained batch 430 batch loss 1.77235126 epoch total loss 1.75490308\n",
      "Trained batch 431 batch loss 1.6810956 epoch total loss 1.75473189\n",
      "Trained batch 432 batch loss 1.78319907 epoch total loss 1.75479782\n",
      "Trained batch 433 batch loss 1.77593577 epoch total loss 1.75484657\n",
      "Trained batch 434 batch loss 1.76125097 epoch total loss 1.75486135\n",
      "Trained batch 435 batch loss 1.74573278 epoch total loss 1.75484025\n",
      "Trained batch 436 batch loss 1.77557969 epoch total loss 1.75488782\n",
      "Trained batch 437 batch loss 1.80478549 epoch total loss 1.75500214\n",
      "Trained batch 438 batch loss 1.77857471 epoch total loss 1.7550559\n",
      "Trained batch 439 batch loss 1.72992992 epoch total loss 1.75499868\n",
      "Trained batch 440 batch loss 1.80899024 epoch total loss 1.75512123\n",
      "Trained batch 441 batch loss 1.68480206 epoch total loss 1.75496185\n",
      "Trained batch 442 batch loss 1.77220356 epoch total loss 1.75500083\n",
      "Trained batch 443 batch loss 1.83019829 epoch total loss 1.75517058\n",
      "Trained batch 444 batch loss 1.81633782 epoch total loss 1.75530839\n",
      "Trained batch 445 batch loss 1.75983894 epoch total loss 1.75531852\n",
      "Trained batch 446 batch loss 1.64300108 epoch total loss 1.75506675\n",
      "Trained batch 447 batch loss 1.4808656 epoch total loss 1.75445342\n",
      "Trained batch 448 batch loss 1.56471014 epoch total loss 1.75402987\n",
      "Trained batch 449 batch loss 1.58629072 epoch total loss 1.75365627\n",
      "Trained batch 450 batch loss 1.65716588 epoch total loss 1.75344181\n",
      "Trained batch 451 batch loss 1.54897833 epoch total loss 1.75298846\n",
      "Trained batch 452 batch loss 1.62392461 epoch total loss 1.75270283\n",
      "Trained batch 453 batch loss 1.66916394 epoch total loss 1.75251842\n",
      "Trained batch 454 batch loss 1.66761756 epoch total loss 1.75233138\n",
      "Trained batch 455 batch loss 1.61977017 epoch total loss 1.75204\n",
      "Trained batch 456 batch loss 1.77679062 epoch total loss 1.75209439\n",
      "Trained batch 457 batch loss 1.80966032 epoch total loss 1.75222027\n",
      "Trained batch 458 batch loss 1.7846607 epoch total loss 1.75229108\n",
      "Trained batch 459 batch loss 1.82287312 epoch total loss 1.75244486\n",
      "Trained batch 460 batch loss 1.81630778 epoch total loss 1.75258362\n",
      "Trained batch 461 batch loss 1.83024716 epoch total loss 1.75275218\n",
      "Trained batch 462 batch loss 1.79055071 epoch total loss 1.75283396\n",
      "Trained batch 463 batch loss 1.81689727 epoch total loss 1.75297225\n",
      "Trained batch 464 batch loss 1.74265909 epoch total loss 1.75295007\n",
      "Trained batch 465 batch loss 1.75020075 epoch total loss 1.75294411\n",
      "Trained batch 466 batch loss 1.85369074 epoch total loss 1.75316036\n",
      "Trained batch 467 batch loss 1.79713714 epoch total loss 1.75325441\n",
      "Trained batch 468 batch loss 1.82067597 epoch total loss 1.75339854\n",
      "Trained batch 469 batch loss 1.74836707 epoch total loss 1.75338781\n",
      "Trained batch 470 batch loss 1.70210719 epoch total loss 1.75327861\n",
      "Trained batch 471 batch loss 1.75476193 epoch total loss 1.75328183\n",
      "Trained batch 472 batch loss 1.80678964 epoch total loss 1.75339508\n",
      "Trained batch 473 batch loss 1.71633518 epoch total loss 1.75331664\n",
      "Trained batch 474 batch loss 1.68112171 epoch total loss 1.75316429\n",
      "Trained batch 475 batch loss 1.78150702 epoch total loss 1.7532239\n",
      "Trained batch 476 batch loss 1.77797556 epoch total loss 1.75327587\n",
      "Trained batch 477 batch loss 1.79755473 epoch total loss 1.75336874\n",
      "Trained batch 478 batch loss 1.76029217 epoch total loss 1.75338328\n",
      "Trained batch 479 batch loss 1.68336 epoch total loss 1.75323701\n",
      "Trained batch 480 batch loss 1.76039255 epoch total loss 1.75325191\n",
      "Trained batch 481 batch loss 1.70913255 epoch total loss 1.75316012\n",
      "Trained batch 482 batch loss 1.76389027 epoch total loss 1.75318241\n",
      "Trained batch 483 batch loss 1.79412723 epoch total loss 1.75326717\n",
      "Trained batch 484 batch loss 1.74341178 epoch total loss 1.75324678\n",
      "Trained batch 485 batch loss 1.7317555 epoch total loss 1.75320256\n",
      "Trained batch 486 batch loss 1.68192601 epoch total loss 1.75305593\n",
      "Trained batch 487 batch loss 1.63385653 epoch total loss 1.75281107\n",
      "Trained batch 488 batch loss 1.67263567 epoch total loss 1.7526468\n",
      "Trained batch 489 batch loss 1.79180586 epoch total loss 1.75272691\n",
      "Trained batch 490 batch loss 1.6182394 epoch total loss 1.75245237\n",
      "Trained batch 491 batch loss 1.68161261 epoch total loss 1.75230813\n",
      "Trained batch 492 batch loss 1.82475281 epoch total loss 1.75245547\n",
      "Trained batch 493 batch loss 1.78373206 epoch total loss 1.75251889\n",
      "Trained batch 494 batch loss 1.57043588 epoch total loss 1.7521503\n",
      "Trained batch 495 batch loss 1.63042855 epoch total loss 1.75190437\n",
      "Trained batch 496 batch loss 1.76759 epoch total loss 1.75193608\n",
      "Trained batch 497 batch loss 1.75666308 epoch total loss 1.7519455\n",
      "Trained batch 498 batch loss 1.83622885 epoch total loss 1.75211477\n",
      "Trained batch 499 batch loss 1.74585676 epoch total loss 1.75210226\n",
      "Trained batch 500 batch loss 1.59105039 epoch total loss 1.75178015\n",
      "Trained batch 501 batch loss 1.66348219 epoch total loss 1.75160384\n",
      "Trained batch 502 batch loss 1.79544818 epoch total loss 1.75169122\n",
      "Trained batch 503 batch loss 1.74580503 epoch total loss 1.75167954\n",
      "Trained batch 504 batch loss 1.8047899 epoch total loss 1.75178492\n",
      "Trained batch 505 batch loss 1.81956244 epoch total loss 1.75191915\n",
      "Trained batch 506 batch loss 1.82974207 epoch total loss 1.75207293\n",
      "Trained batch 507 batch loss 1.7536726 epoch total loss 1.75207603\n",
      "Trained batch 508 batch loss 1.76497984 epoch total loss 1.75210142\n",
      "Trained batch 509 batch loss 1.82529163 epoch total loss 1.75224519\n",
      "Trained batch 510 batch loss 1.83141708 epoch total loss 1.75240052\n",
      "Trained batch 511 batch loss 1.780267 epoch total loss 1.752455\n",
      "Trained batch 512 batch loss 1.80875576 epoch total loss 1.75256503\n",
      "Trained batch 513 batch loss 1.85074067 epoch total loss 1.75275648\n",
      "Trained batch 514 batch loss 1.84039307 epoch total loss 1.75292695\n",
      "Trained batch 515 batch loss 1.79895723 epoch total loss 1.75301635\n",
      "Trained batch 516 batch loss 1.80921698 epoch total loss 1.75312519\n",
      "Trained batch 517 batch loss 1.80827773 epoch total loss 1.75323188\n",
      "Trained batch 518 batch loss 1.82854867 epoch total loss 1.75337732\n",
      "Trained batch 519 batch loss 1.82281756 epoch total loss 1.75351107\n",
      "Trained batch 520 batch loss 1.85731912 epoch total loss 1.75371075\n",
      "Trained batch 521 batch loss 1.80552351 epoch total loss 1.75381017\n",
      "Trained batch 522 batch loss 1.74161303 epoch total loss 1.75378692\n",
      "Trained batch 523 batch loss 1.74339783 epoch total loss 1.75376701\n",
      "Trained batch 524 batch loss 1.75390828 epoch total loss 1.75376725\n",
      "Trained batch 525 batch loss 1.80150759 epoch total loss 1.75385821\n",
      "Trained batch 526 batch loss 1.81403112 epoch total loss 1.75397265\n",
      "Trained batch 527 batch loss 1.81507993 epoch total loss 1.75408852\n",
      "Trained batch 528 batch loss 1.78735363 epoch total loss 1.75415158\n",
      "Trained batch 529 batch loss 1.64508522 epoch total loss 1.75394535\n",
      "Trained batch 530 batch loss 1.55524421 epoch total loss 1.75357044\n",
      "Trained batch 531 batch loss 1.52507138 epoch total loss 1.75314009\n",
      "Trained batch 532 batch loss 1.61428642 epoch total loss 1.75287914\n",
      "Trained batch 533 batch loss 1.58768153 epoch total loss 1.7525692\n",
      "Trained batch 534 batch loss 1.51360881 epoch total loss 1.75212169\n",
      "Trained batch 535 batch loss 1.42922139 epoch total loss 1.75151813\n",
      "Trained batch 536 batch loss 1.39000821 epoch total loss 1.75084364\n",
      "Trained batch 537 batch loss 1.55555892 epoch total loss 1.75047994\n",
      "Trained batch 538 batch loss 1.85823834 epoch total loss 1.75068021\n",
      "Trained batch 539 batch loss 1.77987933 epoch total loss 1.75073445\n",
      "Trained batch 540 batch loss 1.74295914 epoch total loss 1.75072014\n",
      "Trained batch 541 batch loss 1.80540705 epoch total loss 1.75082123\n",
      "Trained batch 542 batch loss 1.76507306 epoch total loss 1.75084746\n",
      "Trained batch 543 batch loss 1.86472178 epoch total loss 1.75105727\n",
      "Trained batch 544 batch loss 1.86100125 epoch total loss 1.75125945\n",
      "Trained batch 545 batch loss 1.85831642 epoch total loss 1.7514559\n",
      "Trained batch 546 batch loss 1.68267548 epoch total loss 1.7513299\n",
      "Trained batch 547 batch loss 1.69815588 epoch total loss 1.75123274\n",
      "Trained batch 548 batch loss 1.71961832 epoch total loss 1.75117505\n",
      "Trained batch 549 batch loss 1.78611815 epoch total loss 1.7512387\n",
      "Trained batch 550 batch loss 1.80502963 epoch total loss 1.75133657\n",
      "Trained batch 551 batch loss 1.73357689 epoch total loss 1.75130439\n",
      "Trained batch 552 batch loss 1.77721059 epoch total loss 1.75135124\n",
      "Trained batch 553 batch loss 1.73680127 epoch total loss 1.75132501\n",
      "Trained batch 554 batch loss 1.72819126 epoch total loss 1.75128329\n",
      "Trained batch 555 batch loss 1.78529334 epoch total loss 1.75134456\n",
      "Trained batch 556 batch loss 1.80960417 epoch total loss 1.75144935\n",
      "Trained batch 557 batch loss 1.82707107 epoch total loss 1.75158513\n",
      "Trained batch 558 batch loss 1.82339168 epoch total loss 1.75171375\n",
      "Trained batch 559 batch loss 1.80254102 epoch total loss 1.75180471\n",
      "Trained batch 560 batch loss 1.76941037 epoch total loss 1.75183618\n",
      "Trained batch 561 batch loss 1.80859637 epoch total loss 1.75193739\n",
      "Trained batch 562 batch loss 1.76899195 epoch total loss 1.75196767\n",
      "Trained batch 563 batch loss 1.82453501 epoch total loss 1.75209653\n",
      "Trained batch 564 batch loss 1.81679201 epoch total loss 1.75221121\n",
      "Trained batch 565 batch loss 1.70696294 epoch total loss 1.7521311\n",
      "Trained batch 566 batch loss 1.70505643 epoch total loss 1.75204802\n",
      "Trained batch 567 batch loss 1.73891664 epoch total loss 1.75202477\n",
      "Trained batch 568 batch loss 1.7183311 epoch total loss 1.75196552\n",
      "Trained batch 569 batch loss 1.79788065 epoch total loss 1.75204611\n",
      "Trained batch 570 batch loss 1.83044493 epoch total loss 1.75218368\n",
      "Trained batch 571 batch loss 1.78644133 epoch total loss 1.75224364\n",
      "Trained batch 572 batch loss 1.75601351 epoch total loss 1.75225031\n",
      "Trained batch 573 batch loss 1.77121735 epoch total loss 1.75228345\n",
      "Trained batch 574 batch loss 1.67997217 epoch total loss 1.75215745\n",
      "Trained batch 575 batch loss 1.75971723 epoch total loss 1.75217068\n",
      "Trained batch 576 batch loss 1.53579473 epoch total loss 1.75179493\n",
      "Trained batch 577 batch loss 1.56265235 epoch total loss 1.75146711\n",
      "Trained batch 578 batch loss 1.59445047 epoch total loss 1.75119531\n",
      "Trained batch 579 batch loss 1.67051399 epoch total loss 1.75105608\n",
      "Trained batch 580 batch loss 1.65860462 epoch total loss 1.75089669\n",
      "Trained batch 581 batch loss 1.72716796 epoch total loss 1.7508558\n",
      "Trained batch 582 batch loss 1.78150845 epoch total loss 1.75090849\n",
      "Trained batch 583 batch loss 1.67792511 epoch total loss 1.75078332\n",
      "Trained batch 584 batch loss 1.52238214 epoch total loss 1.7503922\n",
      "Trained batch 585 batch loss 1.64890718 epoch total loss 1.75021875\n",
      "Trained batch 586 batch loss 1.63327873 epoch total loss 1.75001919\n",
      "Trained batch 587 batch loss 1.67620945 epoch total loss 1.74989355\n",
      "Trained batch 588 batch loss 1.87062752 epoch total loss 1.75009882\n",
      "Trained batch 589 batch loss 1.66605151 epoch total loss 1.74995601\n",
      "Trained batch 590 batch loss 1.70864034 epoch total loss 1.74988604\n",
      "Trained batch 591 batch loss 1.7722317 epoch total loss 1.74992383\n",
      "Trained batch 592 batch loss 1.81467772 epoch total loss 1.75003314\n",
      "Trained batch 593 batch loss 1.77425027 epoch total loss 1.75007415\n",
      "Trained batch 594 batch loss 1.78440118 epoch total loss 1.75013196\n",
      "Trained batch 595 batch loss 1.82226741 epoch total loss 1.7502532\n",
      "Trained batch 596 batch loss 1.67376339 epoch total loss 1.75012469\n",
      "Trained batch 597 batch loss 1.58430314 epoch total loss 1.74984705\n",
      "Trained batch 598 batch loss 1.74754477 epoch total loss 1.74984324\n",
      "Trained batch 599 batch loss 1.74789274 epoch total loss 1.74984\n",
      "Trained batch 600 batch loss 1.78760707 epoch total loss 1.74990296\n",
      "Trained batch 601 batch loss 1.8047899 epoch total loss 1.74999428\n",
      "Trained batch 602 batch loss 1.72033501 epoch total loss 1.74994504\n",
      "Trained batch 603 batch loss 1.72602296 epoch total loss 1.74990547\n",
      "Trained batch 604 batch loss 1.61979747 epoch total loss 1.74968994\n",
      "Trained batch 605 batch loss 1.76295745 epoch total loss 1.74971187\n",
      "Trained batch 606 batch loss 1.82852054 epoch total loss 1.74984193\n",
      "Trained batch 607 batch loss 1.71480584 epoch total loss 1.74978423\n",
      "Trained batch 608 batch loss 1.75105977 epoch total loss 1.74978638\n",
      "Trained batch 609 batch loss 1.70776689 epoch total loss 1.74971735\n",
      "Trained batch 610 batch loss 1.76253796 epoch total loss 1.74973845\n",
      "Trained batch 611 batch loss 1.72513235 epoch total loss 1.74969816\n",
      "Trained batch 612 batch loss 1.76725674 epoch total loss 1.74972677\n",
      "Trained batch 613 batch loss 1.80578494 epoch total loss 1.74981821\n",
      "Trained batch 614 batch loss 1.80204153 epoch total loss 1.7499032\n",
      "Trained batch 615 batch loss 1.77385306 epoch total loss 1.74994206\n",
      "Trained batch 616 batch loss 1.7950114 epoch total loss 1.75001526\n",
      "Trained batch 617 batch loss 1.73713136 epoch total loss 1.74999452\n",
      "Trained batch 618 batch loss 1.78270578 epoch total loss 1.75004745\n",
      "Trained batch 619 batch loss 1.83502638 epoch total loss 1.75018477\n",
      "Trained batch 620 batch loss 1.72297323 epoch total loss 1.75014102\n",
      "Trained batch 621 batch loss 1.79403973 epoch total loss 1.75021172\n",
      "Trained batch 622 batch loss 1.75728619 epoch total loss 1.75022316\n",
      "Trained batch 623 batch loss 1.82738256 epoch total loss 1.75034702\n",
      "Trained batch 624 batch loss 1.86810327 epoch total loss 1.75053585\n",
      "Trained batch 625 batch loss 1.88309097 epoch total loss 1.7507478\n",
      "Trained batch 626 batch loss 1.92206168 epoch total loss 1.75102162\n",
      "Trained batch 627 batch loss 1.89128017 epoch total loss 1.75124526\n",
      "Trained batch 628 batch loss 1.79120326 epoch total loss 1.75130892\n",
      "Trained batch 629 batch loss 1.8055495 epoch total loss 1.75139523\n",
      "Trained batch 630 batch loss 1.69857264 epoch total loss 1.75131142\n",
      "Trained batch 631 batch loss 1.68421865 epoch total loss 1.75120509\n",
      "Trained batch 632 batch loss 1.60711682 epoch total loss 1.75097716\n",
      "Trained batch 633 batch loss 1.61307681 epoch total loss 1.75075924\n",
      "Trained batch 634 batch loss 1.5684011 epoch total loss 1.75047147\n",
      "Trained batch 635 batch loss 1.80761123 epoch total loss 1.75056148\n",
      "Trained batch 636 batch loss 1.91731584 epoch total loss 1.75082374\n",
      "Trained batch 637 batch loss 1.86625183 epoch total loss 1.75100493\n",
      "Trained batch 638 batch loss 1.86723876 epoch total loss 1.75118709\n",
      "Trained batch 639 batch loss 1.84535956 epoch total loss 1.75133431\n",
      "Trained batch 640 batch loss 1.87705266 epoch total loss 1.75153089\n",
      "Trained batch 641 batch loss 1.80419743 epoch total loss 1.75161302\n",
      "Trained batch 642 batch loss 1.74954665 epoch total loss 1.75160968\n",
      "Trained batch 643 batch loss 1.79037654 epoch total loss 1.75167012\n",
      "Trained batch 644 batch loss 1.7730875 epoch total loss 1.75170326\n",
      "Trained batch 645 batch loss 1.80973721 epoch total loss 1.75179315\n",
      "Trained batch 646 batch loss 1.79397058 epoch total loss 1.75185847\n",
      "Trained batch 647 batch loss 1.82759714 epoch total loss 1.75197554\n",
      "Trained batch 648 batch loss 1.82342577 epoch total loss 1.75208592\n",
      "Trained batch 649 batch loss 1.80049479 epoch total loss 1.75216055\n",
      "Trained batch 650 batch loss 1.7500639 epoch total loss 1.75215745\n",
      "Trained batch 651 batch loss 1.71489501 epoch total loss 1.75210011\n",
      "Trained batch 652 batch loss 1.59650362 epoch total loss 1.75186157\n",
      "Trained batch 653 batch loss 1.65546227 epoch total loss 1.75171399\n",
      "Trained batch 654 batch loss 1.75977468 epoch total loss 1.75172639\n",
      "Trained batch 655 batch loss 1.73733044 epoch total loss 1.75170434\n",
      "Trained batch 656 batch loss 1.76642418 epoch total loss 1.75172687\n",
      "Trained batch 657 batch loss 1.73498201 epoch total loss 1.75170135\n",
      "Trained batch 658 batch loss 1.51021361 epoch total loss 1.75133443\n",
      "Trained batch 659 batch loss 1.7421577 epoch total loss 1.7513206\n",
      "Trained batch 660 batch loss 1.73296785 epoch total loss 1.75129271\n",
      "Trained batch 661 batch loss 1.81086397 epoch total loss 1.75138283\n",
      "Trained batch 662 batch loss 1.77995908 epoch total loss 1.75142598\n",
      "Trained batch 663 batch loss 1.68404281 epoch total loss 1.75132442\n",
      "Trained batch 664 batch loss 1.71059096 epoch total loss 1.75126302\n",
      "Trained batch 665 batch loss 1.6224618 epoch total loss 1.75106931\n",
      "Trained batch 666 batch loss 1.80393672 epoch total loss 1.7511487\n",
      "Trained batch 667 batch loss 1.76128149 epoch total loss 1.75116384\n",
      "Trained batch 668 batch loss 1.80935788 epoch total loss 1.75125086\n",
      "Trained batch 669 batch loss 1.75135684 epoch total loss 1.75125098\n",
      "Trained batch 670 batch loss 1.75865436 epoch total loss 1.75126207\n",
      "Trained batch 671 batch loss 1.78738725 epoch total loss 1.75131583\n",
      "Trained batch 672 batch loss 1.81580758 epoch total loss 1.7514118\n",
      "Trained batch 673 batch loss 1.82638562 epoch total loss 1.75152326\n",
      "Trained batch 674 batch loss 1.78581131 epoch total loss 1.75157404\n",
      "Trained batch 675 batch loss 1.69332528 epoch total loss 1.75148785\n",
      "Trained batch 676 batch loss 1.73526514 epoch total loss 1.75146377\n",
      "Trained batch 677 batch loss 1.78848135 epoch total loss 1.75151837\n",
      "Trained batch 678 batch loss 1.65144944 epoch total loss 1.75137091\n",
      "Trained batch 679 batch loss 1.68538976 epoch total loss 1.75127375\n",
      "Trained batch 680 batch loss 1.80859649 epoch total loss 1.75135803\n",
      "Trained batch 681 batch loss 1.78511798 epoch total loss 1.75140762\n",
      "Trained batch 682 batch loss 1.77951026 epoch total loss 1.75144887\n",
      "Trained batch 683 batch loss 1.84622133 epoch total loss 1.75158763\n",
      "Trained batch 684 batch loss 1.81261492 epoch total loss 1.75167692\n",
      "Trained batch 685 batch loss 1.86169982 epoch total loss 1.75183749\n",
      "Trained batch 686 batch loss 1.70006752 epoch total loss 1.75176203\n",
      "Trained batch 687 batch loss 1.74929357 epoch total loss 1.75175834\n",
      "Trained batch 688 batch loss 1.66878092 epoch total loss 1.75163782\n",
      "Trained batch 689 batch loss 1.73658705 epoch total loss 1.751616\n",
      "Trained batch 690 batch loss 1.75114822 epoch total loss 1.75161517\n",
      "Trained batch 691 batch loss 1.72531486 epoch total loss 1.75157714\n",
      "Trained batch 692 batch loss 1.78274512 epoch total loss 1.7516222\n",
      "Trained batch 693 batch loss 1.83703327 epoch total loss 1.75174546\n",
      "Trained batch 694 batch loss 1.86410248 epoch total loss 1.75190735\n",
      "Trained batch 695 batch loss 1.92231691 epoch total loss 1.75215268\n",
      "Trained batch 696 batch loss 1.85093486 epoch total loss 1.75229466\n",
      "Trained batch 697 batch loss 1.72020149 epoch total loss 1.75224853\n",
      "Trained batch 698 batch loss 1.77307713 epoch total loss 1.75227845\n",
      "Trained batch 699 batch loss 1.73656392 epoch total loss 1.75225592\n",
      "Trained batch 700 batch loss 1.66624856 epoch total loss 1.75213313\n",
      "Trained batch 701 batch loss 1.63605 epoch total loss 1.75196755\n",
      "Trained batch 702 batch loss 1.70579016 epoch total loss 1.75190187\n",
      "Trained batch 703 batch loss 1.68477869 epoch total loss 1.75180638\n",
      "Trained batch 704 batch loss 1.75373793 epoch total loss 1.75180924\n",
      "Trained batch 705 batch loss 1.86397958 epoch total loss 1.75196838\n",
      "Trained batch 706 batch loss 1.81256413 epoch total loss 1.75205421\n",
      "Trained batch 707 batch loss 1.80000305 epoch total loss 1.75212216\n",
      "Trained batch 708 batch loss 1.76683378 epoch total loss 1.75214291\n",
      "Trained batch 709 batch loss 1.81205916 epoch total loss 1.75222743\n",
      "Trained batch 710 batch loss 1.77400017 epoch total loss 1.75225818\n",
      "Trained batch 711 batch loss 1.74603796 epoch total loss 1.75224948\n",
      "Trained batch 712 batch loss 1.71678412 epoch total loss 1.75219965\n",
      "Trained batch 713 batch loss 1.76960731 epoch total loss 1.75222409\n",
      "Trained batch 714 batch loss 1.76392686 epoch total loss 1.75224054\n",
      "Trained batch 715 batch loss 1.66566849 epoch total loss 1.75211942\n",
      "Trained batch 716 batch loss 1.69900298 epoch total loss 1.75204515\n",
      "Trained batch 717 batch loss 1.74317837 epoch total loss 1.75203276\n",
      "Trained batch 718 batch loss 1.77706718 epoch total loss 1.75206769\n",
      "Trained batch 719 batch loss 1.71761584 epoch total loss 1.75201988\n",
      "Trained batch 720 batch loss 1.72841859 epoch total loss 1.75198698\n",
      "Trained batch 721 batch loss 1.7596488 epoch total loss 1.75199771\n",
      "Trained batch 722 batch loss 1.75663936 epoch total loss 1.75200403\n",
      "Trained batch 723 batch loss 1.71629524 epoch total loss 1.75195467\n",
      "Trained batch 724 batch loss 1.49569929 epoch total loss 1.75160074\n",
      "Trained batch 725 batch loss 1.58705747 epoch total loss 1.75137377\n",
      "Trained batch 726 batch loss 1.70689189 epoch total loss 1.75131249\n",
      "Trained batch 727 batch loss 1.71709287 epoch total loss 1.75126541\n",
      "Trained batch 728 batch loss 1.74115491 epoch total loss 1.75125158\n",
      "Trained batch 729 batch loss 1.82510626 epoch total loss 1.75135279\n",
      "Trained batch 730 batch loss 1.75039101 epoch total loss 1.75135148\n",
      "Trained batch 731 batch loss 1.76619124 epoch total loss 1.75137186\n",
      "Trained batch 732 batch loss 1.7252785 epoch total loss 1.7513361\n",
      "Trained batch 733 batch loss 1.70497108 epoch total loss 1.7512728\n",
      "Trained batch 734 batch loss 1.75246859 epoch total loss 1.75127447\n",
      "Trained batch 735 batch loss 1.68378127 epoch total loss 1.75118268\n",
      "Trained batch 736 batch loss 1.73146987 epoch total loss 1.75115585\n",
      "Trained batch 737 batch loss 1.81412172 epoch total loss 1.75124121\n",
      "Trained batch 738 batch loss 1.75827396 epoch total loss 1.75125086\n",
      "Trained batch 739 batch loss 1.80758822 epoch total loss 1.75132704\n",
      "Trained batch 740 batch loss 1.73385823 epoch total loss 1.75130355\n",
      "Trained batch 741 batch loss 1.66502976 epoch total loss 1.75118709\n",
      "Trained batch 742 batch loss 1.76326776 epoch total loss 1.75120342\n",
      "Trained batch 743 batch loss 1.7197628 epoch total loss 1.7511611\n",
      "Trained batch 744 batch loss 1.67082906 epoch total loss 1.75105298\n",
      "Trained batch 745 batch loss 1.6260736 epoch total loss 1.75088525\n",
      "Trained batch 746 batch loss 1.62582421 epoch total loss 1.75071764\n",
      "Trained batch 747 batch loss 1.8601079 epoch total loss 1.75086415\n",
      "Trained batch 748 batch loss 1.80844092 epoch total loss 1.75094116\n",
      "Trained batch 749 batch loss 1.85534871 epoch total loss 1.75108051\n",
      "Trained batch 750 batch loss 1.81949914 epoch total loss 1.75117171\n",
      "Trained batch 751 batch loss 1.86533809 epoch total loss 1.7513237\n",
      "Trained batch 752 batch loss 1.85409188 epoch total loss 1.75146043\n",
      "Trained batch 753 batch loss 1.78197992 epoch total loss 1.75150096\n",
      "Trained batch 754 batch loss 1.79423094 epoch total loss 1.75155759\n",
      "Trained batch 755 batch loss 1.78613365 epoch total loss 1.75160336\n",
      "Trained batch 756 batch loss 1.7976886 epoch total loss 1.7516644\n",
      "Trained batch 757 batch loss 1.84287739 epoch total loss 1.75178492\n",
      "Trained batch 758 batch loss 1.79787791 epoch total loss 1.75184572\n",
      "Trained batch 759 batch loss 1.81768346 epoch total loss 1.75193238\n",
      "Trained batch 760 batch loss 1.757954 epoch total loss 1.75194025\n",
      "Trained batch 761 batch loss 1.78374696 epoch total loss 1.75198197\n",
      "Trained batch 762 batch loss 1.81122541 epoch total loss 1.75205982\n",
      "Trained batch 763 batch loss 1.69277883 epoch total loss 1.75198209\n",
      "Trained batch 764 batch loss 1.69004023 epoch total loss 1.75190103\n",
      "Trained batch 765 batch loss 1.62781906 epoch total loss 1.75173879\n",
      "Trained batch 766 batch loss 1.66860247 epoch total loss 1.75163031\n",
      "Trained batch 767 batch loss 1.70628583 epoch total loss 1.75157118\n",
      "Trained batch 768 batch loss 1.66803551 epoch total loss 1.75146246\n",
      "Trained batch 769 batch loss 1.46287811 epoch total loss 1.75108719\n",
      "Trained batch 770 batch loss 1.64103985 epoch total loss 1.75094426\n",
      "Trained batch 771 batch loss 1.62973166 epoch total loss 1.75078702\n",
      "Trained batch 772 batch loss 1.62024593 epoch total loss 1.75061798\n",
      "Trained batch 773 batch loss 1.71862125 epoch total loss 1.75057662\n",
      "Trained batch 774 batch loss 1.78638887 epoch total loss 1.75062287\n",
      "Trained batch 775 batch loss 1.77353215 epoch total loss 1.75065243\n",
      "Trained batch 776 batch loss 1.73860812 epoch total loss 1.75063694\n",
      "Trained batch 777 batch loss 1.81383622 epoch total loss 1.75071824\n",
      "Trained batch 778 batch loss 1.82114339 epoch total loss 1.75080884\n",
      "Trained batch 779 batch loss 1.89264369 epoch total loss 1.75099099\n",
      "Trained batch 780 batch loss 1.80934858 epoch total loss 1.75106573\n",
      "Trained batch 781 batch loss 1.73063636 epoch total loss 1.75103951\n",
      "Trained batch 782 batch loss 1.74617195 epoch total loss 1.75103343\n",
      "Trained batch 783 batch loss 1.75915408 epoch total loss 1.7510438\n",
      "Trained batch 784 batch loss 1.70529032 epoch total loss 1.75098538\n",
      "Trained batch 785 batch loss 1.67529607 epoch total loss 1.75088906\n",
      "Trained batch 786 batch loss 1.6818583 epoch total loss 1.75080121\n",
      "Trained batch 787 batch loss 1.69871902 epoch total loss 1.75073504\n",
      "Trained batch 788 batch loss 1.65702021 epoch total loss 1.75061607\n",
      "Trained batch 789 batch loss 1.66688633 epoch total loss 1.75051\n",
      "Trained batch 790 batch loss 1.65748155 epoch total loss 1.7503922\n",
      "Trained batch 791 batch loss 1.57746804 epoch total loss 1.75017357\n",
      "Trained batch 792 batch loss 1.63114417 epoch total loss 1.75002325\n",
      "Trained batch 793 batch loss 1.75345898 epoch total loss 1.75002754\n",
      "Trained batch 794 batch loss 1.78731954 epoch total loss 1.75007451\n",
      "Trained batch 795 batch loss 1.77707684 epoch total loss 1.7501086\n",
      "Trained batch 796 batch loss 1.63704681 epoch total loss 1.74996662\n",
      "Trained batch 797 batch loss 1.67509699 epoch total loss 1.74987257\n",
      "Trained batch 798 batch loss 1.83366704 epoch total loss 1.74997747\n",
      "Trained batch 799 batch loss 1.7378962 epoch total loss 1.74996245\n",
      "Trained batch 800 batch loss 1.69851542 epoch total loss 1.74989808\n",
      "Trained batch 801 batch loss 1.72100389 epoch total loss 1.74986196\n",
      "Trained batch 802 batch loss 1.69156599 epoch total loss 1.74978924\n",
      "Trained batch 803 batch loss 1.8261019 epoch total loss 1.74988413\n",
      "Trained batch 804 batch loss 1.81634092 epoch total loss 1.74996674\n",
      "Trained batch 805 batch loss 1.7930758 epoch total loss 1.75002027\n",
      "Trained batch 806 batch loss 1.63948083 epoch total loss 1.74988317\n",
      "Trained batch 807 batch loss 1.79795444 epoch total loss 1.74994278\n",
      "Trained batch 808 batch loss 1.72546792 epoch total loss 1.7499125\n",
      "Trained batch 809 batch loss 1.7663672 epoch total loss 1.74993289\n",
      "Trained batch 810 batch loss 1.79972386 epoch total loss 1.74999428\n",
      "Trained batch 811 batch loss 1.6699115 epoch total loss 1.74989557\n",
      "Trained batch 812 batch loss 1.68249309 epoch total loss 1.74981248\n",
      "Trained batch 813 batch loss 1.77362752 epoch total loss 1.74984193\n",
      "Trained batch 814 batch loss 1.78467464 epoch total loss 1.74988472\n",
      "Trained batch 815 batch loss 1.79026127 epoch total loss 1.7499342\n",
      "Trained batch 816 batch loss 1.8059864 epoch total loss 1.75000298\n",
      "Trained batch 817 batch loss 1.7612375 epoch total loss 1.75001669\n",
      "Trained batch 818 batch loss 1.8109175 epoch total loss 1.7500912\n",
      "Trained batch 819 batch loss 1.77852666 epoch total loss 1.750126\n",
      "Trained batch 820 batch loss 1.72681439 epoch total loss 1.75009751\n",
      "Trained batch 821 batch loss 1.69016314 epoch total loss 1.75002456\n",
      "Trained batch 822 batch loss 1.62893307 epoch total loss 1.74987721\n",
      "Trained batch 823 batch loss 1.62206364 epoch total loss 1.74972188\n",
      "Trained batch 824 batch loss 1.70762014 epoch total loss 1.74967086\n",
      "Trained batch 825 batch loss 1.75334024 epoch total loss 1.74967527\n",
      "Trained batch 826 batch loss 1.7401216 epoch total loss 1.74966359\n",
      "Trained batch 827 batch loss 1.63773382 epoch total loss 1.74952829\n",
      "Trained batch 828 batch loss 1.6255455 epoch total loss 1.74937844\n",
      "Trained batch 829 batch loss 1.67490065 epoch total loss 1.74928868\n",
      "Trained batch 830 batch loss 1.76940608 epoch total loss 1.74931288\n",
      "Trained batch 831 batch loss 1.77304876 epoch total loss 1.74934149\n",
      "Trained batch 832 batch loss 1.57621217 epoch total loss 1.74913335\n",
      "Trained batch 833 batch loss 1.77323806 epoch total loss 1.7491622\n",
      "Trained batch 834 batch loss 1.68435812 epoch total loss 1.74908447\n",
      "Trained batch 835 batch loss 1.68595624 epoch total loss 1.74900877\n",
      "Trained batch 836 batch loss 1.66620255 epoch total loss 1.74890983\n",
      "Trained batch 837 batch loss 1.67323554 epoch total loss 1.74881935\n",
      "Trained batch 838 batch loss 1.54502654 epoch total loss 1.74857628\n",
      "Trained batch 839 batch loss 1.75203419 epoch total loss 1.74858046\n",
      "Trained batch 840 batch loss 1.70202422 epoch total loss 1.74852502\n",
      "Trained batch 841 batch loss 1.67726827 epoch total loss 1.74844027\n",
      "Trained batch 842 batch loss 1.70767152 epoch total loss 1.74839175\n",
      "Trained batch 843 batch loss 1.65977144 epoch total loss 1.74828672\n",
      "Trained batch 844 batch loss 1.63209295 epoch total loss 1.74814904\n",
      "Trained batch 845 batch loss 1.68813372 epoch total loss 1.74807799\n",
      "Trained batch 846 batch loss 1.65570641 epoch total loss 1.74796879\n",
      "Trained batch 847 batch loss 1.57892489 epoch total loss 1.74776936\n",
      "Trained batch 848 batch loss 1.69771504 epoch total loss 1.74771035\n",
      "Trained batch 849 batch loss 1.64195549 epoch total loss 1.74758577\n",
      "Trained batch 850 batch loss 1.52958333 epoch total loss 1.74732924\n",
      "Trained batch 851 batch loss 1.51311672 epoch total loss 1.74705398\n",
      "Trained batch 852 batch loss 1.53451467 epoch total loss 1.74680459\n",
      "Trained batch 853 batch loss 1.6743083 epoch total loss 1.7467196\n",
      "Trained batch 854 batch loss 1.68284917 epoch total loss 1.74664474\n",
      "Trained batch 855 batch loss 1.81444442 epoch total loss 1.74672413\n",
      "Trained batch 856 batch loss 1.80808198 epoch total loss 1.74679577\n",
      "Trained batch 857 batch loss 1.82720256 epoch total loss 1.74688959\n",
      "Trained batch 858 batch loss 1.79837894 epoch total loss 1.74694955\n",
      "Trained batch 859 batch loss 1.77623606 epoch total loss 1.74698365\n",
      "Trained batch 860 batch loss 1.64266896 epoch total loss 1.74686241\n",
      "Trained batch 861 batch loss 1.57671809 epoch total loss 1.74666464\n",
      "Trained batch 862 batch loss 1.70756876 epoch total loss 1.74661922\n",
      "Trained batch 863 batch loss 1.72843158 epoch total loss 1.74659812\n",
      "Trained batch 864 batch loss 1.76757658 epoch total loss 1.74662244\n",
      "Trained batch 865 batch loss 1.77794623 epoch total loss 1.74665868\n",
      "Trained batch 866 batch loss 1.76600611 epoch total loss 1.74668097\n",
      "Trained batch 867 batch loss 1.7676425 epoch total loss 1.74670517\n",
      "Trained batch 868 batch loss 1.75167406 epoch total loss 1.74671102\n",
      "Trained batch 869 batch loss 1.63960266 epoch total loss 1.74658775\n",
      "Trained batch 870 batch loss 1.63088417 epoch total loss 1.74645472\n",
      "Trained batch 871 batch loss 1.69660401 epoch total loss 1.74639761\n",
      "Trained batch 872 batch loss 1.76225972 epoch total loss 1.74641573\n",
      "Trained batch 873 batch loss 1.80221987 epoch total loss 1.74647963\n",
      "Trained batch 874 batch loss 1.74863422 epoch total loss 1.74648213\n",
      "Trained batch 875 batch loss 1.6816386 epoch total loss 1.7464081\n",
      "Trained batch 876 batch loss 1.65530777 epoch total loss 1.74630404\n",
      "Trained batch 877 batch loss 1.65676606 epoch total loss 1.74620187\n",
      "Trained batch 878 batch loss 1.7249496 epoch total loss 1.74617767\n",
      "Trained batch 879 batch loss 1.79353118 epoch total loss 1.74623168\n",
      "Trained batch 880 batch loss 1.77710354 epoch total loss 1.74626672\n",
      "Trained batch 881 batch loss 1.80815184 epoch total loss 1.74633694\n",
      "Trained batch 882 batch loss 1.81027472 epoch total loss 1.74640942\n",
      "Trained batch 883 batch loss 1.81593025 epoch total loss 1.74648821\n",
      "Trained batch 884 batch loss 1.80568981 epoch total loss 1.74655509\n",
      "Trained batch 885 batch loss 1.7867322 epoch total loss 1.74660051\n",
      "Trained batch 886 batch loss 1.76764011 epoch total loss 1.74662435\n",
      "Trained batch 887 batch loss 1.71993709 epoch total loss 1.74659431\n",
      "Trained batch 888 batch loss 1.76086485 epoch total loss 1.7466104\n",
      "Trained batch 889 batch loss 1.75378263 epoch total loss 1.74661839\n",
      "Trained batch 890 batch loss 1.73582566 epoch total loss 1.74660635\n",
      "Trained batch 891 batch loss 1.76834619 epoch total loss 1.74663067\n",
      "Trained batch 892 batch loss 1.59737921 epoch total loss 1.74646342\n",
      "Trained batch 893 batch loss 1.56865311 epoch total loss 1.74626422\n",
      "Trained batch 894 batch loss 1.59281301 epoch total loss 1.74609256\n",
      "Trained batch 895 batch loss 1.71379256 epoch total loss 1.74605644\n",
      "Trained batch 896 batch loss 1.50393057 epoch total loss 1.74578607\n",
      "Trained batch 897 batch loss 1.72099221 epoch total loss 1.74575841\n",
      "Trained batch 898 batch loss 1.71842337 epoch total loss 1.7457279\n",
      "Trained batch 899 batch loss 1.66672111 epoch total loss 1.74564\n",
      "Trained batch 900 batch loss 1.71876073 epoch total loss 1.74561024\n",
      "Trained batch 901 batch loss 1.75887036 epoch total loss 1.74562502\n",
      "Trained batch 902 batch loss 1.78638184 epoch total loss 1.7456702\n",
      "Trained batch 903 batch loss 1.81684756 epoch total loss 1.745749\n",
      "Trained batch 904 batch loss 1.56730878 epoch total loss 1.74555159\n",
      "Trained batch 905 batch loss 1.33850861 epoch total loss 1.74510181\n",
      "Trained batch 906 batch loss 1.4703604 epoch total loss 1.74479854\n",
      "Trained batch 907 batch loss 1.59041119 epoch total loss 1.74462831\n",
      "Trained batch 908 batch loss 1.80879223 epoch total loss 1.74469912\n",
      "Trained batch 909 batch loss 1.91666341 epoch total loss 1.74488819\n",
      "Trained batch 910 batch loss 1.85617578 epoch total loss 1.7450105\n",
      "Trained batch 911 batch loss 1.72519445 epoch total loss 1.7449888\n",
      "Trained batch 912 batch loss 1.7884016 epoch total loss 1.74503648\n",
      "Trained batch 913 batch loss 1.88443482 epoch total loss 1.74518907\n",
      "Trained batch 914 batch loss 1.77331519 epoch total loss 1.74522\n",
      "Trained batch 915 batch loss 1.85255301 epoch total loss 1.74533713\n",
      "Trained batch 916 batch loss 1.82434678 epoch total loss 1.74542344\n",
      "Trained batch 917 batch loss 1.86191678 epoch total loss 1.74555051\n",
      "Trained batch 918 batch loss 1.82188845 epoch total loss 1.7456336\n",
      "Trained batch 919 batch loss 1.7105428 epoch total loss 1.74559546\n",
      "Trained batch 920 batch loss 1.76370633 epoch total loss 1.74561512\n",
      "Trained batch 921 batch loss 1.73737645 epoch total loss 1.7456063\n",
      "Trained batch 922 batch loss 1.72512805 epoch total loss 1.74558401\n",
      "Trained batch 923 batch loss 1.74622035 epoch total loss 1.74558473\n",
      "Trained batch 924 batch loss 1.74356592 epoch total loss 1.74558246\n",
      "Trained batch 925 batch loss 1.74275064 epoch total loss 1.74557948\n",
      "Trained batch 926 batch loss 1.63380909 epoch total loss 1.74545872\n",
      "Trained batch 927 batch loss 1.70906353 epoch total loss 1.7454195\n",
      "Trained batch 928 batch loss 1.78355479 epoch total loss 1.74546063\n",
      "Trained batch 929 batch loss 1.78141117 epoch total loss 1.74549925\n",
      "Trained batch 930 batch loss 1.79941261 epoch total loss 1.74555731\n",
      "Trained batch 931 batch loss 1.8150053 epoch total loss 1.74563193\n",
      "Trained batch 932 batch loss 1.81361592 epoch total loss 1.74570489\n",
      "Trained batch 933 batch loss 1.68727291 epoch total loss 1.74564219\n",
      "Trained batch 934 batch loss 1.47216225 epoch total loss 1.74534941\n",
      "Trained batch 935 batch loss 1.70531797 epoch total loss 1.74530661\n",
      "Trained batch 936 batch loss 1.80473781 epoch total loss 1.74537\n",
      "Trained batch 937 batch loss 1.71449828 epoch total loss 1.74533713\n",
      "Trained batch 938 batch loss 1.75966251 epoch total loss 1.74535239\n",
      "Trained batch 939 batch loss 1.85836542 epoch total loss 1.74547279\n",
      "Trained batch 940 batch loss 1.83803499 epoch total loss 1.74557114\n",
      "Trained batch 941 batch loss 1.76280737 epoch total loss 1.74558949\n",
      "Trained batch 942 batch loss 1.83015347 epoch total loss 1.74567938\n",
      "Trained batch 943 batch loss 1.87698209 epoch total loss 1.7458185\n",
      "Trained batch 944 batch loss 1.83532846 epoch total loss 1.74591339\n",
      "Trained batch 945 batch loss 1.77026272 epoch total loss 1.74593914\n",
      "Trained batch 946 batch loss 1.73529077 epoch total loss 1.74592793\n",
      "Trained batch 947 batch loss 1.64579654 epoch total loss 1.74582219\n",
      "Trained batch 948 batch loss 1.60060954 epoch total loss 1.74566901\n",
      "Trained batch 949 batch loss 1.7039 epoch total loss 1.7456249\n",
      "Trained batch 950 batch loss 1.70253849 epoch total loss 1.74557948\n",
      "Trained batch 951 batch loss 1.67171514 epoch total loss 1.74550188\n",
      "Trained batch 952 batch loss 1.71419096 epoch total loss 1.74546909\n",
      "Trained batch 953 batch loss 1.71208131 epoch total loss 1.74543393\n",
      "Trained batch 954 batch loss 1.71195316 epoch total loss 1.74539876\n",
      "Trained batch 955 batch loss 1.66642952 epoch total loss 1.74531603\n",
      "Trained batch 956 batch loss 1.76667094 epoch total loss 1.74533844\n",
      "Trained batch 957 batch loss 1.69734979 epoch total loss 1.74528837\n",
      "Trained batch 958 batch loss 1.79913723 epoch total loss 1.74534464\n",
      "Trained batch 959 batch loss 1.73304176 epoch total loss 1.74533176\n",
      "Trained batch 960 batch loss 1.7775656 epoch total loss 1.74536538\n",
      "Trained batch 961 batch loss 1.71510339 epoch total loss 1.74533391\n",
      "Trained batch 962 batch loss 1.47334325 epoch total loss 1.74505115\n",
      "Trained batch 963 batch loss 1.39697838 epoch total loss 1.7446897\n",
      "Trained batch 964 batch loss 1.44599605 epoch total loss 1.74438\n",
      "Trained batch 965 batch loss 1.51075709 epoch total loss 1.74413788\n",
      "Trained batch 966 batch loss 1.65711653 epoch total loss 1.74404776\n",
      "Trained batch 967 batch loss 1.80932 epoch total loss 1.74411523\n",
      "Trained batch 968 batch loss 1.61506295 epoch total loss 1.74398196\n",
      "Trained batch 969 batch loss 1.72844052 epoch total loss 1.74396586\n",
      "Trained batch 970 batch loss 1.80336761 epoch total loss 1.74402714\n",
      "Trained batch 971 batch loss 1.60540783 epoch total loss 1.74388444\n",
      "Trained batch 972 batch loss 1.58754194 epoch total loss 1.74372351\n",
      "Trained batch 973 batch loss 1.65899718 epoch total loss 1.74363649\n",
      "Trained batch 974 batch loss 1.5803237 epoch total loss 1.74346888\n",
      "Trained batch 975 batch loss 1.64967966 epoch total loss 1.74337268\n",
      "Trained batch 976 batch loss 1.71566606 epoch total loss 1.74334431\n",
      "Trained batch 977 batch loss 1.82159281 epoch total loss 1.7434243\n",
      "Trained batch 978 batch loss 1.83389699 epoch total loss 1.7435168\n",
      "Trained batch 979 batch loss 1.81519675 epoch total loss 1.74359\n",
      "Trained batch 980 batch loss 1.80942965 epoch total loss 1.74365723\n",
      "Trained batch 981 batch loss 1.78515315 epoch total loss 1.74369955\n",
      "Trained batch 982 batch loss 1.77785516 epoch total loss 1.74373424\n",
      "Trained batch 983 batch loss 1.78748751 epoch total loss 1.74377871\n",
      "Trained batch 984 batch loss 1.78830695 epoch total loss 1.74382401\n",
      "Trained batch 985 batch loss 1.77142596 epoch total loss 1.74385214\n",
      "Trained batch 986 batch loss 1.8326664 epoch total loss 1.74394214\n",
      "Trained batch 987 batch loss 1.82066154 epoch total loss 1.74401987\n",
      "Trained batch 988 batch loss 1.83194 epoch total loss 1.74410892\n",
      "Trained batch 989 batch loss 1.8117094 epoch total loss 1.74417734\n",
      "Trained batch 990 batch loss 1.76554656 epoch total loss 1.7441988\n",
      "Trained batch 991 batch loss 1.68139064 epoch total loss 1.7441355\n",
      "Trained batch 992 batch loss 1.7803843 epoch total loss 1.74417198\n",
      "Trained batch 993 batch loss 1.76352358 epoch total loss 1.74419153\n",
      "Trained batch 994 batch loss 1.7991364 epoch total loss 1.74424684\n",
      "Trained batch 995 batch loss 1.85555077 epoch total loss 1.74435878\n",
      "Trained batch 996 batch loss 1.77141058 epoch total loss 1.74438584\n",
      "Trained batch 997 batch loss 1.72651148 epoch total loss 1.74436796\n",
      "Trained batch 998 batch loss 1.78255749 epoch total loss 1.74440634\n",
      "Trained batch 999 batch loss 1.80120075 epoch total loss 1.74446309\n",
      "Trained batch 1000 batch loss 1.80881667 epoch total loss 1.74452746\n",
      "Trained batch 1001 batch loss 1.81147051 epoch total loss 1.74459434\n",
      "Trained batch 1002 batch loss 1.82757199 epoch total loss 1.74467719\n",
      "Trained batch 1003 batch loss 1.72451365 epoch total loss 1.74465704\n",
      "Trained batch 1004 batch loss 1.80294096 epoch total loss 1.74471509\n",
      "Trained batch 1005 batch loss 1.68966818 epoch total loss 1.74466038\n",
      "Trained batch 1006 batch loss 1.6890204 epoch total loss 1.74460495\n",
      "Trained batch 1007 batch loss 1.68398261 epoch total loss 1.74454474\n",
      "Trained batch 1008 batch loss 1.66655123 epoch total loss 1.74446738\n",
      "Trained batch 1009 batch loss 1.73083782 epoch total loss 1.74445379\n",
      "Trained batch 1010 batch loss 1.65443373 epoch total loss 1.74436474\n",
      "Trained batch 1011 batch loss 1.69789052 epoch total loss 1.74431872\n",
      "Trained batch 1012 batch loss 1.68645334 epoch total loss 1.7442615\n",
      "Trained batch 1013 batch loss 1.71338987 epoch total loss 1.74423099\n",
      "Trained batch 1014 batch loss 1.61837637 epoch total loss 1.74410689\n",
      "Trained batch 1015 batch loss 1.69887662 epoch total loss 1.7440623\n",
      "Trained batch 1016 batch loss 1.68739462 epoch total loss 1.74400651\n",
      "Trained batch 1017 batch loss 1.70116901 epoch total loss 1.74396443\n",
      "Trained batch 1018 batch loss 1.71219182 epoch total loss 1.7439332\n",
      "Trained batch 1019 batch loss 1.73753428 epoch total loss 1.74392688\n",
      "Trained batch 1020 batch loss 1.63399911 epoch total loss 1.74381912\n",
      "Trained batch 1021 batch loss 1.61081719 epoch total loss 1.74368894\n",
      "Trained batch 1022 batch loss 1.74258077 epoch total loss 1.74368787\n",
      "Trained batch 1023 batch loss 1.77149546 epoch total loss 1.74371505\n",
      "Trained batch 1024 batch loss 1.72782707 epoch total loss 1.74369943\n",
      "Trained batch 1025 batch loss 1.68767667 epoch total loss 1.74364471\n",
      "Trained batch 1026 batch loss 1.65390277 epoch total loss 1.74355733\n",
      "Trained batch 1027 batch loss 1.65323675 epoch total loss 1.74346924\n",
      "Trained batch 1028 batch loss 1.64753461 epoch total loss 1.74337602\n",
      "Trained batch 1029 batch loss 1.64215183 epoch total loss 1.74327767\n",
      "Trained batch 1030 batch loss 1.68570209 epoch total loss 1.74322176\n",
      "Trained batch 1031 batch loss 1.62775648 epoch total loss 1.74310982\n",
      "Trained batch 1032 batch loss 1.61478639 epoch total loss 1.74298549\n",
      "Trained batch 1033 batch loss 1.48778069 epoch total loss 1.74273837\n",
      "Trained batch 1034 batch loss 1.68395984 epoch total loss 1.74268162\n",
      "Trained batch 1035 batch loss 1.65890872 epoch total loss 1.74260068\n",
      "Trained batch 1036 batch loss 1.66101944 epoch total loss 1.74252188\n",
      "Trained batch 1037 batch loss 1.73719192 epoch total loss 1.74251676\n",
      "Trained batch 1038 batch loss 1.72820807 epoch total loss 1.74250293\n",
      "Trained batch 1039 batch loss 1.76119232 epoch total loss 1.74252093\n",
      "Trained batch 1040 batch loss 1.76587546 epoch total loss 1.74254334\n",
      "Trained batch 1041 batch loss 1.78653216 epoch total loss 1.74258566\n",
      "Trained batch 1042 batch loss 1.80847538 epoch total loss 1.74264884\n",
      "Trained batch 1043 batch loss 2.15430832 epoch total loss 1.74304354\n",
      "Trained batch 1044 batch loss 2.03474712 epoch total loss 1.74332297\n",
      "Trained batch 1045 batch loss 1.54702723 epoch total loss 1.74313509\n",
      "Trained batch 1046 batch loss 1.70590937 epoch total loss 1.74309957\n",
      "Trained batch 1047 batch loss 2.00006723 epoch total loss 1.74334502\n",
      "Trained batch 1048 batch loss 1.77867436 epoch total loss 1.74337876\n",
      "Trained batch 1049 batch loss 1.87469435 epoch total loss 1.74350381\n",
      "Trained batch 1050 batch loss 2.04107809 epoch total loss 1.74378729\n",
      "Trained batch 1051 batch loss 1.97776508 epoch total loss 1.74401\n",
      "Trained batch 1052 batch loss 1.8743602 epoch total loss 1.74413395\n",
      "Trained batch 1053 batch loss 1.93399119 epoch total loss 1.74431419\n",
      "Trained batch 1054 batch loss 1.93349075 epoch total loss 1.7444936\n",
      "Trained batch 1055 batch loss 1.84951234 epoch total loss 1.74459314\n",
      "Trained batch 1056 batch loss 1.80159068 epoch total loss 1.74464715\n",
      "Trained batch 1057 batch loss 1.80631638 epoch total loss 1.74470544\n",
      "Trained batch 1058 batch loss 1.85259628 epoch total loss 1.74480736\n",
      "Trained batch 1059 batch loss 1.71564293 epoch total loss 1.74478\n",
      "Trained batch 1060 batch loss 1.72194147 epoch total loss 1.74475837\n",
      "Trained batch 1061 batch loss 1.7917397 epoch total loss 1.74480259\n",
      "Trained batch 1062 batch loss 1.7023226 epoch total loss 1.74476254\n",
      "Trained batch 1063 batch loss 1.68483067 epoch total loss 1.74470615\n",
      "Trained batch 1064 batch loss 1.65820503 epoch total loss 1.74462485\n",
      "Trained batch 1065 batch loss 1.73107517 epoch total loss 1.74461222\n",
      "Trained batch 1066 batch loss 1.75851667 epoch total loss 1.74462521\n",
      "Trained batch 1067 batch loss 1.82250929 epoch total loss 1.74469829\n",
      "Trained batch 1068 batch loss 1.84465063 epoch total loss 1.74479175\n",
      "Trained batch 1069 batch loss 1.85392106 epoch total loss 1.74489379\n",
      "Trained batch 1070 batch loss 1.76681232 epoch total loss 1.74491429\n",
      "Trained batch 1071 batch loss 1.57079923 epoch total loss 1.74475181\n",
      "Trained batch 1072 batch loss 1.60315061 epoch total loss 1.74461973\n",
      "Trained batch 1073 batch loss 1.71955323 epoch total loss 1.74459636\n",
      "Trained batch 1074 batch loss 1.73725605 epoch total loss 1.74458957\n",
      "Trained batch 1075 batch loss 1.7958256 epoch total loss 1.74463725\n",
      "Trained batch 1076 batch loss 1.81194329 epoch total loss 1.74469972\n",
      "Trained batch 1077 batch loss 1.79718745 epoch total loss 1.74474847\n",
      "Trained batch 1078 batch loss 1.80048227 epoch total loss 1.74480021\n",
      "Trained batch 1079 batch loss 1.78012288 epoch total loss 1.74483299\n",
      "Trained batch 1080 batch loss 1.80368733 epoch total loss 1.74488747\n",
      "Trained batch 1081 batch loss 1.7974633 epoch total loss 1.74493623\n",
      "Trained batch 1082 batch loss 1.77371204 epoch total loss 1.74496269\n",
      "Trained batch 1083 batch loss 1.7937119 epoch total loss 1.74500775\n",
      "Trained batch 1084 batch loss 1.7193563 epoch total loss 1.74498403\n",
      "Trained batch 1085 batch loss 1.76807714 epoch total loss 1.74500537\n",
      "Trained batch 1086 batch loss 1.71905398 epoch total loss 1.74498141\n",
      "Trained batch 1087 batch loss 1.68744361 epoch total loss 1.74492848\n",
      "Trained batch 1088 batch loss 1.76873589 epoch total loss 1.74495029\n",
      "Trained batch 1089 batch loss 1.74066281 epoch total loss 1.74494648\n",
      "Trained batch 1090 batch loss 1.73982906 epoch total loss 1.74494183\n",
      "Trained batch 1091 batch loss 1.73986685 epoch total loss 1.74493718\n",
      "Trained batch 1092 batch loss 1.72051048 epoch total loss 1.74491477\n",
      "Trained batch 1093 batch loss 1.6057446 epoch total loss 1.74478734\n",
      "Trained batch 1094 batch loss 1.64325964 epoch total loss 1.74469459\n",
      "Trained batch 1095 batch loss 1.73727071 epoch total loss 1.74468791\n",
      "Trained batch 1096 batch loss 1.80762482 epoch total loss 1.74474525\n",
      "Trained batch 1097 batch loss 1.75468171 epoch total loss 1.74475431\n",
      "Trained batch 1098 batch loss 1.78211069 epoch total loss 1.74478829\n",
      "Trained batch 1099 batch loss 1.78824949 epoch total loss 1.74482787\n",
      "Trained batch 1100 batch loss 1.62814116 epoch total loss 1.74472177\n",
      "Trained batch 1101 batch loss 1.75987828 epoch total loss 1.7447356\n",
      "Trained batch 1102 batch loss 1.78559566 epoch total loss 1.74477267\n",
      "Trained batch 1103 batch loss 1.69037795 epoch total loss 1.74472344\n",
      "Trained batch 1104 batch loss 1.62088203 epoch total loss 1.74461126\n",
      "Trained batch 1105 batch loss 1.70458412 epoch total loss 1.74457502\n",
      "Trained batch 1106 batch loss 1.70405149 epoch total loss 1.74453843\n",
      "Trained batch 1107 batch loss 1.71564651 epoch total loss 1.74451232\n",
      "Trained batch 1108 batch loss 1.72629905 epoch total loss 1.74449599\n",
      "Trained batch 1109 batch loss 1.79488051 epoch total loss 1.74454141\n",
      "Trained batch 1110 batch loss 1.63190007 epoch total loss 1.74444\n",
      "Trained batch 1111 batch loss 1.49717534 epoch total loss 1.7442174\n",
      "Trained batch 1112 batch loss 1.54511702 epoch total loss 1.74403846\n",
      "Trained batch 1113 batch loss 1.64999604 epoch total loss 1.74395394\n",
      "Trained batch 1114 batch loss 1.78569579 epoch total loss 1.74399137\n",
      "Trained batch 1115 batch loss 1.80785501 epoch total loss 1.74404871\n",
      "Trained batch 1116 batch loss 1.79164398 epoch total loss 1.74409127\n",
      "Trained batch 1117 batch loss 1.81337965 epoch total loss 1.74415326\n",
      "Trained batch 1118 batch loss 1.82134783 epoch total loss 1.74422228\n",
      "Trained batch 1119 batch loss 1.81613517 epoch total loss 1.74428654\n",
      "Trained batch 1120 batch loss 1.7766614 epoch total loss 1.74431551\n",
      "Trained batch 1121 batch loss 1.75242209 epoch total loss 1.74432266\n",
      "Trained batch 1122 batch loss 1.75934136 epoch total loss 1.74433613\n",
      "Trained batch 1123 batch loss 1.74212873 epoch total loss 1.74433422\n",
      "Trained batch 1124 batch loss 1.72448969 epoch total loss 1.74431658\n",
      "Trained batch 1125 batch loss 1.80050015 epoch total loss 1.74436653\n",
      "Trained batch 1126 batch loss 1.72043288 epoch total loss 1.74434531\n",
      "Trained batch 1127 batch loss 1.72725141 epoch total loss 1.74433017\n",
      "Trained batch 1128 batch loss 1.44188023 epoch total loss 1.74406207\n",
      "Trained batch 1129 batch loss 1.37675822 epoch total loss 1.74373662\n",
      "Trained batch 1130 batch loss 1.62867677 epoch total loss 1.74363482\n",
      "Trained batch 1131 batch loss 1.64982796 epoch total loss 1.74355185\n",
      "Trained batch 1132 batch loss 1.68382287 epoch total loss 1.74349916\n",
      "Trained batch 1133 batch loss 1.72517824 epoch total loss 1.74348295\n",
      "Trained batch 1134 batch loss 1.81806421 epoch total loss 1.74354875\n",
      "Trained batch 1135 batch loss 1.78719902 epoch total loss 1.74358726\n",
      "Trained batch 1136 batch loss 1.72192907 epoch total loss 1.74356818\n",
      "Trained batch 1137 batch loss 1.69247222 epoch total loss 1.74352324\n",
      "Trained batch 1138 batch loss 1.68131649 epoch total loss 1.74346864\n",
      "Trained batch 1139 batch loss 1.71217108 epoch total loss 1.7434411\n",
      "Trained batch 1140 batch loss 1.62938952 epoch total loss 1.74334109\n",
      "Trained batch 1141 batch loss 1.70048106 epoch total loss 1.74330342\n",
      "Trained batch 1142 batch loss 1.70915806 epoch total loss 1.7432735\n",
      "Trained batch 1143 batch loss 1.67211318 epoch total loss 1.74321127\n",
      "Trained batch 1144 batch loss 1.71711183 epoch total loss 1.7431885\n",
      "Trained batch 1145 batch loss 1.76785028 epoch total loss 1.74321\n",
      "Trained batch 1146 batch loss 1.77785492 epoch total loss 1.74324024\n",
      "Trained batch 1147 batch loss 1.57441258 epoch total loss 1.74309313\n",
      "Trained batch 1148 batch loss 1.5723815 epoch total loss 1.74294436\n",
      "Trained batch 1149 batch loss 1.53944802 epoch total loss 1.74276721\n",
      "Trained batch 1150 batch loss 1.47844052 epoch total loss 1.74253738\n",
      "Trained batch 1151 batch loss 1.59319067 epoch total loss 1.74240756\n",
      "Trained batch 1152 batch loss 1.66113973 epoch total loss 1.74233699\n",
      "Trained batch 1153 batch loss 1.71793389 epoch total loss 1.74231577\n",
      "Trained batch 1154 batch loss 1.7494446 epoch total loss 1.74232197\n",
      "Trained batch 1155 batch loss 1.73258722 epoch total loss 1.7423135\n",
      "Trained batch 1156 batch loss 1.7916373 epoch total loss 1.74235618\n",
      "Trained batch 1157 batch loss 1.7921766 epoch total loss 1.74239922\n",
      "Trained batch 1158 batch loss 1.78040874 epoch total loss 1.74243212\n",
      "Trained batch 1159 batch loss 1.76199961 epoch total loss 1.74244893\n",
      "Trained batch 1160 batch loss 1.80432832 epoch total loss 1.74250221\n",
      "Trained batch 1161 batch loss 1.69658279 epoch total loss 1.74246264\n",
      "Trained batch 1162 batch loss 1.71110797 epoch total loss 1.74243557\n",
      "Trained batch 1163 batch loss 1.70932007 epoch total loss 1.7424072\n",
      "Trained batch 1164 batch loss 1.66095304 epoch total loss 1.74233723\n",
      "Trained batch 1165 batch loss 1.72004402 epoch total loss 1.74231815\n",
      "Trained batch 1166 batch loss 1.79389477 epoch total loss 1.74236238\n",
      "Trained batch 1167 batch loss 1.72219253 epoch total loss 1.74234509\n",
      "Trained batch 1168 batch loss 1.64345193 epoch total loss 1.74226046\n",
      "Trained batch 1169 batch loss 1.61207163 epoch total loss 1.74214911\n",
      "Trained batch 1170 batch loss 1.64363921 epoch total loss 1.74206495\n",
      "Trained batch 1171 batch loss 1.68747866 epoch total loss 1.74201834\n",
      "Trained batch 1172 batch loss 1.71044207 epoch total loss 1.7419914\n",
      "Trained batch 1173 batch loss 1.7676661 epoch total loss 1.74201334\n",
      "Trained batch 1174 batch loss 1.73649335 epoch total loss 1.74200857\n",
      "Trained batch 1175 batch loss 1.79291666 epoch total loss 1.74205196\n",
      "Trained batch 1176 batch loss 1.77926934 epoch total loss 1.74208355\n",
      "Trained batch 1177 batch loss 1.78029633 epoch total loss 1.74211597\n",
      "Trained batch 1178 batch loss 1.79316866 epoch total loss 1.74215937\n",
      "Trained batch 1179 batch loss 1.81344378 epoch total loss 1.74221981\n",
      "Trained batch 1180 batch loss 1.77120113 epoch total loss 1.74224436\n",
      "Trained batch 1181 batch loss 1.78757977 epoch total loss 1.74228275\n",
      "Trained batch 1182 batch loss 1.77695096 epoch total loss 1.74231207\n",
      "Trained batch 1183 batch loss 1.78763366 epoch total loss 1.74235034\n",
      "Trained batch 1184 batch loss 1.76243186 epoch total loss 1.74236727\n",
      "Trained batch 1185 batch loss 1.72193658 epoch total loss 1.7423501\n",
      "Trained batch 1186 batch loss 1.7476331 epoch total loss 1.74235439\n",
      "Trained batch 1187 batch loss 1.66480017 epoch total loss 1.74228907\n",
      "Trained batch 1188 batch loss 1.78932691 epoch total loss 1.74232864\n",
      "Trained batch 1189 batch loss 1.76130521 epoch total loss 1.74234462\n",
      "Trained batch 1190 batch loss 1.73678815 epoch total loss 1.74234\n",
      "Trained batch 1191 batch loss 1.7366544 epoch total loss 1.74233508\n",
      "Trained batch 1192 batch loss 1.70627117 epoch total loss 1.7423048\n",
      "Trained batch 1193 batch loss 1.52462232 epoch total loss 1.74212241\n",
      "Trained batch 1194 batch loss 1.43009353 epoch total loss 1.7418611\n",
      "Trained batch 1195 batch loss 1.45206416 epoch total loss 1.74161875\n",
      "Trained batch 1196 batch loss 1.61276233 epoch total loss 1.74151099\n",
      "Trained batch 1197 batch loss 1.81010067 epoch total loss 1.74156833\n",
      "Trained batch 1198 batch loss 1.86276186 epoch total loss 1.74166942\n",
      "Trained batch 1199 batch loss 1.71048689 epoch total loss 1.74164343\n",
      "Trained batch 1200 batch loss 1.5949074 epoch total loss 1.74152124\n",
      "Trained batch 1201 batch loss 1.68847275 epoch total loss 1.74147701\n",
      "Trained batch 1202 batch loss 1.71044135 epoch total loss 1.74145126\n",
      "Trained batch 1203 batch loss 1.77429664 epoch total loss 1.74147856\n",
      "Trained batch 1204 batch loss 1.81978929 epoch total loss 1.74154365\n",
      "Trained batch 1205 batch loss 1.80471778 epoch total loss 1.7415961\n",
      "Trained batch 1206 batch loss 1.79988 epoch total loss 1.74164438\n",
      "Trained batch 1207 batch loss 1.82046664 epoch total loss 1.74170971\n",
      "Trained batch 1208 batch loss 1.74951267 epoch total loss 1.74171615\n",
      "Trained batch 1209 batch loss 1.79394329 epoch total loss 1.74175942\n",
      "Trained batch 1210 batch loss 1.66152096 epoch total loss 1.74169314\n",
      "Trained batch 1211 batch loss 1.78772271 epoch total loss 1.74173129\n",
      "Trained batch 1212 batch loss 1.80717027 epoch total loss 1.74178529\n",
      "Trained batch 1213 batch loss 1.78140843 epoch total loss 1.74181795\n",
      "Trained batch 1214 batch loss 1.60150599 epoch total loss 1.74170244\n",
      "Trained batch 1215 batch loss 1.62327933 epoch total loss 1.74160492\n",
      "Trained batch 1216 batch loss 1.66007543 epoch total loss 1.74153805\n",
      "Trained batch 1217 batch loss 1.76026881 epoch total loss 1.74155343\n",
      "Trained batch 1218 batch loss 1.61588633 epoch total loss 1.74145031\n",
      "Trained batch 1219 batch loss 1.72525692 epoch total loss 1.74143708\n",
      "Trained batch 1220 batch loss 1.75295615 epoch total loss 1.7414465\n",
      "Trained batch 1221 batch loss 1.68055642 epoch total loss 1.74139667\n",
      "Trained batch 1222 batch loss 1.69721067 epoch total loss 1.74136055\n",
      "Trained batch 1223 batch loss 1.68123567 epoch total loss 1.74131131\n",
      "Trained batch 1224 batch loss 1.63716698 epoch total loss 1.74122632\n",
      "Trained batch 1225 batch loss 1.68271971 epoch total loss 1.74117839\n",
      "Trained batch 1226 batch loss 1.70799899 epoch total loss 1.74115145\n",
      "Trained batch 1227 batch loss 1.76223183 epoch total loss 1.7411685\n",
      "Trained batch 1228 batch loss 1.72190022 epoch total loss 1.74115288\n",
      "Trained batch 1229 batch loss 1.7629838 epoch total loss 1.74117064\n",
      "Trained batch 1230 batch loss 1.76502585 epoch total loss 1.74119008\n",
      "Trained batch 1231 batch loss 1.77996588 epoch total loss 1.74122167\n",
      "Trained batch 1232 batch loss 1.69746482 epoch total loss 1.74118614\n",
      "Trained batch 1233 batch loss 1.66775799 epoch total loss 1.74112654\n",
      "Trained batch 1234 batch loss 1.72294366 epoch total loss 1.74111187\n",
      "Trained batch 1235 batch loss 1.67940295 epoch total loss 1.74106193\n",
      "Trained batch 1236 batch loss 1.67358255 epoch total loss 1.74100733\n",
      "Trained batch 1237 batch loss 1.61771619 epoch total loss 1.74090755\n",
      "Trained batch 1238 batch loss 1.6682148 epoch total loss 1.7408489\n",
      "Trained batch 1239 batch loss 1.59544897 epoch total loss 1.74073148\n",
      "Trained batch 1240 batch loss 1.64770234 epoch total loss 1.7406565\n",
      "Trained batch 1241 batch loss 1.60243142 epoch total loss 1.74054515\n",
      "Trained batch 1242 batch loss 1.58108437 epoch total loss 1.74041677\n",
      "Trained batch 1243 batch loss 1.68112469 epoch total loss 1.74036908\n",
      "Trained batch 1244 batch loss 1.72671688 epoch total loss 1.74035823\n",
      "Trained batch 1245 batch loss 1.6781987 epoch total loss 1.74030828\n",
      "Trained batch 1246 batch loss 1.70803726 epoch total loss 1.74028242\n",
      "Trained batch 1247 batch loss 1.72938335 epoch total loss 1.74027371\n",
      "Trained batch 1248 batch loss 1.75714278 epoch total loss 1.74028718\n",
      "Trained batch 1249 batch loss 1.56898975 epoch total loss 1.74015009\n",
      "Trained batch 1250 batch loss 1.74875927 epoch total loss 1.74015701\n",
      "Trained batch 1251 batch loss 1.71560872 epoch total loss 1.74013734\n",
      "Trained batch 1252 batch loss 1.75029552 epoch total loss 1.74014544\n",
      "Trained batch 1253 batch loss 1.75567675 epoch total loss 1.74015784\n",
      "Trained batch 1254 batch loss 1.75377703 epoch total loss 1.74016857\n",
      "Trained batch 1255 batch loss 1.71299374 epoch total loss 1.74014688\n",
      "Trained batch 1256 batch loss 1.65315342 epoch total loss 1.7400775\n",
      "Trained batch 1257 batch loss 1.56462669 epoch total loss 1.73993802\n",
      "Trained batch 1258 batch loss 1.8094728 epoch total loss 1.73999333\n",
      "Trained batch 1259 batch loss 1.78928852 epoch total loss 1.74003255\n",
      "Trained batch 1260 batch loss 1.79185426 epoch total loss 1.74007356\n",
      "Trained batch 1261 batch loss 1.80154812 epoch total loss 1.74012232\n",
      "Trained batch 1262 batch loss 1.78616166 epoch total loss 1.74015868\n",
      "Trained batch 1263 batch loss 1.69105625 epoch total loss 1.74011993\n",
      "Trained batch 1264 batch loss 1.7712791 epoch total loss 1.74014461\n",
      "Trained batch 1265 batch loss 1.59454453 epoch total loss 1.74002945\n",
      "Trained batch 1266 batch loss 1.70865488 epoch total loss 1.74000466\n",
      "Trained batch 1267 batch loss 1.64540207 epoch total loss 1.73993015\n",
      "Trained batch 1268 batch loss 1.68923521 epoch total loss 1.7398901\n",
      "Trained batch 1269 batch loss 1.74574661 epoch total loss 1.73989487\n",
      "Trained batch 1270 batch loss 1.6409688 epoch total loss 1.73981678\n",
      "Trained batch 1271 batch loss 1.69094968 epoch total loss 1.7397784\n",
      "Trained batch 1272 batch loss 1.61357379 epoch total loss 1.7396791\n",
      "Trained batch 1273 batch loss 1.64759469 epoch total loss 1.73960686\n",
      "Trained batch 1274 batch loss 1.69354701 epoch total loss 1.73957074\n",
      "Trained batch 1275 batch loss 1.69373918 epoch total loss 1.73953485\n",
      "Trained batch 1276 batch loss 1.64529574 epoch total loss 1.73946106\n",
      "Trained batch 1277 batch loss 1.6071173 epoch total loss 1.73935747\n",
      "Trained batch 1278 batch loss 1.58714557 epoch total loss 1.73923838\n",
      "Trained batch 1279 batch loss 1.6124202 epoch total loss 1.73913908\n",
      "Trained batch 1280 batch loss 1.68194008 epoch total loss 1.73909438\n",
      "Trained batch 1281 batch loss 1.63702703 epoch total loss 1.73901463\n",
      "Trained batch 1282 batch loss 1.78575861 epoch total loss 1.73905098\n",
      "Trained batch 1283 batch loss 1.7160573 epoch total loss 1.7390331\n",
      "Trained batch 1284 batch loss 1.76286614 epoch total loss 1.7390517\n",
      "Trained batch 1285 batch loss 1.77460539 epoch total loss 1.73907936\n",
      "Trained batch 1286 batch loss 1.66909766 epoch total loss 1.73902512\n",
      "Trained batch 1287 batch loss 1.78012955 epoch total loss 1.73905694\n",
      "Trained batch 1288 batch loss 1.77550054 epoch total loss 1.73908508\n",
      "Trained batch 1289 batch loss 1.74954355 epoch total loss 1.73909318\n",
      "Trained batch 1290 batch loss 1.72956812 epoch total loss 1.73908579\n",
      "Trained batch 1291 batch loss 1.635674 epoch total loss 1.73900568\n",
      "Trained batch 1292 batch loss 1.66135097 epoch total loss 1.7389456\n",
      "Trained batch 1293 batch loss 1.69424248 epoch total loss 1.73891115\n",
      "Trained batch 1294 batch loss 1.71562958 epoch total loss 1.73889315\n",
      "Trained batch 1295 batch loss 1.77109861 epoch total loss 1.73891795\n",
      "Trained batch 1296 batch loss 1.71203017 epoch total loss 1.73889709\n",
      "Trained batch 1297 batch loss 1.81979632 epoch total loss 1.73895943\n",
      "Trained batch 1298 batch loss 1.78082991 epoch total loss 1.73899162\n",
      "Trained batch 1299 batch loss 1.70047283 epoch total loss 1.73896194\n",
      "Trained batch 1300 batch loss 1.73894978 epoch total loss 1.73896205\n",
      "Trained batch 1301 batch loss 1.73764324 epoch total loss 1.73896098\n",
      "Trained batch 1302 batch loss 1.79477549 epoch total loss 1.73900378\n",
      "Trained batch 1303 batch loss 1.77350307 epoch total loss 1.73903012\n",
      "Trained batch 1304 batch loss 1.79693019 epoch total loss 1.73907447\n",
      "Trained batch 1305 batch loss 1.73088038 epoch total loss 1.73906827\n",
      "Trained batch 1306 batch loss 1.8423686 epoch total loss 1.73914731\n",
      "Trained batch 1307 batch loss 1.84565353 epoch total loss 1.73922884\n",
      "Trained batch 1308 batch loss 1.61337817 epoch total loss 1.73913252\n",
      "Trained batch 1309 batch loss 1.66334224 epoch total loss 1.73907471\n",
      "Trained batch 1310 batch loss 1.66008 epoch total loss 1.73901439\n",
      "Trained batch 1311 batch loss 1.49417377 epoch total loss 1.73882759\n",
      "Trained batch 1312 batch loss 1.49653435 epoch total loss 1.73864305\n",
      "Trained batch 1313 batch loss 1.57679462 epoch total loss 1.73851979\n",
      "Trained batch 1314 batch loss 1.47157979 epoch total loss 1.73831677\n",
      "Trained batch 1315 batch loss 1.38407016 epoch total loss 1.73804736\n",
      "Trained batch 1316 batch loss 1.4000268 epoch total loss 1.73779058\n",
      "Trained batch 1317 batch loss 1.62348056 epoch total loss 1.7377038\n",
      "Trained batch 1318 batch loss 1.56249905 epoch total loss 1.73757088\n",
      "Trained batch 1319 batch loss 1.77508545 epoch total loss 1.73759937\n",
      "Trained batch 1320 batch loss 1.81483674 epoch total loss 1.7376579\n",
      "Trained batch 1321 batch loss 1.84308493 epoch total loss 1.73773766\n",
      "Trained batch 1322 batch loss 1.84183061 epoch total loss 1.73781645\n",
      "Trained batch 1323 batch loss 1.78661799 epoch total loss 1.73785329\n",
      "Trained batch 1324 batch loss 1.80237818 epoch total loss 1.73790216\n",
      "Trained batch 1325 batch loss 1.70907509 epoch total loss 1.73788035\n",
      "Trained batch 1326 batch loss 1.51651454 epoch total loss 1.73771346\n",
      "Trained batch 1327 batch loss 1.37940967 epoch total loss 1.73744345\n",
      "Trained batch 1328 batch loss 1.68482399 epoch total loss 1.73740375\n",
      "Trained batch 1329 batch loss 1.71898234 epoch total loss 1.73738992\n",
      "Trained batch 1330 batch loss 1.73367393 epoch total loss 1.73738706\n",
      "Trained batch 1331 batch loss 1.67705917 epoch total loss 1.73734176\n",
      "Trained batch 1332 batch loss 1.74853337 epoch total loss 1.73735011\n",
      "Trained batch 1333 batch loss 1.77116132 epoch total loss 1.73737562\n",
      "Trained batch 1334 batch loss 1.65525937 epoch total loss 1.73731399\n",
      "Trained batch 1335 batch loss 1.63856781 epoch total loss 1.73724008\n",
      "Trained batch 1336 batch loss 1.7019155 epoch total loss 1.73721373\n",
      "Trained batch 1337 batch loss 1.67296672 epoch total loss 1.73716557\n",
      "Trained batch 1338 batch loss 1.68925953 epoch total loss 1.73712969\n",
      "Trained batch 1339 batch loss 1.67413449 epoch total loss 1.7370826\n",
      "Trained batch 1340 batch loss 1.73266435 epoch total loss 1.73707926\n",
      "Trained batch 1341 batch loss 1.73997116 epoch total loss 1.73708153\n",
      "Trained batch 1342 batch loss 1.72382236 epoch total loss 1.73707163\n",
      "Trained batch 1343 batch loss 1.76661575 epoch total loss 1.73709369\n",
      "Trained batch 1344 batch loss 1.76015389 epoch total loss 1.73711085\n",
      "Trained batch 1345 batch loss 1.74765229 epoch total loss 1.7371186\n",
      "Trained batch 1346 batch loss 1.72815788 epoch total loss 1.73711205\n",
      "Trained batch 1347 batch loss 1.59601927 epoch total loss 1.73700726\n",
      "Trained batch 1348 batch loss 1.71357405 epoch total loss 1.73698986\n",
      "Trained batch 1349 batch loss 1.72269249 epoch total loss 1.73697925\n",
      "Trained batch 1350 batch loss 1.76211965 epoch total loss 1.73699796\n",
      "Trained batch 1351 batch loss 1.65813088 epoch total loss 1.73693967\n",
      "Trained batch 1352 batch loss 1.73836899 epoch total loss 1.73694062\n",
      "Trained batch 1353 batch loss 1.5222913 epoch total loss 1.73678195\n",
      "Trained batch 1354 batch loss 1.58863711 epoch total loss 1.73667252\n",
      "Trained batch 1355 batch loss 1.7808795 epoch total loss 1.73670506\n",
      "Trained batch 1356 batch loss 1.72992635 epoch total loss 1.7367\n",
      "Trained batch 1357 batch loss 1.76489222 epoch total loss 1.73672092\n",
      "Trained batch 1358 batch loss 1.78920531 epoch total loss 1.73675954\n",
      "Trained batch 1359 batch loss 1.86264193 epoch total loss 1.73685217\n",
      "Trained batch 1360 batch loss 1.83298135 epoch total loss 1.73692286\n",
      "Trained batch 1361 batch loss 1.76676798 epoch total loss 1.73694479\n",
      "Trained batch 1362 batch loss 1.6614219 epoch total loss 1.73688936\n",
      "Trained batch 1363 batch loss 1.55016243 epoch total loss 1.73675227\n",
      "Trained batch 1364 batch loss 1.41236186 epoch total loss 1.73651445\n",
      "Trained batch 1365 batch loss 1.47086513 epoch total loss 1.7363199\n",
      "Trained batch 1366 batch loss 1.57314467 epoch total loss 1.73620057\n",
      "Trained batch 1367 batch loss 1.6870352 epoch total loss 1.73616457\n",
      "Trained batch 1368 batch loss 1.63320732 epoch total loss 1.73608935\n",
      "Trained batch 1369 batch loss 1.71162128 epoch total loss 1.73607147\n",
      "Trained batch 1370 batch loss 1.52593756 epoch total loss 1.73591805\n",
      "Trained batch 1371 batch loss 1.58638287 epoch total loss 1.73580909\n",
      "Trained batch 1372 batch loss 1.71165979 epoch total loss 1.73579144\n",
      "Trained batch 1373 batch loss 1.63132334 epoch total loss 1.73571539\n",
      "Trained batch 1374 batch loss 1.46709192 epoch total loss 1.73551989\n",
      "Trained batch 1375 batch loss 1.49087143 epoch total loss 1.73534203\n",
      "Trained batch 1376 batch loss 1.46170676 epoch total loss 1.73514307\n",
      "Trained batch 1377 batch loss 1.65765655 epoch total loss 1.7350868\n",
      "Trained batch 1378 batch loss 1.53719926 epoch total loss 1.73494315\n",
      "Trained batch 1379 batch loss 1.53103161 epoch total loss 1.73479533\n",
      "Trained batch 1380 batch loss 1.69370532 epoch total loss 1.73476541\n",
      "Trained batch 1381 batch loss 1.64917374 epoch total loss 1.73470342\n",
      "Trained batch 1382 batch loss 1.68692875 epoch total loss 1.73466897\n",
      "Trained batch 1383 batch loss 1.59195757 epoch total loss 1.73456585\n",
      "Trained batch 1384 batch loss 1.66663289 epoch total loss 1.73451686\n",
      "Trained batch 1385 batch loss 1.64948738 epoch total loss 1.73445535\n",
      "Trained batch 1386 batch loss 1.7627281 epoch total loss 1.73447573\n",
      "Trained batch 1387 batch loss 1.84109664 epoch total loss 1.73455262\n",
      "Trained batch 1388 batch loss 1.80529952 epoch total loss 1.73460364\n",
      "Epoch 2 train loss 1.7346036434173584\n",
      "Validated batch 1 batch loss 1.79068804\n",
      "Validated batch 2 batch loss 1.78273678\n",
      "Validated batch 3 batch loss 1.68866706\n",
      "Validated batch 4 batch loss 1.57881212\n",
      "Validated batch 5 batch loss 1.70807528\n",
      "Validated batch 6 batch loss 1.71303892\n",
      "Validated batch 7 batch loss 1.63528967\n",
      "Validated batch 8 batch loss 1.6469506\n",
      "Validated batch 9 batch loss 1.68993795\n",
      "Validated batch 10 batch loss 1.73624396\n",
      "Validated batch 11 batch loss 1.67135274\n",
      "Validated batch 12 batch loss 1.61891079\n",
      "Validated batch 13 batch loss 1.68209755\n",
      "Validated batch 14 batch loss 1.6698103\n",
      "Validated batch 15 batch loss 1.74041855\n",
      "Validated batch 16 batch loss 1.70376015\n",
      "Validated batch 17 batch loss 1.73537624\n",
      "Validated batch 18 batch loss 1.75258636\n",
      "Validated batch 19 batch loss 1.73137307\n",
      "Validated batch 20 batch loss 1.62674081\n",
      "Validated batch 21 batch loss 1.77679586\n",
      "Validated batch 22 batch loss 1.51072657\n",
      "Validated batch 23 batch loss 1.83784676\n",
      "Validated batch 24 batch loss 1.69547617\n",
      "Validated batch 25 batch loss 1.62355042\n",
      "Validated batch 26 batch loss 1.74003148\n",
      "Validated batch 27 batch loss 1.76170397\n",
      "Validated batch 28 batch loss 1.70644212\n",
      "Validated batch 29 batch loss 1.74407578\n",
      "Validated batch 30 batch loss 1.74371493\n",
      "Validated batch 31 batch loss 1.79849851\n",
      "Validated batch 32 batch loss 1.7180469\n",
      "Validated batch 33 batch loss 1.75664592\n",
      "Validated batch 34 batch loss 1.67769361\n",
      "Validated batch 35 batch loss 1.70799196\n",
      "Validated batch 36 batch loss 1.71554565\n",
      "Validated batch 37 batch loss 1.77935719\n",
      "Validated batch 38 batch loss 1.75714529\n",
      "Validated batch 39 batch loss 1.68911946\n",
      "Validated batch 40 batch loss 1.79007626\n",
      "Validated batch 41 batch loss 1.60193276\n",
      "Validated batch 42 batch loss 1.78093457\n",
      "Validated batch 43 batch loss 1.80158401\n",
      "Validated batch 44 batch loss 1.7464776\n",
      "Validated batch 45 batch loss 1.79504609\n",
      "Validated batch 46 batch loss 1.68358636\n",
      "Validated batch 47 batch loss 1.80235553\n",
      "Validated batch 48 batch loss 1.72191834\n",
      "Validated batch 49 batch loss 1.69760382\n",
      "Validated batch 50 batch loss 1.66919494\n",
      "Validated batch 51 batch loss 1.64529097\n",
      "Validated batch 52 batch loss 1.73052943\n",
      "Validated batch 53 batch loss 1.69899178\n",
      "Validated batch 54 batch loss 1.76334143\n",
      "Validated batch 55 batch loss 1.79658043\n",
      "Validated batch 56 batch loss 1.72631788\n",
      "Validated batch 57 batch loss 1.71606207\n",
      "Validated batch 58 batch loss 1.62396085\n",
      "Validated batch 59 batch loss 1.76249075\n",
      "Validated batch 60 batch loss 1.69854522\n",
      "Validated batch 61 batch loss 1.78686893\n",
      "Validated batch 62 batch loss 1.69738936\n",
      "Validated batch 63 batch loss 1.705181\n",
      "Validated batch 64 batch loss 1.56978011\n",
      "Validated batch 65 batch loss 1.67085695\n",
      "Validated batch 66 batch loss 1.75931478\n",
      "Validated batch 67 batch loss 1.65796185\n",
      "Validated batch 68 batch loss 1.67629969\n",
      "Validated batch 69 batch loss 1.69249415\n",
      "Validated batch 70 batch loss 1.60838282\n",
      "Validated batch 71 batch loss 1.82078278\n",
      "Validated batch 72 batch loss 1.74016953\n",
      "Validated batch 73 batch loss 1.77657795\n",
      "Validated batch 74 batch loss 1.71471763\n",
      "Validated batch 75 batch loss 1.80193019\n",
      "Validated batch 76 batch loss 1.69513297\n",
      "Validated batch 77 batch loss 1.83810604\n",
      "Validated batch 78 batch loss 1.69168901\n",
      "Validated batch 79 batch loss 1.70766985\n",
      "Validated batch 80 batch loss 1.71125925\n",
      "Validated batch 81 batch loss 1.65226483\n",
      "Validated batch 82 batch loss 1.54250753\n",
      "Validated batch 83 batch loss 1.7969594\n",
      "Validated batch 84 batch loss 1.78582549\n",
      "Validated batch 85 batch loss 1.68282533\n",
      "Validated batch 86 batch loss 1.77177095\n",
      "Validated batch 87 batch loss 1.78489566\n",
      "Validated batch 88 batch loss 1.78146482\n",
      "Validated batch 89 batch loss 1.83228731\n",
      "Validated batch 90 batch loss 1.8044939\n",
      "Validated batch 91 batch loss 1.72568727\n",
      "Validated batch 92 batch loss 1.63803458\n",
      "Validated batch 93 batch loss 1.80850065\n",
      "Validated batch 94 batch loss 1.75515378\n",
      "Validated batch 95 batch loss 1.76931405\n",
      "Validated batch 96 batch loss 1.73765016\n",
      "Validated batch 97 batch loss 1.7655406\n",
      "Validated batch 98 batch loss 1.81953275\n",
      "Validated batch 99 batch loss 1.72730052\n",
      "Validated batch 100 batch loss 1.66880763\n",
      "Validated batch 101 batch loss 1.68874407\n",
      "Validated batch 102 batch loss 1.70534337\n",
      "Validated batch 103 batch loss 1.75539851\n",
      "Validated batch 104 batch loss 1.72890449\n",
      "Validated batch 105 batch loss 1.64491487\n",
      "Validated batch 106 batch loss 1.6428504\n",
      "Validated batch 107 batch loss 1.69562531\n",
      "Validated batch 108 batch loss 1.7898227\n",
      "Validated batch 109 batch loss 1.8043263\n",
      "Validated batch 110 batch loss 1.74667323\n",
      "Validated batch 111 batch loss 1.84483969\n",
      "Validated batch 112 batch loss 1.90552413\n",
      "Validated batch 113 batch loss 1.8429141\n",
      "Validated batch 114 batch loss 1.69858694\n",
      "Validated batch 115 batch loss 1.64697242\n",
      "Validated batch 116 batch loss 1.75777984\n",
      "Validated batch 117 batch loss 1.66401744\n",
      "Validated batch 118 batch loss 1.66863513\n",
      "Validated batch 119 batch loss 1.67471361\n",
      "Validated batch 120 batch loss 1.70207882\n",
      "Validated batch 121 batch loss 1.74633408\n",
      "Validated batch 122 batch loss 1.70346153\n",
      "Validated batch 123 batch loss 1.7739625\n",
      "Validated batch 124 batch loss 1.6909554\n",
      "Validated batch 125 batch loss 1.78499818\n",
      "Validated batch 126 batch loss 1.8429749\n",
      "Validated batch 127 batch loss 1.77408099\n",
      "Validated batch 128 batch loss 1.70492697\n",
      "Validated batch 129 batch loss 1.70792055\n",
      "Validated batch 130 batch loss 1.77504444\n",
      "Validated batch 131 batch loss 1.69352043\n",
      "Validated batch 132 batch loss 1.78368282\n",
      "Validated batch 133 batch loss 1.66804862\n",
      "Validated batch 134 batch loss 1.76602292\n",
      "Validated batch 135 batch loss 1.67690229\n",
      "Validated batch 136 batch loss 1.69342077\n",
      "Validated batch 137 batch loss 1.7293179\n",
      "Validated batch 138 batch loss 1.75628555\n",
      "Validated batch 139 batch loss 1.7269851\n",
      "Validated batch 140 batch loss 1.70985484\n",
      "Validated batch 141 batch loss 1.65922761\n",
      "Validated batch 142 batch loss 1.55772245\n",
      "Validated batch 143 batch loss 1.69614613\n",
      "Validated batch 144 batch loss 1.77327383\n",
      "Validated batch 145 batch loss 1.63626742\n",
      "Validated batch 146 batch loss 1.77073944\n",
      "Validated batch 147 batch loss 1.77758169\n",
      "Validated batch 148 batch loss 1.7683754\n",
      "Validated batch 149 batch loss 1.8088057\n",
      "Validated batch 150 batch loss 1.73794901\n",
      "Validated batch 151 batch loss 1.64858937\n",
      "Validated batch 152 batch loss 1.73957598\n",
      "Validated batch 153 batch loss 1.75473404\n",
      "Validated batch 154 batch loss 1.80693376\n",
      "Validated batch 155 batch loss 1.72256231\n",
      "Validated batch 156 batch loss 1.76725268\n",
      "Validated batch 157 batch loss 1.78404021\n",
      "Validated batch 158 batch loss 1.70831299\n",
      "Validated batch 159 batch loss 1.76720202\n",
      "Validated batch 160 batch loss 1.77153182\n",
      "Validated batch 161 batch loss 1.77441597\n",
      "Validated batch 162 batch loss 1.73462951\n",
      "Validated batch 163 batch loss 1.72354412\n",
      "Validated batch 164 batch loss 1.74162519\n",
      "Validated batch 165 batch loss 1.72917545\n",
      "Validated batch 166 batch loss 1.85383034\n",
      "Validated batch 167 batch loss 1.8056401\n",
      "Validated batch 168 batch loss 1.77807915\n",
      "Validated batch 169 batch loss 1.72179782\n",
      "Validated batch 170 batch loss 1.79182112\n",
      "Validated batch 171 batch loss 1.78264761\n",
      "Validated batch 172 batch loss 1.8287344\n",
      "Validated batch 173 batch loss 1.82691383\n",
      "Validated batch 174 batch loss 1.60547948\n",
      "Validated batch 175 batch loss 1.77602971\n",
      "Validated batch 176 batch loss 1.76213443\n",
      "Validated batch 177 batch loss 1.71062708\n",
      "Validated batch 178 batch loss 1.72126245\n",
      "Validated batch 179 batch loss 1.62579215\n",
      "Validated batch 180 batch loss 1.61459875\n",
      "Validated batch 181 batch loss 1.81234193\n",
      "Validated batch 182 batch loss 1.73472238\n",
      "Validated batch 183 batch loss 1.7973578\n",
      "Validated batch 184 batch loss 1.74762833\n",
      "Validated batch 185 batch loss 1.79041076\n",
      "Epoch 2 val loss 1.7288371324539185\n",
      "Model ./model_hourglass-epoch-2-loss-1.7288.h5 saved.\n",
      "Start epoch 3 with learning rate 0.5\n",
      "Start distributed traininng...\n",
      "Trained batch 1 batch loss 1.68381977 epoch total loss 1.68381977\n",
      "Trained batch 2 batch loss 1.73782015 epoch total loss 1.71082\n",
      "Trained batch 3 batch loss 1.74871981 epoch total loss 1.72345316\n",
      "Trained batch 4 batch loss 1.68441188 epoch total loss 1.7136929\n",
      "Trained batch 5 batch loss 1.44970834 epoch total loss 1.66089594\n",
      "Trained batch 6 batch loss 1.53997803 epoch total loss 1.6407429\n",
      "Trained batch 7 batch loss 1.64911687 epoch total loss 1.64193916\n",
      "Trained batch 8 batch loss 1.82712936 epoch total loss 1.66508794\n",
      "Trained batch 9 batch loss 1.77746511 epoch total loss 1.67757428\n",
      "Trained batch 10 batch loss 1.76857352 epoch total loss 1.68667412\n",
      "Trained batch 11 batch loss 1.78285873 epoch total loss 1.69541812\n",
      "Trained batch 12 batch loss 1.82648516 epoch total loss 1.70634031\n",
      "Trained batch 13 batch loss 1.78403986 epoch total loss 1.71231723\n",
      "Trained batch 14 batch loss 1.78411567 epoch total loss 1.71744561\n",
      "Trained batch 15 batch loss 1.73122239 epoch total loss 1.71836412\n",
      "Trained batch 16 batch loss 1.71773 epoch total loss 1.71832442\n",
      "Trained batch 17 batch loss 1.71411443 epoch total loss 1.71807683\n",
      "Trained batch 18 batch loss 1.79620147 epoch total loss 1.72241712\n",
      "Trained batch 19 batch loss 1.73215866 epoch total loss 1.72292972\n",
      "Trained batch 20 batch loss 1.75526702 epoch total loss 1.72454667\n",
      "Trained batch 21 batch loss 1.67805767 epoch total loss 1.72233295\n",
      "Trained batch 22 batch loss 1.77188087 epoch total loss 1.72458506\n",
      "Trained batch 23 batch loss 1.68687129 epoch total loss 1.72294533\n",
      "Trained batch 24 batch loss 1.70855403 epoch total loss 1.72234571\n",
      "Trained batch 25 batch loss 1.70308876 epoch total loss 1.72157526\n",
      "Trained batch 26 batch loss 1.66137016 epoch total loss 1.71925974\n",
      "Trained batch 27 batch loss 1.52256596 epoch total loss 1.71197486\n",
      "Trained batch 28 batch loss 1.71435 epoch total loss 1.71205974\n",
      "Trained batch 29 batch loss 1.59611368 epoch total loss 1.70806158\n",
      "Trained batch 30 batch loss 1.63328481 epoch total loss 1.70556903\n",
      "Trained batch 31 batch loss 1.68005371 epoch total loss 1.70474601\n",
      "Trained batch 32 batch loss 1.5410229 epoch total loss 1.69962966\n",
      "Trained batch 33 batch loss 1.55333102 epoch total loss 1.69519627\n",
      "Trained batch 34 batch loss 1.51700914 epoch total loss 1.68995559\n",
      "Trained batch 35 batch loss 1.63702297 epoch total loss 1.68844318\n",
      "Trained batch 36 batch loss 1.60111046 epoch total loss 1.68601739\n",
      "Trained batch 37 batch loss 1.67917931 epoch total loss 1.68583262\n",
      "Trained batch 38 batch loss 1.68483043 epoch total loss 1.68580627\n",
      "Trained batch 39 batch loss 1.74887347 epoch total loss 1.68742335\n",
      "Trained batch 40 batch loss 1.78160906 epoch total loss 1.68977797\n",
      "Trained batch 41 batch loss 1.7788378 epoch total loss 1.6919502\n",
      "Trained batch 42 batch loss 1.76584208 epoch total loss 1.69370937\n",
      "Trained batch 43 batch loss 1.70823109 epoch total loss 1.69404709\n",
      "Trained batch 44 batch loss 1.66106677 epoch total loss 1.69329751\n",
      "Trained batch 45 batch loss 1.70660138 epoch total loss 1.69359314\n",
      "Trained batch 46 batch loss 1.69869387 epoch total loss 1.69370401\n",
      "Trained batch 47 batch loss 1.7668674 epoch total loss 1.69526076\n",
      "Trained batch 48 batch loss 1.80195975 epoch total loss 1.69748354\n",
      "Trained batch 49 batch loss 1.75930691 epoch total loss 1.69874525\n",
      "Trained batch 50 batch loss 1.74870479 epoch total loss 1.69974446\n",
      "Trained batch 51 batch loss 1.75323391 epoch total loss 1.70079327\n",
      "Trained batch 52 batch loss 1.74002373 epoch total loss 1.70154762\n",
      "Trained batch 53 batch loss 1.69642842 epoch total loss 1.70145094\n",
      "Trained batch 54 batch loss 1.7873075 epoch total loss 1.70304096\n",
      "Trained batch 55 batch loss 1.68293488 epoch total loss 1.70267546\n",
      "Trained batch 56 batch loss 1.69087911 epoch total loss 1.70246482\n",
      "Trained batch 57 batch loss 1.6611557 epoch total loss 1.70174\n",
      "Trained batch 58 batch loss 1.64252377 epoch total loss 1.70071912\n",
      "Trained batch 59 batch loss 1.56779695 epoch total loss 1.69846618\n",
      "Trained batch 60 batch loss 1.72074473 epoch total loss 1.6988374\n",
      "Trained batch 61 batch loss 1.71570396 epoch total loss 1.69911397\n",
      "Trained batch 62 batch loss 1.72492456 epoch total loss 1.69953024\n",
      "Trained batch 63 batch loss 1.7151897 epoch total loss 1.69977868\n",
      "Trained batch 64 batch loss 1.6106149 epoch total loss 1.6983856\n",
      "Trained batch 65 batch loss 1.59849346 epoch total loss 1.69684887\n",
      "Trained batch 66 batch loss 1.66810644 epoch total loss 1.69641328\n",
      "Trained batch 67 batch loss 1.75039065 epoch total loss 1.69721889\n",
      "Trained batch 68 batch loss 1.76821983 epoch total loss 1.69826305\n",
      "Trained batch 69 batch loss 1.7383914 epoch total loss 1.69884455\n",
      "Trained batch 70 batch loss 1.74576735 epoch total loss 1.69951487\n",
      "Trained batch 71 batch loss 1.7594254 epoch total loss 1.70035863\n",
      "Trained batch 72 batch loss 1.7600584 epoch total loss 1.70118773\n",
      "Trained batch 73 batch loss 1.77291107 epoch total loss 1.70217025\n",
      "Trained batch 74 batch loss 1.75844014 epoch total loss 1.70293069\n",
      "Trained batch 75 batch loss 1.74217 epoch total loss 1.7034539\n",
      "Trained batch 76 batch loss 1.76410723 epoch total loss 1.70425189\n",
      "Trained batch 77 batch loss 1.73611557 epoch total loss 1.70466578\n",
      "Trained batch 78 batch loss 1.76269627 epoch total loss 1.70540977\n",
      "Trained batch 79 batch loss 1.69437313 epoch total loss 1.70526993\n",
      "Trained batch 80 batch loss 1.74674869 epoch total loss 1.70578837\n",
      "Trained batch 81 batch loss 1.70418561 epoch total loss 1.7057687\n",
      "Trained batch 82 batch loss 1.72258353 epoch total loss 1.70597374\n",
      "Trained batch 83 batch loss 1.77468562 epoch total loss 1.70680165\n",
      "Trained batch 84 batch loss 1.70558727 epoch total loss 1.70678711\n",
      "Trained batch 85 batch loss 1.68832207 epoch total loss 1.70656991\n",
      "Trained batch 86 batch loss 1.74587059 epoch total loss 1.70702684\n",
      "Trained batch 87 batch loss 1.75829482 epoch total loss 1.70761621\n",
      "Trained batch 88 batch loss 1.73001802 epoch total loss 1.7078706\n",
      "Trained batch 89 batch loss 1.70843554 epoch total loss 1.70787704\n",
      "Trained batch 90 batch loss 1.6860292 epoch total loss 1.70763433\n",
      "Trained batch 91 batch loss 1.7378149 epoch total loss 1.70796585\n",
      "Trained batch 92 batch loss 1.72263312 epoch total loss 1.70812523\n",
      "Trained batch 93 batch loss 1.78865075 epoch total loss 1.70899105\n",
      "Trained batch 94 batch loss 1.75082386 epoch total loss 1.70943618\n",
      "Trained batch 95 batch loss 1.67976284 epoch total loss 1.70912385\n",
      "Trained batch 96 batch loss 1.55576026 epoch total loss 1.70752621\n",
      "Trained batch 97 batch loss 1.55907083 epoch total loss 1.70599568\n",
      "Trained batch 98 batch loss 1.52251768 epoch total loss 1.7041235\n",
      "Trained batch 99 batch loss 1.54806244 epoch total loss 1.70254719\n",
      "Trained batch 100 batch loss 1.44159508 epoch total loss 1.69993758\n",
      "Trained batch 101 batch loss 1.44464886 epoch total loss 1.69741011\n",
      "Trained batch 102 batch loss 1.39246035 epoch total loss 1.69442034\n",
      "Trained batch 103 batch loss 1.49969792 epoch total loss 1.6925298\n",
      "Trained batch 104 batch loss 1.68022466 epoch total loss 1.69241142\n",
      "Trained batch 105 batch loss 1.83158159 epoch total loss 1.69373691\n",
      "Trained batch 106 batch loss 1.7585032 epoch total loss 1.69434786\n",
      "Trained batch 107 batch loss 1.7264992 epoch total loss 1.69464839\n",
      "Trained batch 108 batch loss 1.72151613 epoch total loss 1.69489717\n",
      "Trained batch 109 batch loss 1.59558 epoch total loss 1.69398594\n",
      "Trained batch 110 batch loss 1.83504915 epoch total loss 1.69526839\n",
      "Trained batch 111 batch loss 1.80654097 epoch total loss 1.69627082\n",
      "Trained batch 112 batch loss 1.78964782 epoch total loss 1.69710445\n",
      "Trained batch 113 batch loss 1.84417772 epoch total loss 1.69840598\n",
      "Trained batch 114 batch loss 1.83685124 epoch total loss 1.69962049\n",
      "Trained batch 115 batch loss 1.85083616 epoch total loss 1.70093524\n",
      "Trained batch 116 batch loss 1.66800058 epoch total loss 1.70065141\n",
      "Trained batch 117 batch loss 1.71271205 epoch total loss 1.7007544\n",
      "Trained batch 118 batch loss 1.73664379 epoch total loss 1.70105863\n",
      "Trained batch 119 batch loss 1.75286925 epoch total loss 1.70149398\n",
      "Trained batch 120 batch loss 1.69739628 epoch total loss 1.70145988\n",
      "Trained batch 121 batch loss 1.70771742 epoch total loss 1.70151162\n",
      "Trained batch 122 batch loss 1.73445821 epoch total loss 1.70178163\n",
      "Trained batch 123 batch loss 1.70336616 epoch total loss 1.70179451\n",
      "Trained batch 124 batch loss 1.61359143 epoch total loss 1.70108318\n",
      "Trained batch 125 batch loss 1.75220776 epoch total loss 1.70149219\n",
      "Trained batch 126 batch loss 1.82782531 epoch total loss 1.70249474\n",
      "Trained batch 127 batch loss 1.71410012 epoch total loss 1.70258617\n",
      "Trained batch 128 batch loss 1.73120427 epoch total loss 1.70280969\n",
      "Trained batch 129 batch loss 1.82179177 epoch total loss 1.70373201\n",
      "Trained batch 130 batch loss 1.74664927 epoch total loss 1.7040621\n",
      "Trained batch 131 batch loss 1.60701311 epoch total loss 1.70332122\n",
      "Trained batch 132 batch loss 1.60031533 epoch total loss 1.70254087\n",
      "Trained batch 133 batch loss 1.78675938 epoch total loss 1.70317411\n",
      "Trained batch 134 batch loss 1.69956207 epoch total loss 1.70314705\n",
      "Trained batch 135 batch loss 1.74777973 epoch total loss 1.70347762\n",
      "Trained batch 136 batch loss 1.67042708 epoch total loss 1.70323467\n",
      "Trained batch 137 batch loss 1.57657337 epoch total loss 1.70231009\n",
      "Trained batch 138 batch loss 1.71276903 epoch total loss 1.70238578\n",
      "Trained batch 139 batch loss 1.63209426 epoch total loss 1.7018801\n",
      "Trained batch 140 batch loss 1.66844845 epoch total loss 1.70164132\n",
      "Trained batch 141 batch loss 1.65610528 epoch total loss 1.70131838\n",
      "Trained batch 142 batch loss 1.51876807 epoch total loss 1.70003283\n",
      "Trained batch 143 batch loss 1.58980799 epoch total loss 1.69926202\n",
      "Trained batch 144 batch loss 1.51577306 epoch total loss 1.69798791\n",
      "Trained batch 145 batch loss 1.56614041 epoch total loss 1.69707859\n",
      "Trained batch 146 batch loss 1.74406934 epoch total loss 1.69740045\n",
      "Trained batch 147 batch loss 1.66321516 epoch total loss 1.69716787\n",
      "Trained batch 148 batch loss 1.62722158 epoch total loss 1.69669533\n",
      "Trained batch 149 batch loss 1.73034215 epoch total loss 1.69692111\n",
      "Trained batch 150 batch loss 1.74234724 epoch total loss 1.6972239\n",
      "Trained batch 151 batch loss 1.70474172 epoch total loss 1.69727373\n",
      "Trained batch 152 batch loss 1.73626685 epoch total loss 1.69753027\n",
      "Trained batch 153 batch loss 1.70777214 epoch total loss 1.69759715\n",
      "Trained batch 154 batch loss 1.73296893 epoch total loss 1.69782686\n",
      "Trained batch 155 batch loss 1.77447331 epoch total loss 1.69832134\n",
      "Trained batch 156 batch loss 1.72816765 epoch total loss 1.69851279\n",
      "Trained batch 157 batch loss 1.75937271 epoch total loss 1.69890034\n",
      "Trained batch 158 batch loss 1.69383645 epoch total loss 1.69886839\n",
      "Trained batch 159 batch loss 1.52883434 epoch total loss 1.69779897\n",
      "Trained batch 160 batch loss 1.58249557 epoch total loss 1.69707835\n",
      "Trained batch 161 batch loss 1.39366913 epoch total loss 1.69519389\n",
      "Trained batch 162 batch loss 1.38835251 epoch total loss 1.69329989\n",
      "Trained batch 163 batch loss 1.54243577 epoch total loss 1.69237435\n",
      "Trained batch 164 batch loss 1.79763186 epoch total loss 1.69301629\n",
      "Trained batch 165 batch loss 1.79950941 epoch total loss 1.69366157\n",
      "Trained batch 166 batch loss 1.57624125 epoch total loss 1.69295418\n",
      "Trained batch 167 batch loss 1.70203757 epoch total loss 1.69300854\n",
      "Trained batch 168 batch loss 1.72747636 epoch total loss 1.6932137\n",
      "Trained batch 169 batch loss 1.5839783 epoch total loss 1.69256735\n",
      "Trained batch 170 batch loss 1.606915 epoch total loss 1.69206345\n",
      "Trained batch 171 batch loss 1.66549695 epoch total loss 1.69190812\n",
      "Trained batch 172 batch loss 1.5533371 epoch total loss 1.6911025\n",
      "Trained batch 173 batch loss 1.68994951 epoch total loss 1.69109583\n",
      "Trained batch 174 batch loss 1.76959598 epoch total loss 1.69154692\n",
      "Trained batch 175 batch loss 1.75052345 epoch total loss 1.69188392\n",
      "Trained batch 176 batch loss 1.79293704 epoch total loss 1.69245803\n",
      "Trained batch 177 batch loss 1.77641344 epoch total loss 1.69293249\n",
      "Trained batch 178 batch loss 1.7058599 epoch total loss 1.6930052\n",
      "Trained batch 179 batch loss 1.67302 epoch total loss 1.69289362\n",
      "Trained batch 180 batch loss 1.73123729 epoch total loss 1.69310653\n",
      "Trained batch 181 batch loss 1.69283068 epoch total loss 1.6931051\n",
      "Trained batch 182 batch loss 1.66364169 epoch total loss 1.69294322\n",
      "Trained batch 183 batch loss 1.65145588 epoch total loss 1.69271648\n",
      "Trained batch 184 batch loss 1.74175 epoch total loss 1.69298303\n",
      "Trained batch 185 batch loss 1.62037086 epoch total loss 1.69259048\n",
      "Trained batch 186 batch loss 1.63748884 epoch total loss 1.69229424\n",
      "Trained batch 187 batch loss 1.63538468 epoch total loss 1.69198978\n",
      "Trained batch 188 batch loss 1.60851789 epoch total loss 1.69154584\n",
      "Trained batch 189 batch loss 1.70641375 epoch total loss 1.69162452\n",
      "Trained batch 190 batch loss 1.60984135 epoch total loss 1.69119406\n",
      "Trained batch 191 batch loss 1.67225456 epoch total loss 1.69109488\n",
      "Trained batch 192 batch loss 1.64574742 epoch total loss 1.69085872\n",
      "Trained batch 193 batch loss 1.57579732 epoch total loss 1.69026256\n",
      "Trained batch 194 batch loss 1.55599725 epoch total loss 1.68957043\n",
      "Trained batch 195 batch loss 1.59307313 epoch total loss 1.68907559\n",
      "Trained batch 196 batch loss 1.52915931 epoch total loss 1.68825972\n",
      "Trained batch 197 batch loss 1.62615108 epoch total loss 1.68794441\n",
      "Trained batch 198 batch loss 1.64872026 epoch total loss 1.68774629\n",
      "Trained batch 199 batch loss 1.73001373 epoch total loss 1.68795872\n",
      "Trained batch 200 batch loss 1.70827174 epoch total loss 1.68806028\n",
      "Trained batch 201 batch loss 1.75707197 epoch total loss 1.68840373\n",
      "Trained batch 202 batch loss 1.70790899 epoch total loss 1.68850029\n",
      "Trained batch 203 batch loss 1.62736726 epoch total loss 1.68819916\n",
      "Trained batch 204 batch loss 1.77760983 epoch total loss 1.6886375\n",
      "Trained batch 205 batch loss 1.68708193 epoch total loss 1.68862987\n",
      "Trained batch 206 batch loss 1.7380271 epoch total loss 1.68886971\n",
      "Trained batch 207 batch loss 1.76080775 epoch total loss 1.68921721\n",
      "Trained batch 208 batch loss 1.69516492 epoch total loss 1.68924582\n",
      "Trained batch 209 batch loss 1.53697753 epoch total loss 1.68851733\n",
      "Trained batch 210 batch loss 1.61986303 epoch total loss 1.68819046\n",
      "Trained batch 211 batch loss 1.50284493 epoch total loss 1.68731201\n",
      "Trained batch 212 batch loss 1.55425155 epoch total loss 1.68668437\n",
      "Trained batch 213 batch loss 1.7125355 epoch total loss 1.68680573\n",
      "Trained batch 214 batch loss 1.63548553 epoch total loss 1.68656588\n",
      "Trained batch 215 batch loss 1.74502456 epoch total loss 1.68683779\n",
      "Trained batch 216 batch loss 1.81623495 epoch total loss 1.68743682\n",
      "Trained batch 217 batch loss 1.61317325 epoch total loss 1.68709457\n",
      "Trained batch 218 batch loss 1.59807158 epoch total loss 1.68668628\n",
      "Trained batch 219 batch loss 1.60058069 epoch total loss 1.68629313\n",
      "Trained batch 220 batch loss 1.69202363 epoch total loss 1.68631911\n",
      "Trained batch 221 batch loss 1.70208049 epoch total loss 1.6863904\n",
      "Trained batch 222 batch loss 1.72523427 epoch total loss 1.6865654\n",
      "Trained batch 223 batch loss 1.7465663 epoch total loss 1.68683434\n",
      "Trained batch 224 batch loss 1.76578188 epoch total loss 1.68718684\n",
      "Trained batch 225 batch loss 1.76862752 epoch total loss 1.68754864\n",
      "Trained batch 226 batch loss 1.79579973 epoch total loss 1.68802774\n",
      "Trained batch 227 batch loss 1.7409606 epoch total loss 1.68826091\n",
      "Trained batch 228 batch loss 1.74316216 epoch total loss 1.68850172\n",
      "Trained batch 229 batch loss 1.75270891 epoch total loss 1.6887821\n",
      "Trained batch 230 batch loss 1.75708842 epoch total loss 1.68907905\n",
      "Trained batch 231 batch loss 1.81521738 epoch total loss 1.68962514\n",
      "Trained batch 232 batch loss 1.79907084 epoch total loss 1.69009686\n",
      "Trained batch 233 batch loss 1.79500151 epoch total loss 1.69054723\n",
      "Trained batch 234 batch loss 1.74286354 epoch total loss 1.69077075\n",
      "Trained batch 235 batch loss 1.74655104 epoch total loss 1.69100809\n",
      "Trained batch 236 batch loss 1.77593648 epoch total loss 1.69136798\n",
      "Trained batch 237 batch loss 1.81808972 epoch total loss 1.69190264\n",
      "Trained batch 238 batch loss 1.75331306 epoch total loss 1.69216073\n",
      "Trained batch 239 batch loss 1.75870419 epoch total loss 1.69243908\n",
      "Trained batch 240 batch loss 1.69286311 epoch total loss 1.69244087\n",
      "Trained batch 241 batch loss 1.66063619 epoch total loss 1.69230902\n",
      "Trained batch 242 batch loss 1.78905606 epoch total loss 1.69270885\n",
      "Trained batch 243 batch loss 1.75660014 epoch total loss 1.69297171\n",
      "Trained batch 244 batch loss 1.79895198 epoch total loss 1.69340599\n",
      "Trained batch 245 batch loss 1.71464562 epoch total loss 1.69349277\n",
      "Trained batch 246 batch loss 1.68240714 epoch total loss 1.69344771\n",
      "Trained batch 247 batch loss 1.69276822 epoch total loss 1.69344497\n",
      "Trained batch 248 batch loss 1.6567744 epoch total loss 1.69329715\n",
      "Trained batch 249 batch loss 1.69678545 epoch total loss 1.6933111\n",
      "Trained batch 250 batch loss 1.69638216 epoch total loss 1.69332337\n",
      "Trained batch 251 batch loss 1.62800252 epoch total loss 1.69306302\n",
      "Trained batch 252 batch loss 1.67832768 epoch total loss 1.69300449\n",
      "Trained batch 253 batch loss 1.76683295 epoch total loss 1.69329643\n",
      "Trained batch 254 batch loss 1.75047 epoch total loss 1.6935215\n",
      "Trained batch 255 batch loss 1.72724295 epoch total loss 1.6936537\n",
      "Trained batch 256 batch loss 1.47042227 epoch total loss 1.69278169\n",
      "Trained batch 257 batch loss 1.59891987 epoch total loss 1.69241643\n",
      "Trained batch 258 batch loss 1.74764788 epoch total loss 1.69263053\n",
      "Trained batch 259 batch loss 1.78575861 epoch total loss 1.69299006\n",
      "Trained batch 260 batch loss 1.78916371 epoch total loss 1.69336\n",
      "Trained batch 261 batch loss 1.64951277 epoch total loss 1.69319189\n",
      "Trained batch 262 batch loss 1.63439751 epoch total loss 1.69296753\n",
      "Trained batch 263 batch loss 1.72144687 epoch total loss 1.69307578\n",
      "Trained batch 264 batch loss 1.71012831 epoch total loss 1.69314027\n",
      "Trained batch 265 batch loss 1.77305269 epoch total loss 1.69344187\n",
      "Trained batch 266 batch loss 1.72177291 epoch total loss 1.69354832\n",
      "Trained batch 267 batch loss 1.74196947 epoch total loss 1.69372976\n",
      "Trained batch 268 batch loss 1.58802927 epoch total loss 1.69333529\n",
      "Trained batch 269 batch loss 1.6949228 epoch total loss 1.69334126\n",
      "Trained batch 270 batch loss 1.67898202 epoch total loss 1.69328809\n",
      "Trained batch 271 batch loss 1.73460984 epoch total loss 1.69344056\n",
      "Trained batch 272 batch loss 1.73014116 epoch total loss 1.6935755\n",
      "Trained batch 273 batch loss 1.71564448 epoch total loss 1.69365633\n",
      "Trained batch 274 batch loss 1.80108476 epoch total loss 1.6940484\n",
      "Trained batch 275 batch loss 1.74819446 epoch total loss 1.69424522\n",
      "Trained batch 276 batch loss 1.76105595 epoch total loss 1.69448733\n",
      "Trained batch 277 batch loss 1.79644728 epoch total loss 1.69485545\n",
      "Trained batch 278 batch loss 1.76802969 epoch total loss 1.69511867\n",
      "Trained batch 279 batch loss 1.65524936 epoch total loss 1.69497573\n",
      "Trained batch 280 batch loss 1.70970869 epoch total loss 1.69502831\n",
      "Trained batch 281 batch loss 1.7060461 epoch total loss 1.69506764\n",
      "Trained batch 282 batch loss 1.70800734 epoch total loss 1.69511354\n",
      "Trained batch 283 batch loss 1.70139027 epoch total loss 1.69513559\n",
      "Trained batch 284 batch loss 1.57304323 epoch total loss 1.69470572\n",
      "Trained batch 285 batch loss 1.70793831 epoch total loss 1.6947521\n",
      "Trained batch 286 batch loss 1.76129162 epoch total loss 1.69498479\n",
      "Trained batch 287 batch loss 1.503317 epoch total loss 1.69431698\n",
      "Trained batch 288 batch loss 1.50834107 epoch total loss 1.69367123\n",
      "Trained batch 289 batch loss 1.73008585 epoch total loss 1.69379723\n",
      "Trained batch 290 batch loss 1.78735209 epoch total loss 1.69411981\n",
      "Trained batch 291 batch loss 1.79001284 epoch total loss 1.69444931\n",
      "Trained batch 292 batch loss 1.80171 epoch total loss 1.69481659\n",
      "Trained batch 293 batch loss 1.76890993 epoch total loss 1.69506955\n",
      "Trained batch 294 batch loss 1.78169787 epoch total loss 1.69536424\n",
      "Trained batch 295 batch loss 1.76288164 epoch total loss 1.695593\n",
      "Trained batch 296 batch loss 1.69112706 epoch total loss 1.69557798\n",
      "Trained batch 297 batch loss 1.6824677 epoch total loss 1.69553387\n",
      "Trained batch 298 batch loss 1.71125913 epoch total loss 1.69558668\n",
      "Trained batch 299 batch loss 1.60330784 epoch total loss 1.69527805\n",
      "Trained batch 300 batch loss 1.7112416 epoch total loss 1.69533122\n",
      "Trained batch 301 batch loss 1.75156868 epoch total loss 1.69551802\n",
      "Trained batch 302 batch loss 1.60999715 epoch total loss 1.69523478\n",
      "Trained batch 303 batch loss 1.59030676 epoch total loss 1.69488847\n",
      "Trained batch 304 batch loss 1.69230759 epoch total loss 1.69488\n",
      "Trained batch 305 batch loss 1.70397687 epoch total loss 1.69490981\n",
      "Trained batch 306 batch loss 1.65166128 epoch total loss 1.69476855\n",
      "Trained batch 307 batch loss 1.80013549 epoch total loss 1.69511175\n",
      "Trained batch 308 batch loss 1.74284101 epoch total loss 1.69526672\n",
      "Trained batch 309 batch loss 1.72226191 epoch total loss 1.69535422\n",
      "Trained batch 310 batch loss 1.73957133 epoch total loss 1.6954968\n",
      "Trained batch 311 batch loss 1.76406574 epoch total loss 1.69571722\n",
      "Trained batch 312 batch loss 1.71565461 epoch total loss 1.69578099\n",
      "Trained batch 313 batch loss 1.78113091 epoch total loss 1.69605374\n",
      "Trained batch 314 batch loss 1.7462703 epoch total loss 1.6962136\n",
      "Trained batch 315 batch loss 1.68924 epoch total loss 1.69619155\n",
      "Trained batch 316 batch loss 1.50245476 epoch total loss 1.69557846\n",
      "Trained batch 317 batch loss 1.5304805 epoch total loss 1.69505763\n",
      "Trained batch 318 batch loss 1.55337071 epoch total loss 1.69461191\n",
      "Trained batch 319 batch loss 1.6189369 epoch total loss 1.6943748\n",
      "Trained batch 320 batch loss 1.63438916 epoch total loss 1.6941874\n",
      "Trained batch 321 batch loss 1.52725124 epoch total loss 1.69366717\n",
      "Trained batch 322 batch loss 1.61871338 epoch total loss 1.69343448\n",
      "Trained batch 323 batch loss 1.6227845 epoch total loss 1.69321573\n",
      "Trained batch 324 batch loss 1.70036232 epoch total loss 1.6932379\n",
      "Trained batch 325 batch loss 1.57529831 epoch total loss 1.69287503\n",
      "Trained batch 326 batch loss 1.6443187 epoch total loss 1.69272614\n",
      "Trained batch 327 batch loss 1.71246123 epoch total loss 1.69278657\n",
      "Trained batch 328 batch loss 1.70998049 epoch total loss 1.69283891\n",
      "Trained batch 329 batch loss 1.70576191 epoch total loss 1.69287813\n",
      "Trained batch 330 batch loss 1.78686321 epoch total loss 1.69316292\n",
      "Trained batch 331 batch loss 1.62984288 epoch total loss 1.69297159\n",
      "Trained batch 332 batch loss 1.74443197 epoch total loss 1.69312668\n",
      "Trained batch 333 batch loss 1.6589818 epoch total loss 1.69302416\n",
      "Trained batch 334 batch loss 1.72190452 epoch total loss 1.6931107\n",
      "Trained batch 335 batch loss 1.74949193 epoch total loss 1.69327903\n",
      "Trained batch 336 batch loss 1.71351731 epoch total loss 1.69333923\n",
      "Trained batch 337 batch loss 1.65424764 epoch total loss 1.69322312\n",
      "Trained batch 338 batch loss 1.68704426 epoch total loss 1.693205\n",
      "Trained batch 339 batch loss 1.74522364 epoch total loss 1.69335842\n",
      "Trained batch 340 batch loss 1.76855111 epoch total loss 1.69357967\n",
      "Trained batch 341 batch loss 1.71761966 epoch total loss 1.69365\n",
      "Trained batch 342 batch loss 1.78942406 epoch total loss 1.69393015\n",
      "Trained batch 343 batch loss 1.69074953 epoch total loss 1.69392085\n",
      "Trained batch 344 batch loss 1.76089311 epoch total loss 1.6941154\n",
      "Trained batch 345 batch loss 1.67849672 epoch total loss 1.69407\n",
      "Trained batch 346 batch loss 1.79523492 epoch total loss 1.6943624\n",
      "Trained batch 347 batch loss 1.7399615 epoch total loss 1.69449389\n",
      "Trained batch 348 batch loss 1.63381195 epoch total loss 1.69431949\n",
      "Trained batch 349 batch loss 1.66533697 epoch total loss 1.6942364\n",
      "Trained batch 350 batch loss 1.65168023 epoch total loss 1.6941148\n",
      "Trained batch 351 batch loss 1.57891488 epoch total loss 1.69378662\n",
      "Trained batch 352 batch loss 1.6051935 epoch total loss 1.69353485\n",
      "Trained batch 353 batch loss 1.756423 epoch total loss 1.69371295\n",
      "Trained batch 354 batch loss 1.65960157 epoch total loss 1.69361663\n",
      "Trained batch 355 batch loss 1.72504878 epoch total loss 1.69370508\n",
      "Trained batch 356 batch loss 1.6531986 epoch total loss 1.69359136\n",
      "Trained batch 357 batch loss 1.4905746 epoch total loss 1.69302273\n",
      "Trained batch 358 batch loss 1.4841454 epoch total loss 1.6924392\n",
      "Trained batch 359 batch loss 1.53387761 epoch total loss 1.69199753\n",
      "Trained batch 360 batch loss 1.60492349 epoch total loss 1.69175565\n",
      "Trained batch 361 batch loss 1.58195496 epoch total loss 1.69145155\n",
      "Trained batch 362 batch loss 1.68945312 epoch total loss 1.69144607\n",
      "Trained batch 363 batch loss 1.70366228 epoch total loss 1.69147968\n",
      "Trained batch 364 batch loss 1.65390348 epoch total loss 1.69137657\n",
      "Trained batch 365 batch loss 1.72535324 epoch total loss 1.69146967\n",
      "Trained batch 366 batch loss 1.61102867 epoch total loss 1.69124985\n",
      "Trained batch 367 batch loss 1.77890134 epoch total loss 1.69148874\n",
      "Trained batch 368 batch loss 1.8873266 epoch total loss 1.69202089\n",
      "Trained batch 369 batch loss 1.86775184 epoch total loss 1.69249713\n",
      "Trained batch 370 batch loss 1.83802855 epoch total loss 1.69289041\n",
      "Trained batch 371 batch loss 1.77230942 epoch total loss 1.69310451\n",
      "Trained batch 372 batch loss 1.67271781 epoch total loss 1.69304979\n",
      "Trained batch 373 batch loss 1.6674931 epoch total loss 1.69298124\n",
      "Trained batch 374 batch loss 1.69018602 epoch total loss 1.69297373\n",
      "Trained batch 375 batch loss 1.64412439 epoch total loss 1.69284344\n",
      "Trained batch 376 batch loss 1.68773866 epoch total loss 1.69282985\n",
      "Trained batch 377 batch loss 1.58569396 epoch total loss 1.69254565\n",
      "Trained batch 378 batch loss 1.67817044 epoch total loss 1.69250762\n",
      "Trained batch 379 batch loss 1.78698635 epoch total loss 1.69275689\n",
      "Trained batch 380 batch loss 1.7783674 epoch total loss 1.6929822\n",
      "Trained batch 381 batch loss 1.76000714 epoch total loss 1.69315815\n",
      "Trained batch 382 batch loss 1.74980974 epoch total loss 1.69330645\n",
      "Trained batch 383 batch loss 1.70709789 epoch total loss 1.69334245\n",
      "Trained batch 384 batch loss 1.77794027 epoch total loss 1.69356287\n",
      "Trained batch 385 batch loss 1.70454657 epoch total loss 1.69359136\n",
      "Trained batch 386 batch loss 1.71081281 epoch total loss 1.69363594\n",
      "Trained batch 387 batch loss 1.70954192 epoch total loss 1.69367707\n",
      "Trained batch 388 batch loss 1.68900275 epoch total loss 1.69366503\n",
      "Trained batch 389 batch loss 1.69258094 epoch total loss 1.69366217\n",
      "Trained batch 390 batch loss 1.59846497 epoch total loss 1.69341803\n",
      "Trained batch 391 batch loss 1.68216109 epoch total loss 1.6933893\n",
      "Trained batch 392 batch loss 1.5430243 epoch total loss 1.6930058\n",
      "Trained batch 393 batch loss 1.59340024 epoch total loss 1.69275224\n",
      "Trained batch 394 batch loss 1.7329613 epoch total loss 1.6928544\n",
      "Trained batch 395 batch loss 1.63924134 epoch total loss 1.69271863\n",
      "Trained batch 396 batch loss 1.47909284 epoch total loss 1.69217908\n",
      "Trained batch 397 batch loss 1.52396655 epoch total loss 1.69175541\n",
      "Trained batch 398 batch loss 1.54518986 epoch total loss 1.69138706\n",
      "Trained batch 399 batch loss 1.58885 epoch total loss 1.69113016\n",
      "Trained batch 400 batch loss 1.6858443 epoch total loss 1.69111693\n",
      "Trained batch 401 batch loss 1.78181171 epoch total loss 1.69134307\n",
      "Trained batch 402 batch loss 1.71077466 epoch total loss 1.69139135\n",
      "Trained batch 403 batch loss 1.69296789 epoch total loss 1.6913954\n",
      "Trained batch 404 batch loss 1.55227661 epoch total loss 1.69105089\n",
      "Trained batch 405 batch loss 1.70632958 epoch total loss 1.69108868\n",
      "Trained batch 406 batch loss 1.77220011 epoch total loss 1.69128859\n",
      "Trained batch 407 batch loss 1.63871872 epoch total loss 1.69115937\n",
      "Trained batch 408 batch loss 1.66089678 epoch total loss 1.69108522\n",
      "Trained batch 409 batch loss 1.73978531 epoch total loss 1.69120431\n",
      "Trained batch 410 batch loss 1.7023561 epoch total loss 1.69123149\n",
      "Trained batch 411 batch loss 1.71319413 epoch total loss 1.69128489\n",
      "Trained batch 412 batch loss 1.7396605 epoch total loss 1.69140244\n",
      "Trained batch 413 batch loss 1.69410658 epoch total loss 1.69140887\n",
      "Trained batch 414 batch loss 1.70036077 epoch total loss 1.69143057\n",
      "Trained batch 415 batch loss 1.6291579 epoch total loss 1.69128048\n",
      "Trained batch 416 batch loss 1.71672297 epoch total loss 1.69134164\n",
      "Trained batch 417 batch loss 1.78229773 epoch total loss 1.69155979\n",
      "Trained batch 418 batch loss 1.78389311 epoch total loss 1.69178069\n",
      "Trained batch 419 batch loss 1.79289699 epoch total loss 1.69202197\n",
      "Trained batch 420 batch loss 1.61753047 epoch total loss 1.6918447\n",
      "Trained batch 421 batch loss 1.57714891 epoch total loss 1.69157219\n",
      "Trained batch 422 batch loss 1.62552392 epoch total loss 1.69141579\n",
      "Trained batch 423 batch loss 1.61672986 epoch total loss 1.69123936\n",
      "Trained batch 424 batch loss 1.77114487 epoch total loss 1.69142771\n",
      "Trained batch 425 batch loss 1.7477541 epoch total loss 1.69156015\n",
      "Trained batch 426 batch loss 1.75665271 epoch total loss 1.69171298\n",
      "Trained batch 427 batch loss 1.75582433 epoch total loss 1.69186306\n",
      "Trained batch 428 batch loss 1.75309062 epoch total loss 1.69200623\n",
      "Trained batch 429 batch loss 1.75375056 epoch total loss 1.69215\n",
      "Trained batch 430 batch loss 1.80064356 epoch total loss 1.69240236\n",
      "Trained batch 431 batch loss 1.73933601 epoch total loss 1.6925112\n",
      "Trained batch 432 batch loss 1.75680327 epoch total loss 1.69266\n",
      "Trained batch 433 batch loss 1.70753813 epoch total loss 1.69269431\n",
      "Trained batch 434 batch loss 1.7091136 epoch total loss 1.6927321\n",
      "Trained batch 435 batch loss 1.72099411 epoch total loss 1.69279718\n",
      "Trained batch 436 batch loss 1.72454357 epoch total loss 1.69287\n",
      "Trained batch 437 batch loss 1.73835683 epoch total loss 1.69297397\n",
      "Trained batch 438 batch loss 1.7396903 epoch total loss 1.69308066\n",
      "Trained batch 439 batch loss 1.75402498 epoch total loss 1.69321954\n",
      "Trained batch 440 batch loss 1.68150616 epoch total loss 1.69319296\n",
      "Trained batch 441 batch loss 1.69584858 epoch total loss 1.69319892\n",
      "Trained batch 442 batch loss 1.66549814 epoch total loss 1.69313633\n",
      "Trained batch 443 batch loss 1.76233673 epoch total loss 1.6932925\n",
      "Trained batch 444 batch loss 1.77864552 epoch total loss 1.69348478\n",
      "Trained batch 445 batch loss 1.66069484 epoch total loss 1.69341111\n",
      "Trained batch 446 batch loss 1.63083267 epoch total loss 1.6932708\n",
      "Trained batch 447 batch loss 1.602229 epoch total loss 1.69306719\n",
      "Trained batch 448 batch loss 1.70646143 epoch total loss 1.69309711\n",
      "Trained batch 449 batch loss 1.71167564 epoch total loss 1.69313848\n",
      "Trained batch 450 batch loss 1.70169032 epoch total loss 1.69315743\n",
      "Trained batch 451 batch loss 1.6674881 epoch total loss 1.69310045\n",
      "Trained batch 452 batch loss 1.66471434 epoch total loss 1.69303775\n",
      "Trained batch 453 batch loss 1.71432102 epoch total loss 1.6930846\n",
      "Trained batch 454 batch loss 1.67133045 epoch total loss 1.69303668\n",
      "Trained batch 455 batch loss 1.62029302 epoch total loss 1.69287682\n",
      "Trained batch 456 batch loss 1.69490409 epoch total loss 1.69288123\n",
      "Trained batch 457 batch loss 1.72965598 epoch total loss 1.69296181\n",
      "Trained batch 458 batch loss 1.65079439 epoch total loss 1.69286978\n",
      "Trained batch 459 batch loss 1.64611602 epoch total loss 1.69276786\n",
      "Trained batch 460 batch loss 1.44022751 epoch total loss 1.6922189\n",
      "Trained batch 461 batch loss 1.55786073 epoch total loss 1.69192755\n",
      "Trained batch 462 batch loss 1.60275567 epoch total loss 1.69173455\n",
      "Trained batch 463 batch loss 1.67464483 epoch total loss 1.6916976\n",
      "Trained batch 464 batch loss 1.71308351 epoch total loss 1.69174361\n",
      "Trained batch 465 batch loss 1.73011923 epoch total loss 1.69182611\n",
      "Trained batch 466 batch loss 1.70182657 epoch total loss 1.69184768\n",
      "Trained batch 467 batch loss 1.78192699 epoch total loss 1.69204056\n",
      "Trained batch 468 batch loss 1.75287879 epoch total loss 1.6921705\n",
      "Trained batch 469 batch loss 1.74177384 epoch total loss 1.69227624\n",
      "Trained batch 470 batch loss 1.67324376 epoch total loss 1.69223571\n",
      "Trained batch 471 batch loss 1.73433387 epoch total loss 1.692325\n",
      "Trained batch 472 batch loss 1.65799713 epoch total loss 1.69225228\n",
      "Trained batch 473 batch loss 1.67673659 epoch total loss 1.69221961\n",
      "Trained batch 474 batch loss 1.61695278 epoch total loss 1.69206071\n",
      "Trained batch 475 batch loss 1.64240301 epoch total loss 1.69195616\n",
      "Trained batch 476 batch loss 1.63724172 epoch total loss 1.69184136\n",
      "Trained batch 477 batch loss 1.53954506 epoch total loss 1.691522\n",
      "Trained batch 478 batch loss 1.38526249 epoch total loss 1.69088137\n",
      "Trained batch 479 batch loss 1.4245739 epoch total loss 1.69032538\n",
      "Trained batch 480 batch loss 1.61886466 epoch total loss 1.69017637\n",
      "Trained batch 481 batch loss 1.62026906 epoch total loss 1.69003105\n",
      "Trained batch 482 batch loss 1.6850915 epoch total loss 1.6900208\n",
      "Trained batch 483 batch loss 1.71064079 epoch total loss 1.69006348\n",
      "Trained batch 484 batch loss 1.73116755 epoch total loss 1.69014835\n",
      "Trained batch 485 batch loss 1.72349095 epoch total loss 1.69021714\n",
      "Trained batch 486 batch loss 1.66936707 epoch total loss 1.69017422\n",
      "Trained batch 487 batch loss 1.6701231 epoch total loss 1.69013298\n",
      "Trained batch 488 batch loss 1.72653043 epoch total loss 1.6902076\n",
      "Trained batch 489 batch loss 1.62370133 epoch total loss 1.69007158\n",
      "Trained batch 490 batch loss 1.66295338 epoch total loss 1.69001627\n",
      "Trained batch 491 batch loss 1.64478827 epoch total loss 1.68992412\n",
      "Trained batch 492 batch loss 1.63001335 epoch total loss 1.68980229\n",
      "Trained batch 493 batch loss 1.7442646 epoch total loss 1.6899128\n",
      "Trained batch 494 batch loss 1.77205181 epoch total loss 1.69007909\n",
      "Trained batch 495 batch loss 1.73968875 epoch total loss 1.69017923\n",
      "Trained batch 496 batch loss 1.56979442 epoch total loss 1.68993664\n",
      "Trained batch 497 batch loss 1.59636092 epoch total loss 1.68974841\n",
      "Trained batch 498 batch loss 1.42691362 epoch total loss 1.68922067\n",
      "Trained batch 499 batch loss 1.59655106 epoch total loss 1.68903494\n",
      "Trained batch 500 batch loss 1.7151165 epoch total loss 1.68908703\n",
      "Trained batch 501 batch loss 1.47273326 epoch total loss 1.68865514\n",
      "Trained batch 502 batch loss 1.45824933 epoch total loss 1.68819618\n",
      "Trained batch 503 batch loss 1.44875503 epoch total loss 1.68772006\n",
      "Trained batch 504 batch loss 1.5484308 epoch total loss 1.68744373\n",
      "Trained batch 505 batch loss 1.58550692 epoch total loss 1.68724179\n",
      "Trained batch 506 batch loss 1.73665833 epoch total loss 1.68733943\n",
      "Trained batch 507 batch loss 1.81951952 epoch total loss 1.68760014\n",
      "Trained batch 508 batch loss 1.78049779 epoch total loss 1.68778312\n",
      "Trained batch 509 batch loss 1.7617439 epoch total loss 1.68792832\n",
      "Trained batch 510 batch loss 1.79290771 epoch total loss 1.68813419\n",
      "Trained batch 511 batch loss 1.85053277 epoch total loss 1.68845201\n",
      "Trained batch 512 batch loss 1.77229524 epoch total loss 1.68861568\n",
      "Trained batch 513 batch loss 1.8117528 epoch total loss 1.68885577\n",
      "Trained batch 514 batch loss 1.80585504 epoch total loss 1.68908334\n",
      "Trained batch 515 batch loss 1.80458307 epoch total loss 1.68930757\n",
      "Trained batch 516 batch loss 1.71976507 epoch total loss 1.6893667\n",
      "Trained batch 517 batch loss 1.72769415 epoch total loss 1.68944085\n",
      "Trained batch 518 batch loss 1.70216405 epoch total loss 1.6894654\n",
      "Trained batch 519 batch loss 1.5974015 epoch total loss 1.68928802\n",
      "Trained batch 520 batch loss 1.58191681 epoch total loss 1.68908155\n",
      "Trained batch 521 batch loss 1.69788241 epoch total loss 1.68909836\n",
      "Trained batch 522 batch loss 1.76691651 epoch total loss 1.68924749\n",
      "Trained batch 523 batch loss 1.7549479 epoch total loss 1.68937302\n",
      "Trained batch 524 batch loss 1.7887615 epoch total loss 1.68956268\n",
      "Trained batch 525 batch loss 1.74661422 epoch total loss 1.6896714\n",
      "Trained batch 526 batch loss 1.73440528 epoch total loss 1.68975639\n",
      "Trained batch 527 batch loss 1.75785542 epoch total loss 1.68988574\n",
      "Trained batch 528 batch loss 1.70941448 epoch total loss 1.68992269\n",
      "Trained batch 529 batch loss 1.69705176 epoch total loss 1.68993604\n",
      "Trained batch 530 batch loss 1.63581645 epoch total loss 1.689834\n",
      "Trained batch 531 batch loss 1.69940174 epoch total loss 1.689852\n",
      "Trained batch 532 batch loss 1.69586122 epoch total loss 1.68986332\n",
      "Trained batch 533 batch loss 1.67722249 epoch total loss 1.6898396\n",
      "Trained batch 534 batch loss 1.7396 epoch total loss 1.68993282\n",
      "Trained batch 535 batch loss 1.70567584 epoch total loss 1.68996227\n",
      "Trained batch 536 batch loss 1.70025468 epoch total loss 1.68998146\n",
      "Trained batch 537 batch loss 1.5545516 epoch total loss 1.68972933\n",
      "Trained batch 538 batch loss 1.66169131 epoch total loss 1.68967724\n",
      "Trained batch 539 batch loss 1.70872307 epoch total loss 1.68971252\n",
      "Trained batch 540 batch loss 1.61001742 epoch total loss 1.68956506\n",
      "Trained batch 541 batch loss 1.77864289 epoch total loss 1.68972969\n",
      "Trained batch 542 batch loss 1.6573379 epoch total loss 1.68966985\n",
      "Trained batch 543 batch loss 1.62493086 epoch total loss 1.68955064\n",
      "Trained batch 544 batch loss 1.45421672 epoch total loss 1.68911815\n",
      "Trained batch 545 batch loss 1.69228721 epoch total loss 1.68912387\n",
      "Trained batch 546 batch loss 1.73892283 epoch total loss 1.68921518\n",
      "Trained batch 547 batch loss 1.65853167 epoch total loss 1.68915904\n",
      "Trained batch 548 batch loss 1.75837851 epoch total loss 1.68928528\n",
      "Trained batch 549 batch loss 1.74979508 epoch total loss 1.68939555\n",
      "Trained batch 550 batch loss 1.73200822 epoch total loss 1.68947303\n",
      "Trained batch 551 batch loss 1.77254128 epoch total loss 1.68962371\n",
      "Trained batch 552 batch loss 1.73129821 epoch total loss 1.68969929\n",
      "Trained batch 553 batch loss 1.7120043 epoch total loss 1.68973958\n",
      "Trained batch 554 batch loss 1.65376484 epoch total loss 1.68967462\n",
      "Trained batch 555 batch loss 1.71823 epoch total loss 1.689726\n",
      "Trained batch 556 batch loss 1.6537323 epoch total loss 1.68966126\n",
      "Trained batch 557 batch loss 1.6142571 epoch total loss 1.68952584\n",
      "Trained batch 558 batch loss 1.7563374 epoch total loss 1.68964565\n",
      "Trained batch 559 batch loss 1.74587631 epoch total loss 1.68974614\n",
      "Trained batch 560 batch loss 1.6602273 epoch total loss 1.68969345\n",
      "Trained batch 561 batch loss 1.58459163 epoch total loss 1.68950605\n",
      "Trained batch 562 batch loss 1.66624272 epoch total loss 1.68946469\n",
      "Trained batch 563 batch loss 1.5627619 epoch total loss 1.68923962\n",
      "Trained batch 564 batch loss 1.73711777 epoch total loss 1.68932462\n",
      "Trained batch 565 batch loss 1.69826174 epoch total loss 1.68934035\n",
      "Trained batch 566 batch loss 1.71302199 epoch total loss 1.6893822\n",
      "Trained batch 567 batch loss 1.64574 epoch total loss 1.68930519\n",
      "Trained batch 568 batch loss 1.72050297 epoch total loss 1.68936014\n",
      "Trained batch 569 batch loss 1.66200578 epoch total loss 1.6893121\n",
      "Trained batch 570 batch loss 1.53958654 epoch total loss 1.68904948\n",
      "Trained batch 571 batch loss 1.69990897 epoch total loss 1.68906844\n",
      "Trained batch 572 batch loss 1.69808233 epoch total loss 1.68908417\n",
      "Trained batch 573 batch loss 1.68165219 epoch total loss 1.68907118\n",
      "Trained batch 574 batch loss 1.66755271 epoch total loss 1.68903363\n",
      "Trained batch 575 batch loss 1.71669507 epoch total loss 1.68908167\n",
      "Trained batch 576 batch loss 1.70895886 epoch total loss 1.68911624\n",
      "Trained batch 577 batch loss 1.6959393 epoch total loss 1.68912804\n",
      "Trained batch 578 batch loss 1.70821416 epoch total loss 1.68916106\n",
      "Trained batch 579 batch loss 1.66247547 epoch total loss 1.68911493\n",
      "Trained batch 580 batch loss 1.52657139 epoch total loss 1.68883467\n",
      "Trained batch 581 batch loss 1.68110251 epoch total loss 1.68882132\n",
      "Trained batch 582 batch loss 1.68492699 epoch total loss 1.68881464\n",
      "Trained batch 583 batch loss 1.73618817 epoch total loss 1.68889594\n",
      "Trained batch 584 batch loss 1.72787118 epoch total loss 1.6889627\n",
      "Trained batch 585 batch loss 1.77613413 epoch total loss 1.68911159\n",
      "Trained batch 586 batch loss 1.70934963 epoch total loss 1.68914616\n",
      "Trained batch 587 batch loss 1.68504763 epoch total loss 1.68913925\n",
      "Trained batch 588 batch loss 1.69307423 epoch total loss 1.68914592\n",
      "Trained batch 589 batch loss 1.7400732 epoch total loss 1.68923235\n",
      "Trained batch 590 batch loss 1.75129378 epoch total loss 1.68933749\n",
      "Trained batch 591 batch loss 1.74545097 epoch total loss 1.68943238\n",
      "Trained batch 592 batch loss 1.71063602 epoch total loss 1.68946815\n",
      "Trained batch 593 batch loss 1.66665459 epoch total loss 1.68942964\n",
      "Trained batch 594 batch loss 1.72156215 epoch total loss 1.68948376\n",
      "Trained batch 595 batch loss 1.72137308 epoch total loss 1.68953729\n",
      "Trained batch 596 batch loss 1.74195874 epoch total loss 1.68962526\n",
      "Trained batch 597 batch loss 1.65039504 epoch total loss 1.68955958\n",
      "Trained batch 598 batch loss 1.52814424 epoch total loss 1.68928957\n",
      "Trained batch 599 batch loss 1.68846571 epoch total loss 1.68928826\n",
      "Trained batch 600 batch loss 1.73834729 epoch total loss 1.68937\n",
      "Trained batch 601 batch loss 1.71598649 epoch total loss 1.68941438\n",
      "Trained batch 602 batch loss 1.67920494 epoch total loss 1.68939734\n",
      "Trained batch 603 batch loss 1.74998367 epoch total loss 1.68949783\n",
      "Trained batch 604 batch loss 1.74278021 epoch total loss 1.68958616\n",
      "Trained batch 605 batch loss 1.70177984 epoch total loss 1.68960631\n",
      "Trained batch 606 batch loss 1.73462772 epoch total loss 1.68968058\n",
      "Trained batch 607 batch loss 1.73691022 epoch total loss 1.68975842\n",
      "Trained batch 608 batch loss 1.68734455 epoch total loss 1.68975449\n",
      "Trained batch 609 batch loss 1.71454298 epoch total loss 1.68979526\n",
      "Trained batch 610 batch loss 1.78440821 epoch total loss 1.68995047\n",
      "Trained batch 611 batch loss 1.73061109 epoch total loss 1.69001698\n",
      "Trained batch 612 batch loss 1.73358357 epoch total loss 1.69008815\n",
      "Trained batch 613 batch loss 1.77719176 epoch total loss 1.69023037\n",
      "Trained batch 614 batch loss 1.72899795 epoch total loss 1.69029355\n",
      "Trained batch 615 batch loss 1.62423658 epoch total loss 1.69018614\n",
      "Trained batch 616 batch loss 1.77032018 epoch total loss 1.69031608\n",
      "Trained batch 617 batch loss 1.76343465 epoch total loss 1.69043458\n",
      "Trained batch 618 batch loss 1.78821087 epoch total loss 1.69059289\n",
      "Trained batch 619 batch loss 1.76768565 epoch total loss 1.69071746\n",
      "Trained batch 620 batch loss 1.72062778 epoch total loss 1.69076562\n",
      "Trained batch 621 batch loss 1.73748696 epoch total loss 1.69084072\n",
      "Trained batch 622 batch loss 1.69056571 epoch total loss 1.69084024\n",
      "Trained batch 623 batch loss 1.76223338 epoch total loss 1.6909548\n",
      "Trained batch 624 batch loss 1.77354312 epoch total loss 1.69108713\n",
      "Trained batch 625 batch loss 1.72689438 epoch total loss 1.69114459\n",
      "Trained batch 626 batch loss 1.70994127 epoch total loss 1.69117463\n",
      "Trained batch 627 batch loss 1.6116637 epoch total loss 1.69104779\n",
      "Trained batch 628 batch loss 1.54974365 epoch total loss 1.69082272\n",
      "Trained batch 629 batch loss 1.50920153 epoch total loss 1.69053388\n",
      "Trained batch 630 batch loss 1.63389695 epoch total loss 1.69044399\n",
      "Trained batch 631 batch loss 1.71385694 epoch total loss 1.69048119\n",
      "Trained batch 632 batch loss 1.66751254 epoch total loss 1.69044471\n",
      "Trained batch 633 batch loss 1.77386022 epoch total loss 1.69057643\n",
      "Trained batch 634 batch loss 1.66090727 epoch total loss 1.69052958\n",
      "Trained batch 635 batch loss 1.8216542 epoch total loss 1.69073606\n",
      "Trained batch 636 batch loss 1.75193214 epoch total loss 1.69083238\n",
      "Trained batch 637 batch loss 1.72882628 epoch total loss 1.6908921\n",
      "Trained batch 638 batch loss 1.72064888 epoch total loss 1.69093883\n",
      "Trained batch 639 batch loss 1.72922885 epoch total loss 1.69099879\n",
      "Trained batch 640 batch loss 1.63873863 epoch total loss 1.69091725\n",
      "Trained batch 641 batch loss 1.56745636 epoch total loss 1.69072473\n",
      "Trained batch 642 batch loss 1.52272987 epoch total loss 1.69046295\n",
      "Trained batch 643 batch loss 1.63142347 epoch total loss 1.69037116\n",
      "Trained batch 644 batch loss 1.58265901 epoch total loss 1.69020391\n",
      "Trained batch 645 batch loss 1.56079745 epoch total loss 1.69000328\n",
      "Trained batch 646 batch loss 1.71973753 epoch total loss 1.69004929\n",
      "Trained batch 647 batch loss 1.68761396 epoch total loss 1.6900456\n",
      "Trained batch 648 batch loss 1.58201408 epoch total loss 1.68987882\n",
      "Trained batch 649 batch loss 1.57881069 epoch total loss 1.68970776\n",
      "Trained batch 650 batch loss 1.57801688 epoch total loss 1.68953598\n",
      "Trained batch 651 batch loss 1.77394676 epoch total loss 1.68966556\n",
      "Trained batch 652 batch loss 1.66609192 epoch total loss 1.68962944\n",
      "Trained batch 653 batch loss 1.64211953 epoch total loss 1.68955672\n",
      "Trained batch 654 batch loss 1.72207475 epoch total loss 1.68960631\n",
      "Trained batch 655 batch loss 1.57841694 epoch total loss 1.68943655\n",
      "Trained batch 656 batch loss 1.7048558 epoch total loss 1.68946\n",
      "Trained batch 657 batch loss 1.65646493 epoch total loss 1.68940985\n",
      "Trained batch 658 batch loss 1.6448524 epoch total loss 1.68934214\n",
      "Trained batch 659 batch loss 1.74064922 epoch total loss 1.68942\n",
      "Trained batch 660 batch loss 1.73203778 epoch total loss 1.6894846\n",
      "Trained batch 661 batch loss 1.73316026 epoch total loss 1.68955064\n",
      "Trained batch 662 batch loss 1.6604799 epoch total loss 1.68950677\n",
      "Trained batch 663 batch loss 1.72410285 epoch total loss 1.68955898\n",
      "Trained batch 664 batch loss 1.60957277 epoch total loss 1.68943858\n",
      "Trained batch 665 batch loss 1.64727688 epoch total loss 1.68937516\n",
      "Trained batch 666 batch loss 1.62067497 epoch total loss 1.68927205\n",
      "Trained batch 667 batch loss 1.61688805 epoch total loss 1.68916357\n",
      "Trained batch 668 batch loss 1.58234119 epoch total loss 1.68900383\n",
      "Trained batch 669 batch loss 1.65862107 epoch total loss 1.68895829\n",
      "Trained batch 670 batch loss 1.64877224 epoch total loss 1.68889832\n",
      "Trained batch 671 batch loss 1.69446707 epoch total loss 1.68890667\n",
      "Trained batch 672 batch loss 1.63463092 epoch total loss 1.68882585\n",
      "Trained batch 673 batch loss 1.46201181 epoch total loss 1.68848884\n",
      "Trained batch 674 batch loss 1.40233386 epoch total loss 1.68806434\n",
      "Trained batch 675 batch loss 1.49962139 epoch total loss 1.68778515\n",
      "Trained batch 676 batch loss 1.6717149 epoch total loss 1.68776143\n",
      "Trained batch 677 batch loss 1.77693522 epoch total loss 1.68789327\n",
      "Trained batch 678 batch loss 1.63082898 epoch total loss 1.68780911\n",
      "Trained batch 679 batch loss 1.58774614 epoch total loss 1.68766177\n",
      "Trained batch 680 batch loss 1.59568739 epoch total loss 1.68752658\n",
      "Trained batch 681 batch loss 1.61628008 epoch total loss 1.68742204\n",
      "Trained batch 682 batch loss 1.780761 epoch total loss 1.68755889\n",
      "Trained batch 683 batch loss 1.77772236 epoch total loss 1.68769085\n",
      "Trained batch 684 batch loss 1.77632427 epoch total loss 1.68782055\n",
      "Trained batch 685 batch loss 1.73103023 epoch total loss 1.68788362\n",
      "Trained batch 686 batch loss 1.74342799 epoch total loss 1.68796456\n",
      "Trained batch 687 batch loss 1.6776818 epoch total loss 1.68794978\n",
      "Trained batch 688 batch loss 1.79040575 epoch total loss 1.68809867\n",
      "Trained batch 689 batch loss 1.6442759 epoch total loss 1.68803501\n",
      "Trained batch 690 batch loss 1.68577433 epoch total loss 1.68803179\n",
      "Trained batch 691 batch loss 1.71122491 epoch total loss 1.68806529\n",
      "Trained batch 692 batch loss 1.70820642 epoch total loss 1.6880945\n",
      "Trained batch 693 batch loss 1.59964466 epoch total loss 1.68796682\n",
      "Trained batch 694 batch loss 1.4680295 epoch total loss 1.68764985\n",
      "Trained batch 695 batch loss 1.66717958 epoch total loss 1.68762052\n",
      "Trained batch 696 batch loss 1.65743279 epoch total loss 1.68757713\n",
      "Trained batch 697 batch loss 1.70710146 epoch total loss 1.68760526\n",
      "Trained batch 698 batch loss 1.69068098 epoch total loss 1.68760967\n",
      "Trained batch 699 batch loss 1.74144781 epoch total loss 1.68768668\n",
      "Trained batch 700 batch loss 1.76334822 epoch total loss 1.68779469\n",
      "Trained batch 701 batch loss 1.73150611 epoch total loss 1.68785703\n",
      "Trained batch 702 batch loss 1.65512538 epoch total loss 1.68781042\n",
      "Trained batch 703 batch loss 1.65683341 epoch total loss 1.68776631\n",
      "Trained batch 704 batch loss 1.64830339 epoch total loss 1.68771029\n",
      "Trained batch 705 batch loss 1.68372822 epoch total loss 1.68770468\n",
      "Trained batch 706 batch loss 1.73089957 epoch total loss 1.68776596\n",
      "Trained batch 707 batch loss 1.8054328 epoch total loss 1.68793237\n",
      "Trained batch 708 batch loss 1.77462387 epoch total loss 1.6880548\n",
      "Trained batch 709 batch loss 1.77540588 epoch total loss 1.68817806\n",
      "Trained batch 710 batch loss 1.78583646 epoch total loss 1.68831563\n",
      "Trained batch 711 batch loss 1.78894532 epoch total loss 1.68845713\n",
      "Trained batch 712 batch loss 1.84045589 epoch total loss 1.68867064\n",
      "Trained batch 713 batch loss 1.77684 epoch total loss 1.68879437\n",
      "Trained batch 714 batch loss 1.78397775 epoch total loss 1.68892753\n",
      "Trained batch 715 batch loss 1.73000693 epoch total loss 1.68898499\n",
      "Trained batch 716 batch loss 1.61104584 epoch total loss 1.68887615\n",
      "Trained batch 717 batch loss 1.63749504 epoch total loss 1.68880451\n",
      "Trained batch 718 batch loss 1.59081137 epoch total loss 1.68866801\n",
      "Trained batch 719 batch loss 1.67274642 epoch total loss 1.68864584\n",
      "Trained batch 720 batch loss 1.60037458 epoch total loss 1.68852317\n",
      "Trained batch 721 batch loss 1.58670449 epoch total loss 1.68838191\n",
      "Trained batch 722 batch loss 1.63151407 epoch total loss 1.68830311\n",
      "Trained batch 723 batch loss 1.67254508 epoch total loss 1.68828118\n",
      "Trained batch 724 batch loss 1.61153305 epoch total loss 1.68817532\n",
      "Trained batch 725 batch loss 1.61975551 epoch total loss 1.68808091\n",
      "Trained batch 726 batch loss 1.76578975 epoch total loss 1.68818784\n",
      "Trained batch 727 batch loss 1.6829772 epoch total loss 1.68818069\n",
      "Trained batch 728 batch loss 1.52249742 epoch total loss 1.68795311\n",
      "Trained batch 729 batch loss 1.69188941 epoch total loss 1.68795848\n",
      "Trained batch 730 batch loss 1.70535684 epoch total loss 1.68798232\n",
      "Trained batch 731 batch loss 1.72489834 epoch total loss 1.68803275\n",
      "Trained batch 732 batch loss 1.74741125 epoch total loss 1.68811381\n",
      "Trained batch 733 batch loss 1.69324052 epoch total loss 1.68812084\n",
      "Trained batch 734 batch loss 1.64800882 epoch total loss 1.68806612\n",
      "Trained batch 735 batch loss 1.67705917 epoch total loss 1.6880511\n",
      "Trained batch 736 batch loss 1.64914024 epoch total loss 1.68799818\n",
      "Trained batch 737 batch loss 1.52821422 epoch total loss 1.68778145\n",
      "Trained batch 738 batch loss 1.58156419 epoch total loss 1.68763745\n",
      "Trained batch 739 batch loss 1.6957196 epoch total loss 1.6876483\n",
      "Trained batch 740 batch loss 1.70264649 epoch total loss 1.68766856\n",
      "Trained batch 741 batch loss 1.67354929 epoch total loss 1.68764961\n",
      "Trained batch 742 batch loss 1.54424119 epoch total loss 1.68745625\n",
      "Trained batch 743 batch loss 1.62752521 epoch total loss 1.68737566\n",
      "Trained batch 744 batch loss 1.54390526 epoch total loss 1.6871829\n",
      "Trained batch 745 batch loss 1.78450108 epoch total loss 1.68731356\n",
      "Trained batch 746 batch loss 1.77311504 epoch total loss 1.68742847\n",
      "Trained batch 747 batch loss 1.74503 epoch total loss 1.6875056\n",
      "Trained batch 748 batch loss 1.69780767 epoch total loss 1.68751931\n",
      "Trained batch 749 batch loss 1.68383634 epoch total loss 1.68751431\n",
      "Trained batch 750 batch loss 1.64027572 epoch total loss 1.68745136\n",
      "Trained batch 751 batch loss 1.67866838 epoch total loss 1.68743968\n",
      "Trained batch 752 batch loss 1.65700638 epoch total loss 1.68739915\n",
      "Trained batch 753 batch loss 1.67344368 epoch total loss 1.68738067\n",
      "Trained batch 754 batch loss 1.78985393 epoch total loss 1.68751657\n",
      "Trained batch 755 batch loss 1.79066992 epoch total loss 1.68765306\n",
      "Trained batch 756 batch loss 1.78865767 epoch total loss 1.68778682\n",
      "Trained batch 757 batch loss 1.79082799 epoch total loss 1.68792284\n",
      "Trained batch 758 batch loss 1.67350483 epoch total loss 1.68790376\n",
      "Trained batch 759 batch loss 1.77033484 epoch total loss 1.68801236\n",
      "Trained batch 760 batch loss 1.75024343 epoch total loss 1.68809426\n",
      "Trained batch 761 batch loss 1.7605207 epoch total loss 1.68818939\n",
      "Trained batch 762 batch loss 1.72107661 epoch total loss 1.68823254\n",
      "Trained batch 763 batch loss 1.70763731 epoch total loss 1.68825805\n",
      "Trained batch 764 batch loss 1.73151362 epoch total loss 1.68831468\n",
      "Trained batch 765 batch loss 1.68105876 epoch total loss 1.68830514\n",
      "Trained batch 766 batch loss 1.6776042 epoch total loss 1.68829119\n",
      "Trained batch 767 batch loss 1.70316863 epoch total loss 1.68831062\n",
      "Trained batch 768 batch loss 1.67037702 epoch total loss 1.68828726\n",
      "Trained batch 769 batch loss 1.6677258 epoch total loss 1.68826056\n",
      "Trained batch 770 batch loss 1.68122101 epoch total loss 1.6882515\n",
      "Trained batch 771 batch loss 1.69425738 epoch total loss 1.68825912\n",
      "Trained batch 772 batch loss 1.7586267 epoch total loss 1.68835044\n",
      "Trained batch 773 batch loss 1.74550891 epoch total loss 1.68842435\n",
      "Trained batch 774 batch loss 1.7488035 epoch total loss 1.68850231\n",
      "Trained batch 775 batch loss 1.68640041 epoch total loss 1.68849957\n",
      "Trained batch 776 batch loss 1.71024811 epoch total loss 1.68852758\n",
      "Trained batch 777 batch loss 1.60846484 epoch total loss 1.68842459\n",
      "Trained batch 778 batch loss 1.64263916 epoch total loss 1.68836582\n",
      "Trained batch 779 batch loss 1.51207662 epoch total loss 1.68813944\n",
      "Trained batch 780 batch loss 1.66420865 epoch total loss 1.6881088\n",
      "Trained batch 781 batch loss 1.71351528 epoch total loss 1.68814135\n",
      "Trained batch 782 batch loss 1.68250978 epoch total loss 1.68813407\n",
      "Trained batch 783 batch loss 1.71488976 epoch total loss 1.68816817\n",
      "Trained batch 784 batch loss 1.55531085 epoch total loss 1.68799877\n",
      "Trained batch 785 batch loss 1.69592214 epoch total loss 1.68800879\n",
      "Trained batch 786 batch loss 1.71171427 epoch total loss 1.68803895\n",
      "Trained batch 787 batch loss 1.65240455 epoch total loss 1.68799353\n",
      "Trained batch 788 batch loss 1.60329413 epoch total loss 1.687886\n",
      "Trained batch 789 batch loss 1.57230926 epoch total loss 1.68773949\n",
      "Trained batch 790 batch loss 1.70526 epoch total loss 1.68776155\n",
      "Trained batch 791 batch loss 1.74687696 epoch total loss 1.68783629\n",
      "Trained batch 792 batch loss 1.78351712 epoch total loss 1.68795717\n",
      "Trained batch 793 batch loss 1.73787916 epoch total loss 1.68802011\n",
      "Trained batch 794 batch loss 1.76822639 epoch total loss 1.68812108\n",
      "Trained batch 795 batch loss 1.75262666 epoch total loss 1.68820238\n",
      "Trained batch 796 batch loss 1.75442505 epoch total loss 1.68828547\n",
      "Trained batch 797 batch loss 1.69203639 epoch total loss 1.68829012\n",
      "Trained batch 798 batch loss 1.73100591 epoch total loss 1.68834364\n",
      "Trained batch 799 batch loss 1.76857781 epoch total loss 1.68844402\n",
      "Trained batch 800 batch loss 1.75192356 epoch total loss 1.68852341\n",
      "Trained batch 801 batch loss 1.71539307 epoch total loss 1.68855703\n",
      "Trained batch 802 batch loss 1.7335434 epoch total loss 1.68861306\n",
      "Trained batch 803 batch loss 1.75147605 epoch total loss 1.68869138\n",
      "Trained batch 804 batch loss 1.6390872 epoch total loss 1.68862963\n",
      "Trained batch 805 batch loss 1.74234343 epoch total loss 1.68869627\n",
      "Trained batch 806 batch loss 1.47297728 epoch total loss 1.68842876\n",
      "Trained batch 807 batch loss 1.55186963 epoch total loss 1.68825948\n",
      "Trained batch 808 batch loss 1.55211711 epoch total loss 1.68809104\n",
      "Trained batch 809 batch loss 1.63451695 epoch total loss 1.68802476\n",
      "Trained batch 810 batch loss 1.47119009 epoch total loss 1.68775713\n",
      "Trained batch 811 batch loss 1.4852823 epoch total loss 1.68750739\n",
      "Trained batch 812 batch loss 1.47192729 epoch total loss 1.68724191\n",
      "Trained batch 813 batch loss 1.50108755 epoch total loss 1.68701291\n",
      "Trained batch 814 batch loss 1.51577783 epoch total loss 1.68680251\n",
      "Trained batch 815 batch loss 1.56160116 epoch total loss 1.68664896\n",
      "Trained batch 816 batch loss 1.68496263 epoch total loss 1.68664682\n",
      "Trained batch 817 batch loss 1.62386048 epoch total loss 1.68657\n",
      "Trained batch 818 batch loss 1.60911691 epoch total loss 1.6864754\n",
      "Trained batch 819 batch loss 1.5741818 epoch total loss 1.68633831\n",
      "Trained batch 820 batch loss 1.55311882 epoch total loss 1.68617582\n",
      "Trained batch 821 batch loss 1.72039461 epoch total loss 1.68621743\n",
      "Trained batch 822 batch loss 1.656003 epoch total loss 1.68618071\n",
      "Trained batch 823 batch loss 1.79164517 epoch total loss 1.68630886\n",
      "Trained batch 824 batch loss 1.75891662 epoch total loss 1.68639696\n",
      "Trained batch 825 batch loss 1.68598521 epoch total loss 1.68639648\n",
      "Trained batch 826 batch loss 1.66077828 epoch total loss 1.68636549\n",
      "Trained batch 827 batch loss 1.63411748 epoch total loss 1.6863023\n",
      "Trained batch 828 batch loss 1.62693965 epoch total loss 1.68623066\n",
      "Trained batch 829 batch loss 1.60374928 epoch total loss 1.68613112\n",
      "Trained batch 830 batch loss 1.77454805 epoch total loss 1.68623769\n",
      "Trained batch 831 batch loss 1.78728557 epoch total loss 1.68635917\n",
      "Trained batch 832 batch loss 1.7608285 epoch total loss 1.68644881\n",
      "Trained batch 833 batch loss 1.7057029 epoch total loss 1.68647182\n",
      "Trained batch 834 batch loss 1.67642522 epoch total loss 1.68645978\n",
      "Trained batch 835 batch loss 1.71474183 epoch total loss 1.68649364\n",
      "Trained batch 836 batch loss 1.66608202 epoch total loss 1.68646932\n",
      "Trained batch 837 batch loss 1.72197711 epoch total loss 1.68651164\n",
      "Trained batch 838 batch loss 1.64464271 epoch total loss 1.68646169\n",
      "Trained batch 839 batch loss 1.72709346 epoch total loss 1.68651009\n",
      "Trained batch 840 batch loss 1.76123083 epoch total loss 1.68659902\n",
      "Trained batch 841 batch loss 1.73744428 epoch total loss 1.68665946\n",
      "Trained batch 842 batch loss 1.78472567 epoch total loss 1.6867758\n",
      "Trained batch 843 batch loss 1.63130116 epoch total loss 1.68671012\n",
      "Trained batch 844 batch loss 1.70648611 epoch total loss 1.6867336\n",
      "Trained batch 845 batch loss 1.69621527 epoch total loss 1.68674481\n",
      "Trained batch 846 batch loss 1.68938482 epoch total loss 1.68674779\n",
      "Trained batch 847 batch loss 1.60457683 epoch total loss 1.68665087\n",
      "Trained batch 848 batch loss 1.52430725 epoch total loss 1.68645942\n",
      "Trained batch 849 batch loss 1.53000975 epoch total loss 1.68627512\n",
      "Trained batch 850 batch loss 1.53915775 epoch total loss 1.68610203\n",
      "Trained batch 851 batch loss 1.54943681 epoch total loss 1.68594146\n",
      "Trained batch 852 batch loss 1.5721035 epoch total loss 1.68580794\n",
      "Trained batch 853 batch loss 1.41654193 epoch total loss 1.68549216\n",
      "Trained batch 854 batch loss 1.72905588 epoch total loss 1.68554318\n",
      "Trained batch 855 batch loss 1.75355792 epoch total loss 1.68562269\n",
      "Trained batch 856 batch loss 1.7836076 epoch total loss 1.68573713\n",
      "Trained batch 857 batch loss 1.75856686 epoch total loss 1.68582201\n",
      "Trained batch 858 batch loss 1.73863268 epoch total loss 1.68588364\n",
      "Trained batch 859 batch loss 1.71537423 epoch total loss 1.68591797\n",
      "Trained batch 860 batch loss 1.61810184 epoch total loss 1.68583894\n",
      "Trained batch 861 batch loss 1.50012803 epoch total loss 1.68562329\n",
      "Trained batch 862 batch loss 1.63137448 epoch total loss 1.68556035\n",
      "Trained batch 863 batch loss 1.6965704 epoch total loss 1.6855731\n",
      "Trained batch 864 batch loss 1.65134811 epoch total loss 1.6855334\n",
      "Trained batch 865 batch loss 1.69305539 epoch total loss 1.68554223\n",
      "Trained batch 866 batch loss 1.70886207 epoch total loss 1.68556917\n",
      "Trained batch 867 batch loss 1.60568357 epoch total loss 1.68547702\n",
      "Trained batch 868 batch loss 1.61951184 epoch total loss 1.68540108\n",
      "Trained batch 869 batch loss 1.62556052 epoch total loss 1.68533218\n",
      "Trained batch 870 batch loss 1.76777 epoch total loss 1.68542707\n",
      "Trained batch 871 batch loss 1.69303262 epoch total loss 1.68543577\n",
      "Trained batch 872 batch loss 1.58529305 epoch total loss 1.68532097\n",
      "Trained batch 873 batch loss 1.66670084 epoch total loss 1.68529963\n",
      "Trained batch 874 batch loss 1.70807934 epoch total loss 1.68532574\n",
      "Trained batch 875 batch loss 1.70846844 epoch total loss 1.68535221\n",
      "Trained batch 876 batch loss 1.6911391 epoch total loss 1.68535888\n",
      "Trained batch 877 batch loss 1.71772337 epoch total loss 1.68539584\n",
      "Trained batch 878 batch loss 1.57457614 epoch total loss 1.68526959\n",
      "Trained batch 879 batch loss 1.57714605 epoch total loss 1.68514669\n",
      "Trained batch 880 batch loss 1.72207546 epoch total loss 1.68518853\n",
      "Trained batch 881 batch loss 1.59166765 epoch total loss 1.68508244\n",
      "Trained batch 882 batch loss 1.6510036 epoch total loss 1.68504381\n",
      "Trained batch 883 batch loss 1.6801914 epoch total loss 1.68503833\n",
      "Trained batch 884 batch loss 1.73226845 epoch total loss 1.68509173\n",
      "Trained batch 885 batch loss 1.70858848 epoch total loss 1.68511832\n",
      "Trained batch 886 batch loss 1.64410353 epoch total loss 1.68507195\n",
      "Trained batch 887 batch loss 1.70153356 epoch total loss 1.68509054\n",
      "Trained batch 888 batch loss 1.655738 epoch total loss 1.68505752\n",
      "Trained batch 889 batch loss 1.70481873 epoch total loss 1.68507969\n",
      "Trained batch 890 batch loss 1.68793309 epoch total loss 1.68508303\n",
      "Trained batch 891 batch loss 1.61027706 epoch total loss 1.68499899\n",
      "Trained batch 892 batch loss 1.61576259 epoch total loss 1.68492138\n",
      "Trained batch 893 batch loss 1.71348524 epoch total loss 1.68495333\n",
      "Trained batch 894 batch loss 1.71075916 epoch total loss 1.6849823\n",
      "Trained batch 895 batch loss 1.69436026 epoch total loss 1.68499267\n",
      "Trained batch 896 batch loss 1.70835888 epoch total loss 1.68501878\n",
      "Trained batch 897 batch loss 1.64125788 epoch total loss 1.68497\n",
      "Trained batch 898 batch loss 1.53570592 epoch total loss 1.68480384\n",
      "Trained batch 899 batch loss 1.6806854 epoch total loss 1.68479919\n",
      "Trained batch 900 batch loss 1.65141106 epoch total loss 1.68476212\n",
      "Trained batch 901 batch loss 1.60040605 epoch total loss 1.68466854\n",
      "Trained batch 902 batch loss 1.60074687 epoch total loss 1.68457544\n",
      "Trained batch 903 batch loss 1.52902913 epoch total loss 1.68440318\n",
      "Trained batch 904 batch loss 1.60693598 epoch total loss 1.68431759\n",
      "Trained batch 905 batch loss 1.66752362 epoch total loss 1.68429887\n",
      "Trained batch 906 batch loss 1.75256085 epoch total loss 1.68437421\n",
      "Trained batch 907 batch loss 1.74783516 epoch total loss 1.68444419\n",
      "Trained batch 908 batch loss 1.72637284 epoch total loss 1.68449032\n",
      "Trained batch 909 batch loss 1.66912961 epoch total loss 1.68447351\n",
      "Trained batch 910 batch loss 1.57556343 epoch total loss 1.68435383\n",
      "Trained batch 911 batch loss 1.59420884 epoch total loss 1.68425488\n",
      "Trained batch 912 batch loss 1.5913229 epoch total loss 1.68415296\n",
      "Trained batch 913 batch loss 1.60721695 epoch total loss 1.68406868\n",
      "Trained batch 914 batch loss 1.62736607 epoch total loss 1.68400657\n",
      "Trained batch 915 batch loss 1.60251188 epoch total loss 1.68391752\n",
      "Trained batch 916 batch loss 1.61512423 epoch total loss 1.68384242\n",
      "Trained batch 917 batch loss 1.5734992 epoch total loss 1.68372202\n",
      "Trained batch 918 batch loss 1.66435742 epoch total loss 1.68370092\n",
      "Trained batch 919 batch loss 1.65914142 epoch total loss 1.68367422\n",
      "Trained batch 920 batch loss 1.70825136 epoch total loss 1.68370092\n",
      "Trained batch 921 batch loss 1.69693816 epoch total loss 1.68371534\n",
      "Trained batch 922 batch loss 1.63879657 epoch total loss 1.68366659\n",
      "Trained batch 923 batch loss 1.61583138 epoch total loss 1.68359303\n",
      "Trained batch 924 batch loss 1.63563633 epoch total loss 1.68354118\n",
      "Trained batch 925 batch loss 1.70132113 epoch total loss 1.68356037\n",
      "Trained batch 926 batch loss 1.66480744 epoch total loss 1.68354011\n",
      "Trained batch 927 batch loss 1.70781279 epoch total loss 1.68356621\n",
      "Trained batch 928 batch loss 1.75741649 epoch total loss 1.68364584\n",
      "Trained batch 929 batch loss 1.70634878 epoch total loss 1.68367028\n",
      "Trained batch 930 batch loss 1.58458686 epoch total loss 1.68356371\n",
      "Trained batch 931 batch loss 1.53489459 epoch total loss 1.68340397\n",
      "Trained batch 932 batch loss 1.75436807 epoch total loss 1.68348014\n",
      "Trained batch 933 batch loss 1.69292521 epoch total loss 1.68349028\n",
      "Trained batch 934 batch loss 1.68688631 epoch total loss 1.68349385\n",
      "Trained batch 935 batch loss 1.75284159 epoch total loss 1.683568\n",
      "Trained batch 936 batch loss 1.72235692 epoch total loss 1.68360949\n",
      "Trained batch 937 batch loss 1.76075161 epoch total loss 1.68369186\n",
      "Trained batch 938 batch loss 1.73966789 epoch total loss 1.68375146\n",
      "Trained batch 939 batch loss 1.67327201 epoch total loss 1.68374026\n",
      "Trained batch 940 batch loss 1.67155385 epoch total loss 1.68372726\n",
      "Trained batch 941 batch loss 1.64357316 epoch total loss 1.68368459\n",
      "Trained batch 942 batch loss 1.69189 epoch total loss 1.68369329\n",
      "Trained batch 943 batch loss 1.74505973 epoch total loss 1.68375838\n",
      "Trained batch 944 batch loss 1.75193298 epoch total loss 1.68383062\n",
      "Trained batch 945 batch loss 1.63235831 epoch total loss 1.68377614\n",
      "Trained batch 946 batch loss 1.723791 epoch total loss 1.68381846\n",
      "Trained batch 947 batch loss 1.69141507 epoch total loss 1.68382645\n",
      "Trained batch 948 batch loss 1.71833861 epoch total loss 1.68386292\n",
      "Trained batch 949 batch loss 1.76290321 epoch total loss 1.68394625\n",
      "Trained batch 950 batch loss 1.72537184 epoch total loss 1.68398976\n",
      "Trained batch 951 batch loss 1.70860887 epoch total loss 1.68401563\n",
      "Trained batch 952 batch loss 1.68532252 epoch total loss 1.68401706\n",
      "Trained batch 953 batch loss 1.76821303 epoch total loss 1.6841054\n",
      "Trained batch 954 batch loss 1.75538015 epoch total loss 1.68418\n",
      "Trained batch 955 batch loss 1.78850508 epoch total loss 1.68428922\n",
      "Trained batch 956 batch loss 1.72594786 epoch total loss 1.68433285\n",
      "Trained batch 957 batch loss 1.66856408 epoch total loss 1.6843164\n",
      "Trained batch 958 batch loss 1.6271497 epoch total loss 1.68425667\n",
      "Trained batch 959 batch loss 1.70133805 epoch total loss 1.68427444\n",
      "Trained batch 960 batch loss 1.69055402 epoch total loss 1.68428099\n",
      "Trained batch 961 batch loss 1.65722752 epoch total loss 1.68425286\n",
      "Trained batch 962 batch loss 1.6824261 epoch total loss 1.68425095\n",
      "Trained batch 963 batch loss 1.69312656 epoch total loss 1.68426013\n",
      "Trained batch 964 batch loss 1.6575315 epoch total loss 1.68423235\n",
      "Trained batch 965 batch loss 1.60532 epoch total loss 1.68415058\n",
      "Trained batch 966 batch loss 1.67936897 epoch total loss 1.68414557\n",
      "Trained batch 967 batch loss 1.73504651 epoch total loss 1.68419826\n",
      "Trained batch 968 batch loss 1.77029145 epoch total loss 1.68428719\n",
      "Trained batch 969 batch loss 1.77415943 epoch total loss 1.68437994\n",
      "Trained batch 970 batch loss 1.65752137 epoch total loss 1.68435228\n",
      "Trained batch 971 batch loss 1.50312316 epoch total loss 1.6841656\n",
      "Trained batch 972 batch loss 1.57080126 epoch total loss 1.68404901\n",
      "Trained batch 973 batch loss 1.53974 epoch total loss 1.68390071\n",
      "Trained batch 974 batch loss 1.70016873 epoch total loss 1.68391752\n",
      "Trained batch 975 batch loss 1.67858839 epoch total loss 1.68391204\n",
      "Trained batch 976 batch loss 1.7585789 epoch total loss 1.68398845\n",
      "Trained batch 977 batch loss 1.67069376 epoch total loss 1.68397486\n",
      "Trained batch 978 batch loss 1.71107292 epoch total loss 1.68400252\n",
      "Trained batch 979 batch loss 1.61913264 epoch total loss 1.68393624\n",
      "Trained batch 980 batch loss 1.70308506 epoch total loss 1.68395591\n",
      "Trained batch 981 batch loss 1.71007669 epoch total loss 1.68398249\n",
      "Trained batch 982 batch loss 1.72393608 epoch total loss 1.68402314\n",
      "Trained batch 983 batch loss 1.7193718 epoch total loss 1.68405902\n",
      "Trained batch 984 batch loss 1.70817947 epoch total loss 1.68408346\n",
      "Trained batch 985 batch loss 1.6380167 epoch total loss 1.68403685\n",
      "Trained batch 986 batch loss 1.72976601 epoch total loss 1.6840831\n",
      "Trained batch 987 batch loss 1.67663205 epoch total loss 1.68407559\n",
      "Trained batch 988 batch loss 1.69618702 epoch total loss 1.68408787\n",
      "Trained batch 989 batch loss 1.67365193 epoch total loss 1.68407738\n",
      "Trained batch 990 batch loss 1.58332336 epoch total loss 1.68397558\n",
      "Trained batch 991 batch loss 1.72997355 epoch total loss 1.68402207\n",
      "Trained batch 992 batch loss 1.75915515 epoch total loss 1.68409777\n",
      "Trained batch 993 batch loss 1.76739502 epoch total loss 1.68418157\n",
      "Trained batch 994 batch loss 1.76439381 epoch total loss 1.68426228\n",
      "Trained batch 995 batch loss 1.7854104 epoch total loss 1.68436396\n",
      "Trained batch 996 batch loss 1.74746859 epoch total loss 1.68442726\n",
      "Trained batch 997 batch loss 1.64719224 epoch total loss 1.68439\n",
      "Trained batch 998 batch loss 1.55697083 epoch total loss 1.68426228\n",
      "Trained batch 999 batch loss 1.45727921 epoch total loss 1.68403506\n",
      "Trained batch 1000 batch loss 1.57851505 epoch total loss 1.68392956\n",
      "Trained batch 1001 batch loss 1.69877481 epoch total loss 1.68394434\n",
      "Trained batch 1002 batch loss 1.76253724 epoch total loss 1.68402278\n",
      "Trained batch 1003 batch loss 1.73971617 epoch total loss 1.68407834\n",
      "Trained batch 1004 batch loss 1.62834 epoch total loss 1.68402278\n",
      "Trained batch 1005 batch loss 1.51980174 epoch total loss 1.68385935\n",
      "Trained batch 1006 batch loss 1.65524721 epoch total loss 1.68383098\n",
      "Trained batch 1007 batch loss 1.7169292 epoch total loss 1.68386388\n",
      "Trained batch 1008 batch loss 1.71199536 epoch total loss 1.68389177\n",
      "Trained batch 1009 batch loss 1.69698942 epoch total loss 1.68390477\n",
      "Trained batch 1010 batch loss 1.62959206 epoch total loss 1.68385112\n",
      "Trained batch 1011 batch loss 1.58813834 epoch total loss 1.68375635\n",
      "Trained batch 1012 batch loss 1.63870335 epoch total loss 1.68371189\n",
      "Trained batch 1013 batch loss 1.66031647 epoch total loss 1.68368876\n",
      "Trained batch 1014 batch loss 1.71696484 epoch total loss 1.68372142\n",
      "Trained batch 1015 batch loss 1.70369601 epoch total loss 1.68374121\n",
      "Trained batch 1016 batch loss 1.62836289 epoch total loss 1.68368673\n",
      "Trained batch 1017 batch loss 1.66444349 epoch total loss 1.68366778\n",
      "Trained batch 1018 batch loss 1.68384409 epoch total loss 1.68366802\n",
      "Trained batch 1019 batch loss 1.70640767 epoch total loss 1.68369031\n",
      "Trained batch 1020 batch loss 1.72860932 epoch total loss 1.68373442\n",
      "Trained batch 1021 batch loss 1.73614264 epoch total loss 1.68378568\n",
      "Trained batch 1022 batch loss 1.73100948 epoch total loss 1.68383181\n",
      "Trained batch 1023 batch loss 1.67812586 epoch total loss 1.68382621\n",
      "Trained batch 1024 batch loss 1.73042858 epoch total loss 1.68387175\n",
      "Trained batch 1025 batch loss 1.77745724 epoch total loss 1.68396306\n",
      "Trained batch 1026 batch loss 1.59296203 epoch total loss 1.68387437\n",
      "Trained batch 1027 batch loss 1.62940145 epoch total loss 1.68382132\n",
      "Trained batch 1028 batch loss 1.53166234 epoch total loss 1.68367326\n",
      "Trained batch 1029 batch loss 1.46500707 epoch total loss 1.68346071\n",
      "Trained batch 1030 batch loss 1.55959821 epoch total loss 1.68334043\n",
      "Trained batch 1031 batch loss 1.50039923 epoch total loss 1.68316305\n",
      "Trained batch 1032 batch loss 1.41661429 epoch total loss 1.68290472\n",
      "Trained batch 1033 batch loss 1.466766 epoch total loss 1.68269551\n",
      "Trained batch 1034 batch loss 1.40924323 epoch total loss 1.6824311\n",
      "Trained batch 1035 batch loss 1.4842391 epoch total loss 1.68223965\n",
      "Trained batch 1036 batch loss 1.62418246 epoch total loss 1.68218362\n",
      "Trained batch 1037 batch loss 1.7151413 epoch total loss 1.68221533\n",
      "Trained batch 1038 batch loss 1.77838326 epoch total loss 1.68230796\n",
      "Trained batch 1039 batch loss 1.75952232 epoch total loss 1.68238235\n",
      "Trained batch 1040 batch loss 1.75327396 epoch total loss 1.68245053\n",
      "Trained batch 1041 batch loss 1.79817295 epoch total loss 1.68256176\n",
      "Trained batch 1042 batch loss 1.71007919 epoch total loss 1.6825881\n",
      "Trained batch 1043 batch loss 1.6693958 epoch total loss 1.68257546\n",
      "Trained batch 1044 batch loss 1.65674055 epoch total loss 1.68255079\n",
      "Trained batch 1045 batch loss 1.65829659 epoch total loss 1.68252754\n",
      "Trained batch 1046 batch loss 1.73137438 epoch total loss 1.68257427\n",
      "Trained batch 1047 batch loss 1.62837481 epoch total loss 1.68252254\n",
      "Trained batch 1048 batch loss 1.69580078 epoch total loss 1.68253517\n",
      "Trained batch 1049 batch loss 1.55660808 epoch total loss 1.68241513\n",
      "Trained batch 1050 batch loss 1.66588545 epoch total loss 1.68239939\n",
      "Trained batch 1051 batch loss 1.61302042 epoch total loss 1.68233347\n",
      "Trained batch 1052 batch loss 1.67588699 epoch total loss 1.68232727\n",
      "Trained batch 1053 batch loss 1.67008233 epoch total loss 1.68231571\n",
      "Trained batch 1054 batch loss 1.76285076 epoch total loss 1.682392\n",
      "Trained batch 1055 batch loss 1.95570207 epoch total loss 1.68265104\n",
      "Trained batch 1056 batch loss 2.06476784 epoch total loss 1.68301296\n",
      "Trained batch 1057 batch loss 1.81588221 epoch total loss 1.68313873\n",
      "Trained batch 1058 batch loss 1.81007898 epoch total loss 1.68325865\n",
      "Trained batch 1059 batch loss 1.73909605 epoch total loss 1.68331146\n",
      "Trained batch 1060 batch loss 1.54527903 epoch total loss 1.68318129\n",
      "Trained batch 1061 batch loss 1.46977806 epoch total loss 1.68298006\n",
      "Trained batch 1062 batch loss 1.5202204 epoch total loss 1.68282688\n",
      "Trained batch 1063 batch loss 1.77335119 epoch total loss 1.68291199\n",
      "Trained batch 1064 batch loss 1.74445868 epoch total loss 1.68296981\n",
      "Trained batch 1065 batch loss 1.63010371 epoch total loss 1.68292022\n",
      "Trained batch 1066 batch loss 1.41249776 epoch total loss 1.68266654\n",
      "Trained batch 1067 batch loss 1.52148438 epoch total loss 1.6825155\n",
      "Trained batch 1068 batch loss 1.63248158 epoch total loss 1.68246865\n",
      "Trained batch 1069 batch loss 1.69672644 epoch total loss 1.682482\n",
      "Trained batch 1070 batch loss 1.66551733 epoch total loss 1.68246615\n",
      "Trained batch 1071 batch loss 1.6657865 epoch total loss 1.68245053\n",
      "Trained batch 1072 batch loss 1.73145008 epoch total loss 1.68249631\n",
      "Trained batch 1073 batch loss 1.71782792 epoch total loss 1.68252909\n",
      "Trained batch 1074 batch loss 1.60493743 epoch total loss 1.68245697\n",
      "Trained batch 1075 batch loss 1.69673443 epoch total loss 1.6824702\n",
      "Trained batch 1076 batch loss 1.52914703 epoch total loss 1.68232775\n",
      "Trained batch 1077 batch loss 1.66889095 epoch total loss 1.68231535\n",
      "Trained batch 1078 batch loss 1.68595243 epoch total loss 1.68231869\n",
      "Trained batch 1079 batch loss 1.71992469 epoch total loss 1.68235362\n",
      "Trained batch 1080 batch loss 1.6410265 epoch total loss 1.68231523\n",
      "Trained batch 1081 batch loss 1.70658493 epoch total loss 1.68233776\n",
      "Trained batch 1082 batch loss 1.72868133 epoch total loss 1.68238044\n",
      "Trained batch 1083 batch loss 1.71782172 epoch total loss 1.68241322\n",
      "Trained batch 1084 batch loss 1.71553278 epoch total loss 1.68244374\n",
      "Trained batch 1085 batch loss 1.7535187 epoch total loss 1.6825093\n",
      "Trained batch 1086 batch loss 1.7692256 epoch total loss 1.68258905\n",
      "Trained batch 1087 batch loss 1.71868885 epoch total loss 1.68262219\n",
      "Trained batch 1088 batch loss 1.71347523 epoch total loss 1.68265057\n",
      "Trained batch 1089 batch loss 1.70398 epoch total loss 1.68267024\n",
      "Trained batch 1090 batch loss 1.62237787 epoch total loss 1.68261492\n",
      "Trained batch 1091 batch loss 1.73251224 epoch total loss 1.6826607\n",
      "Trained batch 1092 batch loss 1.73626208 epoch total loss 1.68270969\n",
      "Trained batch 1093 batch loss 1.73971534 epoch total loss 1.68276191\n",
      "Trained batch 1094 batch loss 1.74168217 epoch total loss 1.68281579\n",
      "Trained batch 1095 batch loss 1.78590322 epoch total loss 1.68291\n",
      "Trained batch 1096 batch loss 1.73081398 epoch total loss 1.68295372\n",
      "Trained batch 1097 batch loss 1.74848652 epoch total loss 1.68301344\n",
      "Trained batch 1098 batch loss 1.67220974 epoch total loss 1.68300366\n",
      "Trained batch 1099 batch loss 1.60551822 epoch total loss 1.68293309\n",
      "Trained batch 1100 batch loss 1.70452619 epoch total loss 1.68295264\n",
      "Trained batch 1101 batch loss 1.77501631 epoch total loss 1.68303633\n",
      "Trained batch 1102 batch loss 1.76125503 epoch total loss 1.68310726\n",
      "Trained batch 1103 batch loss 1.75734115 epoch total loss 1.68317449\n",
      "Trained batch 1104 batch loss 1.67987156 epoch total loss 1.68317163\n",
      "Trained batch 1105 batch loss 1.66818964 epoch total loss 1.68315804\n",
      "Trained batch 1106 batch loss 1.66153383 epoch total loss 1.68313849\n",
      "Trained batch 1107 batch loss 1.47085071 epoch total loss 1.68294668\n",
      "Trained batch 1108 batch loss 1.54326761 epoch total loss 1.68282056\n",
      "Trained batch 1109 batch loss 1.67725241 epoch total loss 1.68281555\n",
      "Trained batch 1110 batch loss 1.7314924 epoch total loss 1.6828593\n",
      "Trained batch 1111 batch loss 1.69344473 epoch total loss 1.68286896\n",
      "Trained batch 1112 batch loss 1.6901319 epoch total loss 1.68287551\n",
      "Trained batch 1113 batch loss 1.74945915 epoch total loss 1.68293536\n",
      "Trained batch 1114 batch loss 1.79013145 epoch total loss 1.68303156\n",
      "Trained batch 1115 batch loss 1.68347013 epoch total loss 1.68303204\n",
      "Trained batch 1116 batch loss 1.74582016 epoch total loss 1.6830883\n",
      "Trained batch 1117 batch loss 1.7611388 epoch total loss 1.68315816\n",
      "Trained batch 1118 batch loss 1.72842741 epoch total loss 1.68319857\n",
      "Trained batch 1119 batch loss 1.70347786 epoch total loss 1.68321669\n",
      "Trained batch 1120 batch loss 1.60603 epoch total loss 1.68314791\n",
      "Trained batch 1121 batch loss 1.48777723 epoch total loss 1.68297362\n",
      "Trained batch 1122 batch loss 1.62289071 epoch total loss 1.6829201\n",
      "Trained batch 1123 batch loss 1.63985062 epoch total loss 1.68288171\n",
      "Trained batch 1124 batch loss 1.60562146 epoch total loss 1.68281305\n",
      "Trained batch 1125 batch loss 1.63765049 epoch total loss 1.68277287\n",
      "Trained batch 1126 batch loss 1.62040186 epoch total loss 1.68271744\n",
      "Trained batch 1127 batch loss 1.62046 epoch total loss 1.68266225\n",
      "Trained batch 1128 batch loss 1.63465941 epoch total loss 1.68261969\n",
      "Trained batch 1129 batch loss 1.64249587 epoch total loss 1.68258405\n",
      "Trained batch 1130 batch loss 1.63179612 epoch total loss 1.68253922\n",
      "Trained batch 1131 batch loss 1.63049924 epoch total loss 1.68249321\n",
      "Trained batch 1132 batch loss 1.58919847 epoch total loss 1.68241084\n",
      "Trained batch 1133 batch loss 1.5648253 epoch total loss 1.682307\n",
      "Trained batch 1134 batch loss 1.61290622 epoch total loss 1.68224585\n",
      "Trained batch 1135 batch loss 1.55730057 epoch total loss 1.6821357\n",
      "Trained batch 1136 batch loss 1.49714339 epoch total loss 1.68197286\n",
      "Trained batch 1137 batch loss 1.59414458 epoch total loss 1.68189561\n",
      "Trained batch 1138 batch loss 1.47302818 epoch total loss 1.68171203\n",
      "Trained batch 1139 batch loss 1.65199602 epoch total loss 1.68168592\n",
      "Trained batch 1140 batch loss 1.70173454 epoch total loss 1.68170357\n",
      "Trained batch 1141 batch loss 1.63462853 epoch total loss 1.68166232\n",
      "Trained batch 1142 batch loss 1.65314305 epoch total loss 1.68163741\n",
      "Trained batch 1143 batch loss 1.5700289 epoch total loss 1.68153977\n",
      "Trained batch 1144 batch loss 1.57281935 epoch total loss 1.68144476\n",
      "Trained batch 1145 batch loss 1.68103755 epoch total loss 1.68144441\n",
      "Trained batch 1146 batch loss 1.60606587 epoch total loss 1.68137872\n",
      "Trained batch 1147 batch loss 1.60666418 epoch total loss 1.68131363\n",
      "Trained batch 1148 batch loss 1.64778686 epoch total loss 1.68128443\n",
      "Trained batch 1149 batch loss 1.61333692 epoch total loss 1.68122518\n",
      "Trained batch 1150 batch loss 1.69295526 epoch total loss 1.68123543\n",
      "Trained batch 1151 batch loss 1.65400553 epoch total loss 1.68121183\n",
      "Trained batch 1152 batch loss 1.63877463 epoch total loss 1.68117499\n",
      "Trained batch 1153 batch loss 1.75224376 epoch total loss 1.68123662\n",
      "Trained batch 1154 batch loss 1.69962537 epoch total loss 1.68125248\n",
      "Trained batch 1155 batch loss 1.73629463 epoch total loss 1.68130016\n",
      "Trained batch 1156 batch loss 1.71158695 epoch total loss 1.68132639\n",
      "Trained batch 1157 batch loss 1.75155711 epoch total loss 1.68138707\n",
      "Trained batch 1158 batch loss 1.58521354 epoch total loss 1.68130398\n",
      "Trained batch 1159 batch loss 1.52118921 epoch total loss 1.68116593\n",
      "Trained batch 1160 batch loss 1.59605408 epoch total loss 1.68109262\n",
      "Trained batch 1161 batch loss 1.67023039 epoch total loss 1.68108332\n",
      "Trained batch 1162 batch loss 1.64072299 epoch total loss 1.68104851\n",
      "Trained batch 1163 batch loss 1.55844271 epoch total loss 1.68094313\n",
      "Trained batch 1164 batch loss 1.5809772 epoch total loss 1.68085718\n",
      "Trained batch 1165 batch loss 1.679075 epoch total loss 1.68085575\n",
      "Trained batch 1166 batch loss 1.65462637 epoch total loss 1.68083322\n",
      "Trained batch 1167 batch loss 1.50614643 epoch total loss 1.68068349\n",
      "Trained batch 1168 batch loss 1.6341387 epoch total loss 1.68064368\n",
      "Trained batch 1169 batch loss 1.58720613 epoch total loss 1.68056369\n",
      "Trained batch 1170 batch loss 1.65486193 epoch total loss 1.68054175\n",
      "Trained batch 1171 batch loss 1.71578431 epoch total loss 1.68057191\n",
      "Trained batch 1172 batch loss 1.67456245 epoch total loss 1.68056679\n",
      "Trained batch 1173 batch loss 1.62360275 epoch total loss 1.68051827\n",
      "Trained batch 1174 batch loss 1.68596208 epoch total loss 1.68052292\n",
      "Trained batch 1175 batch loss 1.64719224 epoch total loss 1.68049455\n",
      "Trained batch 1176 batch loss 1.58184171 epoch total loss 1.68041062\n",
      "Trained batch 1177 batch loss 1.67677259 epoch total loss 1.68040752\n",
      "Trained batch 1178 batch loss 1.69181657 epoch total loss 1.68041718\n",
      "Trained batch 1179 batch loss 1.62099528 epoch total loss 1.68036675\n",
      "Trained batch 1180 batch loss 1.64893425 epoch total loss 1.68034\n",
      "Trained batch 1181 batch loss 1.6454066 epoch total loss 1.68031049\n",
      "Trained batch 1182 batch loss 1.59918094 epoch total loss 1.68024182\n",
      "Trained batch 1183 batch loss 1.60658574 epoch total loss 1.68017948\n",
      "Trained batch 1184 batch loss 1.64700603 epoch total loss 1.68015146\n",
      "Trained batch 1185 batch loss 1.55905819 epoch total loss 1.6800493\n",
      "Trained batch 1186 batch loss 1.70362854 epoch total loss 1.68006921\n",
      "Trained batch 1187 batch loss 1.60613918 epoch total loss 1.68000686\n",
      "Trained batch 1188 batch loss 1.60567212 epoch total loss 1.67994428\n",
      "Trained batch 1189 batch loss 1.71043324 epoch total loss 1.67996991\n",
      "Trained batch 1190 batch loss 1.54830217 epoch total loss 1.67985928\n",
      "Trained batch 1191 batch loss 1.66136014 epoch total loss 1.67984378\n",
      "Trained batch 1192 batch loss 1.60595202 epoch total loss 1.67978179\n",
      "Trained batch 1193 batch loss 1.529737 epoch total loss 1.67965603\n",
      "Trained batch 1194 batch loss 1.73450899 epoch total loss 1.67970204\n",
      "Trained batch 1195 batch loss 1.59501076 epoch total loss 1.67963111\n",
      "Trained batch 1196 batch loss 1.48397982 epoch total loss 1.67946756\n",
      "Trained batch 1197 batch loss 1.4908098 epoch total loss 1.67931\n",
      "Trained batch 1198 batch loss 1.57084632 epoch total loss 1.67921937\n",
      "Trained batch 1199 batch loss 1.50720286 epoch total loss 1.67907596\n",
      "Trained batch 1200 batch loss 1.54230726 epoch total loss 1.67896199\n",
      "Trained batch 1201 batch loss 1.59197307 epoch total loss 1.67888951\n",
      "Trained batch 1202 batch loss 1.56453347 epoch total loss 1.67879438\n",
      "Trained batch 1203 batch loss 1.51960099 epoch total loss 1.67866218\n",
      "Trained batch 1204 batch loss 1.69009316 epoch total loss 1.6786716\n",
      "Trained batch 1205 batch loss 1.61937857 epoch total loss 1.67862236\n",
      "Trained batch 1206 batch loss 1.7036612 epoch total loss 1.67864311\n",
      "Trained batch 1207 batch loss 1.61172962 epoch total loss 1.67858768\n",
      "Trained batch 1208 batch loss 1.46487689 epoch total loss 1.67841077\n",
      "Trained batch 1209 batch loss 1.24873865 epoch total loss 1.67805541\n",
      "Trained batch 1210 batch loss 1.39683318 epoch total loss 1.67782295\n",
      "Trained batch 1211 batch loss 1.56323719 epoch total loss 1.6777283\n",
      "Trained batch 1212 batch loss 1.78312588 epoch total loss 1.6778152\n",
      "Trained batch 1213 batch loss 1.76806188 epoch total loss 1.6778897\n",
      "Trained batch 1214 batch loss 1.71580672 epoch total loss 1.67792094\n",
      "Trained batch 1215 batch loss 1.73755097 epoch total loss 1.67796993\n",
      "Trained batch 1216 batch loss 1.73407054 epoch total loss 1.67801619\n",
      "Trained batch 1217 batch loss 1.75784302 epoch total loss 1.67808175\n",
      "Trained batch 1218 batch loss 1.74303687 epoch total loss 1.67813504\n",
      "Trained batch 1219 batch loss 1.70214701 epoch total loss 1.67815471\n",
      "Trained batch 1220 batch loss 1.71344304 epoch total loss 1.67818367\n",
      "Trained batch 1221 batch loss 1.65866458 epoch total loss 1.6781677\n",
      "Trained batch 1222 batch loss 1.73122859 epoch total loss 1.67821109\n",
      "Trained batch 1223 batch loss 1.65240014 epoch total loss 1.67818987\n",
      "Trained batch 1224 batch loss 1.68133187 epoch total loss 1.6781925\n",
      "Trained batch 1225 batch loss 1.71103334 epoch total loss 1.67821932\n",
      "Trained batch 1226 batch loss 1.72501063 epoch total loss 1.67825747\n",
      "Trained batch 1227 batch loss 1.5874548 epoch total loss 1.67818344\n",
      "Trained batch 1228 batch loss 1.69819808 epoch total loss 1.67819977\n",
      "Trained batch 1229 batch loss 1.7249105 epoch total loss 1.6782378\n",
      "Trained batch 1230 batch loss 1.69034231 epoch total loss 1.67824769\n",
      "Trained batch 1231 batch loss 1.68397331 epoch total loss 1.67825246\n",
      "Trained batch 1232 batch loss 1.56984067 epoch total loss 1.67816436\n",
      "Trained batch 1233 batch loss 1.59611654 epoch total loss 1.67809796\n",
      "Trained batch 1234 batch loss 1.71156561 epoch total loss 1.67812514\n",
      "Trained batch 1235 batch loss 1.68000448 epoch total loss 1.67812657\n",
      "Trained batch 1236 batch loss 1.77317095 epoch total loss 1.67820346\n",
      "Trained batch 1237 batch loss 1.67714834 epoch total loss 1.67820275\n",
      "Trained batch 1238 batch loss 1.66105974 epoch total loss 1.67818892\n",
      "Trained batch 1239 batch loss 1.69145226 epoch total loss 1.67819965\n",
      "Trained batch 1240 batch loss 1.53465319 epoch total loss 1.6780839\n",
      "Trained batch 1241 batch loss 1.69375205 epoch total loss 1.67809653\n",
      "Trained batch 1242 batch loss 1.65777433 epoch total loss 1.6780802\n",
      "Trained batch 1243 batch loss 1.5579741 epoch total loss 1.6779834\n",
      "Trained batch 1244 batch loss 1.63835275 epoch total loss 1.67795157\n",
      "Trained batch 1245 batch loss 1.63024259 epoch total loss 1.67791319\n",
      "Trained batch 1246 batch loss 1.58803916 epoch total loss 1.67784119\n",
      "Trained batch 1247 batch loss 1.70485258 epoch total loss 1.67786276\n",
      "Trained batch 1248 batch loss 1.64907527 epoch total loss 1.67783988\n",
      "Trained batch 1249 batch loss 1.66641021 epoch total loss 1.6778307\n",
      "Trained batch 1250 batch loss 1.66197348 epoch total loss 1.67781794\n",
      "Trained batch 1251 batch loss 1.7011131 epoch total loss 1.67783666\n",
      "Trained batch 1252 batch loss 1.69261193 epoch total loss 1.67784846\n",
      "Trained batch 1253 batch loss 1.66601658 epoch total loss 1.67783904\n",
      "Trained batch 1254 batch loss 1.62728083 epoch total loss 1.67779863\n",
      "Trained batch 1255 batch loss 1.63376808 epoch total loss 1.67776358\n",
      "Trained batch 1256 batch loss 1.64458323 epoch total loss 1.67773712\n",
      "Trained batch 1257 batch loss 1.4602592 epoch total loss 1.67756402\n",
      "Trained batch 1258 batch loss 1.59471273 epoch total loss 1.67749822\n",
      "Trained batch 1259 batch loss 1.6728363 epoch total loss 1.67749453\n",
      "Trained batch 1260 batch loss 1.68016112 epoch total loss 1.67749667\n",
      "Trained batch 1261 batch loss 1.60508382 epoch total loss 1.67743909\n",
      "Trained batch 1262 batch loss 1.68077922 epoch total loss 1.67744172\n",
      "Trained batch 1263 batch loss 1.65938044 epoch total loss 1.67742741\n",
      "Trained batch 1264 batch loss 1.69540787 epoch total loss 1.6774416\n",
      "Trained batch 1265 batch loss 1.56557262 epoch total loss 1.67735326\n",
      "Trained batch 1266 batch loss 1.47132063 epoch total loss 1.67719054\n",
      "Trained batch 1267 batch loss 1.53627074 epoch total loss 1.67707944\n",
      "Trained batch 1268 batch loss 1.57790732 epoch total loss 1.67700124\n",
      "Trained batch 1269 batch loss 1.64651763 epoch total loss 1.67697716\n",
      "Trained batch 1270 batch loss 1.61003613 epoch total loss 1.67692447\n",
      "Trained batch 1271 batch loss 1.70833564 epoch total loss 1.67694914\n",
      "Trained batch 1272 batch loss 1.67298138 epoch total loss 1.67694604\n",
      "Trained batch 1273 batch loss 1.66427231 epoch total loss 1.67693615\n",
      "Trained batch 1274 batch loss 1.60446858 epoch total loss 1.67687929\n",
      "Trained batch 1275 batch loss 1.61179376 epoch total loss 1.67682827\n",
      "Trained batch 1276 batch loss 1.67166591 epoch total loss 1.67682421\n",
      "Trained batch 1277 batch loss 1.58057165 epoch total loss 1.67674887\n",
      "Trained batch 1278 batch loss 1.67694032 epoch total loss 1.67674899\n",
      "Trained batch 1279 batch loss 1.7160635 epoch total loss 1.67677975\n",
      "Trained batch 1280 batch loss 1.6722337 epoch total loss 1.67677617\n",
      "Trained batch 1281 batch loss 1.65641177 epoch total loss 1.67676032\n",
      "Trained batch 1282 batch loss 1.64137125 epoch total loss 1.67673266\n",
      "Trained batch 1283 batch loss 1.64650559 epoch total loss 1.67670906\n",
      "Trained batch 1284 batch loss 1.67587256 epoch total loss 1.67670834\n",
      "Trained batch 1285 batch loss 1.68870056 epoch total loss 1.67671776\n",
      "Trained batch 1286 batch loss 1.63166094 epoch total loss 1.67668259\n",
      "Trained batch 1287 batch loss 1.65319133 epoch total loss 1.67666423\n",
      "Trained batch 1288 batch loss 1.66234136 epoch total loss 1.67665315\n",
      "Trained batch 1289 batch loss 1.69015551 epoch total loss 1.67666364\n",
      "Trained batch 1290 batch loss 1.709108 epoch total loss 1.67668891\n",
      "Trained batch 1291 batch loss 1.76197147 epoch total loss 1.67675495\n",
      "Trained batch 1292 batch loss 1.74454761 epoch total loss 1.67680752\n",
      "Trained batch 1293 batch loss 1.71564841 epoch total loss 1.67683744\n",
      "Trained batch 1294 batch loss 1.69806767 epoch total loss 1.6768539\n",
      "Trained batch 1295 batch loss 1.71873069 epoch total loss 1.6768862\n",
      "Trained batch 1296 batch loss 1.64644349 epoch total loss 1.67686272\n",
      "Trained batch 1297 batch loss 1.68941045 epoch total loss 1.67687249\n",
      "Trained batch 1298 batch loss 1.66740513 epoch total loss 1.67686522\n",
      "Trained batch 1299 batch loss 1.67142987 epoch total loss 1.67686105\n",
      "Trained batch 1300 batch loss 1.65699255 epoch total loss 1.67684567\n",
      "Trained batch 1301 batch loss 1.6598109 epoch total loss 1.67683268\n",
      "Trained batch 1302 batch loss 1.62662625 epoch total loss 1.67679417\n",
      "Trained batch 1303 batch loss 1.71420932 epoch total loss 1.67682278\n",
      "Trained batch 1304 batch loss 1.66535854 epoch total loss 1.67681396\n",
      "Trained batch 1305 batch loss 1.70375419 epoch total loss 1.6768347\n",
      "Trained batch 1306 batch loss 1.6010617 epoch total loss 1.67677665\n",
      "Trained batch 1307 batch loss 1.63315117 epoch total loss 1.67674327\n",
      "Trained batch 1308 batch loss 1.69932389 epoch total loss 1.67676044\n",
      "Trained batch 1309 batch loss 1.69497633 epoch total loss 1.67677438\n",
      "Trained batch 1310 batch loss 1.67550874 epoch total loss 1.67677343\n",
      "Trained batch 1311 batch loss 1.69267774 epoch total loss 1.67678559\n",
      "Trained batch 1312 batch loss 1.72809899 epoch total loss 1.67682457\n",
      "Trained batch 1313 batch loss 1.69604301 epoch total loss 1.67683923\n",
      "Trained batch 1314 batch loss 1.69254684 epoch total loss 1.67685127\n",
      "Trained batch 1315 batch loss 1.68432128 epoch total loss 1.67685699\n",
      "Trained batch 1316 batch loss 1.611889 epoch total loss 1.67680752\n",
      "Trained batch 1317 batch loss 1.63698399 epoch total loss 1.67677724\n",
      "Trained batch 1318 batch loss 1.6519959 epoch total loss 1.67675853\n",
      "Trained batch 1319 batch loss 1.62091279 epoch total loss 1.67671621\n",
      "Trained batch 1320 batch loss 1.66517484 epoch total loss 1.67670751\n",
      "Trained batch 1321 batch loss 1.55558217 epoch total loss 1.67661583\n",
      "Trained batch 1322 batch loss 1.45017552 epoch total loss 1.67644465\n",
      "Trained batch 1323 batch loss 1.49459934 epoch total loss 1.6763072\n",
      "Trained batch 1324 batch loss 1.55784965 epoch total loss 1.67621768\n",
      "Trained batch 1325 batch loss 1.59046435 epoch total loss 1.67615306\n",
      "Trained batch 1326 batch loss 1.66650701 epoch total loss 1.67614579\n",
      "Trained batch 1327 batch loss 1.69073534 epoch total loss 1.67615676\n",
      "Trained batch 1328 batch loss 1.65815139 epoch total loss 1.67614329\n",
      "Trained batch 1329 batch loss 1.62710476 epoch total loss 1.67610645\n",
      "Trained batch 1330 batch loss 1.62487686 epoch total loss 1.67606783\n",
      "Trained batch 1331 batch loss 1.59475291 epoch total loss 1.67600667\n",
      "Trained batch 1332 batch loss 1.57758641 epoch total loss 1.67593288\n",
      "Trained batch 1333 batch loss 1.60259223 epoch total loss 1.67587781\n",
      "Trained batch 1334 batch loss 1.71470404 epoch total loss 1.67590678\n",
      "Trained batch 1335 batch loss 1.6529665 epoch total loss 1.67588973\n",
      "Trained batch 1336 batch loss 1.67104983 epoch total loss 1.67588615\n",
      "Trained batch 1337 batch loss 1.75016284 epoch total loss 1.67594171\n",
      "Trained batch 1338 batch loss 1.5923624 epoch total loss 1.67587924\n",
      "Trained batch 1339 batch loss 1.62010455 epoch total loss 1.67583764\n",
      "Trained batch 1340 batch loss 1.66620934 epoch total loss 1.67583048\n",
      "Trained batch 1341 batch loss 1.66119218 epoch total loss 1.67581952\n",
      "Trained batch 1342 batch loss 1.5906322 epoch total loss 1.67575598\n",
      "Trained batch 1343 batch loss 1.53633034 epoch total loss 1.67565215\n",
      "Trained batch 1344 batch loss 1.64063168 epoch total loss 1.67562616\n",
      "Trained batch 1345 batch loss 1.67863441 epoch total loss 1.67562842\n",
      "Trained batch 1346 batch loss 1.75374544 epoch total loss 1.67568636\n",
      "Trained batch 1347 batch loss 1.74041271 epoch total loss 1.67573452\n",
      "Trained batch 1348 batch loss 1.67920518 epoch total loss 1.67573702\n",
      "Trained batch 1349 batch loss 1.55126536 epoch total loss 1.67564476\n",
      "Trained batch 1350 batch loss 1.63507688 epoch total loss 1.67561471\n",
      "Trained batch 1351 batch loss 1.6396054 epoch total loss 1.67558801\n",
      "Trained batch 1352 batch loss 1.6279614 epoch total loss 1.67555285\n",
      "Trained batch 1353 batch loss 1.62737584 epoch total loss 1.6755172\n",
      "Trained batch 1354 batch loss 1.62955403 epoch total loss 1.67548335\n",
      "Trained batch 1355 batch loss 1.6644603 epoch total loss 1.67547536\n",
      "Trained batch 1356 batch loss 1.59349728 epoch total loss 1.6754148\n",
      "Trained batch 1357 batch loss 1.67021704 epoch total loss 1.67541099\n",
      "Trained batch 1358 batch loss 1.70947456 epoch total loss 1.67543602\n",
      "Trained batch 1359 batch loss 1.67287779 epoch total loss 1.67543411\n",
      "Trained batch 1360 batch loss 1.69132972 epoch total loss 1.67544591\n",
      "Trained batch 1361 batch loss 1.65460503 epoch total loss 1.67543054\n",
      "Trained batch 1362 batch loss 1.69382453 epoch total loss 1.67544413\n",
      "Trained batch 1363 batch loss 1.66397429 epoch total loss 1.67543578\n",
      "Trained batch 1364 batch loss 1.62924349 epoch total loss 1.67540181\n",
      "Trained batch 1365 batch loss 1.70671523 epoch total loss 1.67542481\n",
      "Trained batch 1366 batch loss 1.69687486 epoch total loss 1.67544043\n",
      "Trained batch 1367 batch loss 1.67387497 epoch total loss 1.67543924\n",
      "Trained batch 1368 batch loss 1.5786196 epoch total loss 1.67536843\n",
      "Trained batch 1369 batch loss 1.54755461 epoch total loss 1.67527509\n",
      "Trained batch 1370 batch loss 1.51430035 epoch total loss 1.67515767\n",
      "Trained batch 1371 batch loss 1.57206166 epoch total loss 1.67508245\n",
      "Trained batch 1372 batch loss 1.70610273 epoch total loss 1.67510509\n",
      "Trained batch 1373 batch loss 1.69081557 epoch total loss 1.67511654\n",
      "Trained batch 1374 batch loss 1.65067244 epoch total loss 1.67509878\n",
      "Trained batch 1375 batch loss 1.70848751 epoch total loss 1.6751231\n",
      "Trained batch 1376 batch loss 1.7321099 epoch total loss 1.67516446\n",
      "Trained batch 1377 batch loss 1.73668742 epoch total loss 1.67520916\n",
      "Trained batch 1378 batch loss 1.68021977 epoch total loss 1.67521274\n",
      "Trained batch 1379 batch loss 1.61247718 epoch total loss 1.67516732\n",
      "Trained batch 1380 batch loss 1.68627751 epoch total loss 1.67517531\n",
      "Trained batch 1381 batch loss 1.63517261 epoch total loss 1.67514646\n",
      "Trained batch 1382 batch loss 1.61997294 epoch total loss 1.67510641\n",
      "Trained batch 1383 batch loss 1.67347622 epoch total loss 1.67510533\n",
      "Trained batch 1384 batch loss 1.67573762 epoch total loss 1.67510581\n",
      "Trained batch 1385 batch loss 1.62218618 epoch total loss 1.67506754\n",
      "Trained batch 1386 batch loss 1.64958405 epoch total loss 1.67504919\n",
      "Trained batch 1387 batch loss 1.64344954 epoch total loss 1.67502642\n",
      "Trained batch 1388 batch loss 1.6743803 epoch total loss 1.67502594\n",
      "Epoch 3 train loss 1.6750259399414062\n",
      "Validated batch 1 batch loss 1.62167084\n",
      "Validated batch 2 batch loss 1.54660726\n",
      "Validated batch 3 batch loss 1.69306946\n",
      "Validated batch 4 batch loss 1.63038993\n",
      "Validated batch 5 batch loss 1.61466622\n",
      "Validated batch 6 batch loss 1.6868639\n",
      "Validated batch 7 batch loss 1.65671802\n",
      "Validated batch 8 batch loss 1.70731199\n",
      "Validated batch 9 batch loss 1.65012097\n",
      "Validated batch 10 batch loss 1.67702699\n",
      "Validated batch 11 batch loss 1.58518553\n",
      "Validated batch 12 batch loss 1.62661529\n",
      "Validated batch 13 batch loss 1.63144732\n",
      "Validated batch 14 batch loss 1.71143091\n",
      "Validated batch 15 batch loss 1.68379736\n",
      "Validated batch 16 batch loss 1.63751197\n",
      "Validated batch 17 batch loss 1.6771611\n",
      "Validated batch 18 batch loss 1.50517559\n",
      "Validated batch 19 batch loss 1.68542278\n",
      "Validated batch 20 batch loss 1.69252276\n",
      "Validated batch 21 batch loss 1.66252041\n",
      "Validated batch 22 batch loss 1.6993382\n",
      "Validated batch 23 batch loss 1.59061611\n",
      "Validated batch 24 batch loss 1.78135359\n",
      "Validated batch 25 batch loss 1.66340947\n",
      "Validated batch 26 batch loss 1.6607554\n",
      "Validated batch 27 batch loss 1.63555968\n",
      "Validated batch 28 batch loss 1.6664083\n",
      "Validated batch 29 batch loss 1.67174888\n",
      "Validated batch 30 batch loss 1.64319015\n",
      "Validated batch 31 batch loss 1.58973086\n",
      "Validated batch 32 batch loss 1.62496221\n",
      "Validated batch 33 batch loss 1.6068511\n",
      "Validated batch 34 batch loss 1.66453099\n",
      "Validated batch 35 batch loss 1.5918678\n",
      "Validated batch 36 batch loss 1.61527991\n",
      "Validated batch 37 batch loss 1.59715796\n",
      "Validated batch 38 batch loss 1.64228809\n",
      "Validated batch 39 batch loss 1.70889926\n",
      "Validated batch 40 batch loss 1.69862843\n",
      "Validated batch 41 batch loss 1.69562709\n",
      "Validated batch 42 batch loss 1.78977811\n",
      "Validated batch 43 batch loss 1.79817533\n",
      "Validated batch 44 batch loss 1.74668717\n",
      "Validated batch 45 batch loss 1.63775027\n",
      "Validated batch 46 batch loss 1.577178\n",
      "Validated batch 47 batch loss 1.63342214\n",
      "Validated batch 48 batch loss 1.62891769\n",
      "Validated batch 49 batch loss 1.56022382\n",
      "Validated batch 50 batch loss 1.6049974\n",
      "Validated batch 51 batch loss 1.65413523\n",
      "Validated batch 52 batch loss 1.65719712\n",
      "Validated batch 53 batch loss 1.6566757\n",
      "Validated batch 54 batch loss 1.63309169\n",
      "Validated batch 55 batch loss 1.64216471\n",
      "Validated batch 56 batch loss 1.71997809\n",
      "Validated batch 57 batch loss 1.71968389\n",
      "Validated batch 58 batch loss 1.6815815\n",
      "Validated batch 59 batch loss 1.63632822\n",
      "Validated batch 60 batch loss 1.62377369\n",
      "Validated batch 61 batch loss 1.67138124\n",
      "Validated batch 62 batch loss 1.6556406\n",
      "Validated batch 63 batch loss 1.66186035\n",
      "Validated batch 64 batch loss 1.65136588\n",
      "Validated batch 65 batch loss 1.60016274\n",
      "Validated batch 66 batch loss 1.67086411\n",
      "Validated batch 67 batch loss 1.57091\n",
      "Validated batch 68 batch loss 1.67399311\n",
      "Validated batch 69 batch loss 1.69200408\n",
      "Validated batch 70 batch loss 1.60809731\n",
      "Validated batch 71 batch loss 1.64805532\n",
      "Validated batch 72 batch loss 1.56411\n",
      "Validated batch 73 batch loss 1.47954774\n",
      "Validated batch 74 batch loss 1.56818342\n",
      "Validated batch 75 batch loss 1.66967285\n",
      "Validated batch 76 batch loss 1.56106377\n",
      "Validated batch 77 batch loss 1.63099599\n",
      "Validated batch 78 batch loss 1.68801188\n",
      "Validated batch 79 batch loss 1.6878103\n",
      "Validated batch 80 batch loss 1.65883219\n",
      "Validated batch 81 batch loss 1.6019026\n",
      "Validated batch 82 batch loss 1.69537985\n",
      "Validated batch 83 batch loss 1.63991952\n",
      "Validated batch 84 batch loss 1.67900062\n",
      "Validated batch 85 batch loss 1.74060416\n",
      "Validated batch 86 batch loss 1.616184\n",
      "Validated batch 87 batch loss 1.70943248\n",
      "Validated batch 88 batch loss 1.71340108\n",
      "Validated batch 89 batch loss 1.62133133\n",
      "Validated batch 90 batch loss 1.6567378\n",
      "Validated batch 91 batch loss 1.66852069\n",
      "Validated batch 92 batch loss 1.72896743\n",
      "Validated batch 93 batch loss 1.5576359\n",
      "Validated batch 94 batch loss 1.68749201\n",
      "Validated batch 95 batch loss 1.68146265\n",
      "Validated batch 96 batch loss 1.67473269\n",
      "Validated batch 97 batch loss 1.66530716\n",
      "Validated batch 98 batch loss 1.71835446\n",
      "Validated batch 99 batch loss 1.61851823\n",
      "Validated batch 100 batch loss 1.75514925\n",
      "Validated batch 101 batch loss 1.60532296\n",
      "Validated batch 102 batch loss 1.65996552\n",
      "Validated batch 103 batch loss 1.68829691\n",
      "Validated batch 104 batch loss 1.57125616\n",
      "Validated batch 105 batch loss 1.4595226\n",
      "Validated batch 106 batch loss 1.68070459\n",
      "Validated batch 107 batch loss 1.70343733\n",
      "Validated batch 108 batch loss 1.67201722\n",
      "Validated batch 109 batch loss 1.66327322\n",
      "Validated batch 110 batch loss 1.70419455\n",
      "Validated batch 111 batch loss 1.68677592\n",
      "Validated batch 112 batch loss 1.7199862\n",
      "Validated batch 113 batch loss 1.6869694\n",
      "Validated batch 114 batch loss 1.66400325\n",
      "Validated batch 115 batch loss 1.58739352\n",
      "Validated batch 116 batch loss 1.59916639\n",
      "Validated batch 117 batch loss 1.69075894\n",
      "Validated batch 118 batch loss 1.5546236\n",
      "Validated batch 119 batch loss 1.7180953\n",
      "Validated batch 120 batch loss 1.70455742\n",
      "Validated batch 121 batch loss 1.68562102\n",
      "Validated batch 122 batch loss 1.67788768\n",
      "Validated batch 123 batch loss 1.69211113\n",
      "Validated batch 124 batch loss 1.72157609\n",
      "Validated batch 125 batch loss 1.66652703\n",
      "Validated batch 126 batch loss 1.72832632\n",
      "Validated batch 127 batch loss 1.72625756\n",
      "Validated batch 128 batch loss 1.53838289\n",
      "Validated batch 129 batch loss 1.70051169\n",
      "Validated batch 130 batch loss 1.64151859\n",
      "Validated batch 131 batch loss 1.66653872\n",
      "Validated batch 132 batch loss 1.67609835\n",
      "Validated batch 133 batch loss 1.5639689\n",
      "Validated batch 134 batch loss 1.52426529\n",
      "Validated batch 135 batch loss 1.70601285\n",
      "Validated batch 136 batch loss 1.6087954\n",
      "Validated batch 137 batch loss 1.73588216\n",
      "Validated batch 138 batch loss 1.64304614\n",
      "Validated batch 139 batch loss 1.73117805\n",
      "Validated batch 140 batch loss 1.66133189\n",
      "Validated batch 141 batch loss 1.6192863\n",
      "Validated batch 142 batch loss 1.61694217\n",
      "Validated batch 143 batch loss 1.58666205\n",
      "Validated batch 144 batch loss 1.68201125\n",
      "Validated batch 145 batch loss 1.56843615\n",
      "Validated batch 146 batch loss 1.64849651\n",
      "Validated batch 147 batch loss 1.70770907\n",
      "Validated batch 148 batch loss 1.66283488\n",
      "Validated batch 149 batch loss 1.60837162\n",
      "Validated batch 150 batch loss 1.60199809\n",
      "Validated batch 151 batch loss 1.58822417\n",
      "Validated batch 152 batch loss 1.66727984\n",
      "Validated batch 153 batch loss 1.67314088\n",
      "Validated batch 154 batch loss 1.63034797\n",
      "Validated batch 155 batch loss 1.59818959\n",
      "Validated batch 156 batch loss 1.56584656\n",
      "Validated batch 157 batch loss 1.54178858\n",
      "Validated batch 158 batch loss 1.65608\n",
      "Validated batch 159 batch loss 1.59798825\n",
      "Validated batch 160 batch loss 1.54537153\n",
      "Validated batch 161 batch loss 1.63094199\n",
      "Validated batch 162 batch loss 1.71119881\n",
      "Validated batch 163 batch loss 1.73248065\n",
      "Validated batch 164 batch loss 1.61110389\n",
      "Validated batch 165 batch loss 1.50855184\n",
      "Validated batch 166 batch loss 1.6371007\n",
      "Validated batch 167 batch loss 1.58187377\n",
      "Validated batch 168 batch loss 1.57818425\n",
      "Validated batch 169 batch loss 1.58550751\n",
      "Validated batch 170 batch loss 1.5404377\n",
      "Validated batch 171 batch loss 1.68644261\n",
      "Validated batch 172 batch loss 1.59284353\n",
      "Validated batch 173 batch loss 1.53058875\n",
      "Validated batch 174 batch loss 1.51889038\n",
      "Validated batch 175 batch loss 1.74448657\n",
      "Validated batch 176 batch loss 1.61508512\n",
      "Validated batch 177 batch loss 1.68168545\n",
      "Validated batch 178 batch loss 1.61326981\n",
      "Validated batch 179 batch loss 1.6937139\n",
      "Validated batch 180 batch loss 1.59572887\n",
      "Validated batch 181 batch loss 1.64103961\n",
      "Validated batch 182 batch loss 1.70611715\n",
      "Validated batch 183 batch loss 1.47643542\n",
      "Validated batch 184 batch loss 1.6421771\n",
      "Validated batch 185 batch loss 1.80370033\n",
      "Epoch 3 val loss 1.646586537361145\n",
      "Model ./model_hourglass-epoch-3-loss-1.6466.h5 saved.\n",
      "Start epoch 4 with learning rate 0.5\n",
      "Start distributed traininng...\n",
      "Trained batch 1 batch loss 1.62268138 epoch total loss 1.62268138\n",
      "Trained batch 2 batch loss 1.55900133 epoch total loss 1.59084129\n",
      "Trained batch 3 batch loss 1.61687446 epoch total loss 1.59951913\n",
      "Trained batch 4 batch loss 1.62083769 epoch total loss 1.60484874\n",
      "Trained batch 5 batch loss 1.56314874 epoch total loss 1.59650874\n",
      "Trained batch 6 batch loss 1.60097742 epoch total loss 1.59725344\n",
      "Trained batch 7 batch loss 1.61519992 epoch total loss 1.59981728\n",
      "Trained batch 8 batch loss 1.58795369 epoch total loss 1.59833431\n",
      "Trained batch 9 batch loss 1.58938956 epoch total loss 1.59734046\n",
      "Trained batch 10 batch loss 1.7138077 epoch total loss 1.60898721\n",
      "Trained batch 11 batch loss 1.60656035 epoch total loss 1.60876656\n",
      "Trained batch 12 batch loss 1.53571057 epoch total loss 1.60267866\n",
      "Trained batch 13 batch loss 1.61015177 epoch total loss 1.60325348\n",
      "Trained batch 14 batch loss 1.64454138 epoch total loss 1.60620248\n",
      "Trained batch 15 batch loss 1.70328748 epoch total loss 1.61267483\n",
      "Trained batch 16 batch loss 1.69625545 epoch total loss 1.61789858\n",
      "Trained batch 17 batch loss 1.67687941 epoch total loss 1.62136805\n",
      "Trained batch 18 batch loss 1.5303936 epoch total loss 1.61631393\n",
      "Trained batch 19 batch loss 1.65846062 epoch total loss 1.61853218\n",
      "Trained batch 20 batch loss 1.59976101 epoch total loss 1.61759353\n",
      "Trained batch 21 batch loss 1.61918533 epoch total loss 1.61766946\n",
      "Trained batch 22 batch loss 1.54516029 epoch total loss 1.61437368\n",
      "Trained batch 23 batch loss 1.63010931 epoch total loss 1.61505771\n",
      "Trained batch 24 batch loss 1.63350391 epoch total loss 1.61582625\n",
      "Trained batch 25 batch loss 1.5418725 epoch total loss 1.61286819\n",
      "Trained batch 26 batch loss 1.51227558 epoch total loss 1.60899925\n",
      "Trained batch 27 batch loss 1.60913873 epoch total loss 1.60900438\n",
      "Trained batch 28 batch loss 1.58931279 epoch total loss 1.60830116\n",
      "Trained batch 29 batch loss 1.43867373 epoch total loss 1.60245204\n",
      "Trained batch 30 batch loss 1.42866993 epoch total loss 1.59665918\n",
      "Trained batch 31 batch loss 1.33384895 epoch total loss 1.5881815\n",
      "Trained batch 32 batch loss 1.54347587 epoch total loss 1.58678448\n",
      "Trained batch 33 batch loss 1.47739899 epoch total loss 1.58346975\n",
      "Trained batch 34 batch loss 1.44999194 epoch total loss 1.57954395\n",
      "Trained batch 35 batch loss 1.6472621 epoch total loss 1.58147871\n",
      "Trained batch 36 batch loss 1.600003 epoch total loss 1.58199334\n",
      "Trained batch 37 batch loss 1.53815401 epoch total loss 1.58080852\n",
      "Trained batch 38 batch loss 1.51012015 epoch total loss 1.57894826\n",
      "Trained batch 39 batch loss 1.54908144 epoch total loss 1.57818246\n",
      "Trained batch 40 batch loss 1.63912725 epoch total loss 1.57970595\n",
      "Trained batch 41 batch loss 1.619753 epoch total loss 1.58068275\n",
      "Trained batch 42 batch loss 1.69260967 epoch total loss 1.58334768\n",
      "Trained batch 43 batch loss 1.72263193 epoch total loss 1.58658683\n",
      "Trained batch 44 batch loss 1.70163417 epoch total loss 1.58920169\n",
      "Trained batch 45 batch loss 1.74659991 epoch total loss 1.59269929\n",
      "Trained batch 46 batch loss 1.71568394 epoch total loss 1.59537292\n",
      "Trained batch 47 batch loss 1.72422624 epoch total loss 1.59811449\n",
      "Trained batch 48 batch loss 1.71004689 epoch total loss 1.60044634\n",
      "Trained batch 49 batch loss 1.7137742 epoch total loss 1.60275924\n",
      "Trained batch 50 batch loss 1.69568229 epoch total loss 1.6046176\n",
      "Trained batch 51 batch loss 1.72080433 epoch total loss 1.6068958\n",
      "Trained batch 52 batch loss 1.61873126 epoch total loss 1.60712326\n",
      "Trained batch 53 batch loss 1.67256474 epoch total loss 1.60835803\n",
      "Trained batch 54 batch loss 1.73676622 epoch total loss 1.61073589\n",
      "Trained batch 55 batch loss 1.61954057 epoch total loss 1.61089587\n",
      "Trained batch 56 batch loss 1.65215373 epoch total loss 1.61163259\n",
      "Trained batch 57 batch loss 1.69488478 epoch total loss 1.61309314\n",
      "Trained batch 58 batch loss 1.71284533 epoch total loss 1.61481309\n",
      "Trained batch 59 batch loss 1.63277471 epoch total loss 1.61511743\n",
      "Trained batch 60 batch loss 1.76298785 epoch total loss 1.61758196\n",
      "Trained batch 61 batch loss 1.58277726 epoch total loss 1.61701143\n",
      "Trained batch 62 batch loss 1.6248188 epoch total loss 1.61713731\n",
      "Trained batch 63 batch loss 1.71033049 epoch total loss 1.61861646\n",
      "Trained batch 64 batch loss 1.65316463 epoch total loss 1.61915636\n",
      "Trained batch 65 batch loss 1.72104788 epoch total loss 1.62072384\n",
      "Trained batch 66 batch loss 1.61340821 epoch total loss 1.6206131\n",
      "Trained batch 67 batch loss 1.65007663 epoch total loss 1.62105286\n",
      "Trained batch 68 batch loss 1.62820673 epoch total loss 1.621158\n",
      "Trained batch 69 batch loss 1.68086851 epoch total loss 1.62202346\n",
      "Trained batch 70 batch loss 1.68997037 epoch total loss 1.62299407\n",
      "Trained batch 71 batch loss 1.76990724 epoch total loss 1.6250633\n",
      "Trained batch 72 batch loss 1.670964 epoch total loss 1.62570083\n",
      "Trained batch 73 batch loss 1.51880205 epoch total loss 1.62423646\n",
      "Trained batch 74 batch loss 1.5911032 epoch total loss 1.62378871\n",
      "Trained batch 75 batch loss 1.59708536 epoch total loss 1.62343264\n",
      "Trained batch 76 batch loss 1.65939116 epoch total loss 1.62390578\n",
      "Trained batch 77 batch loss 1.64712214 epoch total loss 1.62420738\n",
      "Trained batch 78 batch loss 1.68669724 epoch total loss 1.62500846\n",
      "Trained batch 79 batch loss 1.67855847 epoch total loss 1.62568641\n",
      "Trained batch 80 batch loss 1.69747806 epoch total loss 1.62658381\n",
      "Trained batch 81 batch loss 1.65670884 epoch total loss 1.62695575\n",
      "Trained batch 82 batch loss 1.65730596 epoch total loss 1.62732589\n",
      "Trained batch 83 batch loss 1.66609573 epoch total loss 1.62779295\n",
      "Trained batch 84 batch loss 1.68209028 epoch total loss 1.62843919\n",
      "Trained batch 85 batch loss 1.66999853 epoch total loss 1.62892818\n",
      "Trained batch 86 batch loss 1.64386177 epoch total loss 1.62910175\n",
      "Trained batch 87 batch loss 1.65255213 epoch total loss 1.6293714\n",
      "Trained batch 88 batch loss 1.66183376 epoch total loss 1.62974024\n",
      "Trained batch 89 batch loss 1.50474441 epoch total loss 1.62833583\n",
      "Trained batch 90 batch loss 1.44297934 epoch total loss 1.62627625\n",
      "Trained batch 91 batch loss 1.41800344 epoch total loss 1.62398756\n",
      "Trained batch 92 batch loss 1.40632331 epoch total loss 1.62162173\n",
      "Trained batch 93 batch loss 1.641909 epoch total loss 1.62183976\n",
      "Trained batch 94 batch loss 1.70655465 epoch total loss 1.6227411\n",
      "Trained batch 95 batch loss 1.79091752 epoch total loss 1.62451136\n",
      "Trained batch 96 batch loss 1.70199955 epoch total loss 1.62531853\n",
      "Trained batch 97 batch loss 1.76159382 epoch total loss 1.62672341\n",
      "Trained batch 98 batch loss 1.7049073 epoch total loss 1.62752128\n",
      "Trained batch 99 batch loss 1.70885658 epoch total loss 1.62834287\n",
      "Trained batch 100 batch loss 1.73351657 epoch total loss 1.62939465\n",
      "Trained batch 101 batch loss 1.72403 epoch total loss 1.63033164\n",
      "Trained batch 102 batch loss 1.7215724 epoch total loss 1.63122618\n",
      "Trained batch 103 batch loss 1.77363646 epoch total loss 1.63260877\n",
      "Trained batch 104 batch loss 1.658494 epoch total loss 1.63285768\n",
      "Trained batch 105 batch loss 1.63647187 epoch total loss 1.63289213\n",
      "Trained batch 106 batch loss 1.69569099 epoch total loss 1.6334846\n",
      "Trained batch 107 batch loss 1.55428433 epoch total loss 1.63274443\n",
      "Trained batch 108 batch loss 1.53573167 epoch total loss 1.63184631\n",
      "Trained batch 109 batch loss 1.44205499 epoch total loss 1.63010514\n",
      "Trained batch 110 batch loss 1.66968501 epoch total loss 1.63046491\n",
      "Trained batch 111 batch loss 1.6253829 epoch total loss 1.63041902\n",
      "Trained batch 112 batch loss 1.68056107 epoch total loss 1.63086677\n",
      "Trained batch 113 batch loss 1.66003692 epoch total loss 1.63112485\n",
      "Trained batch 114 batch loss 1.6418246 epoch total loss 1.63121879\n",
      "Trained batch 115 batch loss 1.68012536 epoch total loss 1.63164413\n",
      "Trained batch 116 batch loss 1.62064683 epoch total loss 1.63154936\n",
      "Trained batch 117 batch loss 1.78997064 epoch total loss 1.63290346\n",
      "Trained batch 118 batch loss 1.69212067 epoch total loss 1.63340521\n",
      "Trained batch 119 batch loss 1.68167901 epoch total loss 1.633811\n",
      "Trained batch 120 batch loss 1.70032191 epoch total loss 1.6343652\n",
      "Trained batch 121 batch loss 1.66500723 epoch total loss 1.6346184\n",
      "Trained batch 122 batch loss 1.54474199 epoch total loss 1.63388169\n",
      "Trained batch 123 batch loss 1.50048637 epoch total loss 1.63279724\n",
      "Trained batch 124 batch loss 1.48587811 epoch total loss 1.63161242\n",
      "Trained batch 125 batch loss 1.41030788 epoch total loss 1.62984204\n",
      "Trained batch 126 batch loss 1.55475926 epoch total loss 1.62924623\n",
      "Trained batch 127 batch loss 1.65499961 epoch total loss 1.62944901\n",
      "Trained batch 128 batch loss 1.69489729 epoch total loss 1.6299603\n",
      "Trained batch 129 batch loss 1.72237396 epoch total loss 1.63067675\n",
      "Trained batch 130 batch loss 1.72389042 epoch total loss 1.63139379\n",
      "Trained batch 131 batch loss 1.70806396 epoch total loss 1.63197911\n",
      "Trained batch 132 batch loss 1.57478023 epoch total loss 1.63154578\n",
      "Trained batch 133 batch loss 1.4921416 epoch total loss 1.63049769\n",
      "Trained batch 134 batch loss 1.71503091 epoch total loss 1.63112843\n",
      "Trained batch 135 batch loss 1.54315162 epoch total loss 1.63047683\n",
      "Trained batch 136 batch loss 1.56632936 epoch total loss 1.63000512\n",
      "Trained batch 137 batch loss 1.54559207 epoch total loss 1.62938893\n",
      "Trained batch 138 batch loss 1.619524 epoch total loss 1.62931752\n",
      "Trained batch 139 batch loss 1.6666342 epoch total loss 1.62958598\n",
      "Trained batch 140 batch loss 1.62327051 epoch total loss 1.62954092\n",
      "Trained batch 141 batch loss 1.63973939 epoch total loss 1.62961328\n",
      "Trained batch 142 batch loss 1.64991188 epoch total loss 1.62975621\n",
      "Trained batch 143 batch loss 1.61305881 epoch total loss 1.62963939\n",
      "Trained batch 144 batch loss 1.65767515 epoch total loss 1.62983406\n",
      "Trained batch 145 batch loss 1.62876 epoch total loss 1.62982666\n",
      "Trained batch 146 batch loss 1.59969902 epoch total loss 1.62962031\n",
      "Trained batch 147 batch loss 1.67309153 epoch total loss 1.62991607\n",
      "Trained batch 148 batch loss 1.67130816 epoch total loss 1.63019574\n",
      "Trained batch 149 batch loss 1.66844749 epoch total loss 1.63045239\n",
      "Trained batch 150 batch loss 1.45838666 epoch total loss 1.62930536\n",
      "Trained batch 151 batch loss 1.57464647 epoch total loss 1.62894332\n",
      "Trained batch 152 batch loss 1.67640293 epoch total loss 1.62925565\n",
      "Trained batch 153 batch loss 1.64119112 epoch total loss 1.62933362\n",
      "Trained batch 154 batch loss 1.60555816 epoch total loss 1.62917924\n",
      "Trained batch 155 batch loss 1.67463434 epoch total loss 1.62947249\n",
      "Trained batch 156 batch loss 1.62995028 epoch total loss 1.62947547\n",
      "Trained batch 157 batch loss 1.6568954 epoch total loss 1.62965012\n",
      "Trained batch 158 batch loss 1.63621902 epoch total loss 1.62969184\n",
      "Trained batch 159 batch loss 1.58642602 epoch total loss 1.62941968\n",
      "Trained batch 160 batch loss 1.55420756 epoch total loss 1.62894952\n",
      "Trained batch 161 batch loss 1.58800805 epoch total loss 1.62869525\n",
      "Trained batch 162 batch loss 1.61099672 epoch total loss 1.62858605\n",
      "Trained batch 163 batch loss 1.71377754 epoch total loss 1.62910867\n",
      "Trained batch 164 batch loss 1.66538 epoch total loss 1.6293298\n",
      "Trained batch 165 batch loss 1.63702691 epoch total loss 1.62937641\n",
      "Trained batch 166 batch loss 1.65550411 epoch total loss 1.62953389\n",
      "Trained batch 167 batch loss 1.61817217 epoch total loss 1.62946582\n",
      "Trained batch 168 batch loss 1.64618397 epoch total loss 1.62956524\n",
      "Trained batch 169 batch loss 1.66128969 epoch total loss 1.62975299\n",
      "Trained batch 170 batch loss 1.61690688 epoch total loss 1.62967741\n",
      "Trained batch 171 batch loss 1.52476871 epoch total loss 1.62906396\n",
      "Trained batch 172 batch loss 1.57023013 epoch total loss 1.62872195\n",
      "Trained batch 173 batch loss 1.57528186 epoch total loss 1.62841308\n",
      "Trained batch 174 batch loss 1.6443429 epoch total loss 1.62850463\n",
      "Trained batch 175 batch loss 1.62361884 epoch total loss 1.62847674\n",
      "Trained batch 176 batch loss 1.7265718 epoch total loss 1.62903404\n",
      "Trained batch 177 batch loss 1.6697681 epoch total loss 1.62926424\n",
      "Trained batch 178 batch loss 1.48055112 epoch total loss 1.62842882\n",
      "Trained batch 179 batch loss 1.48581564 epoch total loss 1.62763202\n",
      "Trained batch 180 batch loss 1.5207572 epoch total loss 1.62703824\n",
      "Trained batch 181 batch loss 1.68335211 epoch total loss 1.62734938\n",
      "Trained batch 182 batch loss 1.69633591 epoch total loss 1.62772846\n",
      "Trained batch 183 batch loss 1.6645503 epoch total loss 1.62792969\n",
      "Trained batch 184 batch loss 1.72907245 epoch total loss 1.62847936\n",
      "Trained batch 185 batch loss 1.71163559 epoch total loss 1.6289289\n",
      "Trained batch 186 batch loss 1.75938702 epoch total loss 1.62963033\n",
      "Trained batch 187 batch loss 1.6891681 epoch total loss 1.62994874\n",
      "Trained batch 188 batch loss 1.68787599 epoch total loss 1.63025677\n",
      "Trained batch 189 batch loss 1.69335055 epoch total loss 1.63059068\n",
      "Trained batch 190 batch loss 1.68882489 epoch total loss 1.63089716\n",
      "Trained batch 191 batch loss 1.62272191 epoch total loss 1.63085425\n",
      "Trained batch 192 batch loss 1.74282861 epoch total loss 1.63143742\n",
      "Trained batch 193 batch loss 1.69807231 epoch total loss 1.63178265\n",
      "Trained batch 194 batch loss 1.65840137 epoch total loss 1.63191974\n",
      "Trained batch 195 batch loss 1.59916842 epoch total loss 1.63175189\n",
      "Trained batch 196 batch loss 1.62714183 epoch total loss 1.63172829\n",
      "Trained batch 197 batch loss 1.62545896 epoch total loss 1.63169646\n",
      "Trained batch 198 batch loss 1.61739123 epoch total loss 1.63162434\n",
      "Trained batch 199 batch loss 1.53753495 epoch total loss 1.63115156\n",
      "Trained batch 200 batch loss 1.57238626 epoch total loss 1.63085771\n",
      "Trained batch 201 batch loss 1.58904183 epoch total loss 1.63064969\n",
      "Trained batch 202 batch loss 1.50099468 epoch total loss 1.63000786\n",
      "Trained batch 203 batch loss 1.57787609 epoch total loss 1.62975109\n",
      "Trained batch 204 batch loss 1.53946567 epoch total loss 1.62930846\n",
      "Trained batch 205 batch loss 1.56005228 epoch total loss 1.62897074\n",
      "Trained batch 206 batch loss 1.50666177 epoch total loss 1.62837696\n",
      "Trained batch 207 batch loss 1.47832489 epoch total loss 1.62765205\n",
      "Trained batch 208 batch loss 1.40860438 epoch total loss 1.62659895\n",
      "Trained batch 209 batch loss 1.62171066 epoch total loss 1.62657547\n",
      "Trained batch 210 batch loss 1.5854528 epoch total loss 1.62637973\n",
      "Trained batch 211 batch loss 1.54367328 epoch total loss 1.62598765\n",
      "Trained batch 212 batch loss 1.6158042 epoch total loss 1.62593973\n",
      "Trained batch 213 batch loss 1.66480327 epoch total loss 1.62612212\n",
      "Trained batch 214 batch loss 1.7308321 epoch total loss 1.62661147\n",
      "Trained batch 215 batch loss 1.68199956 epoch total loss 1.62686908\n",
      "Trained batch 216 batch loss 1.67198157 epoch total loss 1.62707794\n",
      "Trained batch 217 batch loss 1.63050485 epoch total loss 1.62709367\n",
      "Trained batch 218 batch loss 1.57500291 epoch total loss 1.62685466\n",
      "Trained batch 219 batch loss 1.68808389 epoch total loss 1.62713432\n",
      "Trained batch 220 batch loss 1.63433492 epoch total loss 1.62716699\n",
      "Trained batch 221 batch loss 1.57788396 epoch total loss 1.62694407\n",
      "Trained batch 222 batch loss 1.57667112 epoch total loss 1.62671745\n",
      "Trained batch 223 batch loss 1.60495353 epoch total loss 1.62661994\n",
      "Trained batch 224 batch loss 1.55178368 epoch total loss 1.62628579\n",
      "Trained batch 225 batch loss 1.53632259 epoch total loss 1.62588596\n",
      "Trained batch 226 batch loss 1.59043169 epoch total loss 1.62572908\n",
      "Trained batch 227 batch loss 1.44874823 epoch total loss 1.62494946\n",
      "Trained batch 228 batch loss 1.58790541 epoch total loss 1.62478685\n",
      "Trained batch 229 batch loss 1.64933383 epoch total loss 1.62489402\n",
      "Trained batch 230 batch loss 1.64301157 epoch total loss 1.62497282\n",
      "Trained batch 231 batch loss 1.64638245 epoch total loss 1.62506557\n",
      "Trained batch 232 batch loss 1.61555743 epoch total loss 1.62502456\n",
      "Trained batch 233 batch loss 1.45111632 epoch total loss 1.62427819\n",
      "Trained batch 234 batch loss 1.63707268 epoch total loss 1.6243329\n",
      "Trained batch 235 batch loss 1.63328481 epoch total loss 1.62437093\n",
      "Trained batch 236 batch loss 1.49954498 epoch total loss 1.623842\n",
      "Trained batch 237 batch loss 1.69705796 epoch total loss 1.62415087\n",
      "Trained batch 238 batch loss 1.65822625 epoch total loss 1.62429416\n",
      "Trained batch 239 batch loss 1.61671984 epoch total loss 1.62426245\n",
      "Trained batch 240 batch loss 1.60645831 epoch total loss 1.62418818\n",
      "Trained batch 241 batch loss 1.55004239 epoch total loss 1.62388062\n",
      "Trained batch 242 batch loss 1.5965476 epoch total loss 1.62376773\n",
      "Trained batch 243 batch loss 1.67193353 epoch total loss 1.62396586\n",
      "Trained batch 244 batch loss 1.72684646 epoch total loss 1.6243875\n",
      "Trained batch 245 batch loss 1.68316174 epoch total loss 1.62462747\n",
      "Trained batch 246 batch loss 1.6605804 epoch total loss 1.62477362\n",
      "Trained batch 247 batch loss 1.63105202 epoch total loss 1.62479901\n",
      "Trained batch 248 batch loss 1.53584456 epoch total loss 1.62444031\n",
      "Trained batch 249 batch loss 1.59099233 epoch total loss 1.62430608\n",
      "Trained batch 250 batch loss 1.65257335 epoch total loss 1.62441921\n",
      "Trained batch 251 batch loss 1.56667948 epoch total loss 1.62418914\n",
      "Trained batch 252 batch loss 1.63655949 epoch total loss 1.62423825\n",
      "Trained batch 253 batch loss 1.70389724 epoch total loss 1.62455308\n",
      "Trained batch 254 batch loss 1.63287139 epoch total loss 1.62458587\n",
      "Trained batch 255 batch loss 1.63339734 epoch total loss 1.62462044\n",
      "Trained batch 256 batch loss 1.55093622 epoch total loss 1.62433255\n",
      "Trained batch 257 batch loss 1.55209279 epoch total loss 1.62405145\n",
      "Trained batch 258 batch loss 1.66125202 epoch total loss 1.62419569\n",
      "Trained batch 259 batch loss 1.68885064 epoch total loss 1.62444532\n",
      "Trained batch 260 batch loss 1.70379925 epoch total loss 1.62475049\n",
      "Trained batch 261 batch loss 1.62030256 epoch total loss 1.62473345\n",
      "Trained batch 262 batch loss 1.71393275 epoch total loss 1.62507379\n",
      "Trained batch 263 batch loss 1.68870199 epoch total loss 1.62531579\n",
      "Trained batch 264 batch loss 1.64342415 epoch total loss 1.62538433\n",
      "Trained batch 265 batch loss 1.62378943 epoch total loss 1.62537825\n",
      "Trained batch 266 batch loss 1.63159859 epoch total loss 1.62540162\n",
      "Trained batch 267 batch loss 1.60591662 epoch total loss 1.62532878\n",
      "Trained batch 268 batch loss 1.60283518 epoch total loss 1.62524486\n",
      "Trained batch 269 batch loss 1.72015285 epoch total loss 1.6255976\n",
      "Trained batch 270 batch loss 1.53923178 epoch total loss 1.62527788\n",
      "Trained batch 271 batch loss 1.5851841 epoch total loss 1.62512982\n",
      "Trained batch 272 batch loss 1.54412353 epoch total loss 1.62483203\n",
      "Trained batch 273 batch loss 1.58665514 epoch total loss 1.6246922\n",
      "Trained batch 274 batch loss 1.63665318 epoch total loss 1.62473595\n",
      "Trained batch 275 batch loss 1.60520148 epoch total loss 1.6246649\n",
      "Trained batch 276 batch loss 1.50035536 epoch total loss 1.62421453\n",
      "Trained batch 277 batch loss 1.47405219 epoch total loss 1.62367249\n",
      "Trained batch 278 batch loss 1.63949049 epoch total loss 1.62372935\n",
      "Trained batch 279 batch loss 1.68529105 epoch total loss 1.62395\n",
      "Trained batch 280 batch loss 1.59637511 epoch total loss 1.62385154\n",
      "Trained batch 281 batch loss 1.68709934 epoch total loss 1.6240766\n",
      "Trained batch 282 batch loss 1.685269 epoch total loss 1.62429368\n",
      "Trained batch 283 batch loss 1.59961689 epoch total loss 1.62420642\n",
      "Trained batch 284 batch loss 1.59700727 epoch total loss 1.6241107\n",
      "Trained batch 285 batch loss 1.57777643 epoch total loss 1.62394822\n",
      "Trained batch 286 batch loss 1.5034554 epoch total loss 1.62352681\n",
      "Trained batch 287 batch loss 1.61758637 epoch total loss 1.62350607\n",
      "Trained batch 288 batch loss 1.63388586 epoch total loss 1.62354219\n",
      "Trained batch 289 batch loss 1.67358017 epoch total loss 1.62371528\n",
      "Trained batch 290 batch loss 1.50550807 epoch total loss 1.62330759\n",
      "Trained batch 291 batch loss 1.65484095 epoch total loss 1.62341607\n",
      "Trained batch 292 batch loss 1.59225965 epoch total loss 1.62330925\n",
      "Trained batch 293 batch loss 1.58059502 epoch total loss 1.62316358\n",
      "Trained batch 294 batch loss 1.61351585 epoch total loss 1.6231308\n",
      "Trained batch 295 batch loss 1.53685784 epoch total loss 1.62283838\n",
      "Trained batch 296 batch loss 1.55828774 epoch total loss 1.62262022\n",
      "Trained batch 297 batch loss 1.59842098 epoch total loss 1.62253881\n",
      "Trained batch 298 batch loss 1.66078663 epoch total loss 1.62266719\n",
      "Trained batch 299 batch loss 1.6184876 epoch total loss 1.62265325\n",
      "Trained batch 300 batch loss 1.58875084 epoch total loss 1.62254024\n",
      "Trained batch 301 batch loss 1.6271379 epoch total loss 1.62255549\n",
      "Trained batch 302 batch loss 1.64662135 epoch total loss 1.62263513\n",
      "Trained batch 303 batch loss 1.54132724 epoch total loss 1.62236667\n",
      "Trained batch 304 batch loss 1.5955584 epoch total loss 1.62227857\n",
      "Trained batch 305 batch loss 1.57842541 epoch total loss 1.6221348\n",
      "Trained batch 306 batch loss 1.60117579 epoch total loss 1.62206626\n",
      "Trained batch 307 batch loss 1.62589753 epoch total loss 1.62207866\n",
      "Trained batch 308 batch loss 1.62922144 epoch total loss 1.62210178\n",
      "Trained batch 309 batch loss 1.66420865 epoch total loss 1.62223816\n",
      "Trained batch 310 batch loss 1.63805461 epoch total loss 1.62228918\n",
      "Trained batch 311 batch loss 1.66831648 epoch total loss 1.62243712\n",
      "Trained batch 312 batch loss 1.56620276 epoch total loss 1.62225688\n",
      "Trained batch 313 batch loss 1.48655188 epoch total loss 1.62182319\n",
      "Trained batch 314 batch loss 1.67262495 epoch total loss 1.62198508\n",
      "Trained batch 315 batch loss 1.61535633 epoch total loss 1.62196398\n",
      "Trained batch 316 batch loss 1.61850154 epoch total loss 1.62195313\n",
      "Trained batch 317 batch loss 1.48190641 epoch total loss 1.62151134\n",
      "Trained batch 318 batch loss 1.5776031 epoch total loss 1.62137318\n",
      "Trained batch 319 batch loss 1.57465601 epoch total loss 1.62122667\n",
      "Trained batch 320 batch loss 1.65552199 epoch total loss 1.62133384\n",
      "Trained batch 321 batch loss 1.72063684 epoch total loss 1.6216433\n",
      "Trained batch 322 batch loss 1.67727649 epoch total loss 1.62181592\n",
      "Trained batch 323 batch loss 1.70217431 epoch total loss 1.62206459\n",
      "Trained batch 324 batch loss 1.53411722 epoch total loss 1.62179315\n",
      "Trained batch 325 batch loss 1.42280304 epoch total loss 1.62118089\n",
      "Trained batch 326 batch loss 1.46678305 epoch total loss 1.62070727\n",
      "Trained batch 327 batch loss 1.55091596 epoch total loss 1.62049389\n",
      "Trained batch 328 batch loss 1.60698807 epoch total loss 1.62045264\n",
      "Trained batch 329 batch loss 1.66666389 epoch total loss 1.62059319\n",
      "Trained batch 330 batch loss 1.57760012 epoch total loss 1.62046289\n",
      "Trained batch 331 batch loss 1.61841869 epoch total loss 1.6204567\n",
      "Trained batch 332 batch loss 1.67923844 epoch total loss 1.62063372\n",
      "Trained batch 333 batch loss 1.72581983 epoch total loss 1.62094963\n",
      "Trained batch 334 batch loss 1.68863547 epoch total loss 1.6211524\n",
      "Trained batch 335 batch loss 1.67947662 epoch total loss 1.62132657\n",
      "Trained batch 336 batch loss 1.63086128 epoch total loss 1.62135494\n",
      "Trained batch 337 batch loss 1.620471 epoch total loss 1.62135243\n",
      "Trained batch 338 batch loss 1.63485944 epoch total loss 1.62139237\n",
      "Trained batch 339 batch loss 1.5356102 epoch total loss 1.62113929\n",
      "Trained batch 340 batch loss 1.60870552 epoch total loss 1.62110269\n",
      "Trained batch 341 batch loss 1.62283015 epoch total loss 1.6211077\n",
      "Trained batch 342 batch loss 1.66040742 epoch total loss 1.62122262\n",
      "Trained batch 343 batch loss 1.60989118 epoch total loss 1.62118948\n",
      "Trained batch 344 batch loss 1.60456014 epoch total loss 1.62114108\n",
      "Trained batch 345 batch loss 1.52677667 epoch total loss 1.62086761\n",
      "Trained batch 346 batch loss 1.44178021 epoch total loss 1.62035\n",
      "Trained batch 347 batch loss 1.74196458 epoch total loss 1.62070048\n",
      "Trained batch 348 batch loss 1.66548514 epoch total loss 1.62082911\n",
      "Trained batch 349 batch loss 1.70065963 epoch total loss 1.62105787\n",
      "Trained batch 350 batch loss 1.73802102 epoch total loss 1.62139213\n",
      "Trained batch 351 batch loss 1.7544955 epoch total loss 1.62177145\n",
      "Trained batch 352 batch loss 1.73160279 epoch total loss 1.62208354\n",
      "Trained batch 353 batch loss 1.62840712 epoch total loss 1.62210143\n",
      "Trained batch 354 batch loss 1.68820667 epoch total loss 1.62228823\n",
      "Trained batch 355 batch loss 1.59166741 epoch total loss 1.62220204\n",
      "Trained batch 356 batch loss 1.67276025 epoch total loss 1.62234414\n",
      "Trained batch 357 batch loss 1.72424078 epoch total loss 1.62262952\n",
      "Trained batch 358 batch loss 1.64884067 epoch total loss 1.62270284\n",
      "Trained batch 359 batch loss 1.68568742 epoch total loss 1.62287819\n",
      "Trained batch 360 batch loss 1.68607092 epoch total loss 1.62305379\n",
      "Trained batch 361 batch loss 1.64422083 epoch total loss 1.62311244\n",
      "Trained batch 362 batch loss 1.6488508 epoch total loss 1.62318361\n",
      "Trained batch 363 batch loss 1.70668626 epoch total loss 1.62341356\n",
      "Trained batch 364 batch loss 1.7086004 epoch total loss 1.62364769\n",
      "Trained batch 365 batch loss 1.73210573 epoch total loss 1.62394488\n",
      "Trained batch 366 batch loss 1.68971682 epoch total loss 1.62412453\n",
      "Trained batch 367 batch loss 1.6644969 epoch total loss 1.62423444\n",
      "Trained batch 368 batch loss 1.75268161 epoch total loss 1.62458348\n",
      "Trained batch 369 batch loss 1.7326045 epoch total loss 1.62487626\n",
      "Trained batch 370 batch loss 1.73056722 epoch total loss 1.62516201\n",
      "Trained batch 371 batch loss 1.70897853 epoch total loss 1.62538791\n",
      "Trained batch 372 batch loss 1.69294333 epoch total loss 1.62556946\n",
      "Trained batch 373 batch loss 1.65736198 epoch total loss 1.6256547\n",
      "Trained batch 374 batch loss 1.66377521 epoch total loss 1.62575662\n",
      "Trained batch 375 batch loss 1.64157343 epoch total loss 1.62579882\n",
      "Trained batch 376 batch loss 1.66389608 epoch total loss 1.62590015\n",
      "Trained batch 377 batch loss 1.67105508 epoch total loss 1.62602\n",
      "Trained batch 378 batch loss 1.68979144 epoch total loss 1.62618876\n",
      "Trained batch 379 batch loss 1.69530392 epoch total loss 1.62637115\n",
      "Trained batch 380 batch loss 1.73069048 epoch total loss 1.62664568\n",
      "Trained batch 381 batch loss 1.69656789 epoch total loss 1.62682927\n",
      "Trained batch 382 batch loss 1.64351 epoch total loss 1.6268729\n",
      "Trained batch 383 batch loss 1.55635095 epoch total loss 1.62668872\n",
      "Trained batch 384 batch loss 1.57011163 epoch total loss 1.6265415\n",
      "Trained batch 385 batch loss 1.76197278 epoch total loss 1.62689316\n",
      "Trained batch 386 batch loss 1.70830023 epoch total loss 1.62710416\n",
      "Trained batch 387 batch loss 1.66340899 epoch total loss 1.62719786\n",
      "Trained batch 388 batch loss 1.68210566 epoch total loss 1.62733948\n",
      "Trained batch 389 batch loss 1.60238576 epoch total loss 1.62727523\n",
      "Trained batch 390 batch loss 1.66232204 epoch total loss 1.62736499\n",
      "Trained batch 391 batch loss 1.67120075 epoch total loss 1.62747717\n",
      "Trained batch 392 batch loss 1.72143376 epoch total loss 1.6277169\n",
      "Trained batch 393 batch loss 1.58280158 epoch total loss 1.62760258\n",
      "Trained batch 394 batch loss 1.61281419 epoch total loss 1.62756503\n",
      "Trained batch 395 batch loss 1.66721559 epoch total loss 1.6276654\n",
      "Trained batch 396 batch loss 1.63208807 epoch total loss 1.62767661\n",
      "Trained batch 397 batch loss 1.63548696 epoch total loss 1.62769628\n",
      "Trained batch 398 batch loss 1.60549116 epoch total loss 1.62764049\n",
      "Trained batch 399 batch loss 1.60836625 epoch total loss 1.62759209\n",
      "Trained batch 400 batch loss 1.6418792 epoch total loss 1.62762785\n",
      "Trained batch 401 batch loss 1.51886499 epoch total loss 1.62735665\n",
      "Trained batch 402 batch loss 1.70640826 epoch total loss 1.62755334\n",
      "Trained batch 403 batch loss 1.70717514 epoch total loss 1.62775087\n",
      "Trained batch 404 batch loss 1.73304164 epoch total loss 1.62801147\n",
      "Trained batch 405 batch loss 1.75089502 epoch total loss 1.62831485\n",
      "Trained batch 406 batch loss 1.63910961 epoch total loss 1.62834144\n",
      "Trained batch 407 batch loss 1.50740981 epoch total loss 1.62804425\n",
      "Trained batch 408 batch loss 1.46481395 epoch total loss 1.6276443\n",
      "Trained batch 409 batch loss 1.59039319 epoch total loss 1.62755322\n",
      "Trained batch 410 batch loss 1.74081933 epoch total loss 1.62782955\n",
      "Trained batch 411 batch loss 1.6565944 epoch total loss 1.62789953\n",
      "Trained batch 412 batch loss 1.75845397 epoch total loss 1.62821651\n",
      "Trained batch 413 batch loss 1.73627043 epoch total loss 1.62847817\n",
      "Trained batch 414 batch loss 1.52929878 epoch total loss 1.62823856\n",
      "Trained batch 415 batch loss 1.48321509 epoch total loss 1.62788904\n",
      "Trained batch 416 batch loss 1.41913152 epoch total loss 1.62738729\n",
      "Trained batch 417 batch loss 1.56512558 epoch total loss 1.62723792\n",
      "Trained batch 418 batch loss 1.63523543 epoch total loss 1.62725711\n",
      "Trained batch 419 batch loss 1.64217877 epoch total loss 1.62729263\n",
      "Trained batch 420 batch loss 1.6396625 epoch total loss 1.62732208\n",
      "Trained batch 421 batch loss 1.5972774 epoch total loss 1.62725079\n",
      "Trained batch 422 batch loss 1.66950166 epoch total loss 1.62735081\n",
      "Trained batch 423 batch loss 1.63527882 epoch total loss 1.62736952\n",
      "Trained batch 424 batch loss 1.51951814 epoch total loss 1.62711525\n",
      "Trained batch 425 batch loss 1.5433898 epoch total loss 1.6269182\n",
      "Trained batch 426 batch loss 1.6534307 epoch total loss 1.62698054\n",
      "Trained batch 427 batch loss 1.62840438 epoch total loss 1.62698388\n",
      "Trained batch 428 batch loss 1.63843274 epoch total loss 1.62701058\n",
      "Trained batch 429 batch loss 1.59952295 epoch total loss 1.62694657\n",
      "Trained batch 430 batch loss 1.65038574 epoch total loss 1.62700105\n",
      "Trained batch 431 batch loss 1.64869595 epoch total loss 1.62705135\n",
      "Trained batch 432 batch loss 1.69439197 epoch total loss 1.62720728\n",
      "Trained batch 433 batch loss 1.66731656 epoch total loss 1.6272999\n",
      "Trained batch 434 batch loss 1.67968261 epoch total loss 1.62742054\n",
      "Trained batch 435 batch loss 1.63224077 epoch total loss 1.62743175\n",
      "Trained batch 436 batch loss 1.54238212 epoch total loss 1.6272366\n",
      "Trained batch 437 batch loss 1.50704551 epoch total loss 1.62696147\n",
      "Trained batch 438 batch loss 1.47009039 epoch total loss 1.62660336\n",
      "Trained batch 439 batch loss 1.46827376 epoch total loss 1.62624264\n",
      "Trained batch 440 batch loss 1.50318503 epoch total loss 1.62596297\n",
      "Trained batch 441 batch loss 1.52901053 epoch total loss 1.62574303\n",
      "Trained batch 442 batch loss 1.65790558 epoch total loss 1.62581587\n",
      "Trained batch 443 batch loss 1.56697631 epoch total loss 1.62568295\n",
      "Trained batch 444 batch loss 1.55321383 epoch total loss 1.62551975\n",
      "Trained batch 445 batch loss 1.61739361 epoch total loss 1.62550139\n",
      "Trained batch 446 batch loss 1.63272703 epoch total loss 1.62551773\n",
      "Trained batch 447 batch loss 1.61554837 epoch total loss 1.62549543\n",
      "Trained batch 448 batch loss 1.58924246 epoch total loss 1.62541449\n",
      "Trained batch 449 batch loss 1.36253214 epoch total loss 1.62482893\n",
      "Trained batch 450 batch loss 1.29945934 epoch total loss 1.62410593\n",
      "Trained batch 451 batch loss 1.43415618 epoch total loss 1.62368464\n",
      "Trained batch 452 batch loss 1.60579908 epoch total loss 1.62364507\n",
      "Trained batch 453 batch loss 1.70747733 epoch total loss 1.62383008\n",
      "Trained batch 454 batch loss 1.71820283 epoch total loss 1.62403798\n",
      "Trained batch 455 batch loss 1.69742334 epoch total loss 1.62419927\n",
      "Trained batch 456 batch loss 1.70750856 epoch total loss 1.62438202\n",
      "Trained batch 457 batch loss 1.6307466 epoch total loss 1.62439597\n",
      "Trained batch 458 batch loss 1.61782181 epoch total loss 1.62438154\n",
      "Trained batch 459 batch loss 1.58576226 epoch total loss 1.62429738\n",
      "Trained batch 460 batch loss 1.60671413 epoch total loss 1.62425911\n",
      "Trained batch 461 batch loss 1.65038586 epoch total loss 1.62431574\n",
      "Trained batch 462 batch loss 1.80052304 epoch total loss 1.62469721\n",
      "Trained batch 463 batch loss 1.64842987 epoch total loss 1.62474847\n",
      "Trained batch 464 batch loss 1.71410358 epoch total loss 1.62494111\n",
      "Trained batch 465 batch loss 1.63230824 epoch total loss 1.62495697\n",
      "Trained batch 466 batch loss 1.6843369 epoch total loss 1.6250844\n",
      "Trained batch 467 batch loss 1.67688727 epoch total loss 1.62519526\n",
      "Trained batch 468 batch loss 1.63390326 epoch total loss 1.62521386\n",
      "Trained batch 469 batch loss 1.6760428 epoch total loss 1.62532222\n",
      "Trained batch 470 batch loss 1.63605762 epoch total loss 1.62534499\n",
      "Trained batch 471 batch loss 1.61730039 epoch total loss 1.62532794\n",
      "Trained batch 472 batch loss 1.72975552 epoch total loss 1.6255492\n",
      "Trained batch 473 batch loss 1.74485219 epoch total loss 1.62580144\n",
      "Trained batch 474 batch loss 1.62924337 epoch total loss 1.62580884\n",
      "Trained batch 475 batch loss 1.57001138 epoch total loss 1.62569129\n",
      "Trained batch 476 batch loss 1.59539628 epoch total loss 1.62562764\n",
      "Trained batch 477 batch loss 1.67238212 epoch total loss 1.62572563\n",
      "Trained batch 478 batch loss 1.73003626 epoch total loss 1.6259439\n",
      "Trained batch 479 batch loss 1.7392143 epoch total loss 1.62618029\n",
      "Trained batch 480 batch loss 1.73762655 epoch total loss 1.62641251\n",
      "Trained batch 481 batch loss 1.59500277 epoch total loss 1.62634718\n",
      "Trained batch 482 batch loss 1.59806299 epoch total loss 1.62628853\n",
      "Trained batch 483 batch loss 1.57816696 epoch total loss 1.62618899\n",
      "Trained batch 484 batch loss 1.60563707 epoch total loss 1.62614655\n",
      "Trained batch 485 batch loss 1.59305978 epoch total loss 1.62607837\n",
      "Trained batch 486 batch loss 1.62434447 epoch total loss 1.62607479\n",
      "Trained batch 487 batch loss 1.61425149 epoch total loss 1.62605047\n",
      "Trained batch 488 batch loss 1.61017179 epoch total loss 1.62601793\n",
      "Trained batch 489 batch loss 1.55525815 epoch total loss 1.62587321\n",
      "Trained batch 490 batch loss 1.68505383 epoch total loss 1.62599397\n",
      "Trained batch 491 batch loss 1.68369329 epoch total loss 1.62611151\n",
      "Trained batch 492 batch loss 1.66565609 epoch total loss 1.62619197\n",
      "Trained batch 493 batch loss 1.65344727 epoch total loss 1.62624717\n",
      "Trained batch 494 batch loss 1.64785028 epoch total loss 1.62629092\n",
      "Trained batch 495 batch loss 1.64044333 epoch total loss 1.62631953\n",
      "Trained batch 496 batch loss 1.61857 epoch total loss 1.62630391\n",
      "Trained batch 497 batch loss 1.62182605 epoch total loss 1.62629485\n",
      "Trained batch 498 batch loss 1.64915061 epoch total loss 1.62634087\n",
      "Trained batch 499 batch loss 1.73607826 epoch total loss 1.62656069\n",
      "Trained batch 500 batch loss 1.6058383 epoch total loss 1.62651932\n",
      "Trained batch 501 batch loss 1.66091239 epoch total loss 1.62658787\n",
      "Trained batch 502 batch loss 1.60342681 epoch total loss 1.62654185\n",
      "Trained batch 503 batch loss 1.62544775 epoch total loss 1.62653959\n",
      "Trained batch 504 batch loss 1.66600037 epoch total loss 1.62661791\n",
      "Trained batch 505 batch loss 1.67830253 epoch total loss 1.62672019\n",
      "Trained batch 506 batch loss 1.64505076 epoch total loss 1.62675655\n",
      "Trained batch 507 batch loss 1.65155458 epoch total loss 1.62680542\n",
      "Trained batch 508 batch loss 1.59603333 epoch total loss 1.62674475\n",
      "Trained batch 509 batch loss 1.60201144 epoch total loss 1.62669611\n",
      "Trained batch 510 batch loss 1.62042773 epoch total loss 1.62668383\n",
      "Trained batch 511 batch loss 1.57381034 epoch total loss 1.62658036\n",
      "Trained batch 512 batch loss 1.62014079 epoch total loss 1.62656772\n",
      "Trained batch 513 batch loss 1.65649712 epoch total loss 1.62662601\n",
      "Trained batch 514 batch loss 1.66431057 epoch total loss 1.62669933\n",
      "Trained batch 515 batch loss 1.709934 epoch total loss 1.6268611\n",
      "Trained batch 516 batch loss 1.78065872 epoch total loss 1.62715912\n",
      "Trained batch 517 batch loss 1.71409535 epoch total loss 1.6273272\n",
      "Trained batch 518 batch loss 1.69973636 epoch total loss 1.62746692\n",
      "Trained batch 519 batch loss 1.6391691 epoch total loss 1.62748945\n",
      "Trained batch 520 batch loss 1.59215045 epoch total loss 1.62742162\n",
      "Trained batch 521 batch loss 1.4859699 epoch total loss 1.62715\n",
      "Trained batch 522 batch loss 1.41135073 epoch total loss 1.62673664\n",
      "Trained batch 523 batch loss 1.66303396 epoch total loss 1.62680614\n",
      "Trained batch 524 batch loss 1.70591569 epoch total loss 1.62695706\n",
      "Trained batch 525 batch loss 1.69356942 epoch total loss 1.6270839\n",
      "Trained batch 526 batch loss 1.70656359 epoch total loss 1.62723494\n",
      "Trained batch 527 batch loss 1.72603142 epoch total loss 1.62742245\n",
      "Trained batch 528 batch loss 1.73218608 epoch total loss 1.62762082\n",
      "Trained batch 529 batch loss 1.72409928 epoch total loss 1.62780321\n",
      "Trained batch 530 batch loss 1.72919214 epoch total loss 1.62799454\n",
      "Trained batch 531 batch loss 1.70301092 epoch total loss 1.6281358\n",
      "Trained batch 532 batch loss 1.69416285 epoch total loss 1.6282599\n",
      "Trained batch 533 batch loss 1.64549 epoch total loss 1.6282922\n",
      "Trained batch 534 batch loss 1.60958099 epoch total loss 1.62825716\n",
      "Trained batch 535 batch loss 1.48057282 epoch total loss 1.62798119\n",
      "Trained batch 536 batch loss 1.49990523 epoch total loss 1.62774217\n",
      "Trained batch 537 batch loss 1.64167321 epoch total loss 1.62776804\n",
      "Trained batch 538 batch loss 1.60946071 epoch total loss 1.62773395\n",
      "Trained batch 539 batch loss 1.63547516 epoch total loss 1.62774837\n",
      "Trained batch 540 batch loss 1.59246135 epoch total loss 1.62768304\n",
      "Trained batch 541 batch loss 1.61313021 epoch total loss 1.62765622\n",
      "Trained batch 542 batch loss 1.58581 epoch total loss 1.62757897\n",
      "Trained batch 543 batch loss 1.61769843 epoch total loss 1.62756073\n",
      "Trained batch 544 batch loss 1.55004656 epoch total loss 1.62741828\n",
      "Trained batch 545 batch loss 1.57681131 epoch total loss 1.62732542\n",
      "Trained batch 546 batch loss 1.6095283 epoch total loss 1.62729287\n",
      "Trained batch 547 batch loss 1.64572883 epoch total loss 1.62732661\n",
      "Trained batch 548 batch loss 1.64791322 epoch total loss 1.62736416\n",
      "Trained batch 549 batch loss 1.64754808 epoch total loss 1.62740088\n",
      "Trained batch 550 batch loss 1.60386837 epoch total loss 1.62735808\n",
      "Trained batch 551 batch loss 1.72211027 epoch total loss 1.62753\n",
      "Trained batch 552 batch loss 1.60433853 epoch total loss 1.6274879\n",
      "Trained batch 553 batch loss 1.65194416 epoch total loss 1.62753212\n",
      "Trained batch 554 batch loss 1.56557965 epoch total loss 1.62742031\n",
      "Trained batch 555 batch loss 1.52827132 epoch total loss 1.62724161\n",
      "Trained batch 556 batch loss 1.46244264 epoch total loss 1.62694526\n",
      "Trained batch 557 batch loss 1.57173145 epoch total loss 1.62684608\n",
      "Trained batch 558 batch loss 1.53363645 epoch total loss 1.62667906\n",
      "Trained batch 559 batch loss 1.54499531 epoch total loss 1.62653291\n",
      "Trained batch 560 batch loss 1.69528151 epoch total loss 1.62665558\n",
      "Trained batch 561 batch loss 1.58069205 epoch total loss 1.62657368\n",
      "Trained batch 562 batch loss 1.50536394 epoch total loss 1.62635803\n",
      "Trained batch 563 batch loss 1.57428324 epoch total loss 1.62626553\n",
      "Trained batch 564 batch loss 1.66642475 epoch total loss 1.62633669\n",
      "Trained batch 565 batch loss 1.64567375 epoch total loss 1.62637103\n",
      "Trained batch 566 batch loss 1.52916121 epoch total loss 1.62619925\n",
      "Trained batch 567 batch loss 1.58692253 epoch total loss 1.62613\n",
      "Trained batch 568 batch loss 1.62727976 epoch total loss 1.62613189\n",
      "Trained batch 569 batch loss 1.54071581 epoch total loss 1.62598181\n",
      "Trained batch 570 batch loss 1.48760414 epoch total loss 1.6257391\n",
      "Trained batch 571 batch loss 1.49572158 epoch total loss 1.62551141\n",
      "Trained batch 572 batch loss 1.49773097 epoch total loss 1.62528801\n",
      "Trained batch 573 batch loss 1.62701941 epoch total loss 1.62529099\n",
      "Trained batch 574 batch loss 1.61719322 epoch total loss 1.62527692\n",
      "Trained batch 575 batch loss 1.56403744 epoch total loss 1.62517035\n",
      "Trained batch 576 batch loss 1.61054111 epoch total loss 1.62514496\n",
      "Trained batch 577 batch loss 1.57229722 epoch total loss 1.62505341\n",
      "Trained batch 578 batch loss 1.64338553 epoch total loss 1.62508512\n",
      "Trained batch 579 batch loss 1.57080126 epoch total loss 1.6249913\n",
      "Trained batch 580 batch loss 1.52255619 epoch total loss 1.62481475\n",
      "Trained batch 581 batch loss 1.65765333 epoch total loss 1.62487125\n",
      "Trained batch 582 batch loss 1.56765902 epoch total loss 1.62477303\n",
      "Trained batch 583 batch loss 1.6202929 epoch total loss 1.6247654\n",
      "Trained batch 584 batch loss 1.71228015 epoch total loss 1.62491524\n",
      "Trained batch 585 batch loss 1.68296278 epoch total loss 1.62501454\n",
      "Trained batch 586 batch loss 1.76657963 epoch total loss 1.62525606\n",
      "Trained batch 587 batch loss 1.69945478 epoch total loss 1.62538254\n",
      "Trained batch 588 batch loss 1.64372849 epoch total loss 1.62541378\n",
      "Trained batch 589 batch loss 1.71286833 epoch total loss 1.62556231\n",
      "Trained batch 590 batch loss 1.63480282 epoch total loss 1.62557793\n",
      "Trained batch 591 batch loss 1.60260022 epoch total loss 1.62553906\n",
      "Trained batch 592 batch loss 1.59057391 epoch total loss 1.62548\n",
      "Trained batch 593 batch loss 1.61479545 epoch total loss 1.62546206\n",
      "Trained batch 594 batch loss 1.64676237 epoch total loss 1.62549794\n",
      "Trained batch 595 batch loss 1.61423159 epoch total loss 1.6254791\n",
      "Trained batch 596 batch loss 1.63449192 epoch total loss 1.62549424\n",
      "Trained batch 597 batch loss 1.5292145 epoch total loss 1.62533295\n",
      "Trained batch 598 batch loss 1.61239362 epoch total loss 1.62531126\n",
      "Trained batch 599 batch loss 1.60887802 epoch total loss 1.62528384\n",
      "Trained batch 600 batch loss 1.56139052 epoch total loss 1.62517738\n",
      "Trained batch 601 batch loss 1.61242807 epoch total loss 1.62515616\n",
      "Trained batch 602 batch loss 1.5386852 epoch total loss 1.62501252\n",
      "Trained batch 603 batch loss 1.62112594 epoch total loss 1.6250062\n",
      "Trained batch 604 batch loss 1.55098796 epoch total loss 1.62488353\n",
      "Trained batch 605 batch loss 1.48978138 epoch total loss 1.62466037\n",
      "Trained batch 606 batch loss 1.47221088 epoch total loss 1.62440884\n",
      "Trained batch 607 batch loss 1.520625 epoch total loss 1.62423778\n",
      "Trained batch 608 batch loss 1.59133124 epoch total loss 1.62418365\n",
      "Trained batch 609 batch loss 1.57209945 epoch total loss 1.62409806\n",
      "Trained batch 610 batch loss 1.63862205 epoch total loss 1.6241219\n",
      "Trained batch 611 batch loss 1.80429375 epoch total loss 1.62441683\n",
      "Trained batch 612 batch loss 1.73333955 epoch total loss 1.62459481\n",
      "Trained batch 613 batch loss 1.70272446 epoch total loss 1.62472224\n",
      "Trained batch 614 batch loss 1.69331121 epoch total loss 1.62483394\n",
      "Trained batch 615 batch loss 1.69870913 epoch total loss 1.6249541\n",
      "Trained batch 616 batch loss 1.48957181 epoch total loss 1.62473428\n",
      "Trained batch 617 batch loss 1.53377092 epoch total loss 1.62458682\n",
      "Trained batch 618 batch loss 1.59663773 epoch total loss 1.62454152\n",
      "Trained batch 619 batch loss 1.68753076 epoch total loss 1.62464333\n",
      "Trained batch 620 batch loss 1.58653 epoch total loss 1.62458193\n",
      "Trained batch 621 batch loss 1.55596578 epoch total loss 1.62447143\n",
      "Trained batch 622 batch loss 1.60543609 epoch total loss 1.62444079\n",
      "Trained batch 623 batch loss 1.5261234 epoch total loss 1.62428296\n",
      "Trained batch 624 batch loss 1.59750032 epoch total loss 1.62424\n",
      "Trained batch 625 batch loss 1.53470111 epoch total loss 1.62409675\n",
      "Trained batch 626 batch loss 1.60512 epoch total loss 1.62406647\n",
      "Trained batch 627 batch loss 1.48341584 epoch total loss 1.62384212\n",
      "Trained batch 628 batch loss 1.63598 epoch total loss 1.62386143\n",
      "Trained batch 629 batch loss 1.73197103 epoch total loss 1.62403333\n",
      "Trained batch 630 batch loss 1.814574 epoch total loss 1.62433577\n",
      "Trained batch 631 batch loss 1.74020398 epoch total loss 1.62451947\n",
      "Trained batch 632 batch loss 1.65713131 epoch total loss 1.62457097\n",
      "Trained batch 633 batch loss 1.63148212 epoch total loss 1.62458193\n",
      "Trained batch 634 batch loss 1.62772191 epoch total loss 1.62458682\n",
      "Trained batch 635 batch loss 1.58575606 epoch total loss 1.62452579\n",
      "Trained batch 636 batch loss 1.5270375 epoch total loss 1.62437236\n",
      "Trained batch 637 batch loss 1.64858794 epoch total loss 1.62441039\n",
      "Trained batch 638 batch loss 1.56948113 epoch total loss 1.6243242\n",
      "Trained batch 639 batch loss 1.68900752 epoch total loss 1.62442541\n",
      "Trained batch 640 batch loss 1.67565322 epoch total loss 1.6245054\n",
      "Trained batch 641 batch loss 1.66641712 epoch total loss 1.62457073\n",
      "Trained batch 642 batch loss 1.66803205 epoch total loss 1.62463856\n",
      "Trained batch 643 batch loss 1.64981699 epoch total loss 1.62467766\n",
      "Trained batch 644 batch loss 1.59183538 epoch total loss 1.62462664\n",
      "Trained batch 645 batch loss 1.64953363 epoch total loss 1.62466526\n",
      "Trained batch 646 batch loss 1.63937449 epoch total loss 1.62468803\n",
      "Trained batch 647 batch loss 1.55919373 epoch total loss 1.62458682\n",
      "Trained batch 648 batch loss 1.63981521 epoch total loss 1.62461019\n",
      "Trained batch 649 batch loss 1.54876089 epoch total loss 1.62449324\n",
      "Trained batch 650 batch loss 1.54231858 epoch total loss 1.62436688\n",
      "Trained batch 651 batch loss 1.52205157 epoch total loss 1.62420988\n",
      "Trained batch 652 batch loss 1.61600077 epoch total loss 1.62419713\n",
      "Trained batch 653 batch loss 1.62320948 epoch total loss 1.62419558\n",
      "Trained batch 654 batch loss 1.66148472 epoch total loss 1.62425268\n",
      "Trained batch 655 batch loss 1.69130492 epoch total loss 1.62435496\n",
      "Trained batch 656 batch loss 1.66492581 epoch total loss 1.62441683\n",
      "Trained batch 657 batch loss 1.67150879 epoch total loss 1.62448847\n",
      "Trained batch 658 batch loss 1.64921594 epoch total loss 1.62452602\n",
      "Trained batch 659 batch loss 1.66753376 epoch total loss 1.62459123\n",
      "Trained batch 660 batch loss 1.65317047 epoch total loss 1.6246345\n",
      "Trained batch 661 batch loss 1.6732949 epoch total loss 1.62470818\n",
      "Trained batch 662 batch loss 1.62410855 epoch total loss 1.62470734\n",
      "Trained batch 663 batch loss 1.58513141 epoch total loss 1.62464762\n",
      "Trained batch 664 batch loss 1.58768606 epoch total loss 1.62459183\n",
      "Trained batch 665 batch loss 1.59054363 epoch total loss 1.62454069\n",
      "Trained batch 666 batch loss 1.49448431 epoch total loss 1.62434542\n",
      "Trained batch 667 batch loss 1.55999804 epoch total loss 1.6242491\n",
      "Trained batch 668 batch loss 1.71751761 epoch total loss 1.62438869\n",
      "Trained batch 669 batch loss 1.65722859 epoch total loss 1.62443781\n",
      "Trained batch 670 batch loss 1.63826227 epoch total loss 1.62445855\n",
      "Trained batch 671 batch loss 1.55499125 epoch total loss 1.62435496\n",
      "Trained batch 672 batch loss 1.57199335 epoch total loss 1.624277\n",
      "Trained batch 673 batch loss 1.61575651 epoch total loss 1.62426436\n",
      "Trained batch 674 batch loss 1.62509871 epoch total loss 1.62426555\n",
      "Trained batch 675 batch loss 1.62784421 epoch total loss 1.6242708\n",
      "Trained batch 676 batch loss 1.64195013 epoch total loss 1.62429702\n",
      "Trained batch 677 batch loss 1.62509143 epoch total loss 1.62429821\n",
      "Trained batch 678 batch loss 1.65338063 epoch total loss 1.62434101\n",
      "Trained batch 679 batch loss 1.57146442 epoch total loss 1.62426305\n",
      "Trained batch 680 batch loss 1.633811 epoch total loss 1.62427711\n",
      "Trained batch 681 batch loss 1.57545209 epoch total loss 1.62420535\n",
      "Trained batch 682 batch loss 1.62253881 epoch total loss 1.62420297\n",
      "Trained batch 683 batch loss 1.68855894 epoch total loss 1.62429726\n",
      "Trained batch 684 batch loss 1.69439375 epoch total loss 1.62439966\n",
      "Trained batch 685 batch loss 1.70001864 epoch total loss 1.62451017\n",
      "Trained batch 686 batch loss 1.65891516 epoch total loss 1.62456036\n",
      "Trained batch 687 batch loss 1.71407139 epoch total loss 1.62469065\n",
      "Trained batch 688 batch loss 1.64499176 epoch total loss 1.62472022\n",
      "Trained batch 689 batch loss 1.6345861 epoch total loss 1.62473464\n",
      "Trained batch 690 batch loss 1.6560626 epoch total loss 1.62477994\n",
      "Trained batch 691 batch loss 1.6374867 epoch total loss 1.6247983\n",
      "Trained batch 692 batch loss 1.70846665 epoch total loss 1.62491918\n",
      "Trained batch 693 batch loss 1.59742141 epoch total loss 1.62487948\n",
      "Trained batch 694 batch loss 1.58708107 epoch total loss 1.624825\n",
      "Trained batch 695 batch loss 1.67367816 epoch total loss 1.62489533\n",
      "Trained batch 696 batch loss 1.60137379 epoch total loss 1.62486148\n",
      "Trained batch 697 batch loss 1.5832659 epoch total loss 1.62480175\n",
      "Trained batch 698 batch loss 1.61235929 epoch total loss 1.62478387\n",
      "Trained batch 699 batch loss 1.67213142 epoch total loss 1.62485158\n",
      "Trained batch 700 batch loss 1.54742742 epoch total loss 1.62474108\n",
      "Trained batch 701 batch loss 1.53035569 epoch total loss 1.62460649\n",
      "Trained batch 702 batch loss 1.5620532 epoch total loss 1.62451732\n",
      "Trained batch 703 batch loss 1.63915896 epoch total loss 1.62453806\n",
      "Trained batch 704 batch loss 1.44645143 epoch total loss 1.6242851\n",
      "Trained batch 705 batch loss 1.64195061 epoch total loss 1.62431014\n",
      "Trained batch 706 batch loss 1.67273164 epoch total loss 1.6243788\n",
      "Trained batch 707 batch loss 1.68962955 epoch total loss 1.62447095\n",
      "Trained batch 708 batch loss 1.70793509 epoch total loss 1.62458885\n",
      "Trained batch 709 batch loss 1.73455834 epoch total loss 1.62474394\n",
      "Trained batch 710 batch loss 1.69371009 epoch total loss 1.62484109\n",
      "Trained batch 711 batch loss 1.58984911 epoch total loss 1.62479186\n",
      "Trained batch 712 batch loss 1.51575339 epoch total loss 1.6246388\n",
      "Trained batch 713 batch loss 1.42602801 epoch total loss 1.6243602\n",
      "Trained batch 714 batch loss 1.38393903 epoch total loss 1.62402344\n",
      "Trained batch 715 batch loss 1.38360906 epoch total loss 1.62368727\n",
      "Trained batch 716 batch loss 1.64015198 epoch total loss 1.62371027\n",
      "Trained batch 717 batch loss 1.5653708 epoch total loss 1.62362897\n",
      "Trained batch 718 batch loss 1.60836172 epoch total loss 1.62360775\n",
      "Trained batch 719 batch loss 1.53277695 epoch total loss 1.62348151\n",
      "Trained batch 720 batch loss 1.6522162 epoch total loss 1.62352145\n",
      "Trained batch 721 batch loss 1.55522847 epoch total loss 1.62342668\n",
      "Trained batch 722 batch loss 1.644382 epoch total loss 1.62345564\n",
      "Trained batch 723 batch loss 1.54663181 epoch total loss 1.62334943\n",
      "Trained batch 724 batch loss 1.67562556 epoch total loss 1.62342167\n",
      "Trained batch 725 batch loss 1.62122166 epoch total loss 1.62341869\n",
      "Trained batch 726 batch loss 1.69230676 epoch total loss 1.62351346\n",
      "Trained batch 727 batch loss 1.6213454 epoch total loss 1.62351048\n",
      "Trained batch 728 batch loss 1.59172511 epoch total loss 1.62346673\n",
      "Trained batch 729 batch loss 1.62816191 epoch total loss 1.62347317\n",
      "Trained batch 730 batch loss 1.56334782 epoch total loss 1.62339079\n",
      "Trained batch 731 batch loss 1.52559495 epoch total loss 1.62325716\n",
      "Trained batch 732 batch loss 1.56655681 epoch total loss 1.62317967\n",
      "Trained batch 733 batch loss 1.61947179 epoch total loss 1.62317455\n",
      "Trained batch 734 batch loss 1.58477259 epoch total loss 1.62312222\n",
      "Trained batch 735 batch loss 1.571491 epoch total loss 1.623052\n",
      "Trained batch 736 batch loss 1.65119541 epoch total loss 1.62309039\n",
      "Trained batch 737 batch loss 1.58831215 epoch total loss 1.62304306\n",
      "Trained batch 738 batch loss 1.54065669 epoch total loss 1.62293148\n",
      "Trained batch 739 batch loss 1.44862854 epoch total loss 1.62269557\n",
      "Trained batch 740 batch loss 1.65035629 epoch total loss 1.622733\n",
      "Trained batch 741 batch loss 1.6074357 epoch total loss 1.62271225\n",
      "Trained batch 742 batch loss 1.61661232 epoch total loss 1.62270403\n",
      "Trained batch 743 batch loss 1.61495471 epoch total loss 1.62269366\n",
      "Trained batch 744 batch loss 1.55982363 epoch total loss 1.62260914\n",
      "Trained batch 745 batch loss 1.53600478 epoch total loss 1.62249291\n",
      "Trained batch 746 batch loss 1.50508916 epoch total loss 1.62233555\n",
      "Trained batch 747 batch loss 1.54777718 epoch total loss 1.62223566\n",
      "Trained batch 748 batch loss 1.4874897 epoch total loss 1.62205565\n",
      "Trained batch 749 batch loss 1.45894146 epoch total loss 1.62183785\n",
      "Trained batch 750 batch loss 1.51226985 epoch total loss 1.62169194\n",
      "Trained batch 751 batch loss 1.46680784 epoch total loss 1.62148559\n",
      "Trained batch 752 batch loss 1.42930722 epoch total loss 1.62123013\n",
      "Trained batch 753 batch loss 1.62329578 epoch total loss 1.62123287\n",
      "Trained batch 754 batch loss 1.52058625 epoch total loss 1.62109947\n",
      "Trained batch 755 batch loss 1.50843191 epoch total loss 1.62095022\n",
      "Trained batch 756 batch loss 1.63443244 epoch total loss 1.62096798\n",
      "Trained batch 757 batch loss 1.65290141 epoch total loss 1.62101018\n",
      "Trained batch 758 batch loss 1.6279515 epoch total loss 1.62101936\n",
      "Trained batch 759 batch loss 1.57422459 epoch total loss 1.62095773\n",
      "Trained batch 760 batch loss 1.59507036 epoch total loss 1.62092364\n",
      "Trained batch 761 batch loss 1.5724988 epoch total loss 1.62086\n",
      "Trained batch 762 batch loss 1.63979316 epoch total loss 1.6208849\n",
      "Trained batch 763 batch loss 1.56407726 epoch total loss 1.62081039\n",
      "Trained batch 764 batch loss 1.50198662 epoch total loss 1.62065482\n",
      "Trained batch 765 batch loss 1.46751964 epoch total loss 1.62045467\n",
      "Trained batch 766 batch loss 1.59606624 epoch total loss 1.62042284\n",
      "Trained batch 767 batch loss 1.60292268 epoch total loss 1.6204\n",
      "Trained batch 768 batch loss 1.71321535 epoch total loss 1.62052095\n",
      "Trained batch 769 batch loss 1.60483932 epoch total loss 1.62050056\n",
      "Trained batch 770 batch loss 1.58111167 epoch total loss 1.6204493\n",
      "Trained batch 771 batch loss 1.6069684 epoch total loss 1.62043178\n",
      "Trained batch 772 batch loss 1.54719603 epoch total loss 1.62033701\n",
      "Trained batch 773 batch loss 1.54357743 epoch total loss 1.62023771\n",
      "Trained batch 774 batch loss 1.55712378 epoch total loss 1.62015617\n",
      "Trained batch 775 batch loss 1.66066456 epoch total loss 1.62020838\n",
      "Trained batch 776 batch loss 1.71046329 epoch total loss 1.62032473\n",
      "Trained batch 777 batch loss 1.72820544 epoch total loss 1.62046349\n",
      "Trained batch 778 batch loss 1.73095429 epoch total loss 1.62060547\n",
      "Trained batch 779 batch loss 1.74681962 epoch total loss 1.62076747\n",
      "Trained batch 780 batch loss 1.63942719 epoch total loss 1.62079144\n",
      "Trained batch 781 batch loss 1.59547818 epoch total loss 1.62075901\n",
      "Trained batch 782 batch loss 1.63308525 epoch total loss 1.62077463\n",
      "Trained batch 783 batch loss 1.56256616 epoch total loss 1.62070036\n",
      "Trained batch 784 batch loss 1.60200083 epoch total loss 1.62067664\n",
      "Trained batch 785 batch loss 1.5795908 epoch total loss 1.6206243\n",
      "Trained batch 786 batch loss 1.55599058 epoch total loss 1.62054205\n",
      "Trained batch 787 batch loss 1.61958385 epoch total loss 1.62054098\n",
      "Trained batch 788 batch loss 1.54367864 epoch total loss 1.62044346\n",
      "Trained batch 789 batch loss 1.40585065 epoch total loss 1.62017155\n",
      "Trained batch 790 batch loss 1.34221601 epoch total loss 1.61981964\n",
      "Trained batch 791 batch loss 1.29486871 epoch total loss 1.61940885\n",
      "Trained batch 792 batch loss 1.54658294 epoch total loss 1.61931694\n",
      "Trained batch 793 batch loss 1.69987595 epoch total loss 1.6194185\n",
      "Trained batch 794 batch loss 1.6829927 epoch total loss 1.61949849\n",
      "Trained batch 795 batch loss 1.58990932 epoch total loss 1.61946142\n",
      "Trained batch 796 batch loss 1.64770603 epoch total loss 1.61949682\n",
      "Trained batch 797 batch loss 1.58401382 epoch total loss 1.61945236\n",
      "Trained batch 798 batch loss 1.51336575 epoch total loss 1.61931932\n",
      "Trained batch 799 batch loss 1.56201506 epoch total loss 1.61924756\n",
      "Trained batch 800 batch loss 1.56668854 epoch total loss 1.61918187\n",
      "Trained batch 801 batch loss 1.57897007 epoch total loss 1.61913168\n",
      "Trained batch 802 batch loss 1.4654609 epoch total loss 1.61894\n",
      "Trained batch 803 batch loss 1.54466176 epoch total loss 1.61884749\n",
      "Trained batch 804 batch loss 1.61653173 epoch total loss 1.61884475\n",
      "Trained batch 805 batch loss 1.61514616 epoch total loss 1.6188401\n",
      "Trained batch 806 batch loss 1.60643435 epoch total loss 1.61882472\n",
      "Trained batch 807 batch loss 1.62889457 epoch total loss 1.61883724\n",
      "Trained batch 808 batch loss 1.34763646 epoch total loss 1.61850154\n",
      "Trained batch 809 batch loss 1.63068473 epoch total loss 1.61851668\n",
      "Trained batch 810 batch loss 1.60104644 epoch total loss 1.61849511\n",
      "Trained batch 811 batch loss 1.67562544 epoch total loss 1.61856568\n",
      "Trained batch 812 batch loss 1.67217541 epoch total loss 1.6186316\n",
      "Trained batch 813 batch loss 1.63380432 epoch total loss 1.6186502\n",
      "Trained batch 814 batch loss 1.63434386 epoch total loss 1.61866963\n",
      "Trained batch 815 batch loss 1.59861839 epoch total loss 1.61864495\n",
      "Trained batch 816 batch loss 1.59792972 epoch total loss 1.61861956\n",
      "Trained batch 817 batch loss 1.54852557 epoch total loss 1.61853385\n",
      "Trained batch 818 batch loss 1.60318673 epoch total loss 1.61851501\n",
      "Trained batch 819 batch loss 1.59295535 epoch total loss 1.61848378\n",
      "Trained batch 820 batch loss 1.6508944 epoch total loss 1.61852324\n",
      "Trained batch 821 batch loss 1.70968294 epoch total loss 1.61863434\n",
      "Trained batch 822 batch loss 1.6186316 epoch total loss 1.61863434\n",
      "Trained batch 823 batch loss 1.59831476 epoch total loss 1.61860967\n",
      "Trained batch 824 batch loss 1.56750143 epoch total loss 1.61854756\n",
      "Trained batch 825 batch loss 1.59647763 epoch total loss 1.61852086\n",
      "Trained batch 826 batch loss 1.62509608 epoch total loss 1.61852884\n",
      "Trained batch 827 batch loss 1.50326014 epoch total loss 1.61838949\n",
      "Trained batch 828 batch loss 1.55405188 epoch total loss 1.61831176\n",
      "Trained batch 829 batch loss 1.53522778 epoch total loss 1.61821163\n",
      "Trained batch 830 batch loss 1.43866837 epoch total loss 1.61799538\n",
      "Trained batch 831 batch loss 1.51937544 epoch total loss 1.61787677\n",
      "Trained batch 832 batch loss 1.49835646 epoch total loss 1.61773312\n",
      "Trained batch 833 batch loss 1.48692739 epoch total loss 1.61757612\n",
      "Trained batch 834 batch loss 1.57495785 epoch total loss 1.61752498\n",
      "Trained batch 835 batch loss 1.69989228 epoch total loss 1.61762369\n",
      "Trained batch 836 batch loss 1.62841129 epoch total loss 1.61763668\n",
      "Trained batch 837 batch loss 1.50310755 epoch total loss 1.61749971\n",
      "Trained batch 838 batch loss 1.54147613 epoch total loss 1.61740911\n",
      "Trained batch 839 batch loss 1.59842527 epoch total loss 1.61738634\n",
      "Trained batch 840 batch loss 1.62492275 epoch total loss 1.61739528\n",
      "Trained batch 841 batch loss 1.60600722 epoch total loss 1.61738169\n",
      "Trained batch 842 batch loss 1.53600121 epoch total loss 1.61728501\n",
      "Trained batch 843 batch loss 1.56504607 epoch total loss 1.61722314\n",
      "Trained batch 844 batch loss 1.56916523 epoch total loss 1.61716628\n",
      "Trained batch 845 batch loss 1.5237 epoch total loss 1.61705565\n",
      "Trained batch 846 batch loss 1.50289822 epoch total loss 1.61692071\n",
      "Trained batch 847 batch loss 1.28656912 epoch total loss 1.61653078\n",
      "Trained batch 848 batch loss 1.36454582 epoch total loss 1.61623359\n",
      "Trained batch 849 batch loss 1.52170289 epoch total loss 1.61612225\n",
      "Trained batch 850 batch loss 1.75239384 epoch total loss 1.61628258\n",
      "Trained batch 851 batch loss 1.80479693 epoch total loss 1.61650419\n",
      "Trained batch 852 batch loss 1.70367622 epoch total loss 1.61660647\n",
      "Trained batch 853 batch loss 1.49876583 epoch total loss 1.61646843\n",
      "Trained batch 854 batch loss 1.47962499 epoch total loss 1.61630809\n",
      "Trained batch 855 batch loss 1.69026136 epoch total loss 1.61639464\n",
      "Trained batch 856 batch loss 1.74832106 epoch total loss 1.61654878\n",
      "Trained batch 857 batch loss 1.68579316 epoch total loss 1.6166296\n",
      "Trained batch 858 batch loss 1.654459 epoch total loss 1.61667359\n",
      "Trained batch 859 batch loss 1.71469796 epoch total loss 1.61678779\n",
      "Trained batch 860 batch loss 1.6872592 epoch total loss 1.61686969\n",
      "Trained batch 861 batch loss 1.60966384 epoch total loss 1.61686122\n",
      "Trained batch 862 batch loss 1.6412127 epoch total loss 1.6168896\n",
      "Trained batch 863 batch loss 1.60417926 epoch total loss 1.61687481\n",
      "Trained batch 864 batch loss 1.58876526 epoch total loss 1.61684215\n",
      "Trained batch 865 batch loss 1.67011249 epoch total loss 1.6169039\n",
      "Trained batch 866 batch loss 1.67869389 epoch total loss 1.61697519\n",
      "Trained batch 867 batch loss 1.43847656 epoch total loss 1.61676931\n",
      "Trained batch 868 batch loss 1.4776032 epoch total loss 1.6166091\n",
      "Trained batch 869 batch loss 1.60897446 epoch total loss 1.61660028\n",
      "Trained batch 870 batch loss 1.61902785 epoch total loss 1.61660314\n",
      "Trained batch 871 batch loss 1.62170553 epoch total loss 1.61660898\n",
      "Trained batch 872 batch loss 1.66129518 epoch total loss 1.61666012\n",
      "Trained batch 873 batch loss 1.64909697 epoch total loss 1.61669731\n",
      "Trained batch 874 batch loss 1.56116319 epoch total loss 1.61663377\n",
      "Trained batch 875 batch loss 1.58142316 epoch total loss 1.61659348\n",
      "Trained batch 876 batch loss 1.63491905 epoch total loss 1.61661434\n",
      "Trained batch 877 batch loss 1.65809 epoch total loss 1.61666167\n",
      "Trained batch 878 batch loss 1.65379119 epoch total loss 1.61670399\n",
      "Trained batch 879 batch loss 1.53158128 epoch total loss 1.61660719\n",
      "Trained batch 880 batch loss 1.48814607 epoch total loss 1.61646116\n",
      "Trained batch 881 batch loss 1.57040989 epoch total loss 1.61640894\n",
      "Trained batch 882 batch loss 1.64809537 epoch total loss 1.61644483\n",
      "Trained batch 883 batch loss 1.65272546 epoch total loss 1.61648595\n",
      "Trained batch 884 batch loss 1.60208225 epoch total loss 1.61646962\n",
      "Trained batch 885 batch loss 1.63171732 epoch total loss 1.61648679\n",
      "Trained batch 886 batch loss 1.69159937 epoch total loss 1.61657166\n",
      "Trained batch 887 batch loss 1.63794732 epoch total loss 1.61659575\n",
      "Trained batch 888 batch loss 1.63908339 epoch total loss 1.61662102\n",
      "Trained batch 889 batch loss 1.5628885 epoch total loss 1.61656058\n",
      "Trained batch 890 batch loss 1.54885864 epoch total loss 1.6164844\n",
      "Trained batch 891 batch loss 1.59526181 epoch total loss 1.61646056\n",
      "Trained batch 892 batch loss 1.63084209 epoch total loss 1.61647666\n",
      "Trained batch 893 batch loss 1.61866677 epoch total loss 1.61647916\n",
      "Trained batch 894 batch loss 1.70575917 epoch total loss 1.61657906\n",
      "Trained batch 895 batch loss 1.65101743 epoch total loss 1.61661756\n",
      "Trained batch 896 batch loss 1.62642384 epoch total loss 1.61662853\n",
      "Trained batch 897 batch loss 1.56814218 epoch total loss 1.61657441\n",
      "Trained batch 898 batch loss 1.57186449 epoch total loss 1.6165247\n",
      "Trained batch 899 batch loss 1.60747111 epoch total loss 1.61651456\n",
      "Trained batch 900 batch loss 1.6526947 epoch total loss 1.61655474\n",
      "Trained batch 901 batch loss 1.62302017 epoch total loss 1.61656201\n",
      "Trained batch 902 batch loss 1.69042087 epoch total loss 1.61664391\n",
      "Trained batch 903 batch loss 1.79754639 epoch total loss 1.61684418\n",
      "Trained batch 904 batch loss 1.71720779 epoch total loss 1.61695516\n",
      "Trained batch 905 batch loss 1.70293164 epoch total loss 1.61705\n",
      "Trained batch 906 batch loss 1.67757928 epoch total loss 1.61711693\n",
      "Trained batch 907 batch loss 1.75390124 epoch total loss 1.61726773\n",
      "Trained batch 908 batch loss 1.72273815 epoch total loss 1.61738396\n",
      "Trained batch 909 batch loss 1.70892167 epoch total loss 1.61748457\n",
      "Trained batch 910 batch loss 1.66662693 epoch total loss 1.61753857\n",
      "Trained batch 911 batch loss 1.48720706 epoch total loss 1.61739552\n",
      "Trained batch 912 batch loss 1.60581374 epoch total loss 1.61738276\n",
      "Trained batch 913 batch loss 1.63848495 epoch total loss 1.61740589\n",
      "Trained batch 914 batch loss 1.51479864 epoch total loss 1.6172936\n",
      "Trained batch 915 batch loss 1.62662601 epoch total loss 1.61730373\n",
      "Trained batch 916 batch loss 1.62950492 epoch total loss 1.61731708\n",
      "Trained batch 917 batch loss 1.60902011 epoch total loss 1.61730802\n",
      "Trained batch 918 batch loss 1.57097781 epoch total loss 1.61725748\n",
      "Trained batch 919 batch loss 1.68280566 epoch total loss 1.61732888\n",
      "Trained batch 920 batch loss 1.66460419 epoch total loss 1.61738014\n",
      "Trained batch 921 batch loss 1.60029137 epoch total loss 1.61736166\n",
      "Trained batch 922 batch loss 1.41795576 epoch total loss 1.61714542\n",
      "Trained batch 923 batch loss 1.55550551 epoch total loss 1.61707866\n",
      "Trained batch 924 batch loss 1.58487153 epoch total loss 1.61704373\n",
      "Trained batch 925 batch loss 1.6409 epoch total loss 1.61706948\n",
      "Trained batch 926 batch loss 1.59642911 epoch total loss 1.61704719\n",
      "Trained batch 927 batch loss 1.60081565 epoch total loss 1.61702979\n",
      "Trained batch 928 batch loss 1.60497284 epoch total loss 1.61701679\n",
      "Trained batch 929 batch loss 1.67275476 epoch total loss 1.61707675\n",
      "Trained batch 930 batch loss 1.63936973 epoch total loss 1.61710072\n",
      "Trained batch 931 batch loss 1.66180182 epoch total loss 1.61714864\n",
      "Trained batch 932 batch loss 1.57444227 epoch total loss 1.61710286\n",
      "Trained batch 933 batch loss 1.59693444 epoch total loss 1.61708128\n",
      "Trained batch 934 batch loss 1.51489925 epoch total loss 1.61697185\n",
      "Trained batch 935 batch loss 1.71500838 epoch total loss 1.61707664\n",
      "Trained batch 936 batch loss 1.47136819 epoch total loss 1.61692095\n",
      "Trained batch 937 batch loss 1.42960727 epoch total loss 1.61672103\n",
      "Trained batch 938 batch loss 1.42013955 epoch total loss 1.61651146\n",
      "Trained batch 939 batch loss 1.52708447 epoch total loss 1.61641622\n",
      "Trained batch 940 batch loss 1.57968032 epoch total loss 1.61637712\n",
      "Trained batch 941 batch loss 1.65740502 epoch total loss 1.61642075\n",
      "Trained batch 942 batch loss 1.67481875 epoch total loss 1.61648273\n",
      "Trained batch 943 batch loss 1.72579598 epoch total loss 1.61659861\n",
      "Trained batch 944 batch loss 1.6531291 epoch total loss 1.61663723\n",
      "Trained batch 945 batch loss 1.64888954 epoch total loss 1.61667144\n",
      "Trained batch 946 batch loss 1.62747097 epoch total loss 1.61668289\n",
      "Trained batch 947 batch loss 1.49005747 epoch total loss 1.61654913\n",
      "Trained batch 948 batch loss 1.48551798 epoch total loss 1.61641097\n",
      "Trained batch 949 batch loss 1.59418571 epoch total loss 1.61638761\n",
      "Trained batch 950 batch loss 1.65116191 epoch total loss 1.61642408\n",
      "Trained batch 951 batch loss 1.64089525 epoch total loss 1.61644983\n",
      "Trained batch 952 batch loss 1.60038602 epoch total loss 1.61643291\n",
      "Trained batch 953 batch loss 1.60479736 epoch total loss 1.61642075\n",
      "Trained batch 954 batch loss 1.56016517 epoch total loss 1.61636186\n",
      "Trained batch 955 batch loss 1.51747894 epoch total loss 1.61625826\n",
      "Trained batch 956 batch loss 1.45466757 epoch total loss 1.61608922\n",
      "Trained batch 957 batch loss 1.56498063 epoch total loss 1.61603582\n",
      "Trained batch 958 batch loss 1.6493448 epoch total loss 1.61607051\n",
      "Trained batch 959 batch loss 1.61025274 epoch total loss 1.61606443\n",
      "Trained batch 960 batch loss 1.73876882 epoch total loss 1.61619222\n",
      "Trained batch 961 batch loss 1.64314973 epoch total loss 1.61622036\n",
      "Trained batch 962 batch loss 1.62893105 epoch total loss 1.61623347\n",
      "Trained batch 963 batch loss 1.67447984 epoch total loss 1.61629391\n",
      "Trained batch 964 batch loss 1.69959664 epoch total loss 1.61638033\n",
      "Trained batch 965 batch loss 1.61453938 epoch total loss 1.61637843\n",
      "Trained batch 966 batch loss 1.61624825 epoch total loss 1.61637819\n",
      "Trained batch 967 batch loss 1.64724255 epoch total loss 1.61641014\n",
      "Trained batch 968 batch loss 1.73977435 epoch total loss 1.61653757\n",
      "Trained batch 969 batch loss 1.65625167 epoch total loss 1.61657858\n",
      "Trained batch 970 batch loss 1.62919044 epoch total loss 1.61659145\n",
      "Trained batch 971 batch loss 1.61064124 epoch total loss 1.61658537\n",
      "Trained batch 972 batch loss 1.57747328 epoch total loss 1.61654508\n",
      "Trained batch 973 batch loss 1.56866264 epoch total loss 1.61649585\n",
      "Trained batch 974 batch loss 1.5681715 epoch total loss 1.61644614\n",
      "Trained batch 975 batch loss 1.6193409 epoch total loss 1.61644924\n",
      "Trained batch 976 batch loss 1.59482896 epoch total loss 1.61642706\n",
      "Trained batch 977 batch loss 1.64719117 epoch total loss 1.61645854\n",
      "Trained batch 978 batch loss 1.5319705 epoch total loss 1.61637223\n",
      "Trained batch 979 batch loss 1.53133762 epoch total loss 1.61628532\n",
      "Trained batch 980 batch loss 1.52778816 epoch total loss 1.61619508\n",
      "Trained batch 981 batch loss 1.42955887 epoch total loss 1.61600482\n",
      "Trained batch 982 batch loss 1.72630489 epoch total loss 1.61611724\n",
      "Trained batch 983 batch loss 1.7368089 epoch total loss 1.61624\n",
      "Trained batch 984 batch loss 1.68114758 epoch total loss 1.61630595\n",
      "Trained batch 985 batch loss 1.65509796 epoch total loss 1.61634541\n",
      "Trained batch 986 batch loss 1.67148209 epoch total loss 1.61640131\n",
      "Trained batch 987 batch loss 1.65622175 epoch total loss 1.61644173\n",
      "Trained batch 988 batch loss 1.6301403 epoch total loss 1.61645555\n",
      "Trained batch 989 batch loss 1.64622927 epoch total loss 1.61648571\n",
      "Trained batch 990 batch loss 1.60761237 epoch total loss 1.61647677\n",
      "Trained batch 991 batch loss 1.57512212 epoch total loss 1.61643505\n",
      "Trained batch 992 batch loss 1.63717902 epoch total loss 1.61645591\n",
      "Trained batch 993 batch loss 1.59713948 epoch total loss 1.61643648\n",
      "Trained batch 994 batch loss 1.66113162 epoch total loss 1.61648142\n",
      "Trained batch 995 batch loss 1.54857826 epoch total loss 1.61641324\n",
      "Trained batch 996 batch loss 1.59939778 epoch total loss 1.61639607\n",
      "Trained batch 997 batch loss 1.61620319 epoch total loss 1.61639595\n",
      "Trained batch 998 batch loss 1.56874871 epoch total loss 1.61634815\n",
      "Trained batch 999 batch loss 1.61270189 epoch total loss 1.61634445\n",
      "Trained batch 1000 batch loss 1.56266785 epoch total loss 1.61629081\n",
      "Trained batch 1001 batch loss 1.54180956 epoch total loss 1.61621642\n",
      "Trained batch 1002 batch loss 1.58858049 epoch total loss 1.61618888\n",
      "Trained batch 1003 batch loss 1.68594778 epoch total loss 1.61625838\n",
      "Trained batch 1004 batch loss 1.61706769 epoch total loss 1.61625922\n",
      "Trained batch 1005 batch loss 1.57805312 epoch total loss 1.61622119\n",
      "Trained batch 1006 batch loss 1.43822181 epoch total loss 1.61604416\n",
      "Trained batch 1007 batch loss 1.53925288 epoch total loss 1.61596799\n",
      "Trained batch 1008 batch loss 1.5750078 epoch total loss 1.61592734\n",
      "Trained batch 1009 batch loss 1.62851763 epoch total loss 1.61593986\n",
      "Trained batch 1010 batch loss 1.62004268 epoch total loss 1.61594379\n",
      "Trained batch 1011 batch loss 1.66275668 epoch total loss 1.61599\n",
      "Trained batch 1012 batch loss 1.6329273 epoch total loss 1.61600685\n",
      "Trained batch 1013 batch loss 1.58829904 epoch total loss 1.61597943\n",
      "Trained batch 1014 batch loss 1.62950826 epoch total loss 1.61599278\n",
      "Trained batch 1015 batch loss 1.57667077 epoch total loss 1.61595404\n",
      "Trained batch 1016 batch loss 1.61154568 epoch total loss 1.61594975\n",
      "Trained batch 1017 batch loss 1.51026237 epoch total loss 1.6158458\n",
      "Trained batch 1018 batch loss 1.57493925 epoch total loss 1.61580563\n",
      "Trained batch 1019 batch loss 1.64222443 epoch total loss 1.61583149\n",
      "Trained batch 1020 batch loss 1.65510285 epoch total loss 1.61587012\n",
      "Trained batch 1021 batch loss 1.63305962 epoch total loss 1.61588693\n",
      "Trained batch 1022 batch loss 1.72782278 epoch total loss 1.61599636\n",
      "Trained batch 1023 batch loss 1.63031971 epoch total loss 1.61601043\n",
      "Trained batch 1024 batch loss 1.68162656 epoch total loss 1.61607456\n",
      "Trained batch 1025 batch loss 1.65380716 epoch total loss 1.6161114\n",
      "Trained batch 1026 batch loss 1.63369596 epoch total loss 1.61612844\n",
      "Trained batch 1027 batch loss 1.55022335 epoch total loss 1.61606431\n",
      "Trained batch 1028 batch loss 1.59998631 epoch total loss 1.61604857\n",
      "Trained batch 1029 batch loss 1.67962444 epoch total loss 1.61611032\n",
      "Trained batch 1030 batch loss 1.72551203 epoch total loss 1.61621654\n",
      "Trained batch 1031 batch loss 1.69882226 epoch total loss 1.61629665\n",
      "Trained batch 1032 batch loss 1.55170512 epoch total loss 1.61623406\n",
      "Trained batch 1033 batch loss 1.69144249 epoch total loss 1.6163069\n",
      "Trained batch 1034 batch loss 1.61989784 epoch total loss 1.61631036\n",
      "Trained batch 1035 batch loss 1.67872608 epoch total loss 1.61637068\n",
      "Trained batch 1036 batch loss 1.64606953 epoch total loss 1.61639929\n",
      "Trained batch 1037 batch loss 1.56375909 epoch total loss 1.61634851\n",
      "Trained batch 1038 batch loss 1.68361497 epoch total loss 1.61641335\n",
      "Trained batch 1039 batch loss 1.55174184 epoch total loss 1.61635113\n",
      "Trained batch 1040 batch loss 1.4131285 epoch total loss 1.61615562\n",
      "Trained batch 1041 batch loss 1.56459665 epoch total loss 1.61610615\n",
      "Trained batch 1042 batch loss 1.48768318 epoch total loss 1.61598289\n",
      "Trained batch 1155 batch loss 1.64667654 epoch total loss 1.61318958\n",
      "Trained batch 1156 batch loss 1.56545174 epoch total loss 1.61314833\n",
      "Trained batch 1157 batch loss 1.55748332 epoch total loss 1.61310017\n",
      "Trained batch 1158 batch loss 1.50101495 epoch total loss 1.61300337\n",
      "Trained batch 1159 batch loss 1.67774069 epoch total loss 1.61305928\n",
      "Trained batch 1160 batch loss 1.59236312 epoch total loss 1.6130414\n",
      "Trained batch 1161 batch loss 1.64812148 epoch total loss 1.61307156\n",
      "Trained batch 1162 batch loss 1.68253016 epoch total loss 1.61313128\n",
      "Trained batch 1163 batch loss 1.67759466 epoch total loss 1.61318684\n",
      "Trained batch 1164 batch loss 1.64835644 epoch total loss 1.613217\n",
      "Trained batch 1165 batch loss 1.62857282 epoch total loss 1.61323011\n",
      "Trained batch 1166 batch loss 1.64637101 epoch total loss 1.61325848\n",
      "Trained batch 1167 batch loss 1.60155666 epoch total loss 1.61324847\n",
      "Trained batch 1168 batch loss 1.57232988 epoch total loss 1.61321354\n",
      "Trained batch 1169 batch loss 1.54100823 epoch total loss 1.61315179\n",
      "Trained batch 1170 batch loss 1.50012231 epoch total loss 1.61305511\n",
      "Trained batch 1171 batch loss 1.51276362 epoch total loss 1.61296952\n",
      "Trained batch 1172 batch loss 1.42706203 epoch total loss 1.61281085\n",
      "Trained batch 1173 batch loss 1.72398686 epoch total loss 1.61290562\n",
      "Trained batch 1174 batch loss 1.65061462 epoch total loss 1.61293781\n",
      "Trained batch 1175 batch loss 1.70098579 epoch total loss 1.61301267\n",
      "Trained batch 1176 batch loss 1.65738964 epoch total loss 1.61305034\n",
      "Trained batch 1177 batch loss 1.73634315 epoch total loss 1.61315513\n",
      "Trained batch 1178 batch loss 1.6535002 epoch total loss 1.61318934\n",
      "Trained batch 1179 batch loss 1.51328754 epoch total loss 1.61310458\n",
      "Trained batch 1180 batch loss 1.52066219 epoch total loss 1.61302626\n",
      "Trained batch 1181 batch loss 1.64059579 epoch total loss 1.61304963\n",
      "Trained batch 1182 batch loss 1.5868 epoch total loss 1.61302745\n",
      "Trained batch 1183 batch loss 1.61531878 epoch total loss 1.61302936\n",
      "Trained batch 1184 batch loss 1.68726063 epoch total loss 1.61309206\n",
      "Trained batch 1185 batch loss 1.60776138 epoch total loss 1.61308753\n",
      "Trained batch 1186 batch loss 1.5745796 epoch total loss 1.61305511\n",
      "Trained batch 1187 batch loss 1.64157128 epoch total loss 1.61307919\n",
      "Trained batch 1188 batch loss 1.70318687 epoch total loss 1.61315513\n",
      "Trained batch 1189 batch loss 1.63830805 epoch total loss 1.61317623\n",
      "Trained batch 1190 batch loss 1.67906189 epoch total loss 1.61323166\n",
      "Trained batch 1191 batch loss 1.67310619 epoch total loss 1.61328185\n",
      "Trained batch 1192 batch loss 1.68570566 epoch total loss 1.61334264\n",
      "Trained batch 1193 batch loss 1.68985581 epoch total loss 1.61340666\n",
      "Trained batch 1194 batch loss 1.5164578 epoch total loss 1.61332548\n",
      "Trained batch 1195 batch loss 1.58261156 epoch total loss 1.61329985\n",
      "Trained batch 1196 batch loss 1.575351 epoch total loss 1.61326814\n",
      "Trained batch 1197 batch loss 1.51546419 epoch total loss 1.61318636\n",
      "Trained batch 1198 batch loss 1.53933549 epoch total loss 1.61312473\n",
      "Trained batch 1199 batch loss 1.57724476 epoch total loss 1.61309481\n",
      "Trained batch 1200 batch loss 1.63956928 epoch total loss 1.61311686\n",
      "Trained batch 1201 batch loss 1.63585973 epoch total loss 1.61313581\n",
      "Trained batch 1202 batch loss 1.42603087 epoch total loss 1.61298013\n",
      "Trained batch 1203 batch loss 1.46073794 epoch total loss 1.61285353\n",
      "Trained batch 1204 batch loss 1.38709795 epoch total loss 1.61266601\n",
      "Trained batch 1205 batch loss 1.48305273 epoch total loss 1.61255848\n",
      "Trained batch 1206 batch loss 1.54972816 epoch total loss 1.61250627\n",
      "Trained batch 1207 batch loss 1.49251628 epoch total loss 1.61240697\n",
      "Trained batch 1208 batch loss 1.65139377 epoch total loss 1.61243916\n",
      "Trained batch 1209 batch loss 1.58616734 epoch total loss 1.61241746\n",
      "Trained batch 1210 batch loss 1.62391043 epoch total loss 1.612427\n",
      "Trained batch 1211 batch loss 1.57321787 epoch total loss 1.61239457\n",
      "Trained batch 1212 batch loss 1.5658145 epoch total loss 1.61235619\n",
      "Trained batch 1213 batch loss 1.74134731 epoch total loss 1.61246252\n",
      "Trained batch 1214 batch loss 1.68776703 epoch total loss 1.61252451\n",
      "Trained batch 1215 batch loss 1.69144809 epoch total loss 1.61258936\n",
      "Trained batch 1216 batch loss 1.661708 epoch total loss 1.61262989\n",
      "Trained batch 1217 batch loss 1.65868711 epoch total loss 1.61266768\n",
      "Trained batch 1218 batch loss 1.61257088 epoch total loss 1.61266756\n",
      "Trained batch 1219 batch loss 1.618554 epoch total loss 1.61267245\n",
      "Trained batch 1220 batch loss 1.57505059 epoch total loss 1.61264157\n",
      "Trained batch 1221 batch loss 1.62443876 epoch total loss 1.61265123\n",
      "Trained batch 1222 batch loss 1.69493556 epoch total loss 1.61271858\n",
      "Trained batch 1223 batch loss 1.59564424 epoch total loss 1.61270463\n",
      "Trained batch 1224 batch loss 1.64551818 epoch total loss 1.61273146\n",
      "Trained batch 1225 batch loss 1.64345849 epoch total loss 1.61275649\n",
      "Trained batch 1226 batch loss 1.63442326 epoch total loss 1.61277413\n",
      "Trained batch 1227 batch loss 1.55467486 epoch total loss 1.61272681\n",
      "Trained batch 1228 batch loss 1.66815066 epoch total loss 1.61277187\n",
      "Trained batch 1229 batch loss 1.58091724 epoch total loss 1.612746\n",
      "Trained batch 1230 batch loss 1.56107712 epoch total loss 1.61270392\n",
      "Trained batch 1231 batch loss 1.59368575 epoch total loss 1.61268842\n",
      "Trained batch 1232 batch loss 1.62184811 epoch total loss 1.61269581\n",
      "Trained batch 1233 batch loss 1.55016541 epoch total loss 1.61264515\n",
      "Trained batch 1234 batch loss 1.45111763 epoch total loss 1.61251426\n",
      "Trained batch 1235 batch loss 1.50916338 epoch total loss 1.61243057\n",
      "Trained batch 1236 batch loss 1.39061224 epoch total loss 1.61225116\n",
      "Trained batch 1237 batch loss 1.57373595 epoch total loss 1.61222\n",
      "Trained batch 1238 batch loss 1.60649967 epoch total loss 1.6122154\n",
      "Trained batch 1239 batch loss 1.65613723 epoch total loss 1.6122508\n",
      "Trained batch 1240 batch loss 1.63772154 epoch total loss 1.61227131\n",
      "Trained batch 1241 batch loss 1.63598037 epoch total loss 1.61229038\n",
      "Trained batch 1242 batch loss 1.61591983 epoch total loss 1.61229336\n",
      "Trained batch 1243 batch loss 1.61091375 epoch total loss 1.61229229\n",
      "Trained batch 1244 batch loss 1.65541673 epoch total loss 1.61232698\n",
      "Trained batch 1245 batch loss 1.65632725 epoch total loss 1.61236238\n",
      "Trained batch 1246 batch loss 1.63260198 epoch total loss 1.6123786\n",
      "Trained batch 1247 batch loss 1.62330461 epoch total loss 1.6123873\n",
      "Trained batch 1248 batch loss 1.60993063 epoch total loss 1.61238539\n",
      "Trained batch 1249 batch loss 1.62965071 epoch total loss 1.61239922\n",
      "Trained batch 1250 batch loss 1.56444955 epoch total loss 1.61236084\n",
      "Trained batch 1251 batch loss 1.60631216 epoch total loss 1.61235607\n",
      "Trained batch 1252 batch loss 1.54309 epoch total loss 1.61230063\n",
      "Trained batch 1253 batch loss 1.62608159 epoch total loss 1.61231172\n",
      "Trained batch 1254 batch loss 1.64577889 epoch total loss 1.61233842\n",
      "Trained batch 1255 batch loss 1.6338228 epoch total loss 1.61235547\n",
      "Trained batch 1256 batch loss 1.66274309 epoch total loss 1.61239552\n",
      "Trained batch 1257 batch loss 1.56439614 epoch total loss 1.61235738\n",
      "Trained batch 1258 batch loss 1.60576761 epoch total loss 1.61235213\n",
      "Trained batch 1259 batch loss 1.55508268 epoch total loss 1.61230659\n",
      "Trained batch 1260 batch loss 1.61865616 epoch total loss 1.61231172\n",
      "Trained batch 1261 batch loss 1.55619729 epoch total loss 1.61226714\n",
      "Trained batch 1262 batch loss 1.57830822 epoch total loss 1.61224031\n",
      "Trained batch 1263 batch loss 1.52636361 epoch total loss 1.61217225\n",
      "Trained batch 1264 batch loss 1.59513259 epoch total loss 1.61215878\n",
      "Trained batch 1265 batch loss 1.58819854 epoch total loss 1.61213982\n",
      "Trained batch 1266 batch loss 1.50022697 epoch total loss 1.61205149\n",
      "Trained batch 1267 batch loss 1.58915567 epoch total loss 1.61203337\n",
      "Trained batch 1268 batch loss 1.68735707 epoch total loss 1.61209285\n",
      "Trained batch 1269 batch loss 1.66322911 epoch total loss 1.61213303\n",
      "Trained batch 1270 batch loss 1.64625096 epoch total loss 1.61216\n",
      "Trained batch 1271 batch loss 1.68983412 epoch total loss 1.61222112\n",
      "Trained batch 1272 batch loss 1.66021407 epoch total loss 1.61225879\n",
      "Trained batch 1273 batch loss 1.662305 epoch total loss 1.61229813\n",
      "Trained batch 1274 batch loss 1.61153007 epoch total loss 1.61229765\n",
      "Trained batch 1275 batch loss 1.61250734 epoch total loss 1.61229777\n",
      "Trained batch 1276 batch loss 1.66337109 epoch total loss 1.61233783\n",
      "Trained batch 1277 batch loss 1.69698691 epoch total loss 1.61240411\n",
      "Trained batch 1278 batch loss 1.65239108 epoch total loss 1.61243534\n",
      "Trained batch 1279 batch loss 1.58764529 epoch total loss 1.61241603\n",
      "Trained batch 1280 batch loss 1.68564677 epoch total loss 1.61247313\n",
      "Trained batch 1281 batch loss 1.66391838 epoch total loss 1.61251318\n",
      "Trained batch 1282 batch loss 1.68713593 epoch total loss 1.61257148\n",
      "Trained batch 1283 batch loss 1.59769082 epoch total loss 1.61255991\n",
      "Trained batch 1284 batch loss 1.6454401 epoch total loss 1.61258554\n",
      "Trained batch 1285 batch loss 1.60468745 epoch total loss 1.61257946\n",
      "Trained batch 1286 batch loss 1.644714 epoch total loss 1.6126045\n",
      "Trained batch 1287 batch loss 1.66285419 epoch total loss 1.61264348\n",
      "Trained batch 1288 batch loss 1.58864069 epoch total loss 1.61262488\n",
      "Trained batch 1289 batch loss 1.37410104 epoch total loss 1.61243975\n",
      "Trained batch 1290 batch loss 1.42809248 epoch total loss 1.6122967\n",
      "Trained batch 1291 batch loss 1.48548925 epoch total loss 1.61219859\n",
      "Trained batch 1292 batch loss 1.42453969 epoch total loss 1.61205339\n",
      "Trained batch 1293 batch loss 1.34149301 epoch total loss 1.61184418\n",
      "Trained batch 1294 batch loss 1.29041636 epoch total loss 1.61159587\n",
      "Trained batch 1295 batch loss 1.31084394 epoch total loss 1.61136353\n",
      "Trained batch 1296 batch loss 1.44689906 epoch total loss 1.61123657\n",
      "Trained batch 1297 batch loss 1.6487186 epoch total loss 1.61126542\n",
      "Trained batch 1298 batch loss 1.64224553 epoch total loss 1.61128938\n",
      "Trained batch 1299 batch loss 1.63775647 epoch total loss 1.61130965\n",
      "Trained batch 1300 batch loss 1.58697295 epoch total loss 1.61129093\n",
      "Trained batch 1301 batch loss 1.56756067 epoch total loss 1.61125743\n",
      "Trained batch 1302 batch loss 1.64350915 epoch total loss 1.61128223\n",
      "Trained batch 1303 batch loss 1.55439854 epoch total loss 1.6112386\n",
      "Trained batch 1304 batch loss 1.65469491 epoch total loss 1.61127198\n",
      "Trained batch 1305 batch loss 1.65793443 epoch total loss 1.61130774\n",
      "Trained batch 1306 batch loss 1.60408974 epoch total loss 1.61130214\n",
      "Trained batch 1307 batch loss 1.60280061 epoch total loss 1.61129558\n",
      "Trained batch 1308 batch loss 1.69542646 epoch total loss 1.61135983\n",
      "Trained batch 1309 batch loss 1.65865993 epoch total loss 1.61139596\n",
      "Trained batch 1310 batch loss 1.70111823 epoch total loss 1.6114645\n",
      "Trained batch 1311 batch loss 1.6245873 epoch total loss 1.61147451\n",
      "Trained batch 1312 batch loss 1.61421955 epoch total loss 1.61147666\n",
      "Trained batch 1313 batch loss 1.48553634 epoch total loss 1.6113807\n",
      "Trained batch 1314 batch loss 1.45846128 epoch total loss 1.61126435\n",
      "Trained batch 1315 batch loss 1.45810938 epoch total loss 1.61114788\n",
      "Trained batch 1316 batch loss 1.41973257 epoch total loss 1.61100233\n",
      "Trained batch 1317 batch loss 1.55728197 epoch total loss 1.61096168\n",
      "Trained batch 1318 batch loss 1.53811765 epoch total loss 1.61090636\n",
      "Trained batch 1319 batch loss 1.43858969 epoch total loss 1.61077559\n",
      "Trained batch 1320 batch loss 1.5662533 epoch total loss 1.61074185\n",
      "Trained batch 1321 batch loss 1.51928246 epoch total loss 1.61067259\n",
      "Trained batch 1322 batch loss 1.42604375 epoch total loss 1.61053288\n",
      "Trained batch 1323 batch loss 1.52309585 epoch total loss 1.61046684\n",
      "Trained batch 1324 batch loss 1.6341486 epoch total loss 1.61048472\n",
      "Trained batch 1325 batch loss 1.61774027 epoch total loss 1.61049008\n",
      "Trained batch 1326 batch loss 1.6508677 epoch total loss 1.6105206\n",
      "Trained batch 1327 batch loss 1.61246264 epoch total loss 1.61052215\n",
      "Trained batch 1328 batch loss 1.68168521 epoch total loss 1.61057568\n",
      "Trained batch 1329 batch loss 1.59007549 epoch total loss 1.6105603\n",
      "Trained batch 1330 batch loss 1.53384233 epoch total loss 1.6105026\n",
      "Trained batch 1331 batch loss 1.59064519 epoch total loss 1.6104877\n",
      "Trained batch 1332 batch loss 1.63179648 epoch total loss 1.61050367\n",
      "Trained batch 1333 batch loss 1.57815886 epoch total loss 1.61047935\n",
      "Trained batch 1334 batch loss 1.63555157 epoch total loss 1.61049819\n",
      "Trained batch 1335 batch loss 1.58373737 epoch total loss 1.61047816\n",
      "Trained batch 1336 batch loss 1.53658402 epoch total loss 1.61042285\n",
      "Trained batch 1337 batch loss 1.52188241 epoch total loss 1.61035669\n",
      "Trained batch 1338 batch loss 1.62270379 epoch total loss 1.61036599\n",
      "Trained batch 1339 batch loss 1.59415829 epoch total loss 1.61035395\n",
      "Trained batch 1340 batch loss 1.54689968 epoch total loss 1.6103065\n",
      "Trained batch 1341 batch loss 1.55121851 epoch total loss 1.61026251\n",
      "Trained batch 1342 batch loss 1.56578434 epoch total loss 1.61022925\n",
      "Trained batch 1343 batch loss 1.54731083 epoch total loss 1.61018252\n",
      "Trained batch 1344 batch loss 1.50310159 epoch total loss 1.61010289\n",
      "Trained batch 1345 batch loss 1.32832837 epoch total loss 1.60989344\n",
      "Trained batch 1346 batch loss 1.43146932 epoch total loss 1.60976076\n",
      "Trained batch 1347 batch loss 1.46610153 epoch total loss 1.60965407\n",
      "Trained batch 1348 batch loss 1.51367426 epoch total loss 1.6095829\n",
      "Trained batch 1349 batch loss 1.43332231 epoch total loss 1.60945225\n",
      "Trained batch 1350 batch loss 1.56714058 epoch total loss 1.6094209\n",
      "Trained batch 1351 batch loss 1.57310891 epoch total loss 1.60939395\n",
      "Trained batch 1352 batch loss 1.50672674 epoch total loss 1.60931814\n",
      "Trained batch 1353 batch loss 1.63520885 epoch total loss 1.60933733\n",
      "Trained batch 1354 batch loss 1.59389138 epoch total loss 1.609326\n",
      "Trained batch 1355 batch loss 1.54051578 epoch total loss 1.60927522\n",
      "Trained batch 1356 batch loss 1.5432204 epoch total loss 1.60922647\n",
      "Trained batch 1357 batch loss 1.53076458 epoch total loss 1.60916865\n",
      "Trained batch 1358 batch loss 1.56436133 epoch total loss 1.60913575\n",
      "Trained batch 1359 batch loss 1.54056334 epoch total loss 1.6090852\n",
      "Trained batch 1360 batch loss 1.60853601 epoch total loss 1.60908484\n",
      "Trained batch 1361 batch loss 1.61066175 epoch total loss 1.60908604\n",
      "Trained batch 1362 batch loss 1.70399499 epoch total loss 1.60915577\n",
      "Trained batch 1363 batch loss 1.60808098 epoch total loss 1.60915506\n",
      "Trained batch 1364 batch loss 1.44675064 epoch total loss 1.60903597\n",
      "Trained batch 1365 batch loss 1.41473198 epoch total loss 1.60889375\n",
      "Trained batch 1366 batch loss 1.59313202 epoch total loss 1.60888207\n",
      "Trained batch 1367 batch loss 1.56065655 epoch total loss 1.60884666\n",
      "Trained batch 1368 batch loss 1.62796497 epoch total loss 1.60886061\n",
      "Trained batch 1369 batch loss 1.64551568 epoch total loss 1.60888743\n",
      "Trained batch 1370 batch loss 1.56123614 epoch total loss 1.60885262\n",
      "Trained batch 1371 batch loss 1.59680974 epoch total loss 1.60884392\n",
      "Trained batch 1372 batch loss 1.65335774 epoch total loss 1.60887635\n",
      "Trained batch 1373 batch loss 1.53933322 epoch total loss 1.60882568\n",
      "Trained batch 1374 batch loss 1.54278743 epoch total loss 1.60877764\n",
      "Trained batch 1375 batch loss 1.52289987 epoch total loss 1.60871518\n",
      "Trained batch 1376 batch loss 1.61155868 epoch total loss 1.60871732\n",
      "Trained batch 1377 batch loss 1.6125648 epoch total loss 1.60872006\n",
      "Trained batch 1378 batch loss 1.54544342 epoch total loss 1.60867417\n",
      "Trained batch 1379 batch loss 1.53119099 epoch total loss 1.60861802\n",
      "Trained batch 1380 batch loss 1.59436297 epoch total loss 1.60860777\n",
      "Trained batch 1381 batch loss 1.57177007 epoch total loss 1.60858107\n",
      "Trained batch 1382 batch loss 1.70644605 epoch total loss 1.608652\n",
      "Trained batch 1383 batch loss 1.67139637 epoch total loss 1.6086973\n",
      "Trained batch 1384 batch loss 1.65750265 epoch total loss 1.60873258\n",
      "Trained batch 1385 batch loss 1.58598721 epoch total loss 1.60871613\n",
      "Trained batch 1386 batch loss 1.60135067 epoch total loss 1.60871077\n",
      "Trained batch 1387 batch loss 1.64888895 epoch total loss 1.60873973\n",
      "Trained batch 1388 batch loss 1.64764929 epoch total loss 1.60876787\n",
      "Epoch 4 train loss 1.6087678670883179\n",
      "Validated batch 1 batch loss 1.4584744\n",
      "Validated batch 2 batch loss 1.6863451\n",
      "Validated batch 3 batch loss 1.60912871\n",
      "Validated batch 4 batch loss 1.57270157\n",
      "Validated batch 5 batch loss 1.61532831\n",
      "Validated batch 6 batch loss 1.65545225\n",
      "Validated batch 7 batch loss 1.52966928\n",
      "Validated batch 8 batch loss 1.68589258\n",
      "Validated batch 9 batch loss 1.53155744\n",
      "Validated batch 10 batch loss 1.60254109\n",
      "Validated batch 11 batch loss 1.55734837\n",
      "Validated batch 12 batch loss 1.43813229\n",
      "Validated batch 13 batch loss 1.49654198\n",
      "Validated batch 14 batch loss 1.64735174\n",
      "Validated batch 15 batch loss 1.62034118\n",
      "Validated batch 16 batch loss 1.53810358\n",
      "Validated batch 17 batch loss 1.63928866\n",
      "Validated batch 18 batch loss 1.632038\n",
      "Validated batch 19 batch loss 1.62089109\n",
      "Validated batch 20 batch loss 1.70348859\n",
      "Validated batch 21 batch loss 1.63867605\n",
      "Validated batch 22 batch loss 1.58868194\n",
      "Validated batch 23 batch loss 1.51639807\n",
      "Validated batch 24 batch loss 1.53990543\n",
      "Validated batch 25 batch loss 1.60832596\n",
      "Validated batch 26 batch loss 1.5132153\n",
      "Validated batch 27 batch loss 1.6812439\n",
      "Validated batch 28 batch loss 1.62854338\n",
      "Validated batch 29 batch loss 1.59076643\n",
      "Validated batch 30 batch loss 1.64822817\n",
      "Validated batch 31 batch loss 1.60909307\n",
      "Validated batch 32 batch loss 1.6379683\n",
      "Validated batch 33 batch loss 1.63934171\n",
      "Validated batch 34 batch loss 1.69396949\n",
      "Validated batch 35 batch loss 1.60375524\n",
      "Validated batch 36 batch loss 1.54479277\n",
      "Validated batch 37 batch loss 1.63039422\n",
      "Validated batch 38 batch loss 1.59007239\n",
      "Validated batch 39 batch loss 1.556216\n",
      "Validated batch 40 batch loss 1.66900921\n",
      "Validated batch 41 batch loss 1.4110297\n",
      "Validated batch 42 batch loss 1.56300509\n",
      "Validated batch 43 batch loss 1.63108432\n",
      "Validated batch 44 batch loss 1.58268666\n",
      "Validated batch 45 batch loss 1.67922\n",
      "Validated batch 46 batch loss 1.57767427\n",
      "Validated batch 47 batch loss 1.61664331\n",
      "Validated batch 48 batch loss 1.54991198\n",
      "Validated batch 49 batch loss 1.51631761\n",
      "Validated batch 50 batch loss 1.54144144\n",
      "Validated batch 51 batch loss 1.58251011\n",
      "Validated batch 52 batch loss 1.59687972\n",
      "Validated batch 53 batch loss 1.58919132\n",
      "Validated batch 54 batch loss 1.57802606\n",
      "Validated batch 55 batch loss 1.58378792\n",
      "Validated batch 56 batch loss 1.65369952\n",
      "Validated batch 57 batch loss 1.67606294\n",
      "Validated batch 58 batch loss 1.62273431\n",
      "Validated batch 59 batch loss 1.57915711\n",
      "Validated batch 60 batch loss 1.52754068\n",
      "Validated batch 61 batch loss 1.6203301\n",
      "Validated batch 62 batch loss 1.57421589\n",
      "Validated batch 63 batch loss 1.62754869\n",
      "Validated batch 64 batch loss 1.59751666\n",
      "Validated batch 65 batch loss 1.53394222\n",
      "Validated batch 66 batch loss 1.6307739\n",
      "Validated batch 67 batch loss 1.52116966\n",
      "Validated batch 68 batch loss 1.58329213\n",
      "Validated batch 69 batch loss 1.61921942\n",
      "Validated batch 70 batch loss 1.71054804\n",
      "Validated batch 71 batch loss 1.58369541\n",
      "Validated batch 72 batch loss 1.61187673\n",
      "Validated batch 73 batch loss 1.57102203\n",
      "Validated batch 74 batch loss 1.5968473\n",
      "Validated batch 75 batch loss 1.67678022\n",
      "Validated batch 76 batch loss 1.55979228\n",
      "Validated batch 77 batch loss 1.51981342\n",
      "Validated batch 78 batch loss 1.53076625\n",
      "Validated batch 79 batch loss 1.5651561\n",
      "Validated batch 80 batch loss 1.63133764\n",
      "Validated batch 81 batch loss 1.57529163\n",
      "Validated batch 82 batch loss 1.53003573\n",
      "Validated batch 83 batch loss 1.50759709\n",
      "Validated batch 84 batch loss 1.57932436\n",
      "Validated batch 85 batch loss 1.65960431\n",
      "Validated batch 86 batch loss 1.64949691\n",
      "Validated batch 87 batch loss 1.61357212\n",
      "Validated batch 88 batch loss 1.71180797\n",
      "Validated batch 89 batch loss 1.75767267\n",
      "Validated batch 90 batch loss 1.69300425\n",
      "Validated batch 91 batch loss 1.57849\n",
      "Validated batch 92 batch loss 1.49528861\n",
      "Validated batch 93 batch loss 1.51101089\n",
      "Validated batch 94 batch loss 1.56268764\n",
      "Validated batch 95 batch loss 1.52800632\n",
      "Validated batch 96 batch loss 1.39569449\n",
      "Validated batch 97 batch loss 1.49941194\n",
      "Validated batch 98 batch loss 1.66281164\n",
      "Validated batch 99 batch loss 1.50138545\n",
      "Validated batch 100 batch loss 1.58745563\n",
      "Validated batch 101 batch loss 1.61053658\n",
      "Validated batch 102 batch loss 1.62134719\n",
      "Validated batch 103 batch loss 1.62252378\n",
      "Validated batch 104 batch loss 1.53748345\n",
      "Validated batch 105 batch loss 1.60304451\n",
      "Validated batch 106 batch loss 1.58140671\n",
      "Validated batch 107 batch loss 1.59265018\n",
      "Validated batch 108 batch loss 1.71464539\n",
      "Validated batch 109 batch loss 1.53337586\n",
      "Validated batch 110 batch loss 1.65809727\n",
      "Validated batch 111 batch loss 1.60513747\n",
      "Validated batch 112 batch loss 1.57815909\n",
      "Validated batch 113 batch loss 1.62694192\n",
      "Validated batch 114 batch loss 1.61023688\n",
      "Validated batch 115 batch loss 1.66410494\n",
      "Validated batch 116 batch loss 1.66332424\n",
      "Validated batch 117 batch loss 1.60871124\n",
      "Validated batch 118 batch loss 1.5605278\n",
      "Validated batch 119 batch loss 1.52437758\n",
      "Validated batch 120 batch loss 1.54724145\n",
      "Validated batch 121 batch loss 1.61496043\n",
      "Validated batch 122 batch loss 1.51917207\n",
      "Validated batch 123 batch loss 1.62130451\n",
      "Validated batch 124 batch loss 1.62474525\n",
      "Validated batch 125 batch loss 1.59729576\n",
      "Validated batch 126 batch loss 1.55395508\n",
      "Validated batch 127 batch loss 1.50451207\n",
      "Validated batch 128 batch loss 1.55226088\n",
      "Validated batch 129 batch loss 1.61733389\n",
      "Validated batch 130 batch loss 1.61960626\n",
      "Validated batch 131 batch loss 1.58377433\n",
      "Validated batch 132 batch loss 1.56336105\n",
      "Validated batch 133 batch loss 1.49448144\n",
      "Validated batch 134 batch loss 1.48840308\n",
      "Validated batch 135 batch loss 1.60405731\n",
      "Validated batch 136 batch loss 1.52358115\n",
      "Validated batch 137 batch loss 1.52039766\n",
      "Validated batch 138 batch loss 1.56909704\n",
      "Validated batch 139 batch loss 1.67897689\n",
      "Validated batch 140 batch loss 1.68525279\n",
      "Validated batch 141 batch loss 1.55106032\n",
      "Validated batch 142 batch loss 1.46516669\n",
      "Validated batch 143 batch loss 1.58384514\n",
      "Validated batch 144 batch loss 1.51562285\n",
      "Validated batch 145 batch loss 1.47425663\n",
      "Validated batch 146 batch loss 1.55348206\n",
      "Validated batch 147 batch loss 1.47843432\n",
      "Validated batch 148 batch loss 1.65898466\n",
      "Validated batch 149 batch loss 1.52076066\n",
      "Validated batch 150 batch loss 1.48458862\n",
      "Validated batch 151 batch loss 1.47716582\n",
      "Validated batch 152 batch loss 1.67035198\n",
      "Validated batch 153 batch loss 1.54924989\n",
      "Validated batch 154 batch loss 1.65969467\n",
      "Validated batch 155 batch loss 1.53261495\n",
      "Validated batch 156 batch loss 1.63979411\n",
      "Validated batch 157 batch loss 1.55917513\n",
      "Validated batch 158 batch loss 1.583076\n",
      "Validated batch 159 batch loss 1.61820388\n",
      "Validated batch 160 batch loss 1.4036119\n",
      "Validated batch 161 batch loss 1.62902439\n",
      "Validated batch 162 batch loss 1.61061263\n",
      "Validated batch 163 batch loss 1.48253989\n",
      "Validated batch 164 batch loss 1.62379849\n",
      "Validated batch 165 batch loss 1.55509615\n",
      "Validated batch 166 batch loss 1.48535919\n",
      "Validated batch 167 batch loss 1.63870168\n",
      "Validated batch 168 batch loss 1.62727845\n",
      "Validated batch 169 batch loss 1.59238017\n",
      "Validated batch 170 batch loss 1.62999868\n",
      "Validated batch 171 batch loss 1.63087177\n",
      "Validated batch 172 batch loss 1.59913123\n",
      "Validated batch 173 batch loss 1.57635939\n",
      "Validated batch 174 batch loss 1.58836138\n",
      "Validated batch 175 batch loss 1.62118721\n",
      "Validated batch 176 batch loss 1.64190555\n",
      "Validated batch 177 batch loss 1.63000762\n",
      "Validated batch 178 batch loss 1.56964421\n",
      "Validated batch 179 batch loss 1.48980665\n",
      "Validated batch 180 batch loss 1.5614804\n",
      "Validated batch 181 batch loss 1.64135695\n",
      "Validated batch 182 batch loss 1.64271164\n",
      "Validated batch 183 batch loss 1.6178894\n",
      "Validated batch 184 batch loss 1.60319066\n",
      "Validated batch 185 batch loss 1.59622049\n",
      "Epoch 4 val loss 1.5867974758148193\n",
      "Model ./model_hourglass-epoch-4-loss-1.5868.h5 saved.\n",
      "Start epoch 5 with learning rate 0.5\n",
      "Start distributed traininng...\n",
      "Trained batch 1 batch loss 1.64513779 epoch total loss 1.64513779\n",
      "Trained batch 2 batch loss 1.61235619 epoch total loss 1.62874699\n",
      "Trained batch 3 batch loss 1.62752354 epoch total loss 1.62833917\n",
      "Trained batch 4 batch loss 1.60345197 epoch total loss 1.62211728\n",
      "Trained batch 5 batch loss 1.61474895 epoch total loss 1.62064362\n",
      "Trained batch 6 batch loss 1.58214819 epoch total loss 1.61422777\n",
      "Trained batch 7 batch loss 1.54682493 epoch total loss 1.60459876\n",
      "Trained batch 8 batch loss 1.58519518 epoch total loss 1.60217333\n",
      "Trained batch 9 batch loss 1.67480206 epoch total loss 1.6102432\n",
      "Trained batch 10 batch loss 1.61729944 epoch total loss 1.6109488\n",
      "Trained batch 11 batch loss 1.54847836 epoch total loss 1.60526967\n",
      "Trained batch 12 batch loss 1.49651015 epoch total loss 1.59620631\n",
      "Trained batch 13 batch loss 1.47527373 epoch total loss 1.58690381\n",
      "Trained batch 14 batch loss 1.65329671 epoch total loss 1.59164619\n",
      "Trained batch 15 batch loss 1.54251575 epoch total loss 1.5883708\n",
      "Trained batch 16 batch loss 1.57753444 epoch total loss 1.58769345\n",
      "Trained batch 17 batch loss 1.515378 epoch total loss 1.58343959\n",
      "Trained batch 18 batch loss 1.47499251 epoch total loss 1.57741475\n",
      "Trained batch 19 batch loss 1.59424555 epoch total loss 1.5783006\n",
      "Trained batch 20 batch loss 1.55299783 epoch total loss 1.57703543\n",
      "Trained batch 21 batch loss 1.58583224 epoch total loss 1.57745433\n",
      "Trained batch 22 batch loss 1.40657783 epoch total loss 1.56968725\n",
      "Trained batch 23 batch loss 1.62052536 epoch total loss 1.57189763\n",
      "Trained batch 24 batch loss 1.60943437 epoch total loss 1.57346165\n",
      "Trained batch 25 batch loss 1.63578451 epoch total loss 1.57595456\n",
      "Trained batch 26 batch loss 1.80760205 epoch total loss 1.58486414\n",
      "Trained batch 27 batch loss 1.71587539 epoch total loss 1.58971632\n",
      "Trained batch 28 batch loss 1.70286441 epoch total loss 1.59375739\n",
      "Trained batch 29 batch loss 1.66546619 epoch total loss 1.59623\n",
      "Trained batch 30 batch loss 1.61805332 epoch total loss 1.59695756\n",
      "Trained batch 31 batch loss 1.47922647 epoch total loss 1.59315968\n",
      "Trained batch 32 batch loss 1.52191222 epoch total loss 1.5909332\n",
      "Trained batch 33 batch loss 1.63290977 epoch total loss 1.59220517\n",
      "Trained batch 34 batch loss 1.55402279 epoch total loss 1.59108222\n",
      "Trained batch 35 batch loss 1.64954853 epoch total loss 1.59275258\n",
      "Trained batch 36 batch loss 1.45822036 epoch total loss 1.5890156\n",
      "Trained batch 37 batch loss 1.52839375 epoch total loss 1.58737719\n",
      "Trained batch 38 batch loss 1.6050024 epoch total loss 1.58784103\n",
      "Trained batch 39 batch loss 1.54052746 epoch total loss 1.58662784\n",
      "Trained batch 40 batch loss 1.65083206 epoch total loss 1.58823299\n",
      "Trained batch 41 batch loss 1.57456851 epoch total loss 1.58789968\n",
      "Trained batch 42 batch loss 1.53825808 epoch total loss 1.58671784\n",
      "Trained batch 43 batch loss 1.62482774 epoch total loss 1.58760405\n",
      "Trained batch 44 batch loss 1.61493456 epoch total loss 1.58822525\n",
      "Trained batch 45 batch loss 1.63583064 epoch total loss 1.58928323\n",
      "Trained batch 46 batch loss 1.59286964 epoch total loss 1.58936131\n",
      "Trained batch 47 batch loss 1.62971902 epoch total loss 1.59022\n",
      "Trained batch 48 batch loss 1.63509226 epoch total loss 1.59115493\n",
      "Trained batch 49 batch loss 1.67187643 epoch total loss 1.59280229\n",
      "Trained batch 50 batch loss 1.55904162 epoch total loss 1.59212708\n",
      "Trained batch 51 batch loss 1.64327013 epoch total loss 1.59312987\n",
      "Trained batch 52 batch loss 1.56780601 epoch total loss 1.5926429\n",
      "Trained batch 53 batch loss 1.62518394 epoch total loss 1.59325683\n",
      "Trained batch 54 batch loss 1.63465953 epoch total loss 1.59402359\n",
      "Trained batch 55 batch loss 1.6822387 epoch total loss 1.59562743\n",
      "Trained batch 56 batch loss 1.68526149 epoch total loss 1.59722805\n",
      "Trained batch 57 batch loss 1.66834819 epoch total loss 1.59847581\n",
      "Trained batch 58 batch loss 1.6736449 epoch total loss 1.59977186\n",
      "Trained batch 59 batch loss 1.67722261 epoch total loss 1.60108459\n",
      "Trained batch 60 batch loss 1.60561025 epoch total loss 1.60116\n",
      "Trained batch 61 batch loss 1.60405457 epoch total loss 1.60120749\n",
      "Trained batch 62 batch loss 1.64053214 epoch total loss 1.60184181\n",
      "Trained batch 63 batch loss 1.61401355 epoch total loss 1.60203505\n",
      "Trained batch 64 batch loss 1.56140375 epoch total loss 1.60140014\n",
      "Trained batch 65 batch loss 1.65436792 epoch total loss 1.60221493\n",
      "Trained batch 66 batch loss 1.70390296 epoch total loss 1.60375571\n",
      "Trained batch 67 batch loss 1.718261 epoch total loss 1.60546482\n",
      "Trained batch 68 batch loss 1.5641638 epoch total loss 1.60485744\n",
      "Trained batch 69 batch loss 1.54269743 epoch total loss 1.60395646\n",
      "Trained batch 70 batch loss 1.60495937 epoch total loss 1.60397077\n",
      "Trained batch 71 batch loss 1.59903359 epoch total loss 1.60390127\n",
      "Trained batch 72 batch loss 1.57920659 epoch total loss 1.6035583\n",
      "Trained batch 73 batch loss 1.56598842 epoch total loss 1.60304368\n",
      "Trained batch 74 batch loss 1.66029811 epoch total loss 1.60381734\n",
      "Trained batch 75 batch loss 1.50523245 epoch total loss 1.60250294\n",
      "Trained batch 76 batch loss 1.59694517 epoch total loss 1.60242987\n",
      "Trained batch 77 batch loss 1.64513612 epoch total loss 1.60298443\n",
      "Trained batch 78 batch loss 1.70053971 epoch total loss 1.60423517\n",
      "Trained batch 79 batch loss 1.74714708 epoch total loss 1.60604417\n",
      "Trained batch 80 batch loss 1.64641464 epoch total loss 1.60654867\n",
      "Trained batch 81 batch loss 1.69946527 epoch total loss 1.60769582\n",
      "Trained batch 82 batch loss 1.46756756 epoch total loss 1.60598695\n",
      "Trained batch 83 batch loss 1.45358372 epoch total loss 1.60415077\n",
      "Trained batch 84 batch loss 1.68907642 epoch total loss 1.60516179\n",
      "Trained batch 85 batch loss 1.67025387 epoch total loss 1.60592759\n",
      "Trained batch 86 batch loss 1.59053957 epoch total loss 1.60574877\n",
      "Trained batch 87 batch loss 1.6608994 epoch total loss 1.60638273\n",
      "Trained batch 88 batch loss 1.56932044 epoch total loss 1.60596156\n",
      "Trained batch 89 batch loss 1.56347895 epoch total loss 1.60548413\n",
      "Trained batch 90 batch loss 1.52063394 epoch total loss 1.6045413\n",
      "Trained batch 91 batch loss 1.63076377 epoch total loss 1.60482955\n",
      "Trained batch 92 batch loss 1.59172261 epoch total loss 1.60468709\n",
      "Trained batch 93 batch loss 1.61840951 epoch total loss 1.60483456\n",
      "Trained batch 94 batch loss 1.54089844 epoch total loss 1.60415435\n",
      "Trained batch 95 batch loss 1.52817178 epoch total loss 1.60335457\n",
      "Trained batch 96 batch loss 1.62839723 epoch total loss 1.6036154\n",
      "Trained batch 97 batch loss 1.49308634 epoch total loss 1.602476\n",
      "Trained batch 98 batch loss 1.63533747 epoch total loss 1.60281122\n",
      "Trained batch 99 batch loss 1.65666354 epoch total loss 1.60335517\n",
      "Trained batch 100 batch loss 1.54646325 epoch total loss 1.6027863\n",
      "Trained batch 101 batch loss 1.41372442 epoch total loss 1.60091436\n",
      "Trained batch 102 batch loss 1.57171524 epoch total loss 1.60062814\n",
      "Trained batch 103 batch loss 1.64125383 epoch total loss 1.60102248\n",
      "Trained batch 104 batch loss 1.65129745 epoch total loss 1.60150588\n",
      "Trained batch 105 batch loss 1.66635 epoch total loss 1.6021235\n",
      "Trained batch 106 batch loss 1.62580752 epoch total loss 1.6023469\n",
      "Trained batch 107 batch loss 1.54453027 epoch total loss 1.60180652\n",
      "Trained batch 108 batch loss 1.45478511 epoch total loss 1.60044527\n",
      "Trained batch 109 batch loss 1.68045115 epoch total loss 1.60117924\n",
      "Trained batch 110 batch loss 1.55971253 epoch total loss 1.60080218\n",
      "Trained batch 111 batch loss 1.61896968 epoch total loss 1.60096586\n",
      "Trained batch 112 batch loss 1.5760299 epoch total loss 1.60074329\n",
      "Trained batch 113 batch loss 1.64393747 epoch total loss 1.60112548\n",
      "Trained batch 114 batch loss 1.68248367 epoch total loss 1.60183918\n",
      "Trained batch 115 batch loss 1.62727344 epoch total loss 1.60206032\n",
      "Trained batch 116 batch loss 1.60409832 epoch total loss 1.60207784\n",
      "Trained batch 117 batch loss 1.55240726 epoch total loss 1.60165346\n",
      "Trained batch 118 batch loss 1.4145205 epoch total loss 1.6000675\n",
      "Trained batch 119 batch loss 1.45328522 epoch total loss 1.59883416\n",
      "Trained batch 120 batch loss 1.47383642 epoch total loss 1.59779239\n",
      "Trained batch 121 batch loss 1.39578116 epoch total loss 1.59612298\n",
      "Trained batch 122 batch loss 1.29613125 epoch total loss 1.59366393\n",
      "Trained batch 123 batch loss 1.34157646 epoch total loss 1.59161448\n",
      "Trained batch 124 batch loss 1.26314616 epoch total loss 1.58896565\n",
      "Trained batch 125 batch loss 1.49428 epoch total loss 1.58820808\n",
      "Trained batch 126 batch loss 1.66486156 epoch total loss 1.5888164\n",
      "Trained batch 127 batch loss 1.66500044 epoch total loss 1.58941627\n",
      "Trained batch 128 batch loss 1.67618632 epoch total loss 1.59009421\n",
      "Trained batch 129 batch loss 1.51597309 epoch total loss 1.58951962\n",
      "Trained batch 130 batch loss 1.64229178 epoch total loss 1.58992553\n",
      "Trained batch 131 batch loss 1.5731647 epoch total loss 1.58979762\n",
      "Trained batch 132 batch loss 1.61501789 epoch total loss 1.58998871\n",
      "Trained batch 133 batch loss 1.59290385 epoch total loss 1.59001064\n",
      "Trained batch 134 batch loss 1.54295182 epoch total loss 1.58965945\n",
      "Trained batch 135 batch loss 1.48741245 epoch total loss 1.58890212\n",
      "Trained batch 136 batch loss 1.53671372 epoch total loss 1.58851838\n",
      "Trained batch 137 batch loss 1.62787533 epoch total loss 1.58880556\n",
      "Trained batch 138 batch loss 1.64093745 epoch total loss 1.58918333\n",
      "Trained batch 139 batch loss 1.66788542 epoch total loss 1.58974957\n",
      "Trained batch 140 batch loss 1.60219932 epoch total loss 1.5898385\n",
      "Trained batch 141 batch loss 1.59789908 epoch total loss 1.58989573\n",
      "Trained batch 142 batch loss 1.63218439 epoch total loss 1.59019351\n",
      "Trained batch 143 batch loss 1.57199407 epoch total loss 1.59006619\n",
      "Trained batch 144 batch loss 1.53485322 epoch total loss 1.58968282\n",
      "Trained batch 145 batch loss 1.57617474 epoch total loss 1.5895896\n",
      "Trained batch 146 batch loss 1.63383198 epoch total loss 1.58989263\n",
      "Trained batch 147 batch loss 1.73622942 epoch total loss 1.59088814\n",
      "Trained batch 148 batch loss 1.72917509 epoch total loss 1.5918225\n",
      "Trained batch 149 batch loss 1.7364819 epoch total loss 1.59279346\n",
      "Trained batch 150 batch loss 1.67603278 epoch total loss 1.59334826\n",
      "Trained batch 151 batch loss 1.64781392 epoch total loss 1.59370899\n",
      "Trained batch 152 batch loss 1.5685463 epoch total loss 1.59354341\n",
      "Trained batch 153 batch loss 1.69263589 epoch total loss 1.59419107\n",
      "Trained batch 154 batch loss 1.71799636 epoch total loss 1.59499502\n",
      "Trained batch 155 batch loss 1.72007072 epoch total loss 1.59580207\n",
      "Trained batch 156 batch loss 1.55970931 epoch total loss 1.59557068\n",
      "Trained batch 157 batch loss 1.53202319 epoch total loss 1.59516597\n",
      "Trained batch 158 batch loss 1.60966587 epoch total loss 1.59525776\n",
      "Trained batch 159 batch loss 1.62067235 epoch total loss 1.5954175\n",
      "Trained batch 160 batch loss 1.55511808 epoch total loss 1.59516561\n",
      "Trained batch 161 batch loss 1.6010344 epoch total loss 1.59520209\n",
      "Trained batch 162 batch loss 1.69200301 epoch total loss 1.5957998\n",
      "Trained batch 163 batch loss 1.49750245 epoch total loss 1.59519672\n",
      "Trained batch 164 batch loss 1.62111151 epoch total loss 1.5953548\n",
      "Trained batch 165 batch loss 1.57079709 epoch total loss 1.59520602\n",
      "Trained batch 166 batch loss 1.61730766 epoch total loss 1.59533918\n",
      "Trained batch 167 batch loss 1.60129666 epoch total loss 1.5953747\n",
      "Trained batch 168 batch loss 1.62725163 epoch total loss 1.59556448\n",
      "Trained batch 169 batch loss 1.60759437 epoch total loss 1.59563577\n",
      "Trained batch 170 batch loss 1.55454385 epoch total loss 1.59539402\n",
      "Trained batch 171 batch loss 1.5296979 epoch total loss 1.5950098\n",
      "Trained batch 172 batch loss 1.56093788 epoch total loss 1.59481168\n",
      "Trained batch 173 batch loss 1.7198211 epoch total loss 1.59553432\n",
      "Trained batch 174 batch loss 1.629848 epoch total loss 1.5957315\n",
      "Trained batch 175 batch loss 1.58938694 epoch total loss 1.59569526\n",
      "Trained batch 176 batch loss 1.60847592 epoch total loss 1.59576797\n",
      "Trained batch 177 batch loss 1.55079794 epoch total loss 1.59551394\n",
      "Trained batch 178 batch loss 1.59917712 epoch total loss 1.59553456\n",
      "Trained batch 179 batch loss 1.682953 epoch total loss 1.59602296\n",
      "Trained batch 180 batch loss 1.56239343 epoch total loss 1.59583616\n",
      "Trained batch 181 batch loss 1.60465264 epoch total loss 1.59588492\n",
      "Trained batch 182 batch loss 1.48382711 epoch total loss 1.5952692\n",
      "Trained batch 183 batch loss 1.46157 epoch total loss 1.59453857\n",
      "Trained batch 184 batch loss 1.51199329 epoch total loss 1.59409\n",
      "Trained batch 185 batch loss 1.61905777 epoch total loss 1.59422493\n",
      "Trained batch 186 batch loss 1.63313842 epoch total loss 1.59443414\n",
      "Trained batch 187 batch loss 1.57043493 epoch total loss 1.59430587\n",
      "Trained batch 188 batch loss 1.61400342 epoch total loss 1.59441066\n",
      "Trained batch 189 batch loss 1.58616567 epoch total loss 1.59436703\n",
      "Trained batch 190 batch loss 1.58765817 epoch total loss 1.59433162\n",
      "Trained batch 191 batch loss 1.5599246 epoch total loss 1.5941515\n",
      "Trained batch 192 batch loss 1.61497021 epoch total loss 1.59425986\n",
      "Trained batch 193 batch loss 1.50502682 epoch total loss 1.59379756\n",
      "Trained batch 194 batch loss 1.56983042 epoch total loss 1.59367406\n",
      "Trained batch 195 batch loss 1.51910186 epoch total loss 1.59329164\n",
      "Trained batch 196 batch loss 1.50898147 epoch total loss 1.59286141\n",
      "Trained batch 197 batch loss 1.56902957 epoch total loss 1.59274042\n",
      "Trained batch 198 batch loss 1.82309341 epoch total loss 1.59390378\n",
      "Trained batch 199 batch loss 1.68524873 epoch total loss 1.59436285\n",
      "Trained batch 200 batch loss 1.73891735 epoch total loss 1.59508562\n",
      "Trained batch 201 batch loss 1.65548611 epoch total loss 1.59538615\n",
      "Trained batch 202 batch loss 1.70122647 epoch total loss 1.59591007\n",
      "Trained batch 203 batch loss 1.4901346 epoch total loss 1.59538913\n",
      "Trained batch 204 batch loss 1.51750755 epoch total loss 1.5950073\n",
      "Trained batch 205 batch loss 1.66741765 epoch total loss 1.59536064\n",
      "Trained batch 206 batch loss 1.67056823 epoch total loss 1.59572566\n",
      "Trained batch 207 batch loss 1.59147644 epoch total loss 1.59570515\n",
      "Trained batch 208 batch loss 1.65912282 epoch total loss 1.59601009\n",
      "Trained batch 209 batch loss 1.55891788 epoch total loss 1.59583259\n",
      "Trained batch 210 batch loss 1.62583721 epoch total loss 1.5959754\n",
      "Trained batch 211 batch loss 1.6114161 epoch total loss 1.59604871\n",
      "Trained batch 212 batch loss 1.56310487 epoch total loss 1.59589326\n",
      "Trained batch 213 batch loss 1.72070134 epoch total loss 1.5964793\n",
      "Trained batch 214 batch loss 1.62582684 epoch total loss 1.59661639\n",
      "Trained batch 215 batch loss 1.66981161 epoch total loss 1.59695673\n",
      "Trained batch 216 batch loss 1.70378602 epoch total loss 1.59745133\n",
      "Trained batch 217 batch loss 1.59511042 epoch total loss 1.5974406\n",
      "Trained batch 218 batch loss 1.6288085 epoch total loss 1.59758461\n",
      "Trained batch 219 batch loss 1.39676976 epoch total loss 1.59666753\n",
      "Trained batch 220 batch loss 1.35958779 epoch total loss 1.59558988\n",
      "Trained batch 221 batch loss 1.35116231 epoch total loss 1.59448397\n",
      "Trained batch 222 batch loss 1.37529373 epoch total loss 1.59349668\n",
      "Trained batch 223 batch loss 1.54259503 epoch total loss 1.59326839\n",
      "Trained batch 224 batch loss 1.70508373 epoch total loss 1.59376752\n",
      "Trained batch 225 batch loss 1.63462043 epoch total loss 1.59394908\n",
      "Trained batch 226 batch loss 1.69099545 epoch total loss 1.59437859\n",
      "Trained batch 227 batch loss 1.63634396 epoch total loss 1.59456348\n",
      "Trained batch 228 batch loss 1.65868008 epoch total loss 1.5948447\n",
      "Trained batch 229 batch loss 1.71618652 epoch total loss 1.59537458\n",
      "Trained batch 230 batch loss 1.68052602 epoch total loss 1.59574473\n",
      "Trained batch 231 batch loss 1.66905427 epoch total loss 1.59606218\n",
      "Trained batch 232 batch loss 1.66072893 epoch total loss 1.59634101\n",
      "Trained batch 233 batch loss 1.76125455 epoch total loss 1.59704876\n",
      "Trained batch 234 batch loss 1.66261542 epoch total loss 1.59732902\n",
      "Trained batch 235 batch loss 1.63947845 epoch total loss 1.59750831\n",
      "Trained batch 236 batch loss 1.59619188 epoch total loss 1.59750271\n",
      "Trained batch 237 batch loss 1.49393761 epoch total loss 1.59706569\n",
      "Trained batch 238 batch loss 1.43131793 epoch total loss 1.59636927\n",
      "Trained batch 239 batch loss 1.47458732 epoch total loss 1.59585965\n",
      "Trained batch 240 batch loss 1.55307496 epoch total loss 1.59568143\n",
      "Trained batch 241 batch loss 1.60311031 epoch total loss 1.59571218\n",
      "Trained batch 242 batch loss 1.6403 epoch total loss 1.59589648\n",
      "Trained batch 243 batch loss 1.6488893 epoch total loss 1.59611452\n",
      "Trained batch 244 batch loss 1.67673993 epoch total loss 1.59644496\n",
      "Trained batch 245 batch loss 1.62549424 epoch total loss 1.59656346\n",
      "Trained batch 246 batch loss 1.56003916 epoch total loss 1.59641492\n",
      "Trained batch 247 batch loss 1.57292569 epoch total loss 1.59631991\n",
      "Trained batch 248 batch loss 1.56935513 epoch total loss 1.59621119\n",
      "Trained batch 249 batch loss 1.59636676 epoch total loss 1.59621191\n",
      "Trained batch 250 batch loss 1.63795495 epoch total loss 1.59637892\n",
      "Trained batch 251 batch loss 1.66907251 epoch total loss 1.59666848\n",
      "Trained batch 252 batch loss 1.77082801 epoch total loss 1.59735954\n",
      "Trained batch 253 batch loss 1.68007946 epoch total loss 1.59768653\n",
      "Trained batch 254 batch loss 1.65389323 epoch total loss 1.59790778\n",
      "Trained batch 255 batch loss 1.71198249 epoch total loss 1.59835517\n",
      "Trained batch 256 batch loss 1.67143059 epoch total loss 1.59864056\n",
      "Trained batch 257 batch loss 1.74213672 epoch total loss 1.59919882\n",
      "Trained batch 258 batch loss 1.68676543 epoch total loss 1.59953833\n",
      "Trained batch 259 batch loss 1.63575232 epoch total loss 1.59967804\n",
      "Trained batch 260 batch loss 1.56819212 epoch total loss 1.59955704\n",
      "Trained batch 261 batch loss 1.59438944 epoch total loss 1.59953725\n",
      "Trained batch 262 batch loss 1.6426996 epoch total loss 1.599702\n",
      "Trained batch 263 batch loss 1.68976498 epoch total loss 1.60004437\n",
      "Trained batch 264 batch loss 1.62388062 epoch total loss 1.60013461\n",
      "Trained batch 265 batch loss 1.76970696 epoch total loss 1.60077453\n",
      "Trained batch 266 batch loss 1.6932404 epoch total loss 1.60112214\n",
      "Trained batch 267 batch loss 1.59935582 epoch total loss 1.60111558\n",
      "Trained batch 268 batch loss 1.48759627 epoch total loss 1.60069203\n",
      "Trained batch 269 batch loss 1.45676851 epoch total loss 1.60015702\n",
      "Trained batch 270 batch loss 1.38250792 epoch total loss 1.59935093\n",
      "Trained batch 271 batch loss 1.41856515 epoch total loss 1.59868383\n",
      "Trained batch 272 batch loss 1.51783311 epoch total loss 1.59838653\n",
      "Trained batch 273 batch loss 1.31341493 epoch total loss 1.59734273\n",
      "Trained batch 274 batch loss 1.31845582 epoch total loss 1.5963248\n",
      "Trained batch 275 batch loss 1.33428335 epoch total loss 1.59537196\n",
      "Trained batch 276 batch loss 1.39214182 epoch total loss 1.59463573\n",
      "Trained batch 277 batch loss 1.5277586 epoch total loss 1.59439433\n",
      "Trained batch 278 batch loss 1.62442255 epoch total loss 1.59450233\n",
      "Trained batch 279 batch loss 1.6478734 epoch total loss 1.59469366\n",
      "Trained batch 280 batch loss 1.68272829 epoch total loss 1.59500813\n",
      "Trained batch 281 batch loss 1.67108071 epoch total loss 1.59527886\n",
      "Trained batch 282 batch loss 1.80480993 epoch total loss 1.59602177\n",
      "Trained batch 283 batch loss 1.68741775 epoch total loss 1.59634471\n",
      "Trained batch 284 batch loss 1.68134761 epoch total loss 1.59664404\n",
      "Trained batch 285 batch loss 1.53755653 epoch total loss 1.59643674\n",
      "Trained batch 286 batch loss 1.69366097 epoch total loss 1.59677672\n",
      "Trained batch 287 batch loss 1.66625774 epoch total loss 1.59701884\n",
      "Trained batch 288 batch loss 1.59997523 epoch total loss 1.59702909\n",
      "Trained batch 289 batch loss 1.62119853 epoch total loss 1.59711266\n",
      "Trained batch 290 batch loss 1.52964795 epoch total loss 1.59688008\n",
      "Trained batch 291 batch loss 1.59452224 epoch total loss 1.59687197\n",
      "Trained batch 292 batch loss 1.61460388 epoch total loss 1.59693265\n",
      "Trained batch 293 batch loss 1.61594534 epoch total loss 1.5969975\n",
      "Trained batch 294 batch loss 1.66215396 epoch total loss 1.59721899\n",
      "Trained batch 295 batch loss 1.65078342 epoch total loss 1.59740067\n",
      "Trained batch 296 batch loss 1.62680411 epoch total loss 1.5975\n",
      "Trained batch 297 batch loss 1.65157557 epoch total loss 1.597682\n",
      "Trained batch 298 batch loss 1.62953746 epoch total loss 1.59778893\n",
      "Trained batch 299 batch loss 1.60400808 epoch total loss 1.59780979\n",
      "Trained batch 300 batch loss 1.58020902 epoch total loss 1.59775102\n",
      "Trained batch 301 batch loss 1.51598799 epoch total loss 1.59747946\n",
      "Trained batch 302 batch loss 1.52245927 epoch total loss 1.59723103\n",
      "Trained batch 303 batch loss 1.54478061 epoch total loss 1.59705794\n",
      "Trained batch 304 batch loss 1.59553266 epoch total loss 1.59705281\n",
      "Trained batch 305 batch loss 1.7451731 epoch total loss 1.59753847\n",
      "Trained batch 306 batch loss 1.66685486 epoch total loss 1.59776497\n",
      "Trained batch 307 batch loss 1.50484407 epoch total loss 1.5974623\n",
      "Trained batch 308 batch loss 1.52048135 epoch total loss 1.59721231\n",
      "Trained batch 309 batch loss 1.51930237 epoch total loss 1.59696019\n",
      "Trained batch 310 batch loss 1.52818739 epoch total loss 1.59673834\n",
      "Trained batch 311 batch loss 1.55375731 epoch total loss 1.59660017\n",
      "Trained batch 312 batch loss 1.52190661 epoch total loss 1.5963608\n",
      "Trained batch 313 batch loss 1.54594564 epoch total loss 1.59619975\n",
      "Trained batch 314 batch loss 1.6582396 epoch total loss 1.5963974\n",
      "Trained batch 315 batch loss 1.63856411 epoch total loss 1.59653115\n",
      "Trained batch 316 batch loss 1.61684322 epoch total loss 1.59659553\n",
      "Trained batch 317 batch loss 1.62270236 epoch total loss 1.5966779\n",
      "Trained batch 318 batch loss 1.60815525 epoch total loss 1.5967139\n",
      "Trained batch 319 batch loss 1.54833221 epoch total loss 1.59656227\n",
      "Trained batch 320 batch loss 1.63272429 epoch total loss 1.59667528\n",
      "Trained batch 321 batch loss 1.56934381 epoch total loss 1.59659016\n",
      "Trained batch 322 batch loss 1.51586449 epoch total loss 1.59633946\n",
      "Trained batch 323 batch loss 1.52235889 epoch total loss 1.59611034\n",
      "Trained batch 324 batch loss 1.51356363 epoch total loss 1.59585547\n",
      "Trained batch 325 batch loss 1.44207311 epoch total loss 1.59538233\n",
      "Trained batch 326 batch loss 1.54204607 epoch total loss 1.59521878\n",
      "Trained batch 327 batch loss 1.59840465 epoch total loss 1.59522843\n",
      "Trained batch 328 batch loss 1.5475986 epoch total loss 1.59508324\n",
      "Trained batch 329 batch loss 1.52206945 epoch total loss 1.59486139\n",
      "Trained batch 330 batch loss 1.59485793 epoch total loss 1.59486139\n",
      "Trained batch 331 batch loss 1.59955764 epoch total loss 1.59487557\n",
      "Trained batch 332 batch loss 1.49013782 epoch total loss 1.59456\n",
      "Trained batch 333 batch loss 1.51529157 epoch total loss 1.59432209\n",
      "Trained batch 334 batch loss 1.4796716 epoch total loss 1.59397876\n",
      "Trained batch 335 batch loss 1.49722254 epoch total loss 1.59368992\n",
      "Trained batch 336 batch loss 1.52021849 epoch total loss 1.59347117\n",
      "Trained batch 337 batch loss 1.46400964 epoch total loss 1.59308696\n",
      "Trained batch 338 batch loss 1.48583901 epoch total loss 1.59276962\n",
      "Trained batch 339 batch loss 1.4412365 epoch total loss 1.59232259\n",
      "Trained batch 340 batch loss 1.48823929 epoch total loss 1.59201646\n",
      "Trained batch 341 batch loss 1.51167452 epoch total loss 1.59178078\n",
      "Trained batch 342 batch loss 1.42166495 epoch total loss 1.59128344\n",
      "Trained batch 343 batch loss 1.49010515 epoch total loss 1.59098852\n",
      "Trained batch 344 batch loss 1.6754607 epoch total loss 1.59123409\n",
      "Trained batch 345 batch loss 1.60520411 epoch total loss 1.59127462\n",
      "Trained batch 346 batch loss 1.63260603 epoch total loss 1.59139419\n",
      "Trained batch 347 batch loss 1.54976225 epoch total loss 1.59127414\n",
      "Trained batch 348 batch loss 1.5526247 epoch total loss 1.59116304\n",
      "Trained batch 349 batch loss 1.5809927 epoch total loss 1.59113383\n",
      "Trained batch 350 batch loss 1.55867839 epoch total loss 1.59104109\n",
      "Trained batch 351 batch loss 1.57744074 epoch total loss 1.59100235\n",
      "Trained batch 352 batch loss 1.61373854 epoch total loss 1.59106684\n",
      "Trained batch 353 batch loss 1.58682752 epoch total loss 1.59105492\n",
      "Trained batch 354 batch loss 1.51027846 epoch total loss 1.59082675\n",
      "Trained batch 355 batch loss 1.6031847 epoch total loss 1.59086156\n",
      "Trained batch 356 batch loss 1.46765506 epoch total loss 1.59051549\n",
      "Trained batch 357 batch loss 1.49551201 epoch total loss 1.5902493\n",
      "Trained batch 358 batch loss 1.63006616 epoch total loss 1.59036052\n",
      "Trained batch 359 batch loss 1.53850508 epoch total loss 1.59021604\n",
      "Trained batch 360 batch loss 1.50385416 epoch total loss 1.58997619\n",
      "Trained batch 361 batch loss 1.60808229 epoch total loss 1.59002638\n",
      "Trained batch 362 batch loss 1.59103489 epoch total loss 1.59002924\n",
      "Trained batch 363 batch loss 1.63683355 epoch total loss 1.59015822\n",
      "Trained batch 364 batch loss 1.46315718 epoch total loss 1.58980918\n",
      "Trained batch 365 batch loss 1.45791125 epoch total loss 1.58944774\n",
      "Trained batch 366 batch loss 1.62247169 epoch total loss 1.5895381\n",
      "Trained batch 367 batch loss 1.60153449 epoch total loss 1.58957088\n",
      "Trained batch 368 batch loss 1.67364955 epoch total loss 1.58979928\n",
      "Trained batch 369 batch loss 1.59544098 epoch total loss 1.58981466\n",
      "Trained batch 370 batch loss 1.62930369 epoch total loss 1.58992147\n",
      "Trained batch 371 batch loss 1.64082992 epoch total loss 1.59005857\n",
      "Trained batch 372 batch loss 1.7893337 epoch total loss 1.59059417\n",
      "Trained batch 373 batch loss 1.71241355 epoch total loss 1.59092081\n",
      "Trained batch 374 batch loss 1.72996259 epoch total loss 1.59129262\n",
      "Trained batch 375 batch loss 1.57871449 epoch total loss 1.59125912\n",
      "Trained batch 376 batch loss 1.58887935 epoch total loss 1.5912528\n",
      "Trained batch 377 batch loss 1.52362108 epoch total loss 1.59107339\n",
      "Trained batch 378 batch loss 1.62814 epoch total loss 1.59117138\n",
      "Trained batch 379 batch loss 1.68672442 epoch total loss 1.59142339\n",
      "Trained batch 380 batch loss 1.60986352 epoch total loss 1.59147191\n",
      "Trained batch 381 batch loss 1.63073623 epoch total loss 1.59157503\n",
      "Trained batch 382 batch loss 1.54377425 epoch total loss 1.59144986\n",
      "Trained batch 383 batch loss 1.50870681 epoch total loss 1.59123385\n",
      "Trained batch 384 batch loss 1.56555319 epoch total loss 1.59116697\n",
      "Trained batch 385 batch loss 1.69498217 epoch total loss 1.59143674\n",
      "Trained batch 386 batch loss 1.6683985 epoch total loss 1.59163606\n",
      "Trained batch 387 batch loss 1.70343399 epoch total loss 1.59192491\n",
      "Trained batch 388 batch loss 1.61193407 epoch total loss 1.59197652\n",
      "Trained batch 389 batch loss 1.75653338 epoch total loss 1.5923996\n",
      "Trained batch 390 batch loss 1.63893211 epoch total loss 1.59251881\n",
      "Trained batch 391 batch loss 1.53877985 epoch total loss 1.59238136\n",
      "Trained batch 392 batch loss 1.36884916 epoch total loss 1.59181106\n",
      "Trained batch 393 batch loss 1.60921144 epoch total loss 1.59185529\n",
      "Trained batch 394 batch loss 1.64513373 epoch total loss 1.59199047\n",
      "Trained batch 395 batch loss 1.64877546 epoch total loss 1.59213436\n",
      "Trained batch 396 batch loss 1.6733489 epoch total loss 1.5923394\n",
      "Trained batch 397 batch loss 1.60516942 epoch total loss 1.5923717\n",
      "Trained batch 398 batch loss 1.65078378 epoch total loss 1.59251845\n",
      "Trained batch 399 batch loss 1.60639739 epoch total loss 1.59255314\n",
      "Trained batch 400 batch loss 1.69498348 epoch total loss 1.59280932\n",
      "Trained batch 401 batch loss 1.66682839 epoch total loss 1.59299386\n",
      "Trained batch 402 batch loss 1.63804662 epoch total loss 1.59310591\n",
      "Trained batch 403 batch loss 1.55422914 epoch total loss 1.59300935\n",
      "Trained batch 404 batch loss 1.61536074 epoch total loss 1.59306467\n",
      "Trained batch 405 batch loss 1.43789148 epoch total loss 1.59268153\n",
      "Trained batch 406 batch loss 1.43388641 epoch total loss 1.5922904\n",
      "Trained batch 407 batch loss 1.59077978 epoch total loss 1.59228671\n",
      "Trained batch 408 batch loss 1.5919888 epoch total loss 1.59228587\n",
      "Trained batch 409 batch loss 1.57034099 epoch total loss 1.59223223\n",
      "Trained batch 410 batch loss 1.60059845 epoch total loss 1.59225261\n",
      "Trained batch 411 batch loss 1.64011669 epoch total loss 1.59236908\n",
      "Trained batch 412 batch loss 1.50223386 epoch total loss 1.59215033\n",
      "Trained batch 413 batch loss 1.53957772 epoch total loss 1.59202302\n",
      "Trained batch 414 batch loss 1.62306011 epoch total loss 1.59209788\n",
      "Trained batch 415 batch loss 1.66552234 epoch total loss 1.5922749\n",
      "Trained batch 416 batch loss 1.58632803 epoch total loss 1.59226048\n",
      "Trained batch 417 batch loss 1.56813717 epoch total loss 1.59220266\n",
      "Trained batch 418 batch loss 1.56168795 epoch total loss 1.59212971\n",
      "Trained batch 419 batch loss 1.50234354 epoch total loss 1.59191525\n",
      "Trained batch 420 batch loss 1.52273 epoch total loss 1.5917505\n",
      "Trained batch 421 batch loss 1.60172892 epoch total loss 1.59177423\n",
      "Trained batch 422 batch loss 1.53916705 epoch total loss 1.59164965\n",
      "Trained batch 423 batch loss 1.54052532 epoch total loss 1.59152877\n",
      "Trained batch 424 batch loss 1.51239276 epoch total loss 1.59134209\n",
      "Trained batch 425 batch loss 1.53263783 epoch total loss 1.59120405\n",
      "Trained batch 426 batch loss 1.47977901 epoch total loss 1.5909425\n",
      "Trained batch 427 batch loss 1.52569294 epoch total loss 1.59078968\n",
      "Trained batch 428 batch loss 1.55264032 epoch total loss 1.59070051\n",
      "Trained batch 429 batch loss 1.42633343 epoch total loss 1.59031737\n",
      "Trained batch 430 batch loss 1.50993943 epoch total loss 1.59013045\n",
      "Trained batch 431 batch loss 1.52557957 epoch total loss 1.58998072\n",
      "Trained batch 432 batch loss 1.50532341 epoch total loss 1.58978474\n",
      "Trained batch 433 batch loss 1.6170826 epoch total loss 1.58984768\n",
      "Trained batch 434 batch loss 1.59297824 epoch total loss 1.58985484\n",
      "Trained batch 435 batch loss 1.65213752 epoch total loss 1.58999813\n",
      "Trained batch 436 batch loss 1.63532805 epoch total loss 1.59010208\n",
      "Trained batch 437 batch loss 1.5974704 epoch total loss 1.59011889\n",
      "Trained batch 438 batch loss 1.54958129 epoch total loss 1.59002626\n",
      "Trained batch 439 batch loss 1.50918519 epoch total loss 1.58984208\n",
      "Trained batch 440 batch loss 1.54048896 epoch total loss 1.58972991\n",
      "Trained batch 441 batch loss 1.61072874 epoch total loss 1.58977747\n",
      "Trained batch 442 batch loss 1.59665263 epoch total loss 1.58979309\n",
      "Trained batch 443 batch loss 1.66804743 epoch total loss 1.58996964\n",
      "Trained batch 444 batch loss 1.74172175 epoch total loss 1.59031141\n",
      "Trained batch 445 batch loss 1.64554536 epoch total loss 1.59043562\n",
      "Trained batch 446 batch loss 1.62805367 epoch total loss 1.59051991\n",
      "Trained batch 447 batch loss 1.57928538 epoch total loss 1.59049475\n",
      "Trained batch 448 batch loss 1.5292877 epoch total loss 1.59035814\n",
      "Trained batch 449 batch loss 1.55363965 epoch total loss 1.59027648\n",
      "Trained batch 450 batch loss 1.64626467 epoch total loss 1.59040082\n",
      "Trained batch 451 batch loss 1.62174892 epoch total loss 1.59047031\n",
      "Trained batch 452 batch loss 1.61642909 epoch total loss 1.59052777\n",
      "Trained batch 453 batch loss 1.6125226 epoch total loss 1.59057641\n",
      "Trained batch 454 batch loss 1.58919418 epoch total loss 1.59057331\n",
      "Trained batch 455 batch loss 1.57558465 epoch total loss 1.59054041\n",
      "Trained batch 456 batch loss 1.56024539 epoch total loss 1.59047389\n",
      "Trained batch 457 batch loss 1.52463198 epoch total loss 1.59032989\n",
      "Trained batch 458 batch loss 1.58425426 epoch total loss 1.59031653\n",
      "Trained batch 459 batch loss 1.55669188 epoch total loss 1.59024334\n",
      "Trained batch 460 batch loss 1.46803975 epoch total loss 1.58997762\n",
      "Trained batch 461 batch loss 1.55875564 epoch total loss 1.58990991\n",
      "Trained batch 462 batch loss 1.5271287 epoch total loss 1.58977401\n",
      "Trained batch 463 batch loss 1.51106155 epoch total loss 1.5896039\n",
      "Trained batch 464 batch loss 1.51782131 epoch total loss 1.58944929\n",
      "Trained batch 465 batch loss 1.55487323 epoch total loss 1.5893749\n",
      "Trained batch 466 batch loss 1.537552 epoch total loss 1.58926368\n",
      "Trained batch 467 batch loss 1.67667735 epoch total loss 1.58945084\n",
      "Trained batch 468 batch loss 1.55236351 epoch total loss 1.58937168\n",
      "Trained batch 469 batch loss 1.61795342 epoch total loss 1.5894326\n",
      "Trained batch 470 batch loss 1.60192394 epoch total loss 1.58945918\n",
      "Trained batch 471 batch loss 1.60882437 epoch total loss 1.58950031\n",
      "Trained batch 472 batch loss 1.62496734 epoch total loss 1.58957541\n",
      "Trained batch 473 batch loss 1.55380845 epoch total loss 1.58949983\n",
      "Trained batch 474 batch loss 1.58872914 epoch total loss 1.58949828\n",
      "Trained batch 475 batch loss 1.50299048 epoch total loss 1.58931613\n",
      "Trained batch 476 batch loss 1.50639653 epoch total loss 1.58914196\n",
      "Trained batch 477 batch loss 1.50326431 epoch total loss 1.58896184\n",
      "Trained batch 478 batch loss 1.57525778 epoch total loss 1.58893323\n",
      "Trained batch 479 batch loss 1.54929113 epoch total loss 1.5888505\n",
      "Trained batch 480 batch loss 1.51611328 epoch total loss 1.58869898\n",
      "Trained batch 481 batch loss 1.54756236 epoch total loss 1.58861339\n",
      "Trained batch 482 batch loss 1.51740932 epoch total loss 1.58846569\n",
      "Trained batch 483 batch loss 1.56760824 epoch total loss 1.58842254\n",
      "Trained batch 484 batch loss 1.49114263 epoch total loss 1.58822155\n",
      "Trained batch 485 batch loss 1.49451923 epoch total loss 1.58802831\n",
      "Trained batch 486 batch loss 1.57301259 epoch total loss 1.58799732\n",
      "Trained batch 487 batch loss 1.57267189 epoch total loss 1.58796597\n",
      "Trained batch 488 batch loss 1.60416698 epoch total loss 1.58799922\n",
      "Trained batch 489 batch loss 1.61932349 epoch total loss 1.58806324\n",
      "Trained batch 490 batch loss 1.59250653 epoch total loss 1.58807242\n",
      "Trained batch 491 batch loss 1.62919235 epoch total loss 1.5881561\n",
      "Trained batch 492 batch loss 1.55609107 epoch total loss 1.58809102\n",
      "Trained batch 493 batch loss 1.54074919 epoch total loss 1.58799493\n",
      "Trained batch 494 batch loss 1.60051084 epoch total loss 1.58802032\n",
      "Trained batch 495 batch loss 1.51153362 epoch total loss 1.58786583\n",
      "Trained batch 496 batch loss 1.52591944 epoch total loss 1.58774102\n",
      "Trained batch 497 batch loss 1.45079756 epoch total loss 1.58746552\n",
      "Trained batch 498 batch loss 1.60422075 epoch total loss 1.58749914\n",
      "Trained batch 499 batch loss 1.44869518 epoch total loss 1.58722091\n",
      "Trained batch 500 batch loss 1.53518188 epoch total loss 1.58711684\n",
      "Trained batch 501 batch loss 1.55356658 epoch total loss 1.58704984\n",
      "Trained batch 502 batch loss 1.47030616 epoch total loss 1.58681726\n",
      "Trained batch 503 batch loss 1.47637439 epoch total loss 1.58659768\n",
      "Trained batch 504 batch loss 1.52098274 epoch total loss 1.5864675\n",
      "Trained batch 505 batch loss 1.54786611 epoch total loss 1.58639109\n",
      "Trained batch 506 batch loss 1.62973857 epoch total loss 1.5864768\n",
      "Trained batch 507 batch loss 1.54995704 epoch total loss 1.58640468\n",
      "Trained batch 508 batch loss 1.50404692 epoch total loss 1.58624256\n",
      "Trained batch 509 batch loss 1.54788649 epoch total loss 1.58616722\n",
      "Trained batch 510 batch loss 1.43502438 epoch total loss 1.58587086\n",
      "Trained batch 511 batch loss 1.55603921 epoch total loss 1.58581245\n",
      "Trained batch 512 batch loss 1.50524676 epoch total loss 1.58565509\n",
      "Trained batch 513 batch loss 1.46989679 epoch total loss 1.58542943\n",
      "Trained batch 514 batch loss 1.57674432 epoch total loss 1.5854125\n",
      "Trained batch 515 batch loss 1.55071902 epoch total loss 1.58534515\n",
      "Trained batch 516 batch loss 1.5949862 epoch total loss 1.58536386\n",
      "Trained batch 517 batch loss 1.54550266 epoch total loss 1.58528674\n",
      "Trained batch 518 batch loss 1.55840683 epoch total loss 1.58523488\n",
      "Trained batch 519 batch loss 1.63451159 epoch total loss 1.58532989\n",
      "Trained batch 520 batch loss 1.46948206 epoch total loss 1.58510709\n",
      "Trained batch 521 batch loss 1.51317072 epoch total loss 1.58496904\n",
      "Trained batch 522 batch loss 1.57597423 epoch total loss 1.58495176\n",
      "Trained batch 523 batch loss 1.63851762 epoch total loss 1.58505416\n",
      "Trained batch 524 batch loss 1.64044595 epoch total loss 1.5851599\n",
      "Trained batch 525 batch loss 1.60195327 epoch total loss 1.58519185\n",
      "Trained batch 526 batch loss 1.67822409 epoch total loss 1.58536863\n",
      "Trained batch 527 batch loss 1.62520528 epoch total loss 1.58544421\n",
      "Trained batch 528 batch loss 1.5731957 epoch total loss 1.58542097\n",
      "Trained batch 529 batch loss 1.5610249 epoch total loss 1.58537495\n",
      "Trained batch 530 batch loss 1.69787478 epoch total loss 1.58558714\n",
      "Trained batch 531 batch loss 1.66102648 epoch total loss 1.58572924\n",
      "Trained batch 532 batch loss 1.59158993 epoch total loss 1.58574033\n",
      "Trained batch 533 batch loss 1.63829696 epoch total loss 1.58583891\n",
      "Trained batch 534 batch loss 1.62281239 epoch total loss 1.58590817\n",
      "Trained batch 535 batch loss 1.55627811 epoch total loss 1.58585274\n",
      "Trained batch 536 batch loss 1.54007924 epoch total loss 1.58576739\n",
      "Trained batch 537 batch loss 1.66713381 epoch total loss 1.5859189\n",
      "Trained batch 538 batch loss 1.61168742 epoch total loss 1.58596683\n",
      "Trained batch 539 batch loss 1.64885044 epoch total loss 1.58608341\n",
      "Trained batch 540 batch loss 1.60319 epoch total loss 1.58611512\n",
      "Trained batch 541 batch loss 1.41545951 epoch total loss 1.58579969\n",
      "Trained batch 542 batch loss 1.34616423 epoch total loss 1.58535767\n",
      "Trained batch 543 batch loss 1.33731 epoch total loss 1.58490086\n",
      "Trained batch 544 batch loss 1.53329206 epoch total loss 1.58480585\n",
      "Trained batch 545 batch loss 1.64648271 epoch total loss 1.5849191\n",
      "Trained batch 546 batch loss 1.59299076 epoch total loss 1.58493388\n",
      "Trained batch 547 batch loss 1.54343593 epoch total loss 1.58485806\n",
      "Trained batch 548 batch loss 1.57732916 epoch total loss 1.58484435\n",
      "Trained batch 549 batch loss 1.64787531 epoch total loss 1.58495915\n",
      "Trained batch 550 batch loss 1.60767066 epoch total loss 1.5850004\n",
      "Trained batch 551 batch loss 1.67868638 epoch total loss 1.58517051\n",
      "Trained batch 552 batch loss 1.64857721 epoch total loss 1.58528531\n",
      "Trained batch 553 batch loss 1.55455136 epoch total loss 1.58522975\n",
      "Trained batch 554 batch loss 1.55880404 epoch total loss 1.58518207\n",
      "Trained batch 555 batch loss 1.54620278 epoch total loss 1.58511186\n",
      "Trained batch 556 batch loss 1.4956696 epoch total loss 1.58495092\n",
      "Trained batch 557 batch loss 1.51769066 epoch total loss 1.58483016\n",
      "Trained batch 558 batch loss 1.62508762 epoch total loss 1.58490229\n",
      "Trained batch 559 batch loss 1.65569651 epoch total loss 1.58502901\n",
      "Trained batch 560 batch loss 1.53514314 epoch total loss 1.58494\n",
      "Trained batch 561 batch loss 1.53989542 epoch total loss 1.58485961\n",
      "Trained batch 562 batch loss 1.47072744 epoch total loss 1.58465648\n",
      "Trained batch 563 batch loss 1.559901 epoch total loss 1.58461249\n",
      "Trained batch 564 batch loss 1.46076679 epoch total loss 1.58439291\n",
      "Trained batch 565 batch loss 1.54583347 epoch total loss 1.58432472\n",
      "Trained batch 566 batch loss 1.66432214 epoch total loss 1.58446598\n",
      "Trained batch 567 batch loss 1.60606623 epoch total loss 1.58450413\n",
      "Trained batch 568 batch loss 1.50636232 epoch total loss 1.58436644\n",
      "Trained batch 569 batch loss 1.61023223 epoch total loss 1.58441198\n",
      "Trained batch 570 batch loss 1.60661411 epoch total loss 1.58445096\n",
      "Trained batch 571 batch loss 1.61425829 epoch total loss 1.58450317\n",
      "Trained batch 572 batch loss 1.60118401 epoch total loss 1.58453226\n",
      "Trained batch 573 batch loss 1.63645244 epoch total loss 1.58462298\n",
      "Trained batch 574 batch loss 1.5459522 epoch total loss 1.58455563\n",
      "Trained batch 575 batch loss 1.45782852 epoch total loss 1.58433521\n",
      "Trained batch 576 batch loss 1.57506633 epoch total loss 1.58431911\n",
      "Trained batch 577 batch loss 1.57044756 epoch total loss 1.58429503\n",
      "Trained batch 578 batch loss 1.58123958 epoch total loss 1.58428979\n",
      "Trained batch 579 batch loss 1.57834661 epoch total loss 1.58427954\n",
      "Trained batch 580 batch loss 1.60438788 epoch total loss 1.58431423\n",
      "Trained batch 581 batch loss 1.5055809 epoch total loss 1.58417857\n",
      "Trained batch 582 batch loss 1.54795098 epoch total loss 1.58411646\n",
      "Trained batch 583 batch loss 1.64787149 epoch total loss 1.58422577\n",
      "Trained batch 584 batch loss 1.57069373 epoch total loss 1.58420265\n",
      "Trained batch 585 batch loss 1.62278497 epoch total loss 1.58426857\n",
      "Trained batch 586 batch loss 1.60372305 epoch total loss 1.58430171\n",
      "Trained batch 587 batch loss 1.59271836 epoch total loss 1.58431602\n",
      "Trained batch 588 batch loss 1.4622823 epoch total loss 1.58410847\n",
      "Trained batch 589 batch loss 1.34083319 epoch total loss 1.58369541\n",
      "Trained batch 590 batch loss 1.45845222 epoch total loss 1.58348322\n",
      "Trained batch 591 batch loss 1.62624359 epoch total loss 1.58355546\n",
      "Trained batch 592 batch loss 1.54267931 epoch total loss 1.58348644\n",
      "Trained batch 593 batch loss 1.52277124 epoch total loss 1.58338404\n",
      "Trained batch 594 batch loss 1.56685472 epoch total loss 1.58335614\n",
      "Trained batch 595 batch loss 1.60376811 epoch total loss 1.58339047\n",
      "Trained batch 596 batch loss 1.55511916 epoch total loss 1.58334303\n",
      "Trained batch 597 batch loss 1.52239525 epoch total loss 1.58324087\n",
      "Trained batch 598 batch loss 1.47895145 epoch total loss 1.58306646\n",
      "Trained batch 599 batch loss 1.48840141 epoch total loss 1.58290851\n",
      "Trained batch 600 batch loss 1.55414641 epoch total loss 1.58286047\n",
      "Trained batch 601 batch loss 1.53814209 epoch total loss 1.58278608\n",
      "Trained batch 602 batch loss 1.54877949 epoch total loss 1.58272958\n",
      "Trained batch 603 batch loss 1.58899271 epoch total loss 1.58274\n",
      "Trained batch 604 batch loss 1.52995491 epoch total loss 1.58265257\n",
      "Trained batch 605 batch loss 1.61836362 epoch total loss 1.58271158\n",
      "Trained batch 606 batch loss 1.59136629 epoch total loss 1.58272588\n",
      "Trained batch 607 batch loss 1.60503387 epoch total loss 1.58276272\n",
      "Trained batch 608 batch loss 1.65557504 epoch total loss 1.5828824\n",
      "Trained batch 609 batch loss 1.5264256 epoch total loss 1.58278978\n",
      "Trained batch 610 batch loss 1.54662073 epoch total loss 1.58273041\n",
      "Trained batch 611 batch loss 1.56476581 epoch total loss 1.58270109\n",
      "Trained batch 612 batch loss 1.47653711 epoch total loss 1.58252764\n",
      "Trained batch 613 batch loss 1.51870346 epoch total loss 1.58242345\n",
      "Trained batch 614 batch loss 1.60280561 epoch total loss 1.58245659\n",
      "Trained batch 615 batch loss 1.45515156 epoch total loss 1.58224964\n",
      "Trained batch 616 batch loss 1.59551692 epoch total loss 1.5822711\n",
      "Trained batch 617 batch loss 1.55151045 epoch total loss 1.58222127\n",
      "Trained batch 618 batch loss 1.60155678 epoch total loss 1.58225262\n",
      "Trained batch 619 batch loss 1.53759742 epoch total loss 1.58218038\n",
      "Trained batch 620 batch loss 1.4503299 epoch total loss 1.58196771\n",
      "Trained batch 621 batch loss 1.54566908 epoch total loss 1.5819093\n",
      "Trained batch 622 batch loss 1.62369132 epoch total loss 1.58197653\n",
      "Trained batch 623 batch loss 1.5814774 epoch total loss 1.5819757\n",
      "Trained batch 624 batch loss 1.63155007 epoch total loss 1.58205509\n",
      "Trained batch 625 batch loss 1.5374769 epoch total loss 1.5819838\n",
      "Trained batch 626 batch loss 1.50781393 epoch total loss 1.58186531\n",
      "Trained batch 627 batch loss 1.51075101 epoch total loss 1.58175182\n",
      "Trained batch 628 batch loss 1.41683912 epoch total loss 1.58148921\n",
      "Trained batch 629 batch loss 1.63806081 epoch total loss 1.58157921\n",
      "Trained batch 630 batch loss 1.65751624 epoch total loss 1.58169973\n",
      "Trained batch 631 batch loss 1.62774825 epoch total loss 1.58177269\n",
      "Trained batch 632 batch loss 1.60477138 epoch total loss 1.58180916\n",
      "Trained batch 633 batch loss 1.71514225 epoch total loss 1.58201981\n",
      "Trained batch 634 batch loss 1.55785179 epoch total loss 1.58198166\n",
      "Trained batch 635 batch loss 1.57812119 epoch total loss 1.58197558\n",
      "Trained batch 636 batch loss 1.53564715 epoch total loss 1.58190274\n",
      "Trained batch 637 batch loss 1.62836337 epoch total loss 1.5819757\n",
      "Trained batch 638 batch loss 1.52976632 epoch total loss 1.58189392\n",
      "Trained batch 639 batch loss 1.59523225 epoch total loss 1.58191466\n",
      "Trained batch 640 batch loss 1.53819406 epoch total loss 1.58184648\n",
      "Trained batch 641 batch loss 1.54953647 epoch total loss 1.58179605\n",
      "Trained batch 642 batch loss 1.51185238 epoch total loss 1.58168709\n",
      "Trained batch 643 batch loss 1.53492272 epoch total loss 1.58161438\n",
      "Trained batch 644 batch loss 1.50184798 epoch total loss 1.58149052\n",
      "Trained batch 645 batch loss 1.54517686 epoch total loss 1.58143413\n",
      "Trained batch 646 batch loss 1.46515322 epoch total loss 1.58125412\n",
      "Trained batch 647 batch loss 1.51810336 epoch total loss 1.58115661\n",
      "Trained batch 648 batch loss 1.48891306 epoch total loss 1.58101416\n",
      "Trained batch 649 batch loss 1.39217663 epoch total loss 1.58072329\n",
      "Trained batch 650 batch loss 1.42593217 epoch total loss 1.58048511\n",
      "Trained batch 651 batch loss 1.3070308 epoch total loss 1.58006501\n",
      "Trained batch 652 batch loss 1.18826818 epoch total loss 1.57946408\n",
      "Trained batch 653 batch loss 1.34992409 epoch total loss 1.57911265\n",
      "Trained batch 654 batch loss 1.54853559 epoch total loss 1.57906592\n",
      "Trained batch 655 batch loss 1.65130877 epoch total loss 1.57917631\n",
      "Trained batch 656 batch loss 1.73726964 epoch total loss 1.57941735\n",
      "Trained batch 657 batch loss 1.61607182 epoch total loss 1.57947314\n",
      "Trained batch 658 batch loss 1.45241368 epoch total loss 1.57928\n",
      "Trained batch 659 batch loss 1.50067496 epoch total loss 1.57916081\n",
      "Trained batch 660 batch loss 1.64889085 epoch total loss 1.57926655\n",
      "Trained batch 661 batch loss 1.67097342 epoch total loss 1.57940531\n",
      "Trained batch 662 batch loss 1.62504387 epoch total loss 1.57947421\n",
      "Trained batch 663 batch loss 1.61059344 epoch total loss 1.57952118\n",
      "Trained batch 664 batch loss 1.6578393 epoch total loss 1.57963908\n",
      "Trained batch 665 batch loss 1.61291552 epoch total loss 1.57968915\n",
      "Trained batch 666 batch loss 1.60445464 epoch total loss 1.57972634\n",
      "Trained batch 667 batch loss 1.58115351 epoch total loss 1.5797286\n",
      "Trained batch 668 batch loss 1.54864061 epoch total loss 1.57968199\n",
      "Trained batch 669 batch loss 1.57101214 epoch total loss 1.579669\n",
      "Trained batch 670 batch loss 1.61563468 epoch total loss 1.57972264\n",
      "Trained batch 671 batch loss 1.59966588 epoch total loss 1.57975233\n",
      "Trained batch 672 batch loss 1.44467032 epoch total loss 1.57955134\n",
      "Trained batch 673 batch loss 1.45759237 epoch total loss 1.57937014\n",
      "Trained batch 674 batch loss 1.58855939 epoch total loss 1.57938373\n",
      "Trained batch 675 batch loss 1.54590583 epoch total loss 1.57933414\n",
      "Trained batch 676 batch loss 1.58533216 epoch total loss 1.57934296\n",
      "Trained batch 677 batch loss 1.44736958 epoch total loss 1.57914805\n",
      "Trained batch 678 batch loss 1.57179666 epoch total loss 1.57913721\n",
      "Trained batch 679 batch loss 1.52794325 epoch total loss 1.57906187\n",
      "Trained batch 680 batch loss 1.50582457 epoch total loss 1.57895422\n",
      "Trained batch 681 batch loss 1.48646784 epoch total loss 1.57881832\n",
      "Trained batch 682 batch loss 1.53754497 epoch total loss 1.57875788\n",
      "Trained batch 683 batch loss 1.52650785 epoch total loss 1.57868135\n",
      "Trained batch 684 batch loss 1.47092628 epoch total loss 1.57852387\n",
      "Trained batch 685 batch loss 1.55020797 epoch total loss 1.57848251\n",
      "Trained batch 686 batch loss 1.59477282 epoch total loss 1.57850611\n",
      "Trained batch 687 batch loss 1.56412458 epoch total loss 1.57848513\n",
      "Trained batch 688 batch loss 1.66362441 epoch total loss 1.57860887\n",
      "Trained batch 689 batch loss 1.62699282 epoch total loss 1.57867897\n",
      "Trained batch 690 batch loss 1.63067341 epoch total loss 1.57875431\n",
      "Trained batch 691 batch loss 1.5892725 epoch total loss 1.57876945\n",
      "Trained batch 692 batch loss 1.57931781 epoch total loss 1.57877028\n",
      "Trained batch 693 batch loss 1.59617031 epoch total loss 1.57879543\n",
      "Trained batch 694 batch loss 1.55691361 epoch total loss 1.57876384\n",
      "Trained batch 695 batch loss 1.69487715 epoch total loss 1.57893085\n",
      "Trained batch 696 batch loss 1.49850297 epoch total loss 1.57881534\n",
      "Trained batch 697 batch loss 1.57299495 epoch total loss 1.578807\n",
      "Trained batch 698 batch loss 1.39063442 epoch total loss 1.57853734\n",
      "Trained batch 699 batch loss 1.50985301 epoch total loss 1.57843912\n",
      "Trained batch 700 batch loss 1.55208218 epoch total loss 1.57840157\n",
      "Trained batch 701 batch loss 1.41716754 epoch total loss 1.57817149\n",
      "Trained batch 702 batch loss 1.41790223 epoch total loss 1.57794309\n",
      "Trained batch 703 batch loss 1.33351159 epoch total loss 1.57759535\n",
      "Trained batch 704 batch loss 1.40307 epoch total loss 1.57734752\n",
      "Trained batch 705 batch loss 1.43842375 epoch total loss 1.57715046\n",
      "Trained batch 706 batch loss 1.35514748 epoch total loss 1.57683599\n",
      "Trained batch 707 batch loss 1.51806331 epoch total loss 1.5767529\n",
      "Trained batch 708 batch loss 1.47232187 epoch total loss 1.57660532\n",
      "Trained batch 709 batch loss 1.50812387 epoch total loss 1.57650876\n",
      "Trained batch 710 batch loss 1.48111129 epoch total loss 1.57637441\n",
      "Trained batch 711 batch loss 1.40975583 epoch total loss 1.57614017\n",
      "Trained batch 712 batch loss 1.54260373 epoch total loss 1.57609296\n",
      "Trained batch 713 batch loss 1.63643551 epoch total loss 1.57617772\n",
      "Trained batch 714 batch loss 1.56036985 epoch total loss 1.57615566\n",
      "Trained batch 715 batch loss 1.57667279 epoch total loss 1.57615638\n",
      "Trained batch 716 batch loss 1.58023286 epoch total loss 1.57616198\n",
      "Trained batch 717 batch loss 1.51474893 epoch total loss 1.57607639\n",
      "Trained batch 718 batch loss 1.55567896 epoch total loss 1.5760479\n",
      "Trained batch 719 batch loss 1.62747633 epoch total loss 1.57611942\n",
      "Trained batch 720 batch loss 1.55079007 epoch total loss 1.57608426\n",
      "Trained batch 721 batch loss 1.56561065 epoch total loss 1.57606959\n",
      "Trained batch 722 batch loss 1.63781023 epoch total loss 1.57615519\n",
      "Trained batch 723 batch loss 1.5758599 epoch total loss 1.57615471\n",
      "Trained batch 724 batch loss 1.60011435 epoch total loss 1.57618773\n",
      "Trained batch 725 batch loss 1.61441827 epoch total loss 1.57624042\n",
      "Trained batch 726 batch loss 1.56393826 epoch total loss 1.57622349\n",
      "Trained batch 727 batch loss 1.51455212 epoch total loss 1.57613862\n",
      "Trained batch 728 batch loss 1.38792479 epoch total loss 1.57588017\n",
      "Trained batch 729 batch loss 1.39047313 epoch total loss 1.57562578\n",
      "Trained batch 730 batch loss 1.48873508 epoch total loss 1.57550681\n",
      "Trained batch 731 batch loss 1.56593156 epoch total loss 1.57549369\n",
      "Trained batch 732 batch loss 1.53013039 epoch total loss 1.57543182\n",
      "Trained batch 733 batch loss 1.44765115 epoch total loss 1.57525742\n",
      "Trained batch 734 batch loss 1.45648623 epoch total loss 1.57509565\n",
      "Trained batch 735 batch loss 1.46068978 epoch total loss 1.57494009\n",
      "Trained batch 736 batch loss 1.50382519 epoch total loss 1.57484341\n",
      "Trained batch 737 batch loss 1.37330365 epoch total loss 1.57457\n",
      "Trained batch 738 batch loss 1.61032605 epoch total loss 1.57461834\n",
      "Trained batch 739 batch loss 1.67792273 epoch total loss 1.57475829\n",
      "Trained batch 740 batch loss 1.66246593 epoch total loss 1.57487679\n",
      "Trained batch 741 batch loss 1.64765048 epoch total loss 1.57497501\n",
      "Trained batch 742 batch loss 1.57969332 epoch total loss 1.57498145\n",
      "Trained batch 743 batch loss 1.56396818 epoch total loss 1.57496667\n",
      "Trained batch 744 batch loss 1.5881871 epoch total loss 1.57498431\n",
      "Trained batch 745 batch loss 1.65947425 epoch total loss 1.57509768\n",
      "Trained batch 746 batch loss 1.58353925 epoch total loss 1.57510889\n",
      "Trained batch 915 batch loss 1.48235142 epoch total loss 1.56984115\n",
      "Trained batch 916 batch loss 1.53651667 epoch total loss 1.56980479\n",
      "Trained batch 917 batch loss 1.51025629 epoch total loss 1.56973982\n",
      "Trained batch 918 batch loss 1.38844919 epoch total loss 1.56954229\n",
      "Trained batch 919 batch loss 1.43154073 epoch total loss 1.56939209\n",
      "Trained batch 920 batch loss 1.50916147 epoch total loss 1.56932664\n",
      "Trained batch 921 batch loss 1.39846778 epoch total loss 1.56914115\n",
      "Trained batch 922 batch loss 1.52252841 epoch total loss 1.5690906\n",
      "Trained batch 923 batch loss 1.37972188 epoch total loss 1.56888545\n",
      "Trained batch 924 batch loss 1.42097616 epoch total loss 1.56872547\n",
      "Trained batch 925 batch loss 1.45955896 epoch total loss 1.56860745\n",
      "Trained batch 926 batch loss 1.50170422 epoch total loss 1.56853521\n",
      "Trained batch 927 batch loss 1.55544221 epoch total loss 1.56852102\n",
      "Trained batch 928 batch loss 1.50721383 epoch total loss 1.56845498\n",
      "Trained batch 929 batch loss 1.60757375 epoch total loss 1.56849706\n",
      "Trained batch 930 batch loss 1.60256565 epoch total loss 1.56853366\n",
      "Trained batch 931 batch loss 1.63884616 epoch total loss 1.56860912\n",
      "Trained batch 932 batch loss 1.56388736 epoch total loss 1.56860399\n",
      "Trained batch 933 batch loss 1.49109292 epoch total loss 1.5685209\n",
      "Trained batch 934 batch loss 1.49792147 epoch total loss 1.56844532\n",
      "Trained batch 935 batch loss 1.5178864 epoch total loss 1.56839132\n",
      "Trained batch 936 batch loss 1.53887427 epoch total loss 1.56835973\n",
      "Trained batch 937 batch loss 1.50404096 epoch total loss 1.56829107\n",
      "Trained batch 938 batch loss 1.57526863 epoch total loss 1.56829858\n",
      "Trained batch 939 batch loss 1.5668571 epoch total loss 1.56829715\n",
      "Trained batch 940 batch loss 1.50836289 epoch total loss 1.56823337\n",
      "Trained batch 941 batch loss 1.51313877 epoch total loss 1.56817496\n",
      "Trained batch 942 batch loss 1.63256299 epoch total loss 1.56824327\n",
      "Trained batch 943 batch loss 1.66727495 epoch total loss 1.56834829\n",
      "Trained batch 944 batch loss 1.67287922 epoch total loss 1.56845891\n",
      "Trained batch 945 batch loss 1.65426302 epoch total loss 1.56854975\n",
      "Trained batch 946 batch loss 1.59758496 epoch total loss 1.56858039\n",
      "Trained batch 947 batch loss 1.61140656 epoch total loss 1.56862569\n",
      "Trained batch 948 batch loss 1.61121 epoch total loss 1.56867063\n",
      "Trained batch 949 batch loss 1.56892037 epoch total loss 1.56867087\n",
      "Trained batch 950 batch loss 1.57909203 epoch total loss 1.56868196\n",
      "Trained batch 951 batch loss 1.70439816 epoch total loss 1.56882453\n",
      "Trained batch 952 batch loss 1.65328193 epoch total loss 1.56891334\n",
      "Trained batch 953 batch loss 1.59111071 epoch total loss 1.56893659\n",
      "Trained batch 954 batch loss 1.51563716 epoch total loss 1.56888068\n",
      "Trained batch 955 batch loss 1.62607336 epoch total loss 1.56894064\n",
      "Trained batch 956 batch loss 1.58318722 epoch total loss 1.56895542\n",
      "Trained batch 957 batch loss 1.64243567 epoch total loss 1.56903219\n",
      "Trained batch 958 batch loss 1.57039094 epoch total loss 1.56903374\n",
      "Trained batch 959 batch loss 1.44239473 epoch total loss 1.56890166\n",
      "Trained batch 960 batch loss 1.40625012 epoch total loss 1.56873214\n",
      "Trained batch 961 batch loss 1.51880538 epoch total loss 1.56868029\n",
      "Trained batch 962 batch loss 1.67893291 epoch total loss 1.56879485\n",
      "Trained batch 963 batch loss 1.61767244 epoch total loss 1.56884563\n",
      "Trained batch 964 batch loss 1.66536975 epoch total loss 1.56894577\n",
      "Trained batch 965 batch loss 1.66282082 epoch total loss 1.56904304\n",
      "Trained batch 966 batch loss 1.7064147 epoch total loss 1.56918526\n",
      "Trained batch 967 batch loss 1.62295651 epoch total loss 1.56924093\n",
      "Trained batch 968 batch loss 1.59799349 epoch total loss 1.56927061\n",
      "Trained batch 969 batch loss 1.61300159 epoch total loss 1.56931579\n",
      "Trained batch 970 batch loss 1.61812019 epoch total loss 1.5693661\n",
      "Trained batch 971 batch loss 1.51817346 epoch total loss 1.56931341\n",
      "Trained batch 972 batch loss 1.66330266 epoch total loss 1.5694102\n",
      "Trained batch 973 batch loss 1.59859419 epoch total loss 1.56944025\n",
      "Trained batch 974 batch loss 1.66174054 epoch total loss 1.56953502\n",
      "Trained batch 975 batch loss 1.52658558 epoch total loss 1.56949091\n",
      "Trained batch 976 batch loss 1.59491253 epoch total loss 1.56951702\n",
      "Trained batch 977 batch loss 1.50715053 epoch total loss 1.56945324\n",
      "Trained batch 978 batch loss 1.48176599 epoch total loss 1.56936359\n",
      "Trained batch 979 batch loss 1.60220575 epoch total loss 1.56939721\n",
      "Trained batch 980 batch loss 1.57476819 epoch total loss 1.56940269\n",
      "Trained batch 981 batch loss 1.63825369 epoch total loss 1.56947291\n",
      "Trained batch 982 batch loss 1.70609879 epoch total loss 1.56961203\n",
      "Trained batch 983 batch loss 1.4797895 epoch total loss 1.56952059\n",
      "Trained batch 984 batch loss 1.51921892 epoch total loss 1.56946945\n",
      "Trained batch 985 batch loss 1.56935143 epoch total loss 1.56946933\n",
      "Trained batch 986 batch loss 1.54501665 epoch total loss 1.56944454\n",
      "Trained batch 987 batch loss 1.56616521 epoch total loss 1.5694412\n",
      "Trained batch 988 batch loss 1.62618959 epoch total loss 1.56949866\n",
      "Trained batch 989 batch loss 1.68535602 epoch total loss 1.56961572\n",
      "Trained batch 990 batch loss 1.64621699 epoch total loss 1.56969309\n",
      "Trained batch 991 batch loss 1.64659142 epoch total loss 1.56977081\n",
      "Trained batch 992 batch loss 1.67346454 epoch total loss 1.56987524\n",
      "Trained batch 993 batch loss 1.59884286 epoch total loss 1.56990445\n",
      "Trained batch 994 batch loss 1.66927791 epoch total loss 1.57000446\n",
      "Trained batch 995 batch loss 1.60976923 epoch total loss 1.5700444\n",
      "Trained batch 996 batch loss 1.63080859 epoch total loss 1.57010555\n",
      "Trained batch 997 batch loss 1.60769582 epoch total loss 1.57014322\n",
      "Trained batch 998 batch loss 1.58081102 epoch total loss 1.57015383\n",
      "Trained batch 999 batch loss 1.63328087 epoch total loss 1.57021701\n",
      "Trained batch 1000 batch loss 1.55639029 epoch total loss 1.5702033\n",
      "Trained batch 1001 batch loss 1.58742476 epoch total loss 1.57022047\n",
      "Trained batch 1002 batch loss 1.5424422 epoch total loss 1.57019269\n",
      "Trained batch 1003 batch loss 1.41611099 epoch total loss 1.57003915\n",
      "Trained batch 1004 batch loss 1.4644928 epoch total loss 1.56993401\n",
      "Trained batch 1005 batch loss 1.55101252 epoch total loss 1.56991518\n",
      "Trained batch 1006 batch loss 1.45922887 epoch total loss 1.56980515\n",
      "Trained batch 1007 batch loss 1.52204418 epoch total loss 1.56975782\n",
      "Trained batch 1008 batch loss 1.51348746 epoch total loss 1.56970191\n",
      "Trained batch 1009 batch loss 1.54565501 epoch total loss 1.56967807\n",
      "Trained batch 1010 batch loss 1.57408309 epoch total loss 1.56968248\n",
      "Trained batch 1011 batch loss 1.63463187 epoch total loss 1.56974673\n",
      "Trained batch 1012 batch loss 1.54649043 epoch total loss 1.56972373\n",
      "Trained batch 1013 batch loss 1.51665497 epoch total loss 1.56967127\n",
      "Trained batch 1014 batch loss 1.52456188 epoch total loss 1.56962681\n",
      "Trained batch 1015 batch loss 1.55695236 epoch total loss 1.56961441\n",
      "Trained batch 1016 batch loss 1.54584503 epoch total loss 1.56959105\n",
      "Trained batch 1017 batch loss 1.60324812 epoch total loss 1.56962419\n",
      "Trained batch 1018 batch loss 1.61200047 epoch total loss 1.56966579\n",
      "Trained batch 1019 batch loss 1.57590115 epoch total loss 1.56967199\n",
      "Trained batch 1020 batch loss 1.4575417 epoch total loss 1.56956196\n",
      "Trained batch 1021 batch loss 1.41370153 epoch total loss 1.56940937\n",
      "Trained batch 1022 batch loss 1.45199978 epoch total loss 1.56929445\n",
      "Trained batch 1023 batch loss 1.63922393 epoch total loss 1.56936288\n",
      "Trained batch 1024 batch loss 1.56828284 epoch total loss 1.56936181\n",
      "Trained batch 1025 batch loss 1.60740721 epoch total loss 1.56939888\n",
      "Trained batch 1026 batch loss 1.62535584 epoch total loss 1.56945348\n",
      "Trained batch 1027 batch loss 1.62651753 epoch total loss 1.56950903\n",
      "Trained batch 1028 batch loss 1.60433185 epoch total loss 1.56954288\n",
      "Trained batch 1029 batch loss 1.65125179 epoch total loss 1.56962228\n",
      "Trained batch 1030 batch loss 1.59376788 epoch total loss 1.56964576\n",
      "Trained batch 1031 batch loss 1.56037331 epoch total loss 1.56963682\n",
      "Trained batch 1032 batch loss 1.57410991 epoch total loss 1.56964111\n",
      "Trained batch 1033 batch loss 1.62650359 epoch total loss 1.56969607\n",
      "Trained batch 1034 batch loss 1.61719811 epoch total loss 1.56974208\n",
      "Trained batch 1035 batch loss 1.54944992 epoch total loss 1.56972241\n",
      "Trained batch 1036 batch loss 1.51718473 epoch total loss 1.56967175\n",
      "Trained batch 1037 batch loss 1.52338183 epoch total loss 1.56962717\n",
      "Trained batch 1038 batch loss 1.53554392 epoch total loss 1.56959426\n",
      "Trained batch 1039 batch loss 1.55586636 epoch total loss 1.56958115\n",
      "Trained batch 1040 batch loss 1.4847194 epoch total loss 1.56949961\n",
      "Trained batch 1041 batch loss 1.58169711 epoch total loss 1.56951129\n",
      "Trained batch 1042 batch loss 1.52617049 epoch total loss 1.56946957\n",
      "Trained batch 1043 batch loss 1.53559744 epoch total loss 1.56943715\n",
      "Trained batch 1044 batch loss 1.5027051 epoch total loss 1.56937325\n",
      "Trained batch 1045 batch loss 1.54916 epoch total loss 1.56935394\n",
      "Trained batch 1046 batch loss 1.53726912 epoch total loss 1.56932318\n",
      "Trained batch 1047 batch loss 1.49067771 epoch total loss 1.5692482\n",
      "Trained batch 1048 batch loss 1.48821294 epoch total loss 1.56917071\n",
      "Trained batch 1049 batch loss 1.52402055 epoch total loss 1.5691278\n",
      "Trained batch 1050 batch loss 1.48905861 epoch total loss 1.5690515\n",
      "Trained batch 1051 batch loss 1.55074191 epoch total loss 1.5690341\n",
      "Trained batch 1052 batch loss 1.58463359 epoch total loss 1.56904888\n",
      "Trained batch 1053 batch loss 1.4799298 epoch total loss 1.56896424\n",
      "Trained batch 1054 batch loss 1.48696887 epoch total loss 1.5688864\n",
      "Trained batch 1055 batch loss 1.56774437 epoch total loss 1.56888533\n",
      "Trained batch 1056 batch loss 1.56626582 epoch total loss 1.56888294\n",
      "Trained batch 1057 batch loss 1.57884753 epoch total loss 1.56889236\n",
      "Trained batch 1058 batch loss 1.56085587 epoch total loss 1.56888485\n",
      "Trained batch 1059 batch loss 1.53587675 epoch total loss 1.56885362\n",
      "Trained batch 1060 batch loss 1.5152756 epoch total loss 1.56880307\n",
      "Trained batch 1061 batch loss 1.56126869 epoch total loss 1.56879604\n",
      "Trained batch 1062 batch loss 1.46865392 epoch total loss 1.56870162\n",
      "Trained batch 1063 batch loss 1.50476229 epoch total loss 1.56864154\n",
      "Trained batch 1064 batch loss 1.54761767 epoch total loss 1.56862175\n",
      "Trained batch 1065 batch loss 1.54170299 epoch total loss 1.56859648\n",
      "Trained batch 1066 batch loss 1.45269871 epoch total loss 1.56848788\n",
      "Trained batch 1067 batch loss 1.34870279 epoch total loss 1.56828189\n",
      "Trained batch 1068 batch loss 1.42180872 epoch total loss 1.56814468\n",
      "Trained batch 1069 batch loss 1.44913936 epoch total loss 1.56803334\n",
      "Trained batch 1070 batch loss 1.50012302 epoch total loss 1.56796992\n",
      "Trained batch 1071 batch loss 1.53125882 epoch total loss 1.56793559\n",
      "Trained batch 1072 batch loss 1.56932437 epoch total loss 1.5679369\n",
      "Trained batch 1073 batch loss 1.52983832 epoch total loss 1.56790137\n",
      "Trained batch 1074 batch loss 1.58531308 epoch total loss 1.56791759\n",
      "Trained batch 1075 batch loss 1.68011117 epoch total loss 1.56802189\n",
      "Trained batch 1076 batch loss 1.53920949 epoch total loss 1.56799507\n",
      "Trained batch 1077 batch loss 1.5636673 epoch total loss 1.56799114\n",
      "Trained batch 1078 batch loss 1.58047462 epoch total loss 1.5680027\n",
      "Trained batch 1079 batch loss 1.48594666 epoch total loss 1.56792665\n",
      "Trained batch 1080 batch loss 1.49360418 epoch total loss 1.56785786\n",
      "Trained batch 1081 batch loss 1.48320115 epoch total loss 1.56777954\n",
      "Trained batch 1082 batch loss 1.58633935 epoch total loss 1.56779659\n",
      "Trained batch 1083 batch loss 1.42234445 epoch total loss 1.56766236\n",
      "Trained batch 1084 batch loss 1.53102076 epoch total loss 1.5676285\n",
      "Trained batch 1085 batch loss 1.51207018 epoch total loss 1.56757736\n",
      "Trained batch 1086 batch loss 1.46797371 epoch total loss 1.56748569\n",
      "Trained batch 1087 batch loss 1.50149035 epoch total loss 1.56742489\n",
      "Trained batch 1088 batch loss 1.48727608 epoch total loss 1.56735134\n",
      "Trained batch 1089 batch loss 1.61409426 epoch total loss 1.56739426\n",
      "Trained batch 1090 batch loss 1.58666265 epoch total loss 1.5674119\n",
      "Trained batch 1091 batch loss 1.77847517 epoch total loss 1.56760538\n",
      "Trained batch 1092 batch loss 1.58807659 epoch total loss 1.56762421\n",
      "Trained batch 1093 batch loss 1.71006227 epoch total loss 1.56775451\n",
      "Trained batch 1094 batch loss 1.59995222 epoch total loss 1.56778395\n",
      "Trained batch 1095 batch loss 1.66441178 epoch total loss 1.56787217\n",
      "Trained batch 1096 batch loss 1.52046049 epoch total loss 1.56782901\n",
      "Trained batch 1097 batch loss 1.63299739 epoch total loss 1.5678885\n",
      "Trained batch 1098 batch loss 1.62335587 epoch total loss 1.56793904\n",
      "Trained batch 1099 batch loss 1.57417107 epoch total loss 1.56794477\n",
      "Trained batch 1100 batch loss 1.5969274 epoch total loss 1.56797111\n",
      "Trained batch 1101 batch loss 1.66235065 epoch total loss 1.56805682\n",
      "Trained batch 1102 batch loss 1.62403822 epoch total loss 1.5681076\n",
      "Trained batch 1103 batch loss 1.54969597 epoch total loss 1.56809092\n",
      "Trained batch 1104 batch loss 1.52228093 epoch total loss 1.56804943\n",
      "Trained batch 1105 batch loss 1.579265 epoch total loss 1.56805956\n",
      "Trained batch 1106 batch loss 1.45405102 epoch total loss 1.56795657\n",
      "Trained batch 1107 batch loss 1.24083447 epoch total loss 1.56766105\n",
      "Trained batch 1108 batch loss 1.33959007 epoch total loss 1.56745517\n",
      "Trained batch 1109 batch loss 1.52624846 epoch total loss 1.56741798\n",
      "Trained batch 1110 batch loss 1.42576671 epoch total loss 1.56729043\n",
      "Trained batch 1111 batch loss 1.50264823 epoch total loss 1.56723225\n",
      "Trained batch 1112 batch loss 1.52401543 epoch total loss 1.56719351\n",
      "Trained batch 1113 batch loss 1.51964343 epoch total loss 1.56715071\n",
      "Trained batch 1114 batch loss 1.5375104 epoch total loss 1.56712413\n",
      "Trained batch 1115 batch loss 1.53919494 epoch total loss 1.56709909\n",
      "Trained batch 1116 batch loss 1.63138294 epoch total loss 1.56715655\n",
      "Trained batch 1117 batch loss 1.5656364 epoch total loss 1.56715524\n",
      "Trained batch 1118 batch loss 1.37064183 epoch total loss 1.56697953\n",
      "Trained batch 1119 batch loss 1.46889067 epoch total loss 1.56689179\n",
      "Trained batch 1120 batch loss 1.56314611 epoch total loss 1.56688845\n",
      "Trained batch 1121 batch loss 1.4960258 epoch total loss 1.56682515\n",
      "Trained batch 1122 batch loss 1.58598971 epoch total loss 1.5668422\n",
      "Trained batch 1123 batch loss 1.63840091 epoch total loss 1.56690598\n",
      "Trained batch 1124 batch loss 1.67317641 epoch total loss 1.56700051\n",
      "Trained batch 1125 batch loss 1.49497724 epoch total loss 1.56693649\n",
      "Trained batch 1126 batch loss 1.46120512 epoch total loss 1.56684256\n",
      "Trained batch 1127 batch loss 1.43797791 epoch total loss 1.56672823\n",
      "Trained batch 1128 batch loss 1.55420947 epoch total loss 1.56671715\n",
      "Trained batch 1129 batch loss 1.55733716 epoch total loss 1.56670892\n",
      "Trained batch 1130 batch loss 1.40799928 epoch total loss 1.56656837\n",
      "Trained batch 1131 batch loss 1.64056849 epoch total loss 1.56663382\n",
      "Trained batch 1132 batch loss 1.67864585 epoch total loss 1.56673276\n",
      "Trained batch 1133 batch loss 1.62512255 epoch total loss 1.56678426\n",
      "Trained batch 1134 batch loss 1.67919326 epoch total loss 1.56688344\n",
      "Trained batch 1135 batch loss 1.57864094 epoch total loss 1.56689382\n",
      "Trained batch 1136 batch loss 1.45063877 epoch total loss 1.56679153\n",
      "Trained batch 1137 batch loss 1.43665957 epoch total loss 1.56667697\n",
      "Trained batch 1138 batch loss 1.58380806 epoch total loss 1.56669211\n",
      "Trained batch 1139 batch loss 1.54937124 epoch total loss 1.56667686\n",
      "Trained batch 1140 batch loss 1.52891302 epoch total loss 1.56664371\n",
      "Trained batch 1141 batch loss 1.56013548 epoch total loss 1.56663811\n",
      "Trained batch 1142 batch loss 1.55553448 epoch total loss 1.56662834\n",
      "Trained batch 1143 batch loss 1.56557906 epoch total loss 1.56662738\n",
      "Trained batch 1144 batch loss 1.505687 epoch total loss 1.56657422\n",
      "Trained batch 1145 batch loss 1.49185836 epoch total loss 1.56650889\n",
      "Trained batch 1146 batch loss 1.55255103 epoch total loss 1.56649673\n",
      "Trained batch 1147 batch loss 1.5535419 epoch total loss 1.5664854\n",
      "Trained batch 1148 batch loss 1.51698101 epoch total loss 1.56644225\n",
      "Trained batch 1149 batch loss 1.49290419 epoch total loss 1.56637836\n",
      "Trained batch 1150 batch loss 1.55842698 epoch total loss 1.56637144\n",
      "Trained batch 1151 batch loss 1.46355271 epoch total loss 1.56628203\n",
      "Trained batch 1152 batch loss 1.45324 epoch total loss 1.56618392\n",
      "Trained batch 1153 batch loss 1.43666792 epoch total loss 1.56607163\n",
      "Trained batch 1154 batch loss 1.4777956 epoch total loss 1.5659951\n",
      "Trained batch 1155 batch loss 1.50612187 epoch total loss 1.56594324\n",
      "Trained batch 1156 batch loss 1.75287545 epoch total loss 1.56610501\n",
      "Trained batch 1157 batch loss 1.68222308 epoch total loss 1.56620538\n",
      "Trained batch 1158 batch loss 1.66365254 epoch total loss 1.56628954\n",
      "Trained batch 1159 batch loss 1.5957191 epoch total loss 1.56631494\n",
      "Trained batch 1160 batch loss 1.65320086 epoch total loss 1.5663898\n",
      "Trained batch 1161 batch loss 1.60604501 epoch total loss 1.56642401\n",
      "Trained batch 1162 batch loss 1.64121938 epoch total loss 1.56648839\n",
      "Trained batch 1163 batch loss 1.50796461 epoch total loss 1.56643808\n",
      "Trained batch 1164 batch loss 1.6331 epoch total loss 1.5664953\n",
      "Trained batch 1165 batch loss 1.62431586 epoch total loss 1.56654489\n",
      "Trained batch 1166 batch loss 1.58887267 epoch total loss 1.56656396\n",
      "Trained batch 1167 batch loss 1.57269847 epoch total loss 1.56656933\n",
      "Trained batch 1168 batch loss 1.55669558 epoch total loss 1.56656086\n",
      "Trained batch 1169 batch loss 1.53487229 epoch total loss 1.5665338\n",
      "Trained batch 1170 batch loss 1.54828811 epoch total loss 1.56651819\n",
      "Trained batch 1171 batch loss 1.54285455 epoch total loss 1.56649804\n",
      "Trained batch 1172 batch loss 1.51489663 epoch total loss 1.56645393\n",
      "Trained batch 1173 batch loss 1.5477221 epoch total loss 1.56643796\n",
      "Trained batch 1174 batch loss 1.50609732 epoch total loss 1.56638658\n",
      "Trained batch 1175 batch loss 1.52599764 epoch total loss 1.56635225\n",
      "Trained batch 1176 batch loss 1.46614122 epoch total loss 1.56626701\n",
      "Trained batch 1177 batch loss 1.48284686 epoch total loss 1.56619608\n",
      "Trained batch 1178 batch loss 1.62276423 epoch total loss 1.56624413\n",
      "Trained batch 1179 batch loss 1.42851031 epoch total loss 1.5661273\n",
      "Trained batch 1180 batch loss 1.3432951 epoch total loss 1.56593847\n",
      "Trained batch 1181 batch loss 1.2250824 epoch total loss 1.56564987\n",
      "Trained batch 1182 batch loss 1.41880178 epoch total loss 1.56552565\n",
      "Trained batch 1183 batch loss 1.54219711 epoch total loss 1.56550598\n",
      "Trained batch 1184 batch loss 1.63979185 epoch total loss 1.56556869\n",
      "Trained batch 1185 batch loss 1.5859102 epoch total loss 1.56558585\n",
      "Trained batch 1186 batch loss 1.55783272 epoch total loss 1.5655793\n",
      "Trained batch 1187 batch loss 1.57154751 epoch total loss 1.5655843\n",
      "Trained batch 1188 batch loss 1.51169944 epoch total loss 1.565539\n",
      "Trained batch 1189 batch loss 1.42858553 epoch total loss 1.56542385\n",
      "Trained batch 1190 batch loss 1.60555327 epoch total loss 1.56545758\n",
      "Trained batch 1191 batch loss 1.45585132 epoch total loss 1.56536555\n",
      "Trained batch 1192 batch loss 1.54875016 epoch total loss 1.56535161\n",
      "Trained batch 1193 batch loss 1.50428748 epoch total loss 1.56530035\n",
      "Trained batch 1194 batch loss 1.51335073 epoch total loss 1.56525683\n",
      "Trained batch 1195 batch loss 1.44738626 epoch total loss 1.56515813\n",
      "Trained batch 1196 batch loss 1.62645578 epoch total loss 1.56520939\n",
      "Trained batch 1197 batch loss 1.57694197 epoch total loss 1.56521916\n",
      "Trained batch 1198 batch loss 1.35441959 epoch total loss 1.56504321\n",
      "Trained batch 1199 batch loss 1.49753249 epoch total loss 1.56498694\n",
      "Trained batch 1200 batch loss 1.53204179 epoch total loss 1.56495941\n",
      "Trained batch 1201 batch loss 1.72595024 epoch total loss 1.56509352\n",
      "Trained batch 1202 batch loss 1.57166457 epoch total loss 1.56509888\n",
      "Trained batch 1203 batch loss 1.52725089 epoch total loss 1.56506741\n",
      "Trained batch 1204 batch loss 1.5668273 epoch total loss 1.56506884\n",
      "Trained batch 1205 batch loss 1.56489897 epoch total loss 1.56506872\n",
      "Trained batch 1206 batch loss 1.50356722 epoch total loss 1.5650177\n",
      "Trained batch 1207 batch loss 1.48142838 epoch total loss 1.56494844\n",
      "Trained batch 1208 batch loss 1.49020064 epoch total loss 1.56488669\n",
      "Trained batch 1209 batch loss 1.52555835 epoch total loss 1.56485403\n",
      "Trained batch 1210 batch loss 1.56712413 epoch total loss 1.56485593\n",
      "Trained batch 1211 batch loss 1.61492276 epoch total loss 1.5648973\n",
      "Trained batch 1212 batch loss 1.58703589 epoch total loss 1.56491554\n",
      "Trained batch 1213 batch loss 1.59247088 epoch total loss 1.56493831\n",
      "Trained batch 1214 batch loss 1.54476047 epoch total loss 1.56492174\n",
      "Trained batch 1215 batch loss 1.55507779 epoch total loss 1.56491363\n",
      "Trained batch 1216 batch loss 1.61534059 epoch total loss 1.56495512\n",
      "Trained batch 1217 batch loss 1.62240028 epoch total loss 1.56500232\n",
      "Trained batch 1218 batch loss 1.64749599 epoch total loss 1.56507\n",
      "Trained batch 1219 batch loss 1.62422657 epoch total loss 1.56511855\n",
      "Trained batch 1220 batch loss 1.59995246 epoch total loss 1.56514716\n",
      "Trained batch 1221 batch loss 1.61808586 epoch total loss 1.56519043\n",
      "Trained batch 1222 batch loss 1.63965058 epoch total loss 1.56525135\n",
      "Trained batch 1223 batch loss 1.61422193 epoch total loss 1.5652914\n",
      "Trained batch 1224 batch loss 1.57774019 epoch total loss 1.56530166\n",
      "Trained batch 1225 batch loss 1.55011332 epoch total loss 1.56528926\n",
      "Trained batch 1226 batch loss 1.54912007 epoch total loss 1.56527603\n",
      "Trained batch 1227 batch loss 1.57927275 epoch total loss 1.56528747\n",
      "Trained batch 1228 batch loss 1.57153356 epoch total loss 1.56529248\n",
      "Trained batch 1229 batch loss 1.53296125 epoch total loss 1.56526625\n",
      "Trained batch 1230 batch loss 1.52461219 epoch total loss 1.56523323\n",
      "Trained batch 1231 batch loss 1.52982879 epoch total loss 1.56520438\n",
      "Trained batch 1232 batch loss 1.54940152 epoch total loss 1.56519163\n",
      "Trained batch 1233 batch loss 1.59367871 epoch total loss 1.56521463\n",
      "Trained batch 1234 batch loss 1.55688846 epoch total loss 1.56520796\n",
      "Trained batch 1235 batch loss 1.61305904 epoch total loss 1.56524658\n",
      "Trained batch 1236 batch loss 1.49139726 epoch total loss 1.56518698\n",
      "Trained batch 1237 batch loss 1.65216136 epoch total loss 1.56525731\n",
      "Trained batch 1238 batch loss 1.4789803 epoch total loss 1.56518757\n",
      "Trained batch 1239 batch loss 1.5398221 epoch total loss 1.56516707\n",
      "Trained batch 1240 batch loss 1.54282069 epoch total loss 1.56514907\n",
      "Trained batch 1241 batch loss 1.57010233 epoch total loss 1.56515312\n",
      "Trained batch 1242 batch loss 1.58721185 epoch total loss 1.56517076\n",
      "Trained batch 1243 batch loss 1.61508107 epoch total loss 1.56521094\n",
      "Trained batch 1244 batch loss 1.5497179 epoch total loss 1.56519854\n",
      "Trained batch 1245 batch loss 1.61029744 epoch total loss 1.56523478\n",
      "Trained batch 1246 batch loss 1.54094 epoch total loss 1.56521523\n",
      "Trained batch 1247 batch loss 1.59313393 epoch total loss 1.56523764\n",
      "Trained batch 1248 batch loss 1.48468041 epoch total loss 1.56517315\n",
      "Trained batch 1249 batch loss 1.4945879 epoch total loss 1.56511664\n",
      "Trained batch 1250 batch loss 1.47400463 epoch total loss 1.56504381\n",
      "Trained batch 1251 batch loss 1.5696367 epoch total loss 1.56504738\n",
      "Trained batch 1252 batch loss 1.44621038 epoch total loss 1.56495237\n",
      "Trained batch 1253 batch loss 1.49055827 epoch total loss 1.56489313\n",
      "Trained batch 1254 batch loss 1.59328675 epoch total loss 1.56491566\n",
      "Trained batch 1255 batch loss 1.55087662 epoch total loss 1.56490457\n",
      "Trained batch 1256 batch loss 1.40629268 epoch total loss 1.56477821\n",
      "Trained batch 1257 batch loss 1.58639383 epoch total loss 1.56479549\n",
      "Trained batch 1258 batch loss 1.49045622 epoch total loss 1.56473637\n",
      "Trained batch 1259 batch loss 1.53168392 epoch total loss 1.56471014\n",
      "Trained batch 1260 batch loss 1.47875929 epoch total loss 1.56464195\n",
      "Trained batch 1261 batch loss 1.40943277 epoch total loss 1.56451881\n",
      "Trained batch 1262 batch loss 1.34127295 epoch total loss 1.56434202\n",
      "Trained batch 1263 batch loss 1.46845615 epoch total loss 1.56426609\n",
      "Trained batch 1264 batch loss 1.52113247 epoch total loss 1.56423199\n",
      "Trained batch 1265 batch loss 1.39482498 epoch total loss 1.564098\n",
      "Trained batch 1266 batch loss 1.48628306 epoch total loss 1.56403661\n",
      "Trained batch 1267 batch loss 1.5449667 epoch total loss 1.56402147\n",
      "Trained batch 1268 batch loss 1.52014494 epoch total loss 1.5639869\n",
      "Trained batch 1269 batch loss 1.53979564 epoch total loss 1.56396782\n",
      "Trained batch 1270 batch loss 1.47771096 epoch total loss 1.56389987\n",
      "Trained batch 1271 batch loss 1.51788354 epoch total loss 1.56386375\n",
      "Trained batch 1272 batch loss 1.55796206 epoch total loss 1.56385911\n",
      "Trained batch 1273 batch loss 1.3699162 epoch total loss 1.56370676\n",
      "Trained batch 1274 batch loss 1.24540579 epoch total loss 1.56345677\n",
      "Trained batch 1275 batch loss 1.3810811 epoch total loss 1.56331384\n",
      "Trained batch 1276 batch loss 1.4113816 epoch total loss 1.56319475\n",
      "Trained batch 1277 batch loss 1.67009532 epoch total loss 1.56327844\n",
      "Trained batch 1278 batch loss 1.73611069 epoch total loss 1.56341362\n",
      "Trained batch 1279 batch loss 1.68503022 epoch total loss 1.56350875\n",
      "Trained batch 1280 batch loss 1.61149216 epoch total loss 1.56354618\n",
      "Trained batch 1281 batch loss 1.60858524 epoch total loss 1.56358135\n",
      "Trained batch 1282 batch loss 1.63333344 epoch total loss 1.56363583\n",
      "Trained batch 1283 batch loss 1.65536106 epoch total loss 1.56370723\n",
      "Trained batch 1284 batch loss 1.55949688 epoch total loss 1.56370401\n",
      "Trained batch 1285 batch loss 1.62225831 epoch total loss 1.56374955\n",
      "Trained batch 1286 batch loss 1.53905082 epoch total loss 1.56373036\n",
      "Trained batch 1287 batch loss 1.6060617 epoch total loss 1.56376326\n",
      "Trained batch 1288 batch loss 1.52643907 epoch total loss 1.56373429\n",
      "Trained batch 1289 batch loss 1.60371 epoch total loss 1.56376541\n",
      "Trained batch 1290 batch loss 1.60559511 epoch total loss 1.56379783\n",
      "Trained batch 1291 batch loss 1.57074332 epoch total loss 1.5638032\n",
      "Trained batch 1292 batch loss 1.54381812 epoch total loss 1.56378782\n",
      "Trained batch 1293 batch loss 1.57788825 epoch total loss 1.56379867\n",
      "Trained batch 1294 batch loss 1.51310325 epoch total loss 1.56375945\n",
      "Trained batch 1295 batch loss 1.58078492 epoch total loss 1.56377268\n",
      "Trained batch 1296 batch loss 1.67463541 epoch total loss 1.56385827\n",
      "Trained batch 1297 batch loss 1.47548914 epoch total loss 1.56379008\n",
      "Trained batch 1298 batch loss 1.62597239 epoch total loss 1.56383801\n",
      "Trained batch 1299 batch loss 1.5541476 epoch total loss 1.56383061\n",
      "Trained batch 1300 batch loss 1.54338813 epoch total loss 1.56381476\n",
      "Trained batch 1301 batch loss 1.5342741 epoch total loss 1.56379211\n",
      "Trained batch 1302 batch loss 1.49167299 epoch total loss 1.56373668\n",
      "Trained batch 1303 batch loss 1.40814269 epoch total loss 1.56361735\n",
      "Trained batch 1304 batch loss 1.55352426 epoch total loss 1.5636096\n",
      "Trained batch 1305 batch loss 1.55766189 epoch total loss 1.56360495\n",
      "Trained batch 1306 batch loss 1.59849513 epoch total loss 1.56363177\n",
      "Trained batch 1307 batch loss 1.57187796 epoch total loss 1.56363809\n",
      "Trained batch 1308 batch loss 1.43927646 epoch total loss 1.56354296\n",
      "Trained batch 1309 batch loss 1.42355466 epoch total loss 1.56343603\n",
      "Trained batch 1310 batch loss 1.54130459 epoch total loss 1.5634191\n",
      "Trained batch 1311 batch loss 1.57413542 epoch total loss 1.56342745\n",
      "Trained batch 1312 batch loss 1.55440164 epoch total loss 1.56342053\n",
      "Trained batch 1313 batch loss 1.46632814 epoch total loss 1.56334662\n",
      "Trained batch 1314 batch loss 1.52749813 epoch total loss 1.56331933\n",
      "Trained batch 1315 batch loss 1.52678609 epoch total loss 1.56329167\n",
      "Trained batch 1316 batch loss 1.55943918 epoch total loss 1.56328869\n",
      "Trained batch 1317 batch loss 1.60095239 epoch total loss 1.5633173\n",
      "Trained batch 1318 batch loss 1.54218137 epoch total loss 1.56330132\n",
      "Trained batch 1319 batch loss 1.53086925 epoch total loss 1.56327665\n",
      "Trained batch 1320 batch loss 1.48915887 epoch total loss 1.56322062\n",
      "Trained batch 1321 batch loss 1.49061155 epoch total loss 1.56316566\n",
      "Trained batch 1322 batch loss 1.50495517 epoch total loss 1.56312156\n",
      "Trained batch 1323 batch loss 1.54383099 epoch total loss 1.56310713\n",
      "Trained batch 1324 batch loss 1.55027151 epoch total loss 1.56309748\n",
      "Trained batch 1325 batch loss 1.5527482 epoch total loss 1.56308961\n",
      "Trained batch 1326 batch loss 1.49297893 epoch total loss 1.56303668\n",
      "Trained batch 1327 batch loss 1.55430925 epoch total loss 1.56303\n",
      "Trained batch 1328 batch loss 1.55266356 epoch total loss 1.56302226\n",
      "Trained batch 1329 batch loss 1.48015857 epoch total loss 1.56296\n",
      "Trained batch 1330 batch loss 1.53707373 epoch total loss 1.5629406\n",
      "Trained batch 1331 batch loss 1.39276087 epoch total loss 1.56281269\n",
      "Trained batch 1332 batch loss 1.54854715 epoch total loss 1.56280208\n",
      "Trained batch 1333 batch loss 1.39321136 epoch total loss 1.56267488\n",
      "Trained batch 1334 batch loss 1.53386354 epoch total loss 1.56265342\n",
      "Trained batch 1335 batch loss 1.68350375 epoch total loss 1.5627439\n",
      "Trained batch 1336 batch loss 1.61967278 epoch total loss 1.56278658\n",
      "Trained batch 1337 batch loss 1.53116453 epoch total loss 1.56276298\n",
      "Trained batch 1338 batch loss 1.46875453 epoch total loss 1.56269264\n",
      "Trained batch 1339 batch loss 1.40347219 epoch total loss 1.56257379\n",
      "Trained batch 1340 batch loss 1.47991896 epoch total loss 1.56251216\n",
      "Trained batch 1341 batch loss 1.50150788 epoch total loss 1.56246674\n",
      "Trained batch 1342 batch loss 1.46081185 epoch total loss 1.5623908\n",
      "Trained batch 1343 batch loss 1.54875922 epoch total loss 1.56238079\n",
      "Trained batch 1344 batch loss 1.56805539 epoch total loss 1.56238496\n",
      "Trained batch 1345 batch loss 1.46101713 epoch total loss 1.56230962\n",
      "Trained batch 1346 batch loss 1.56179464 epoch total loss 1.56230915\n",
      "Trained batch 1347 batch loss 1.5232724 epoch total loss 1.56228018\n",
      "Trained batch 1348 batch loss 1.52279091 epoch total loss 1.56225073\n",
      "Trained batch 1349 batch loss 1.58406889 epoch total loss 1.56226695\n",
      "Trained batch 1350 batch loss 1.62099898 epoch total loss 1.56231046\n",
      "Trained batch 1351 batch loss 1.55425024 epoch total loss 1.5623045\n",
      "Trained batch 1352 batch loss 1.58683026 epoch total loss 1.56232262\n",
      "Trained batch 1353 batch loss 1.55444622 epoch total loss 1.56231689\n",
      "Trained batch 1354 batch loss 1.58738875 epoch total loss 1.56233537\n",
      "Trained batch 1355 batch loss 1.61156535 epoch total loss 1.56237173\n",
      "Trained batch 1356 batch loss 1.56295395 epoch total loss 1.56237221\n",
      "Trained batch 1357 batch loss 1.55467939 epoch total loss 1.56236649\n",
      "Trained batch 1358 batch loss 1.55780602 epoch total loss 1.56236315\n",
      "Trained batch 1359 batch loss 1.49951315 epoch total loss 1.56231689\n",
      "Trained batch 1360 batch loss 1.47037649 epoch total loss 1.56224942\n",
      "Trained batch 1361 batch loss 1.4895463 epoch total loss 1.5621959\n",
      "Trained batch 1362 batch loss 1.66113508 epoch total loss 1.56226861\n",
      "Trained batch 1363 batch loss 1.61003804 epoch total loss 1.56230366\n",
      "Trained batch 1364 batch loss 1.61112106 epoch total loss 1.56233943\n",
      "Trained batch 1365 batch loss 1.46021676 epoch total loss 1.56226468\n",
      "Trained batch 1366 batch loss 1.49189258 epoch total loss 1.56221318\n",
      "Trained batch 1367 batch loss 1.43538833 epoch total loss 1.56212032\n",
      "Trained batch 1368 batch loss 1.54166174 epoch total loss 1.56210542\n",
      "Trained batch 1369 batch loss 1.55951428 epoch total loss 1.56210351\n",
      "Trained batch 1370 batch loss 1.60821867 epoch total loss 1.56213713\n",
      "Trained batch 1371 batch loss 1.59562624 epoch total loss 1.56216168\n",
      "Trained batch 1372 batch loss 1.59502447 epoch total loss 1.56218553\n",
      "Trained batch 1373 batch loss 1.5479213 epoch total loss 1.56217515\n",
      "Trained batch 1374 batch loss 1.46550357 epoch total loss 1.56210482\n",
      "Trained batch 1375 batch loss 1.55228758 epoch total loss 1.56209767\n",
      "Trained batch 1376 batch loss 1.55081272 epoch total loss 1.56208944\n",
      "Trained batch 1377 batch loss 1.54492021 epoch total loss 1.56207693\n",
      "Trained batch 1378 batch loss 1.57564616 epoch total loss 1.56208682\n",
      "Trained batch 1379 batch loss 1.53463745 epoch total loss 1.56206691\n",
      "Trained batch 1380 batch loss 1.42390156 epoch total loss 1.56196678\n",
      "Trained batch 1381 batch loss 1.41152644 epoch total loss 1.56185794\n",
      "Trained batch 1382 batch loss 1.38136911 epoch total loss 1.56172729\n",
      "Trained batch 1383 batch loss 1.36199021 epoch total loss 1.56158292\n",
      "Trained batch 1384 batch loss 1.45659137 epoch total loss 1.56150699\n",
      "Trained batch 1385 batch loss 1.60584068 epoch total loss 1.56153917\n",
      "Trained batch 1386 batch loss 1.54796815 epoch total loss 1.56152928\n",
      "Trained batch 1387 batch loss 1.70650041 epoch total loss 1.56163383\n",
      "Trained batch 1388 batch loss 1.66654277 epoch total loss 1.5617094\n",
      "Epoch 5 val loss 1.574467658996582\n",
      "Model ./model_hourglass-epoch-5-loss-1.5745.h5 saved.\n",
      "Start epoch 6 with learning rate 0.5\n",
      "Start distributed traininng...\n",
      "Trained batch 1 batch loss 1.56822681 epoch total loss 1.56822681\n",
      "Trained batch 2 batch loss 1.50643897 epoch total loss 1.53733289\n",
      "Trained batch 3 batch loss 1.54798281 epoch total loss 1.54088295\n",
      "Trained batch 4 batch loss 1.4862783 epoch total loss 1.52723169\n",
      "Trained batch 5 batch loss 1.63440776 epoch total loss 1.54866695\n",
      "Trained batch 6 batch loss 1.56988466 epoch total loss 1.55220318\n",
      "Trained batch 7 batch loss 1.61215401 epoch total loss 1.56076753\n",
      "Trained batch 8 batch loss 1.60343897 epoch total loss 1.56610155\n",
      "Trained batch 9 batch loss 1.66338968 epoch total loss 1.57691133\n",
      "Trained batch 10 batch loss 1.51513338 epoch total loss 1.57073343\n",
      "Trained batch 11 batch loss 1.58403361 epoch total loss 1.57194257\n",
      "Trained batch 12 batch loss 1.51411653 epoch total loss 1.56712377\n",
      "Trained batch 13 batch loss 1.45416069 epoch total loss 1.55843425\n",
      "Trained batch 14 batch loss 1.44439459 epoch total loss 1.55028856\n",
      "Trained batch 15 batch loss 1.57228065 epoch total loss 1.55175471\n",
      "Trained batch 16 batch loss 1.48351479 epoch total loss 1.54748976\n",
      "Trained batch 17 batch loss 1.63352466 epoch total loss 1.55255055\n",
      "Trained batch 18 batch loss 1.50771701 epoch total loss 1.5500598\n",
      "Trained batch 19 batch loss 1.47333038 epoch total loss 1.54602146\n",
      "Trained batch 20 batch loss 1.45442963 epoch total loss 1.5414418\n",
      "Trained batch 21 batch loss 1.51486611 epoch total loss 1.54017639\n",
      "Trained batch 22 batch loss 1.55467761 epoch total loss 1.5408355\n",
      "Trained batch 23 batch loss 1.59559667 epoch total loss 1.54321635\n",
      "Trained batch 24 batch loss 1.5888598 epoch total loss 1.54511821\n",
      "Trained batch 25 batch loss 1.61853325 epoch total loss 1.54805481\n",
      "Trained batch 26 batch loss 1.7765584 epoch total loss 1.5568434\n",
      "Trained batch 27 batch loss 1.66395581 epoch total loss 1.56081057\n",
      "Trained batch 28 batch loss 1.69292104 epoch total loss 1.56552875\n",
      "Trained batch 29 batch loss 1.52560139 epoch total loss 1.56415188\n",
      "Trained batch 30 batch loss 1.54791057 epoch total loss 1.56361043\n",
      "Trained batch 31 batch loss 1.46689153 epoch total loss 1.56049049\n",
      "Trained batch 32 batch loss 1.46025467 epoch total loss 1.55735815\n",
      "Trained batch 33 batch loss 1.57465363 epoch total loss 1.55788231\n",
      "Trained batch 34 batch loss 1.59125876 epoch total loss 1.55886388\n",
      "Trained batch 35 batch loss 1.51660109 epoch total loss 1.55765641\n",
      "Trained batch 36 batch loss 1.49576175 epoch total loss 1.55593717\n",
      "Trained batch 37 batch loss 1.44761813 epoch total loss 1.55300951\n",
      "Trained batch 38 batch loss 1.5431453 epoch total loss 1.55275\n",
      "Trained batch 1191 batch loss 2489.13428 epoch total loss 421.4039\n",
      "Trained batch 1192 batch loss 2495.57959 epoch total loss 423.143982\n",
      "Trained batch 1193 batch loss 2409.05029 epoch total loss 424.808624\n",
      "Trained batch 1194 batch loss 2306.11084 epoch total loss 426.384277\n",
      "Trained batch 1195 batch loss 2298.40332 epoch total loss 427.950806\n",
      "Trained batch 1196 batch loss 2476.76294 epoch total loss 429.663849\n",
      "Trained batch 1197 batch loss 2527.73291 epoch total loss 431.416626\n",
      "Trained batch 1198 batch loss 2493.66846 epoch total loss 433.138031\n",
      "Trained batch 1199 batch loss 2514.90283 epoch total loss 434.874268\n",
      "Trained batch 1200 batch loss 2448.97778 epoch total loss 436.552673\n",
      "Trained batch 1201 batch loss 2454.95312 epoch total loss 438.233307\n",
      "Trained batch 1202 batch loss 2500.66235 epoch total loss 439.949158\n",
      "Trained batch 1203 batch loss 2562.81152 epoch total loss 441.713776\n",
      "Trained batch 1204 batch loss 2563.19189 epoch total loss 443.4758\n",
      "Trained batch 1205 batch loss 2510.52148 epoch total loss 445.191193\n",
      "Trained batch 1206 batch loss 2560.13867 epoch total loss 446.944855\n",
      "Trained batch 1207 batch loss 2351.16821 epoch total loss 448.522522\n",
      "Trained batch 1208 batch loss 2573.32764 epoch total loss 450.281464\n",
      "Trained batch 1209 batch loss 2575.37573 epoch total loss 452.039185\n",
      "Trained batch 1210 batch loss 2603.19556 epoch total loss 453.817\n",
      "Trained batch 1211 batch loss 2584.03638 epoch total loss 455.57608\n",
      "Trained batch 1212 batch loss 2608.51514 epoch total loss 457.352417\n",
      "Trained batch 1213 batch loss 2626.94824 epoch total loss 459.141022\n",
      "Trained batch 1214 batch loss 2609.53784 epoch total loss 460.912384\n",
      "Trained batch 1215 batch loss 2622.99707 epoch total loss 462.691864\n",
      "Trained batch 1216 batch loss 2554.68677 epoch total loss 464.412262\n",
      "Trained batch 1217 batch loss 2500.84058 epoch total loss 466.085571\n",
      "Trained batch 1218 batch loss 2557.37549 epoch total loss 467.802551\n",
      "Trained batch 1219 batch loss 2561.34717 epoch total loss 469.52\n",
      "Trained batch 1220 batch loss 2474.90088 epoch total loss 471.163727\n",
      "Trained batch 1221 batch loss 2566.1189 epoch total loss 472.879517\n",
      "Trained batch 1222 batch loss 2548.20386 epoch total loss 474.577789\n",
      "Trained batch 1223 batch loss 2488.62134 epoch total loss 476.224609\n",
      "Trained batch 1224 batch loss 2640.55981 epoch total loss 477.992859\n",
      "Trained batch 1225 batch loss 2663.73145 epoch total loss 479.77713\n",
      "Trained batch 1226 batch loss 2654.68091 epoch total loss 481.551117\n",
      "Trained batch 1227 batch loss 2607.37402 epoch total loss 483.283661\n",
      "Trained batch 1228 batch loss 2575.03687 epoch total loss 484.987061\n",
      "Trained batch 1229 batch loss 2625.8833 epoch total loss 486.729034\n",
      "Trained batch 1230 batch loss 2667.92578 epoch total loss 488.50238\n",
      "Trained batch 1231 batch loss 2625.19238 epoch total loss 490.238129\n",
      "Trained batch 1232 batch loss 2606.56689 epoch total loss 491.955902\n",
      "Trained batch 1233 batch loss 2638.62256 epoch total loss 493.69693\n",
      "Trained batch 1234 batch loss 2662.84766 epoch total loss 495.454773\n",
      "Trained batch 1235 batch loss 2669.98462 epoch total loss 497.215546\n",
      "Trained batch 1236 batch loss 2691.92212 epoch total loss 498.991211\n",
      "Trained batch 1237 batch loss 2690.49048 epoch total loss 500.762848\n",
      "Trained batch 1238 batch loss 2692.21387 epoch total loss 502.532959\n",
      "Trained batch 1239 batch loss 2685.95386 epoch total loss 504.295197\n",
      "Trained batch 1240 batch loss 2668.27588 epoch total loss 506.040314\n",
      "Trained batch 1241 batch loss 2679.70117 epoch total loss 507.79184\n",
      "Trained batch 1242 batch loss 2706.52734 epoch total loss 509.562134\n",
      "Trained batch 1243 batch loss 2704.55786 epoch total loss 511.328033\n",
      "Trained batch 1244 batch loss 2649.78 epoch total loss 513.047\n",
      "Trained batch 1245 batch loss 2713.85254 epoch total loss 514.814758\n",
      "Trained batch 1246 batch loss 2712.42 epoch total loss 516.578491\n",
      "Trained batch 1247 batch loss 2663.76807 epoch total loss 518.300354\n",
      "Trained batch 1248 batch loss 2706.87549 epoch total loss 520.054\n",
      "Trained batch 1249 batch loss 2708.552 epoch total loss 521.806274\n",
      "Trained batch 1250 batch loss 2694.26074 epoch total loss 523.544189\n",
      "Trained batch 1251 batch loss 2610.13452 epoch total loss 525.212158\n",
      "Trained batch 1252 batch loss 2647.2124 epoch total loss 526.907\n",
      "Trained batch 1253 batch loss 2702.33862 epoch total loss 528.643127\n",
      "Trained batch 1254 batch loss 2685.33081 epoch total loss 530.363\n",
      "Trained batch 1255 batch loss 2686.37939 epoch total loss 532.080933\n",
      "Trained batch 1256 batch loss 2708.50073 epoch total loss 533.813721\n",
      "Trained batch 1257 batch loss 2647.31226 epoch total loss 535.495117\n",
      "Trained batch 1258 batch loss 2691.47534 epoch total loss 537.209\n",
      "Trained batch 1259 batch loss 2672.74243 epoch total loss 538.905212\n",
      "Trained batch 1260 batch loss 2650.43384 epoch total loss 540.581\n",
      "Trained batch 1261 batch loss 2677.92383 epoch total loss 542.276\n",
      "Trained batch 1262 batch loss 2720.90259 epoch total loss 544.002258\n",
      "Trained batch 1263 batch loss 2667.198 epoch total loss 545.68335\n",
      "Trained batch 1264 batch loss 2684.42114 epoch total loss 547.375366\n",
      "Trained batch 1265 batch loss 2673.57275 epoch total loss 549.056152\n",
      "Trained batch 1266 batch loss 2594.9939 epoch total loss 550.672241\n",
      "Trained batch 1267 batch loss 2636.86401 epoch total loss 552.318787\n",
      "Trained batch 1268 batch loss 2684.36426 epoch total loss 554.000244\n",
      "Trained batch 1269 batch loss 2716.06226 epoch total loss 555.704\n",
      "Trained batch 1270 batch loss 2690.86206 epoch total loss 557.385254\n",
      "Trained batch 1271 batch loss 2685.14062 epoch total loss 559.059326\n",
      "Trained batch 1272 batch loss 2683.36279 epoch total loss 560.72937\n",
      "Trained batch 1273 batch loss 2604.04053 epoch total loss 562.334473\n",
      "Trained batch 1274 batch loss 2692.05688 epoch total loss 564.006165\n",
      "Trained batch 1275 batch loss 2734.00195 epoch total loss 565.70813\n",
      "Trained batch 1276 batch loss 2725.95435 epoch total loss 567.401123\n",
      "Trained batch 1277 batch loss 2612.07056 epoch total loss 569.002258\n",
      "Trained batch 1278 batch loss 2646.5896 epoch total loss 570.627869\n",
      "Trained batch 1279 batch loss 2715.55957 epoch total loss 572.304932\n",
      "Trained batch 1280 batch loss 2650.51074 epoch total loss 573.928528\n",
      "Trained batch 1281 batch loss 2664.28833 epoch total loss 575.560364\n",
      "Trained batch 1282 batch loss 2740.30371 epoch total loss 577.248901\n",
      "Trained batch 1283 batch loss 2738.78345 epoch total loss 578.933716\n",
      "Trained batch 1284 batch loss 2748.22095 epoch total loss 580.623169\n",
      "Trained batch 1285 batch loss 2745.98242 epoch total loss 582.308289\n",
      "Trained batch 1286 batch loss 2713.87646 epoch total loss 583.96582\n",
      "Trained batch 1287 batch loss 2742.85425 epoch total loss 585.643311\n",
      "Trained batch 1288 batch loss 2732.57715 epoch total loss 587.310181\n",
      "Trained batch 1289 batch loss 2711.00952 epoch total loss 588.957703\n",
      "Trained batch 1290 batch loss 2753.8623 epoch total loss 590.635925\n",
      "Trained batch 1291 batch loss 2720.58057 epoch total loss 592.285767\n",
      "Trained batch 1292 batch loss 2709.02124 epoch total loss 593.924072\n",
      "Trained batch 1293 batch loss 2707.39453 epoch total loss 595.558655\n",
      "Trained batch 1294 batch loss 2674.29883 epoch total loss 597.1651\n",
      "Trained batch 1295 batch loss 2605.15527 epoch total loss 598.715637\n",
      "Trained batch 1296 batch loss 2661.47 epoch total loss 600.307312\n",
      "Trained batch 1297 batch loss 2711.0874 epoch total loss 601.934692\n",
      "Trained batch 1298 batch loss 2743.79248 epoch total loss 603.584839\n",
      "Trained batch 1299 batch loss 2696.55762 epoch total loss 605.196045\n",
      "Trained batch 1300 batch loss 2643.60254 epoch total loss 606.764099\n",
      "Trained batch 1301 batch loss 2640.1687 epoch total loss 608.327\n",
      "Trained batch 1302 batch loss 2575.01123 epoch total loss 609.837585\n",
      "Trained batch 1303 batch loss 2604.43384 epoch total loss 611.368347\n",
      "Trained batch 1304 batch loss 2709.7312 epoch total loss 612.977539\n",
      "Trained batch 1305 batch loss 2692.83105 epoch total loss 614.571289\n",
      "Trained batch 1306 batch loss 2718.77393 epoch total loss 616.182434\n",
      "Trained batch 1307 batch loss 2663.44604 epoch total loss 617.748779\n",
      "Trained batch 1308 batch loss 2666.31616 epoch total loss 619.315\n",
      "Trained batch 1309 batch loss 2684.42651 epoch total loss 620.892639\n",
      "Trained batch 1310 batch loss 2659.09204 epoch total loss 622.448486\n",
      "Trained batch 1311 batch loss 2675.54736 epoch total loss 624.014526\n",
      "Trained batch 1312 batch loss 2703.71582 epoch total loss 625.59967\n",
      "Trained batch 1313 batch loss 2635.02051 epoch total loss 627.130066\n",
      "Trained batch 1314 batch loss 2652.91089 epoch total loss 628.671753\n",
      "Trained batch 1315 batch loss 2608.25391 epoch total loss 630.177124\n",
      "Trained batch 1316 batch loss 2606.77954 epoch total loss 631.679077\n",
      "Trained batch 1317 batch loss 2690.23779 epoch total loss 633.242188\n",
      "Trained batch 1318 batch loss 2635.13184 epoch total loss 634.761047\n",
      "Trained batch 1319 batch loss 2675.87 epoch total loss 636.308533\n",
      "Trained batch 1320 batch loss 2698.17944 epoch total loss 637.870544\n",
      "Trained batch 1321 batch loss 2731.06812 epoch total loss 639.455078\n",
      "Trained batch 1322 batch loss 2710.24487 epoch total loss 641.021484\n",
      "Trained batch 1323 batch loss 2715.44238 epoch total loss 642.589478\n",
      "Trained batch 1324 batch loss 2716.7666 epoch total loss 644.156067\n",
      "Trained batch 1325 batch loss 2666.73633 epoch total loss 645.682556\n",
      "Trained batch 1326 batch loss 2687.09 epoch total loss 647.222046\n",
      "Trained batch 1327 batch loss 2612.06348 epoch total loss 648.702698\n",
      "Trained batch 1328 batch loss 2607.93042 epoch total loss 650.17804\n",
      "Trained batch 1329 batch loss 2642.41748 epoch total loss 651.677124\n",
      "Trained batch 1330 batch loss 2595.31909 epoch total loss 653.138489\n",
      "Trained batch 1331 batch loss 2593.06934 epoch total loss 654.596\n",
      "Trained batch 1332 batch loss 2561.8728 epoch total loss 656.027893\n",
      "Trained batch 1333 batch loss 2602.61206 epoch total loss 657.488159\n",
      "Trained batch 1334 batch loss 2654.6145 epoch total loss 658.985291\n",
      "Trained batch 1335 batch loss 2616.40967 epoch total loss 660.451538\n",
      "Trained batch 1336 batch loss 2644.28125 epoch total loss 661.936462\n",
      "Trained batch 1337 batch loss 2694.79541 epoch total loss 663.457\n",
      "Trained batch 1338 batch loss 2681.65039 epoch total loss 664.965271\n",
      "Trained batch 1339 batch loss 2622.93408 epoch total loss 666.427551\n",
      "Trained batch 1340 batch loss 2611.25244 epoch total loss 667.878906\n",
      "Trained batch 1341 batch loss 2688.45605 epoch total loss 669.385681\n",
      "Trained batch 1342 batch loss 2699.67627 epoch total loss 670.89856\n",
      "Trained batch 1343 batch loss 2709.64087 epoch total loss 672.416626\n",
      "Trained batch 1344 batch loss 2727.82446 epoch total loss 673.945923\n",
      "Trained batch 1345 batch loss 2730.30151 epoch total loss 675.474792\n",
      "Trained batch 1346 batch loss 2727.67041 epoch total loss 676.999512\n",
      "Trained batch 1347 batch loss 2605.13623 epoch total loss 678.430908\n",
      "Trained batch 1348 batch loss 2703.9917 epoch total loss 679.933533\n",
      "Trained batch 1349 batch loss 2671.6748 epoch total loss 681.410034\n",
      "Trained batch 1350 batch loss 2647.5813 epoch total loss 682.866455\n",
      "Trained batch 1351 batch loss 2725.80444 epoch total loss 684.378601\n",
      "Trained batch 1352 batch loss 2699.51074 epoch total loss 685.86908\n",
      "Trained batch 1353 batch loss 2644.8064 epoch total loss 687.316956\n",
      "Trained batch 1354 batch loss 2670.94629 epoch total loss 688.781921\n",
      "Trained batch 1355 batch loss 2704.54736 epoch total loss 690.269592\n",
      "Trained batch 1356 batch loss 2678.60205 epoch total loss 691.735962\n",
      "Trained batch 1357 batch loss 2655.21533 epoch total loss 693.182861\n",
      "Trained batch 1358 batch loss 2685.08936 epoch total loss 694.649597\n",
      "Trained batch 1359 batch loss 2681.21021 epoch total loss 696.111389\n",
      "Trained batch 1360 batch loss 2702.76123 epoch total loss 697.586853\n",
      "Trained batch 1361 batch loss 2703.44434 epoch total loss 699.060669\n",
      "Trained batch 1362 batch loss 2670.3562 epoch total loss 700.508057\n",
      "Trained batch 1363 batch loss 2650.23877 epoch total loss 701.938538\n",
      "Trained batch 1364 batch loss 2654.84448 epoch total loss 703.3703\n",
      "Trained batch 1365 batch loss 2672.84644 epoch total loss 704.813171\n",
      "Trained batch 1366 batch loss 2664.0105 epoch total loss 706.247375\n",
      "Trained batch 1367 batch loss 2515.49023 epoch total loss 707.570923\n",
      "Trained batch 1368 batch loss 2611.4668 epoch total loss 708.962646\n",
      "Trained batch 1369 batch loss 2646.78247 epoch total loss 710.378174\n",
      "Trained batch 1370 batch loss 2663.89575 epoch total loss 711.804077\n",
      "Trained batch 1371 batch loss 2632.3125 epoch total loss 713.204895\n",
      "Trained batch 1372 batch loss 2664.53906 epoch total loss 714.627136\n",
      "Trained batch 1373 batch loss 2642.65186 epoch total loss 716.031372\n",
      "Trained batch 1374 batch loss 2610.67725 epoch total loss 717.410278\n",
      "Trained batch 1375 batch loss 2626.34985 epoch total loss 718.798645\n",
      "Trained batch 1376 batch loss 2667.29492 epoch total loss 720.214722\n",
      "Trained batch 1377 batch loss 2636.92944 epoch total loss 721.606689\n",
      "Trained batch 1378 batch loss 2646.24731 epoch total loss 723.003357\n",
      "Trained batch 1379 batch loss 2629.98315 epoch total loss 724.38623\n",
      "Trained batch 1380 batch loss 2541.0979 epoch total loss 725.702698\n",
      "Trained batch 1381 batch loss 2534.95117 epoch total loss 727.012817\n",
      "Trained batch 1382 batch loss 2575.04858 epoch total loss 728.350037\n",
      "Trained batch 1383 batch loss 2557.83789 epoch total loss 729.672852\n",
      "Trained batch 1384 batch loss 2603.94971 epoch total loss 731.0271\n",
      "Trained batch 1385 batch loss 2607.11475 epoch total loss 732.381653\n",
      "Trained batch 1386 batch loss 2634.84326 epoch total loss 733.754272\n",
      "Trained batch 1387 batch loss 2653.44043 epoch total loss 735.138367\n",
      "Trained batch 1388 batch loss 2617.97266 epoch total loss 736.494873\n",
      "Epoch 6 train loss 736.494873046875\n",
      "Validated batch 1 batch loss 2.08371968e+26\n",
      "Validated batch 2 batch loss 2.06740925e+26\n",
      "Validated batch 3 batch loss 2.00872204e+26\n",
      "Validated batch 4 batch loss 1.94429015e+26\n",
      "Validated batch 5 batch loss 2.03063419e+26\n",
      "Validated batch 6 batch loss 2.00398547e+26\n",
      "Validated batch 7 batch loss 1.97105084e+26\n",
      "Validated batch 8 batch loss 2.003155e+26\n",
      "Validated batch 9 batch loss 2.01094081e+26\n",
      "Validated batch 10 batch loss 2.03332059e+26\n",
      "Validated batch 11 batch loss 2.00354201e+26\n",
      "Validated batch 12 batch loss 1.97442585e+26\n",
      "Validated batch 13 batch loss 1.98476931e+26\n",
      "Validated batch 14 batch loss 1.97746256e+26\n",
      "Validated batch 15 batch loss 2.07573168e+26\n",
      "Validated batch 16 batch loss 2.0266294e+26\n",
      "Validated batch 17 batch loss 2.03666646e+26\n",
      "Validated batch 18 batch loss 1.99776822e+27\n",
      "Validated batch 19 batch loss 2.0440826e+26\n",
      "Validated batch 20 batch loss 1.95377085e+26\n",
      "Validated batch 21 batch loss 2.06226371e+26\n",
      "Validated batch 22 batch loss 1.9000163e+26\n",
      "Validated batch 23 batch loss 2.10710074e+26\n",
      "Validated batch 24 batch loss 2.09724077e+26\n",
      "Validated batch 25 batch loss 2.04914753e+26\n",
      "Validated batch 26 batch loss 2.03775924e+26\n",
      "Validated batch 27 batch loss 2.03248311e+26\n",
      "Validated batch 28 batch loss 1.97004918e+26\n",
      "Validated batch 29 batch loss 2.07106724e+26\n",
      "Validated batch 30 batch loss 2.03919385e+26\n",
      "Validated batch 31 batch loss 2.08667097e+26\n",
      "Validated batch 32 batch loss 2.08689012e+26\n",
      "Validated batch 33 batch loss 2.02678066e+26\n",
      "Validated batch 34 batch loss 2.04314662e+26\n",
      "Validated batch 35 batch loss 1.99308824e+26\n",
      "Validated batch 36 batch loss 4.36834483e+27\n",
      "Validated batch 37 batch loss 2.0064767e+26\n",
      "Validated batch 38 batch loss 2.08684142e+26\n",
      "Validated batch 39 batch loss 2.07092852e+26\n",
      "Validated batch 40 batch loss 2.05597817e+26\n",
      "Validated batch 41 batch loss 1.93476591e+26\n",
      "Validated batch 42 batch loss 2.00267391e+26\n",
      "Validated batch 43 batch loss 2.06920283e+26\n",
      "Validated batch 44 batch loss 1.98803088e+26\n",
      "Validated batch 45 batch loss 1.99556748e+26\n",
      "Validated batch 46 batch loss 2.01510757e+26\n",
      "Validated batch 47 batch loss 2.03405e+26\n",
      "Validated batch 48 batch loss 2.04881899e+26\n",
      "Validated batch 49 batch loss 2.01507823e+26\n",
      "Validated batch 50 batch loss 2.04961202e+26\n",
      "Validated batch 51 batch loss 2.10610794e+26\n",
      "Validated batch 52 batch loss 2.08618435e+26\n",
      "Validated batch 53 batch loss 2.05535375e+26\n",
      "Validated batch 54 batch loss 2.06612185e+26\n",
      "Validated batch 55 batch loss 2.07558651e+26\n",
      "Validated batch 56 batch loss 2.11473437e+26\n",
      "Validated batch 57 batch loss 2.09104691e+26\n",
      "Validated batch 58 batch loss 2.08642784e+26\n",
      "Validated batch 59 batch loss 2.00158832e+26\n",
      "Validated batch 60 batch loss 2.10635143e+26\n",
      "Validated batch 61 batch loss 2.04280664e+26\n",
      "Validated batch 62 batch loss 2.04066405e+26\n",
      "Validated batch 63 batch loss 2.08224467e+26\n",
      "Validated batch 64 batch loss 1.91186095e+26\n",
      "Validated batch 65 batch loss 1.99071451e+26\n",
      "Validated batch 66 batch loss 2.10693841e+26\n",
      "Validated batch 67 batch loss 2.05296729e+26\n",
      "Validated batch 68 batch loss 2.08224375e+26\n",
      "Validated batch 69 batch loss 2.06507408e+26\n",
      "Validated batch 70 batch loss 2.02581166e+26\n",
      "Validated batch 71 batch loss 1.970106e+26\n",
      "Validated batch 72 batch loss 2.04556443e+26\n",
      "Validated batch 73 batch loss 2.07990987e+26\n",
      "Validated batch 74 batch loss 2.02503394e+26\n",
      "Validated batch 75 batch loss 2.04662198e+26\n",
      "Validated batch 76 batch loss 2.06728621e+26\n",
      "Validated batch 77 batch loss 2.05041094e+26\n",
      "Validated batch 78 batch loss 2.03658806e+26\n",
      "Validated batch 79 batch loss 2.03706104e+26\n",
      "Validated batch 80 batch loss 2.0360437e+26\n",
      "Validated batch 81 batch loss 2.06900858e+26\n",
      "Validated batch 82 batch loss 2.01157262e+26\n",
      "Validated batch 83 batch loss 2.09491e+26\n",
      "Validated batch 84 batch loss 2.08551325e+26\n",
      "Validated batch 85 batch loss 2.03783654e+26\n",
      "Validated batch 86 batch loss 2.05769833e+26\n",
      "Validated batch 87 batch loss 1.97326e+26\n",
      "Validated batch 88 batch loss 2.06803312e+26\n",
      "Validated batch 89 batch loss 2.09703195e+26\n",
      "Validated batch 90 batch loss 2.07451715e+26\n",
      "Validated batch 91 batch loss 2.08962669e+26\n",
      "Validated batch 92 batch loss 2.00522122e+26\n",
      "Validated batch 93 batch loss 1.97723622e+26\n",
      "Validated batch 94 batch loss 2.09292718e+26\n",
      "Validated batch 95 batch loss 2.8452223e+26\n",
      "Validated batch 96 batch loss 2.09455087e+26\n",
      "Validated batch 97 batch loss 2.04947662e+26\n",
      "Validated batch 98 batch loss 2.07475068e+26\n",
      "Validated batch 99 batch loss 2.05239747e+26\n",
      "Validated batch 100 batch loss 2.11049771e+26\n",
      "Validated batch 101 batch loss 2.01457741e+26\n",
      "Validated batch 102 batch loss 1.98612257e+26\n",
      "Validated batch 103 batch loss 2.05629084e+26\n",
      "Validated batch 104 batch loss 2.0054114e+26\n",
      "Validated batch 105 batch loss 1.9293208e+26\n",
      "Validated batch 106 batch loss 2.09738429e+26\n",
      "Validated batch 107 batch loss 2.07439356e+26\n",
      "Validated batch 108 batch loss 2.06093776e+26\n",
      "Validated batch 109 batch loss 2.05660942e+26\n",
      "Validated batch 110 batch loss 2.08481025e+26\n",
      "Validated batch 111 batch loss 2.09548317e+26\n",
      "Validated batch 112 batch loss 2.1230474e+26\n",
      "Validated batch 113 batch loss 2.11392032e+26\n",
      "Validated batch 114 batch loss 2.060372e+26\n",
      "Validated batch 115 batch loss 1.97422128e+26\n",
      "Validated batch 116 batch loss 2.08694066e+26\n",
      "Validated batch 117 batch loss 1.97894125e+26\n",
      "Validated batch 118 batch loss 2.0237198e+26\n",
      "Validated batch 119 batch loss 2.00981759e+26\n",
      "Validated batch 120 batch loss 2.05741407e+26\n",
      "Validated batch 121 batch loss 2.06633233e+26\n",
      "Validated batch 122 batch loss 1.99893549e+26\n",
      "Validated batch 123 batch loss 2.0677435e+26\n",
      "Validated batch 124 batch loss 2.02599705e+26\n",
      "Validated batch 125 batch loss 2.06962599e+26\n",
      "Validated batch 126 batch loss 2.11188693e+26\n",
      "Validated batch 127 batch loss 2.0920161e+26\n",
      "Validated batch 128 batch loss 2.00600668e+26\n",
      "Validated batch 129 batch loss 2.05350705e+26\n",
      "Validated batch 130 batch loss 2.08407883e+26\n",
      "Validated batch 131 batch loss 2.01549126e+26\n",
      "Validated batch 132 batch loss 2.07170162e+26\n",
      "Validated batch 133 batch loss 2.01254384e+26\n",
      "Validated batch 134 batch loss 2.05984645e+26\n",
      "Validated batch 135 batch loss 1.98283609e+26\n",
      "Validated batch 136 batch loss 2.03793744e+26\n",
      "Validated batch 137 batch loss 1.99527861e+26\n",
      "Validated batch 138 batch loss 2.02224406e+26\n",
      "Validated batch 139 batch loss 2.05864705e+26\n",
      "Validated batch 140 batch loss 2.04740652e+26\n",
      "Validated batch 141 batch loss 1.99524e+26\n",
      "Validated batch 142 batch loss 1.92809464e+26\n",
      "Validated batch 143 batch loss 2.02256835e+26\n",
      "Validated batch 144 batch loss 2.07326941e+26\n",
      "Validated batch 145 batch loss 1.98671821e+26\n",
      "Validated batch 146 batch loss 2.07705634e+26\n",
      "Validated batch 147 batch loss 2.08464312e+26\n",
      "Validated batch 148 batch loss 2.072595e+26\n",
      "Validated batch 149 batch loss 2.08007552e+26\n",
      "Validated batch 150 batch loss 2.02128446e+26\n",
      "Validated batch 151 batch loss 2.03018298e+26\n",
      "Validated batch 152 batch loss 2.03758142e+26\n",
      "Validated batch 153 batch loss 2.08355458e+26\n",
      "Validated batch 154 batch loss 2.10406662e+26\n",
      "Validated batch 155 batch loss 2.05724472e+26\n",
      "Validated batch 156 batch loss 2.08471174e+26\n",
      "Validated batch 157 batch loss 2.08014193e+26\n",
      "Validated batch 158 batch loss 2.03178416e+26\n",
      "Validated batch 159 batch loss 2.08778479e+26\n",
      "Validated batch 160 batch loss 2.09146344e+26\n",
      "Validated batch 161 batch loss 2.07913437e+26\n",
      "Validated batch 162 batch loss 2.11365542e+26\n",
      "Validated batch 163 batch loss 2.05007208e+26\n",
      "Validated batch 164 batch loss 2.06124675e+26\n",
      "Validated batch 165 batch loss 2.07576323e+26\n",
      "Validated batch 166 batch loss 2.10312049e+26\n",
      "Validated batch 167 batch loss 2.11105904e+26\n",
      "Validated batch 168 batch loss 2.09505871e+26\n",
      "Validated batch 169 batch loss 2.01221733e+26\n",
      "Validated batch 170 batch loss 2.01197494e+26\n",
      "Validated batch 171 batch loss 2.02141285e+26\n",
      "Validated batch 172 batch loss 2.0622283e+26\n",
      "Validated batch 173 batch loss 2.09740956e+26\n",
      "Validated batch 174 batch loss 7.07975117e+27\n",
      "Validated batch 175 batch loss 2.02054456e+26\n",
      "Validated batch 176 batch loss 2.01999189e+26\n",
      "Validated batch 177 batch loss 2.0709573e+26\n",
      "Validated batch 178 batch loss 2.11279802e+26\n",
      "Validated batch 179 batch loss 2.09251398e+26\n",
      "Validated batch 180 batch loss 2.11227118e+26\n",
      "Validated batch 181 batch loss 2.18038e+26\n",
      "Validated batch 182 batch loss 2.0879058e+26\n",
      "Validated batch 183 batch loss 2.0641082e+26\n",
      "Validated batch 184 batch loss 1.9768468e+26\n",
      "Validated batch 185 batch loss 2.10550491e+26\n",
      "Epoch 6 val loss 2.7446143122718976e+26\n",
      "Start epoch 7 with learning rate 0.5\n",
      "Start distributed traininng...\n",
      "Trained batch 1 batch loss 2641.45776 epoch total loss 2641.45776\n",
      "Trained batch 2 batch loss 2619.74048 epoch total loss 2630.59912\n",
      "Trained batch 3 batch loss 2658.7644 epoch total loss 2639.98755\n",
      "Trained batch 4 batch loss 2659.15063 epoch total loss 2644.77832\n",
      "Trained batch 5 batch loss 2655.7395 epoch total loss 2646.97046\n",
      "Trained batch 6 batch loss 2662.18506 epoch total loss 2649.5061\n",
      "Trained batch 7 batch loss 2657.17505 epoch total loss 2650.60181\n",
      "Trained batch 8 batch loss 2661.69971 epoch total loss 2651.98901\n",
      "Trained batch 9 batch loss 2656.79541 epoch total loss 2652.52295\n",
      "Trained batch 10 batch loss 2657.56982 epoch total loss 2653.02783\n",
      "Trained batch 11 batch loss 2653.229 epoch total loss 2653.0459\n",
      "Trained batch 12 batch loss 2658.32373 epoch total loss 2653.48584\n",
      "Trained batch 13 batch loss 2611.31885 epoch total loss 2650.24219\n",
      "Trained batch 14 batch loss 2633.12793 epoch total loss 2649.01978\n",
      "Trained batch 15 batch loss 2649.59692 epoch total loss 2649.05835\n",
      "Trained batch 16 batch loss 2614.07568 epoch total loss 2646.87183\n",
      "Trained batch 17 batch loss 2621.71167 epoch total loss 2645.39185\n",
      "Trained batch 18 batch loss 2591.05981 epoch total loss 2642.37329\n",
      "Trained batch 19 batch loss 2622.76758 epoch total loss 2641.34131\n",
      "Trained batch 20 batch loss 2576.30029 epoch total loss 2638.08936\n",
      "Trained batch 21 batch loss 2616.43408 epoch total loss 2637.05811\n",
      "Trained batch 22 batch loss 2552.51221 epoch total loss 2633.21509\n",
      "Trained batch 23 batch loss 2606.58447 epoch total loss 2632.05713\n",
      "Trained batch 24 batch loss 2512.43677 epoch total loss 2627.073\n",
      "Trained batch 25 batch loss 2532.26904 epoch total loss 2623.28101\n",
      "Trained batch 26 batch loss 2507.95532 epoch total loss 2618.84521\n",
      "Trained batch 27 batch loss 2542.92188 epoch total loss 2616.0332\n",
      "Trained batch 28 batch loss 2583.65625 epoch total loss 2614.87695\n",
      "Trained batch 29 batch loss 2607.55688 epoch total loss 2614.62451\n",
      "Trained batch 30 batch loss 2606.53687 epoch total loss 2614.35498\n",
      "Trained batch 31 batch loss 2610.63501 epoch total loss 2614.23486\n",
      "Trained batch 32 batch loss 2640.98877 epoch total loss 2615.07104\n",
      "Trained batch 33 batch loss 2636.65308 epoch total loss 2615.7251\n",
      "Trained batch 34 batch loss 2638.98486 epoch total loss 2616.40918\n",
      "Trained batch 35 batch loss 2647.20361 epoch total loss 2617.28906\n",
      "Trained batch 36 batch loss 2625.80688 epoch total loss 2617.52563\n",
      "Trained batch 37 batch loss 2612.41138 epoch total loss 2617.38745\n",
      "Trained batch 38 batch loss 2651.68066 epoch total loss 2618.28979\n",
      "Trained batch 39 batch loss 2629.25928 epoch total loss 2618.57104\n",
      "Trained batch 40 batch loss 2633.54053 epoch total loss 2618.94531\n",
      "Trained batch 41 batch loss 2613.16504 epoch total loss 2618.8042\n",
      "Trained batch 42 batch loss 2508.70239 epoch total loss 2616.18286\n",
      "Trained batch 43 batch loss 2526.44141 epoch total loss 2614.09595\n",
      "Trained batch 44 batch loss 2570.56421 epoch total loss 2613.10645\n",
      "Trained batch 45 batch loss 2581.41016 epoch total loss 2612.4021\n",
      "Trained batch 46 batch loss 2618.89404 epoch total loss 2612.54321\n",
      "Trained batch 47 batch loss 2550.11084 epoch total loss 2611.21484\n",
      "Trained batch 48 batch loss 2605.7373 epoch total loss 2611.10059\n",
      "Trained batch 49 batch loss 2628.99463 epoch total loss 2611.46582\n",
      "Trained batch 50 batch loss 2635.01782 epoch total loss 2611.93677\n",
      "Trained batch 51 batch loss 2635.50122 epoch total loss 2612.39893\n",
      "Trained batch 52 batch loss 2630.37427 epoch total loss 2612.74463\n",
      "Trained batch 53 batch loss 2604.32617 epoch total loss 2612.58569\n",
      "Trained batch 54 batch loss 2568.52637 epoch total loss 2611.77\n",
      "Trained batch 55 batch loss 2589.13086 epoch total loss 2611.35815\n",
      "Trained batch 56 batch loss 2609.59595 epoch total loss 2611.32666\n",
      "Trained batch 57 batch loss 2585.75806 epoch total loss 2610.87817\n",
      "Trained batch 58 batch loss 2620.06226 epoch total loss 2611.03662\n",
      "Trained batch 59 batch loss 2579.17017 epoch total loss 2610.49658\n",
      "Trained batch 60 batch loss 2609.52686 epoch total loss 2610.48047\n",
      "Trained batch 61 batch loss 2602.28589 epoch total loss 2610.34595\n",
      "Trained batch 62 batch loss 2574.1228 epoch total loss 2609.76196\n",
      "Trained batch 63 batch loss 2602.55933 epoch total loss 2609.64746\n",
      "Trained batch 64 batch loss 2618.17773 epoch total loss 2609.78076\n",
      "Trained batch 65 batch loss 2590.39233 epoch total loss 2609.48242\n",
      "Trained batch 66 batch loss 2619.15454 epoch total loss 2609.62891\n",
      "Trained batch 67 batch loss 2569.25537 epoch total loss 2609.02637\n",
      "Trained batch 68 batch loss 2607.25366 epoch total loss 2609.00024\n",
      "Trained batch 69 batch loss 2616.24634 epoch total loss 2609.10522\n",
      "Trained batch 70 batch loss 2544.41553 epoch total loss 2608.18115\n",
      "Trained batch 71 batch loss 2596.62476 epoch total loss 2608.01855\n",
      "Trained batch 72 batch loss 2618.52148 epoch total loss 2608.16431\n",
      "Trained batch 73 batch loss 2609.44067 epoch total loss 2608.18164\n",
      "Trained batch 74 batch loss 2608.19189 epoch total loss 2608.18188\n",
      "Trained batch 75 batch loss 2559.93945 epoch total loss 2607.53857\n",
      "Trained batch 76 batch loss 2624.04761 epoch total loss 2607.75586\n",
      "Trained batch 77 batch loss 2601.92163 epoch total loss 2607.68\n",
      "Trained batch 78 batch loss 2558.00244 epoch total loss 2607.04297\n",
      "Trained batch 79 batch loss 2628.92603 epoch total loss 2607.32\n",
      "Trained batch 80 batch loss 2622.71606 epoch total loss 2607.51245\n",
      "Trained batch 81 batch loss 2615.3125 epoch total loss 2607.60889\n",
      "Trained batch 82 batch loss 2632.69629 epoch total loss 2607.91479\n",
      "Trained batch 83 batch loss 2592.64893 epoch total loss 2607.73096\n",
      "Trained batch 84 batch loss 2559.55542 epoch total loss 2607.15747\n",
      "Trained batch 85 batch loss 2613.09497 epoch total loss 2607.22729\n",
      "Trained batch 86 batch loss 2617.83691 epoch total loss 2607.35083\n",
      "Trained batch 87 batch loss 2577.64648 epoch total loss 2607.00928\n",
      "Trained batch 88 batch loss 2531.49219 epoch total loss 2606.15137\n",
      "Trained batch 89 batch loss 2550.80542 epoch total loss 2605.52954\n",
      "Trained batch 90 batch loss 2602.24731 epoch total loss 2605.49316\n",
      "Trained batch 91 batch loss 2600.53882 epoch total loss 2605.43848\n",
      "Trained batch 92 batch loss 2573.04248 epoch total loss 2605.08643\n",
      "Trained batch 93 batch loss 2585.65674 epoch total loss 2604.87744\n",
      "Trained batch 94 batch loss 2590.24097 epoch total loss 2604.72168\n",
      "Trained batch 95 batch loss 2609.19409 epoch total loss 2604.7688\n",
      "Trained batch 96 batch loss 2592.05371 epoch total loss 2604.63623\n",
      "Trained batch 97 batch loss 2608.30371 epoch total loss 2604.67407\n",
      "Trained batch 98 batch loss 2601.66895 epoch total loss 2604.64331\n",
      "Trained batch 99 batch loss 2612.7041 epoch total loss 2604.72485\n",
      "Trained batch 100 batch loss 2587.55371 epoch total loss 2604.55298\n",
      "Trained batch 101 batch loss 2622.14844 epoch total loss 2604.72705\n",
      "Trained batch 102 batch loss 2607.0647 epoch total loss 2604.75\n",
      "Trained batch 103 batch loss 2619.29736 epoch total loss 2604.89136\n",
      "Trained batch 104 batch loss 2619.57715 epoch total loss 2605.03247\n",
      "Trained batch 105 batch loss 2623.84717 epoch total loss 2605.21167\n",
      "Trained batch 106 batch loss 2602.07 epoch total loss 2605.18188\n",
      "Trained batch 107 batch loss 2622.37329 epoch total loss 2605.34253\n",
      "Trained batch 108 batch loss 2625.2312 epoch total loss 2605.52661\n",
      "Trained batch 109 batch loss 2625.49902 epoch total loss 2605.71\n",
      "Trained batch 110 batch loss 2596.75415 epoch total loss 2605.62842\n",
      "Trained batch 111 batch loss 2509.11475 epoch total loss 2604.75903\n",
      "Trained batch 112 batch loss 2488.04614 epoch total loss 2603.7168\n",
      "Trained batch 113 batch loss 2496.25391 epoch total loss 2602.76587\n",
      "Trained batch 114 batch loss 2600.94067 epoch total loss 2602.74976\n",
      "Trained batch 115 batch loss 2606.08521 epoch total loss 2602.77881\n",
      "Trained batch 116 batch loss 2594.94434 epoch total loss 2602.71118\n",
      "Trained batch 117 batch loss 2592.35718 epoch total loss 2602.62256\n",
      "Trained batch 118 batch loss 2574.40283 epoch total loss 2602.38354\n",
      "Trained batch 119 batch loss 2558.6377 epoch total loss 2602.01587\n",
      "Trained batch 120 batch loss 2546.95435 epoch total loss 2601.55713\n",
      "Trained batch 121 batch loss 2510.41675 epoch total loss 2600.80371\n",
      "Trained batch 122 batch loss 2563.90552 epoch total loss 2600.50122\n",
      "Trained batch 123 batch loss 2563.48218 epoch total loss 2600.2002\n",
      "Trained batch 124 batch loss 2598.06421 epoch total loss 2600.18286\n",
      "Trained batch 125 batch loss 2602.93091 epoch total loss 2600.20508\n",
      "Trained batch 126 batch loss 2598.37427 epoch total loss 2600.19043\n",
      "Trained batch 127 batch loss 2618.21826 epoch total loss 2600.33252\n",
      "Trained batch 128 batch loss 2617.05103 epoch total loss 2600.46313\n",
      "Trained batch 129 batch loss 2606.18701 epoch total loss 2600.50757\n",
      "Trained batch 130 batch loss 2616.60034 epoch total loss 2600.63135\n",
      "Trained batch 131 batch loss 2553.3877 epoch total loss 2600.27051\n",
      "Trained batch 132 batch loss 2569.8689 epoch total loss 2600.04028\n",
      "Trained batch 133 batch loss 2554.01782 epoch total loss 2599.69434\n",
      "Trained batch 134 batch loss 2540.60498 epoch total loss 2599.25317\n",
      "Trained batch 135 batch loss 2558.06226 epoch total loss 2598.94824\n",
      "Trained batch 136 batch loss 2560.61475 epoch total loss 2598.66626\n",
      "Trained batch 137 batch loss 2540.18311 epoch total loss 2598.2395\n",
      "Trained batch 138 batch loss 2582.02637 epoch total loss 2598.12207\n",
      "Trained batch 139 batch loss 2560.71289 epoch total loss 2597.85303\n",
      "Trained batch 140 batch loss 2523.9314 epoch total loss 2597.32495\n",
      "Trained batch 141 batch loss 2573.40869 epoch total loss 2597.15527\n",
      "Trained batch 142 batch loss 2520.32935 epoch total loss 2596.6145\n",
      "Trained batch 143 batch loss 2556.55542 epoch total loss 2596.33447\n",
      "Trained batch 144 batch loss 2554.9873 epoch total loss 2596.04736\n",
      "Trained batch 145 batch loss 2573.51978 epoch total loss 2595.89209\n",
      "Trained batch 146 batch loss 2487.92944 epoch total loss 2595.15259\n",
      "Trained batch 147 batch loss 2585.87549 epoch total loss 2595.0896\n",
      "Trained batch 148 batch loss 2576.02954 epoch total loss 2594.96069\n",
      "Trained batch 149 batch loss 2529.21216 epoch total loss 2594.51953\n",
      "Trained batch 150 batch loss 2531.30298 epoch total loss 2594.09814\n",
      "Trained batch 151 batch loss 2523.75488 epoch total loss 2593.63232\n",
      "Trained batch 152 batch loss 2540.45947 epoch total loss 2593.28247\n",
      "Trained batch 153 batch loss 2513.9397 epoch total loss 2592.76392\n",
      "Trained batch 154 batch loss 2525.73169 epoch total loss 2592.32861\n",
      "Trained batch 155 batch loss 2547.27759 epoch total loss 2592.03784\n",
      "Trained batch 156 batch loss 2562.7876 epoch total loss 2591.85034\n",
      "Trained batch 157 batch loss 2572.23145 epoch total loss 2591.72534\n",
      "Trained batch 158 batch loss 2490.93433 epoch total loss 2591.0874\n",
      "Trained batch 159 batch loss 2542.32251 epoch total loss 2590.78076\n",
      "Trained batch 160 batch loss 2539.34497 epoch total loss 2590.45923\n",
      "Trained batch 161 batch loss 2593.76392 epoch total loss 2590.47974\n",
      "Trained batch 162 batch loss 2563.20728 epoch total loss 2590.31128\n",
      "Trained batch 163 batch loss 2591.75684 epoch total loss 2590.32\n",
      "Trained batch 164 batch loss 2565.96582 epoch total loss 2590.17163\n",
      "Trained batch 165 batch loss 2563.01636 epoch total loss 2590.00708\n",
      "Trained batch 166 batch loss 2595.97925 epoch total loss 2590.04321\n",
      "Trained batch 167 batch loss 2580.98389 epoch total loss 2589.98877\n",
      "Trained batch 168 batch loss 2561.7373 epoch total loss 2589.8208\n",
      "Trained batch 169 batch loss 2565.75635 epoch total loss 2589.67822\n",
      "Trained batch 170 batch loss 2583.70581 epoch total loss 2589.64331\n",
      "Trained batch 171 batch loss 2580.22168 epoch total loss 2589.58813\n",
      "Trained batch 172 batch loss 2453.33691 epoch total loss 2588.7959\n",
      "Trained batch 173 batch loss 2523.12476 epoch total loss 2588.41626\n",
      "Trained batch 174 batch loss 2580.66284 epoch total loss 2588.37183\n",
      "Trained batch 175 batch loss 2556.04077 epoch total loss 2588.18701\n",
      "Trained batch 176 batch loss 2465.0957 epoch total loss 2587.48755\n",
      "Trained batch 177 batch loss 2545.9375 epoch total loss 2587.25293\n",
      "Trained batch 178 batch loss 2526.05273 epoch total loss 2586.90918\n",
      "Trained batch 179 batch loss 2553.81 epoch total loss 2586.72412\n",
      "Trained batch 180 batch loss 2557.57349 epoch total loss 2586.56226\n",
      "Trained batch 181 batch loss 2586.0791 epoch total loss 2586.55957\n",
      "Trained batch 182 batch loss 2608.55859 epoch total loss 2586.68042\n",
      "Trained batch 183 batch loss 2610.89844 epoch total loss 2586.81274\n",
      "Trained batch 184 batch loss 2610.66089 epoch total loss 2586.94238\n",
      "Trained batch 185 batch loss 2601.53809 epoch total loss 2587.02124\n",
      "Trained batch 186 batch loss 2534.31885 epoch total loss 2586.73779\n",
      "Trained batch 187 batch loss 2607.38184 epoch total loss 2586.84814\n",
      "Trained batch 188 batch loss 2578.55347 epoch total loss 2586.8042\n",
      "Trained batch 189 batch loss 2603.24414 epoch total loss 2586.89111\n",
      "Trained batch 190 batch loss 2606.77246 epoch total loss 2586.99585\n",
      "Trained batch 191 batch loss 2596.31152 epoch total loss 2587.04468\n",
      "Trained batch 192 batch loss 2605.97485 epoch total loss 2587.14331\n",
      "Trained batch 193 batch loss 2548.62915 epoch total loss 2586.9436\n",
      "Trained batch 194 batch loss 2599.33765 epoch total loss 2587.00757\n",
      "Trained batch 195 batch loss 2523.3833 epoch total loss 2586.68115\n",
      "Trained batch 196 batch loss 2503.84961 epoch total loss 2586.25854\n",
      "Trained batch 197 batch loss 2492.44507 epoch total loss 2585.78247\n",
      "Trained batch 198 batch loss 2501.16211 epoch total loss 2585.35498\n",
      "Trained batch 199 batch loss 2595.86572 epoch total loss 2585.40771\n",
      "Trained batch 200 batch loss 2601.4082 epoch total loss 2585.48779\n",
      "Trained batch 201 batch loss 2569.0896 epoch total loss 2585.40625\n",
      "Trained batch 202 batch loss 2527.53857 epoch total loss 2585.11963\n",
      "Trained batch 203 batch loss 2529.52271 epoch total loss 2584.8457\n",
      "Trained batch 204 batch loss 2547.97583 epoch total loss 2584.66504\n",
      "Trained batch 205 batch loss 2590.47876 epoch total loss 2584.6936\n",
      "Trained batch 206 batch loss 2584.90137 epoch total loss 2584.69458\n",
      "Trained batch 207 batch loss 2582.37158 epoch total loss 2584.68335\n",
      "Trained batch 208 batch loss 2552.03711 epoch total loss 2584.52637\n",
      "Trained batch 209 batch loss 2522.89648 epoch total loss 2584.23145\n",
      "Trained batch 210 batch loss 2396.99194 epoch total loss 2583.33984\n",
      "Trained batch 211 batch loss 2572.34302 epoch total loss 2583.2876\n",
      "Trained batch 212 batch loss 2580.2312 epoch total loss 2583.27319\n",
      "Trained batch 213 batch loss 2576.44165 epoch total loss 2583.24121\n",
      "Trained batch 214 batch loss 2572.05786 epoch total loss 2583.18896\n",
      "Trained batch 215 batch loss 2578.90137 epoch total loss 2583.16895\n",
      "Trained batch 216 batch loss 2576.16089 epoch total loss 2583.13647\n",
      "Trained batch 217 batch loss 2574.84546 epoch total loss 2583.09839\n",
      "Trained batch 218 batch loss 2572.12695 epoch total loss 2583.0481\n",
      "Trained batch 219 batch loss 2545.62329 epoch total loss 2582.8772\n",
      "Trained batch 220 batch loss 2564.69312 epoch total loss 2582.79468\n",
      "Trained batch 221 batch loss 2579.86914 epoch total loss 2582.78149\n",
      "Trained batch 222 batch loss 2526.67822 epoch total loss 2582.52881\n",
      "Trained batch 223 batch loss 2560.55591 epoch total loss 2582.43018\n",
      "Trained batch 224 batch loss 2524.30908 epoch total loss 2582.17065\n",
      "Trained batch 225 batch loss 2505.66528 epoch total loss 2581.83081\n",
      "Trained batch 226 batch loss 2544.15576 epoch total loss 2581.66406\n",
      "Trained batch 227 batch loss 2560.22217 epoch total loss 2581.56958\n",
      "Trained batch 228 batch loss 2561.15723 epoch total loss 2581.48022\n",
      "Trained batch 229 batch loss 2553.93774 epoch total loss 2581.36\n",
      "Trained batch 230 batch loss 2560.82739 epoch total loss 2581.27075\n",
      "Trained batch 231 batch loss 2504.60693 epoch total loss 2580.93896\n",
      "Trained batch 232 batch loss 2511.36768 epoch total loss 2580.63892\n",
      "Trained batch 233 batch loss 2545.77026 epoch total loss 2580.48926\n",
      "Trained batch 234 batch loss 2550.01074 epoch total loss 2580.35889\n",
      "Trained batch 235 batch loss 2557.72607 epoch total loss 2580.2627\n",
      "Trained batch 236 batch loss 2522.36279 epoch total loss 2580.01758\n",
      "Trained batch 237 batch loss 2521.43799 epoch total loss 2579.77026\n",
      "Trained batch 238 batch loss 2510.30127 epoch total loss 2579.47852\n",
      "Trained batch 239 batch loss 2505.49951 epoch total loss 2579.16895\n",
      "Trained batch 240 batch loss 2570.84351 epoch total loss 2579.13403\n",
      "Trained batch 241 batch loss 2509.35107 epoch total loss 2578.84473\n",
      "Trained batch 242 batch loss 2544.07642 epoch total loss 2578.70093\n",
      "Trained batch 243 batch loss 2511.47827 epoch total loss 2578.42432\n",
      "Trained batch 244 batch loss 2455.84497 epoch total loss 2577.92212\n",
      "Trained batch 245 batch loss 2488.90527 epoch total loss 2577.55859\n",
      "Trained batch 246 batch loss 2554.70825 epoch total loss 2577.46582\n",
      "Trained batch 247 batch loss 2520.74292 epoch total loss 2577.23608\n",
      "Trained batch 248 batch loss 2480.89014 epoch total loss 2576.84741\n",
      "Trained batch 249 batch loss 2504.07178 epoch total loss 2576.55518\n",
      "Trained batch 250 batch loss 2530.58057 epoch total loss 2576.37134\n",
      "Trained batch 251 batch loss 2504.61157 epoch total loss 2576.08545\n",
      "Trained batch 252 batch loss 2503.99146 epoch total loss 2575.79932\n",
      "Trained batch 253 batch loss 2536.55029 epoch total loss 2575.64429\n",
      "Trained batch 254 batch loss 2552.44849 epoch total loss 2575.55298\n",
      "Trained batch 255 batch loss 2564.03735 epoch total loss 2575.50781\n",
      "Trained batch 256 batch loss 2589.7998 epoch total loss 2575.56372\n",
      "Trained batch 257 batch loss 2591.69458 epoch total loss 2575.62646\n",
      "Trained batch 258 batch loss 2571.45825 epoch total loss 2575.61\n",
      "Trained batch 259 batch loss 2592.87793 epoch total loss 2575.677\n",
      "Trained batch 260 batch loss 2580.80078 epoch total loss 2575.69653\n",
      "Trained batch 261 batch loss 2583.53491 epoch total loss 2575.72681\n",
      "Trained batch 262 batch loss 2584.2959 epoch total loss 2575.75952\n",
      "Trained batch 263 batch loss 2586.02637 epoch total loss 2575.79858\n",
      "Trained batch 264 batch loss 2555.25537 epoch total loss 2575.7207\n",
      "Trained batch 265 batch loss 2568.59961 epoch total loss 2575.69385\n",
      "Trained batch 266 batch loss 2593.4231 epoch total loss 2575.7605\n",
      "Trained batch 267 batch loss 2589.4978 epoch total loss 2575.81201\n",
      "Trained batch 268 batch loss 2572.25 epoch total loss 2575.79883\n",
      "Trained batch 269 batch loss 2570.13647 epoch total loss 2575.77759\n",
      "Trained batch 270 batch loss 2587.36108 epoch total loss 2575.82056\n",
      "Trained batch 271 batch loss 2595.3606 epoch total loss 2575.89282\n",
      "Trained batch 272 batch loss 2581.45288 epoch total loss 2575.91309\n",
      "Trained batch 273 batch loss 2557.07764 epoch total loss 2575.84399\n",
      "Trained batch 274 batch loss 2592.5271 epoch total loss 2575.90479\n",
      "Trained batch 275 batch loss 2552.5437 epoch total loss 2575.82\n",
      "Trained batch 276 batch loss 2596.98438 epoch total loss 2575.89673\n",
      "Trained batch 277 batch loss 2570.66699 epoch total loss 2575.87793\n",
      "Trained batch 278 batch loss 2567.61841 epoch total loss 2575.84814\n",
      "Trained batch 279 batch loss 2585.25513 epoch total loss 2575.88184\n",
      "Trained batch 280 batch loss 2519.35034 epoch total loss 2575.68018\n",
      "Trained batch 281 batch loss 2536.34253 epoch total loss 2575.54\n",
      "Trained batch 282 batch loss 2550.67358 epoch total loss 2575.4519\n",
      "Trained batch 283 batch loss 2554.06958 epoch total loss 2575.37622\n",
      "Trained batch 284 batch loss 2553.55518 epoch total loss 2575.29956\n",
      "Trained batch 285 batch loss 2536.3855 epoch total loss 2575.16284\n",
      "Trained batch 286 batch loss 2560.88525 epoch total loss 2575.11304\n",
      "Trained batch 287 batch loss 2413.68262 epoch total loss 2574.55054\n",
      "Trained batch 288 batch loss 2573.21533 epoch total loss 2574.5459\n",
      "Trained batch 289 batch loss 2507.85425 epoch total loss 2574.31519\n",
      "Trained batch 290 batch loss 2531.72534 epoch total loss 2574.16821\n",
      "Trained batch 291 batch loss 2516.20044 epoch total loss 2573.96899\n",
      "Trained batch 292 batch loss 2509.0896 epoch total loss 2573.74683\n",
      "Trained batch 293 batch loss 2553.21558 epoch total loss 2573.67651\n",
      "Trained batch 294 batch loss 2516.2124 epoch total loss 2573.4812\n",
      "Trained batch 295 batch loss 2562.22021 epoch total loss 2573.44312\n",
      "Trained batch 296 batch loss 2563.88892 epoch total loss 2573.41064\n",
      "Trained batch 297 batch loss 2576.78442 epoch total loss 2573.42212\n",
      "Trained batch 298 batch loss 2579.42041 epoch total loss 2573.44238\n",
      "Trained batch 299 batch loss 2501.60889 epoch total loss 2573.20215\n",
      "Trained batch 300 batch loss 2526.83276 epoch total loss 2573.04761\n",
      "Trained batch 301 batch loss 2574.39893 epoch total loss 2573.052\n",
      "Trained batch 302 batch loss 2583.89966 epoch total loss 2573.08765\n",
      "Trained batch 303 batch loss 2575.26685 epoch total loss 2573.09497\n",
      "Trained batch 304 batch loss 2456.94531 epoch total loss 2572.71289\n",
      "Trained batch 305 batch loss 2430.29907 epoch total loss 2572.24585\n",
      "Trained batch 306 batch loss 2486.49463 epoch total loss 2571.96558\n",
      "Trained batch 307 batch loss 2569.24194 epoch total loss 2571.95679\n",
      "Trained batch 308 batch loss 2513.74487 epoch total loss 2571.76782\n",
      "Trained batch 309 batch loss 2587.97046 epoch total loss 2571.82031\n",
      "Trained batch 310 batch loss 2563.22339 epoch total loss 2571.79272\n",
      "Trained batch 311 batch loss 2557.21729 epoch total loss 2571.74585\n",
      "Trained batch 312 batch loss 2552.0957 epoch total loss 2571.68286\n",
      "Trained batch 313 batch loss 2569.29248 epoch total loss 2571.67529\n",
      "Trained batch 314 batch loss 2563.00806 epoch total loss 2571.64771\n",
      "Trained batch 315 batch loss 2539.41382 epoch total loss 2571.54541\n",
      "Trained batch 316 batch loss 2542.7019 epoch total loss 2571.4541\n",
      "Trained batch 317 batch loss 2561.15503 epoch total loss 2571.42163\n",
      "Trained batch 318 batch loss 2477.58228 epoch total loss 2571.12646\n",
      "Trained batch 319 batch loss 2543.61646 epoch total loss 2571.04028\n",
      "Trained batch 320 batch loss 2574.35815 epoch total loss 2571.05054\n",
      "Trained batch 321 batch loss 2527.80225 epoch total loss 2570.91577\n",
      "Trained batch 322 batch loss 2513.89746 epoch total loss 2570.73877\n",
      "Trained batch 323 batch loss 2514.40649 epoch total loss 2570.56445\n",
      "Trained batch 324 batch loss 2494.71875 epoch total loss 2570.33032\n",
      "Trained batch 325 batch loss 2454.85742 epoch total loss 2569.9751\n",
      "Trained batch 326 batch loss 2541.05737 epoch total loss 2569.88623\n",
      "Trained batch 327 batch loss 2566.94702 epoch total loss 2569.8772\n",
      "Trained batch 328 batch loss 2553.63135 epoch total loss 2569.82764\n",
      "Trained batch 329 batch loss 2550.31104 epoch total loss 2569.76831\n",
      "Trained batch 330 batch loss 2530.8335 epoch total loss 2569.65039\n",
      "Trained batch 331 batch loss 2492.36694 epoch total loss 2569.41699\n",
      "Trained batch 332 batch loss 2559.95752 epoch total loss 2569.38843\n",
      "Trained batch 333 batch loss 2535.82 epoch total loss 2569.2876\n",
      "Trained batch 334 batch loss 2518.57202 epoch total loss 2569.13574\n",
      "Trained batch 335 batch loss 2551.25171 epoch total loss 2569.08228\n",
      "Trained batch 336 batch loss 2563.74243 epoch total loss 2569.06641\n",
      "Trained batch 337 batch loss 2564.35742 epoch total loss 2569.05249\n",
      "Trained batch 338 batch loss 2554.39404 epoch total loss 2569.00903\n",
      "Trained batch 339 batch loss 2558.66211 epoch total loss 2568.97852\n",
      "Trained batch 340 batch loss 2561.11597 epoch total loss 2568.95557\n",
      "Trained batch 341 batch loss 2548.71265 epoch total loss 2568.896\n",
      "Trained batch 342 batch loss 2536.64941 epoch total loss 2568.80176\n",
      "Trained batch 343 batch loss 2538.08936 epoch total loss 2568.71216\n",
      "Trained batch 344 batch loss 2553.29712 epoch total loss 2568.66724\n",
      "Trained batch 345 batch loss 2537.98633 epoch total loss 2568.57837\n",
      "Trained batch 346 batch loss 2530.01782 epoch total loss 2568.46704\n",
      "Trained batch 347 batch loss 2550.30176 epoch total loss 2568.41455\n",
      "Trained batch 348 batch loss 2511.47314 epoch total loss 2568.25098\n",
      "Trained batch 349 batch loss 2536.92188 epoch total loss 2568.16138\n",
      "Trained batch 350 batch loss 2511.15454 epoch total loss 2567.99829\n",
      "Trained batch 351 batch loss 2475.80518 epoch total loss 2567.73584\n",
      "Trained batch 352 batch loss 2542.40527 epoch total loss 2567.66382\n",
      "Trained batch 353 batch loss 2536.48315 epoch total loss 2567.57544\n",
      "Trained batch 354 batch loss 2539.97729 epoch total loss 2567.49756\n",
      "Trained batch 355 batch loss 2537.76807 epoch total loss 2567.41382\n",
      "Trained batch 356 batch loss 2519.19409 epoch total loss 2567.27832\n",
      "Trained batch 357 batch loss 2523.83667 epoch total loss 2567.15649\n",
      "Trained batch 358 batch loss 2501.22485 epoch total loss 2566.97241\n",
      "Trained batch 359 batch loss 2451.45703 epoch total loss 2566.65063\n",
      "Trained batch 360 batch loss 2507.59106 epoch total loss 2566.48657\n",
      "Trained batch 361 batch loss 2513.41211 epoch total loss 2566.3396\n",
      "Trained batch 362 batch loss 2510.15308 epoch total loss 2566.18433\n",
      "Trained batch 363 batch loss 2523.07 epoch total loss 2566.06543\n",
      "Trained batch 364 batch loss 2509.70483 epoch total loss 2565.91064\n",
      "Trained batch 365 batch loss 2529.06152 epoch total loss 2565.80957\n",
      "Trained batch 366 batch loss 2528.15137 epoch total loss 2565.70654\n",
      "Trained batch 367 batch loss 2527.04468 epoch total loss 2565.60132\n",
      "Trained batch 368 batch loss 2517.32837 epoch total loss 2565.47021\n",
      "Trained batch 369 batch loss 2473.89307 epoch total loss 2565.22192\n",
      "Trained batch 370 batch loss 2425.37964 epoch total loss 2564.84399\n",
      "Trained batch 371 batch loss 2466.55444 epoch total loss 2564.5791\n",
      "Trained batch 372 batch loss 2466.71118 epoch total loss 2564.31592\n",
      "Trained batch 373 batch loss 2509.64258 epoch total loss 2564.16919\n",
      "Trained batch 374 batch loss 2460.18286 epoch total loss 2563.89111\n",
      "Trained batch 375 batch loss 2521.19458 epoch total loss 2563.77734\n",
      "Trained batch 376 batch loss 2517.87 epoch total loss 2563.65527\n",
      "Trained batch 377 batch loss 2489.06567 epoch total loss 2563.45728\n",
      "Trained batch 378 batch loss 2445.94849 epoch total loss 2563.14648\n",
      "Trained batch 379 batch loss 2446.38745 epoch total loss 2562.83838\n",
      "Trained batch 380 batch loss 2464.30737 epoch total loss 2562.5791\n",
      "Trained batch 381 batch loss 2501.89868 epoch total loss 2562.41968\n",
      "Trained batch 382 batch loss 2477.26514 epoch total loss 2562.19678\n",
      "Trained batch 383 batch loss 2479.34399 epoch total loss 2561.98047\n",
      "Trained batch 384 batch loss 2503.59521 epoch total loss 2561.82861\n",
      "Trained batch 385 batch loss 2515.92676 epoch total loss 2561.70947\n",
      "Trained batch 386 batch loss 2523.32275 epoch total loss 2561.60986\n",
      "Trained batch 387 batch loss 2468.55151 epoch total loss 2561.36963\n",
      "Trained batch 388 batch loss 2521.1084 epoch total loss 2561.26587\n",
      "Trained batch 389 batch loss 2510.49121 epoch total loss 2561.13525\n",
      "Trained batch 390 batch loss 2490.10767 epoch total loss 2560.95312\n",
      "Trained batch 391 batch loss 2525.71484 epoch total loss 2560.86304\n",
      "Trained batch 392 batch loss 2506.65747 epoch total loss 2560.72485\n",
      "Trained batch 393 batch loss 2527.28613 epoch total loss 2560.64\n",
      "Trained batch 394 batch loss 2543.18677 epoch total loss 2560.59546\n",
      "Trained batch 395 batch loss 2539.08105 epoch total loss 2560.54102\n",
      "Trained batch 396 batch loss 2527.0979 epoch total loss 2560.45654\n",
      "Trained batch 397 batch loss 2489.68628 epoch total loss 2560.27832\n",
      "Trained batch 398 batch loss 2473.1792 epoch total loss 2560.05957\n",
      "Trained batch 399 batch loss 2513.25806 epoch total loss 2559.94214\n",
      "Trained batch 400 batch loss 2532.073 epoch total loss 2559.87256\n",
      "Trained batch 401 batch loss 2494.1958 epoch total loss 2559.70874\n",
      "Trained batch 402 batch loss 2508.29419 epoch total loss 2559.58081\n",
      "Trained batch 403 batch loss 2455.56519 epoch total loss 2559.32275\n",
      "Trained batch 404 batch loss 2496.27271 epoch total loss 2559.1665\n",
      "Trained batch 405 batch loss 2541.0647 epoch total loss 2559.12183\n",
      "Trained batch 406 batch loss 2507.15454 epoch total loss 2558.9939\n",
      "Trained batch 407 batch loss 2527.33496 epoch total loss 2558.91602\n",
      "Trained batch 408 batch loss 2513.7666 epoch total loss 2558.80542\n",
      "Trained batch 409 batch loss 2531.16846 epoch total loss 2558.73779\n",
      "Trained batch 410 batch loss 2548.41895 epoch total loss 2558.7124\n",
      "Trained batch 411 batch loss 2497.16309 epoch total loss 2558.56274\n",
      "Trained batch 412 batch loss 2484.86816 epoch total loss 2558.38379\n",
      "Trained batch 413 batch loss 2453.17407 epoch total loss 2558.12891\n",
      "Trained batch 414 batch loss 2510.13794 epoch total loss 2558.01294\n",
      "Trained batch 415 batch loss 2505.37207 epoch total loss 2557.88623\n",
      "Trained batch 416 batch loss 2512.62891 epoch total loss 2557.77734\n",
      "Trained batch 417 batch loss 2516.43286 epoch total loss 2557.67798\n",
      "Trained batch 418 batch loss 2541.86206 epoch total loss 2557.64014\n",
      "Trained batch 419 batch loss 2531.41919 epoch total loss 2557.57764\n",
      "Trained batch 420 batch loss 2536.39551 epoch total loss 2557.5271\n",
      "Trained batch 421 batch loss 2531.8147 epoch total loss 2557.46606\n",
      "Trained batch 422 batch loss 2506.89136 epoch total loss 2557.34619\n",
      "Trained batch 423 batch loss 2537.74658 epoch total loss 2557.3\n",
      "Trained batch 424 batch loss 2505.88232 epoch total loss 2557.17871\n",
      "Trained batch 425 batch loss 2526.93896 epoch total loss 2557.10767\n",
      "Trained batch 426 batch loss 2520.62646 epoch total loss 2557.02197\n",
      "Trained batch 427 batch loss 2472.44238 epoch total loss 2556.82397\n",
      "Trained batch 428 batch loss 2459.08545 epoch total loss 2556.5957\n",
      "Trained batch 429 batch loss 2517.68359 epoch total loss 2556.50488\n",
      "Trained batch 430 batch loss 2502.57983 epoch total loss 2556.37964\n",
      "Trained batch 431 batch loss 2487.75171 epoch total loss 2556.22046\n",
      "Trained batch 432 batch loss 2492.56226 epoch total loss 2556.073\n",
      "Trained batch 433 batch loss 2506.84839 epoch total loss 2555.95923\n",
      "Trained batch 434 batch loss 2507.27466 epoch total loss 2555.84717\n",
      "Trained batch 435 batch loss 2520.24219 epoch total loss 2555.76514\n",
      "Trained batch 436 batch loss 2512.29565 epoch total loss 2555.66553\n",
      "Trained batch 437 batch loss 2520.3877 epoch total loss 2555.58472\n",
      "Trained batch 438 batch loss 2530.24194 epoch total loss 2555.52686\n",
      "Trained batch 439 batch loss 2529.78345 epoch total loss 2555.46802\n",
      "Trained batch 440 batch loss 2529.2395 epoch total loss 2555.40845\n",
      "Trained batch 441 batch loss 2525.23657 epoch total loss 2555.34\n",
      "Trained batch 442 batch loss 2516.25928 epoch total loss 2555.25171\n",
      "Trained batch 443 batch loss 2456.83374 epoch total loss 2555.02954\n",
      "Trained batch 444 batch loss 2516.91772 epoch total loss 2554.9436\n",
      "Trained batch 445 batch loss 2524.22803 epoch total loss 2554.87476\n",
      "Trained batch 446 batch loss 2523.34033 epoch total loss 2554.8042\n",
      "Trained batch 447 batch loss 2518.07471 epoch total loss 2554.72192\n",
      "Trained batch 448 batch loss 2510.22974 epoch total loss 2554.6228\n",
      "Trained batch 449 batch loss 2518.79443 epoch total loss 2554.54297\n",
      "Trained batch 450 batch loss 2518.06494 epoch total loss 2554.46191\n",
      "Trained batch 451 batch loss 2514.64673 epoch total loss 2554.37354\n",
      "Trained batch 452 batch loss 2518.7085 epoch total loss 2554.29492\n",
      "Trained batch 453 batch loss 2518.16895 epoch total loss 2554.21484\n",
      "Trained batch 454 batch loss 2518.29248 epoch total loss 2554.13574\n",
      "Trained batch 455 batch loss 2502.74536 epoch total loss 2554.02271\n",
      "Trained batch 456 batch loss 2506.12573 epoch total loss 2553.91772\n",
      "Trained batch 457 batch loss 2490.85229 epoch total loss 2553.77979\n",
      "Trained batch 458 batch loss 2504.55591 epoch total loss 2553.67212\n",
      "Trained batch 459 batch loss 2501.77954 epoch total loss 2553.55908\n",
      "Trained batch 460 batch loss 2487.48389 epoch total loss 2553.41553\n",
      "Trained batch 461 batch loss 2517.4314 epoch total loss 2553.3374\n",
      "Trained batch 462 batch loss 2509.71045 epoch total loss 2553.24292\n",
      "Trained batch 463 batch loss 2510.29492 epoch total loss 2553.15015\n",
      "Trained batch 464 batch loss 2453.58447 epoch total loss 2552.93555\n",
      "Trained batch 465 batch loss 2477.96094 epoch total loss 2552.77441\n",
      "Trained batch 466 batch loss 2460.43677 epoch total loss 2552.57617\n",
      "Trained batch 467 batch loss 2493.07935 epoch total loss 2552.44897\n",
      "Trained batch 468 batch loss 2464.72095 epoch total loss 2552.26147\n",
      "Trained batch 469 batch loss 2481.33179 epoch total loss 2552.11035\n",
      "Trained batch 470 batch loss 2449.72705 epoch total loss 2551.89258\n",
      "Trained batch 471 batch loss 2433.47412 epoch total loss 2551.64111\n",
      "Trained batch 472 batch loss 2425.22729 epoch total loss 2551.37329\n",
      "Trained batch 473 batch loss 2508.97705 epoch total loss 2551.28394\n",
      "Trained batch 474 batch loss 2492.1958 epoch total loss 2551.15918\n",
      "Trained batch 475 batch loss 2513.36499 epoch total loss 2551.07983\n",
      "Trained batch 476 batch loss 2502.8208 epoch total loss 2550.97852\n",
      "Trained batch 477 batch loss 2503.63306 epoch total loss 2550.87915\n",
      "Trained batch 478 batch loss 2510.35645 epoch total loss 2550.79443\n",
      "Trained batch 479 batch loss 2460.62402 epoch total loss 2550.6062\n",
      "Trained batch 480 batch loss 2479.84106 epoch total loss 2550.45874\n",
      "Trained batch 481 batch loss 2504.12622 epoch total loss 2550.36255\n",
      "Trained batch 482 batch loss 2462.79395 epoch total loss 2550.18066\n",
      "Trained batch 483 batch loss 2485.5083 epoch total loss 2550.04688\n",
      "Trained batch 484 batch loss 2488.39185 epoch total loss 2549.91943\n",
      "Trained batch 485 batch loss 2512.28979 epoch total loss 2549.8418\n",
      "Trained batch 486 batch loss 2514.58081 epoch total loss 2549.76929\n",
      "Trained batch 487 batch loss 2516.34302 epoch total loss 2549.70068\n",
      "Trained batch 488 batch loss 2514.99243 epoch total loss 2549.62964\n",
      "Trained batch 489 batch loss 2505.11816 epoch total loss 2549.53857\n",
      "Trained batch 490 batch loss 2491.27686 epoch total loss 2549.41968\n",
      "Trained batch 491 batch loss 2458.41455 epoch total loss 2549.23413\n",
      "Trained batch 492 batch loss 2480.19165 epoch total loss 2549.09399\n",
      "Trained batch 493 batch loss 2507.68652 epoch total loss 2549.01\n",
      "Trained batch 494 batch loss 2478.62109 epoch total loss 2548.86743\n",
      "Trained batch 495 batch loss 2487.78271 epoch total loss 2548.7439\n",
      "Trained batch 496 batch loss 2476.51685 epoch total loss 2548.59839\n",
      "Trained batch 497 batch loss 2519.63477 epoch total loss 2548.54\n",
      "Trained batch 498 batch loss 2516.17529 epoch total loss 2548.47485\n",
      "Trained batch 499 batch loss 2512.1814 epoch total loss 2548.4021\n",
      "Trained batch 500 batch loss 2507.38501 epoch total loss 2548.32\n",
      "Trained batch 501 batch loss 2476.08447 epoch total loss 2548.17578\n",
      "Trained batch 502 batch loss 2510.10303 epoch total loss 2548.1\n",
      "Trained batch 503 batch loss 2484.49609 epoch total loss 2547.97363\n",
      "Trained batch 504 batch loss 2497.70825 epoch total loss 2547.87402\n",
      "Trained batch 505 batch loss 2499.68701 epoch total loss 2547.77856\n",
      "Trained batch 506 batch loss 2445.85 epoch total loss 2547.57715\n",
      "Trained batch 507 batch loss 2500.34473 epoch total loss 2547.48389\n",
      "Trained batch 508 batch loss 2498.12573 epoch total loss 2547.38672\n",
      "Trained batch 509 batch loss 2480.49194 epoch total loss 2547.25537\n",
      "Trained batch 510 batch loss 2497.50659 epoch total loss 2547.15796\n",
      "Trained batch 511 batch loss 2474.16919 epoch total loss 2547.01489\n",
      "Trained batch 512 batch loss 2503.40918 epoch total loss 2546.92969\n",
      "Trained batch 513 batch loss 2508.74243 epoch total loss 2546.85522\n",
      "Trained batch 514 batch loss 2493.67529 epoch total loss 2546.75171\n",
      "Trained batch 515 batch loss 2505.71411 epoch total loss 2546.67212\n",
      "Trained batch 516 batch loss 2500.74512 epoch total loss 2546.58301\n",
      "Trained batch 517 batch loss 2496.64722 epoch total loss 2546.48657\n",
      "Trained batch 518 batch loss 2502.94678 epoch total loss 2546.40259\n",
      "Trained batch 519 batch loss 2479.81079 epoch total loss 2546.27417\n",
      "Trained batch 520 batch loss 2432.06055 epoch total loss 2546.05444\n",
      "Trained batch 521 batch loss 2479.98608 epoch total loss 2545.92749\n",
      "Trained batch 522 batch loss 2500.33716 epoch total loss 2545.84033\n",
      "Trained batch 523 batch loss 2462.19922 epoch total loss 2545.68042\n",
      "Trained batch 524 batch loss 2493.13306 epoch total loss 2545.58\n",
      "Trained batch 525 batch loss 2503.03467 epoch total loss 2545.49902\n",
      "Trained batch 526 batch loss 2496.1394 epoch total loss 2545.40527\n",
      "Trained batch 527 batch loss 2505.5061 epoch total loss 2545.32935\n",
      "Trained batch 528 batch loss 2451.85303 epoch total loss 2545.15234\n",
      "Trained batch 529 batch loss 2322.70288 epoch total loss 2544.73193\n",
      "Trained batch 530 batch loss 2439.14673 epoch total loss 2544.53271\n",
      "Trained batch 531 batch loss 2483.24854 epoch total loss 2544.41748\n",
      "Trained batch 532 batch loss 2492.89062 epoch total loss 2544.32056\n",
      "Trained batch 533 batch loss 2502.73438 epoch total loss 2544.24243\n",
      "Trained batch 534 batch loss 2503.11353 epoch total loss 2544.16553\n",
      "Trained batch 535 batch loss 2502.88745 epoch total loss 2544.08838\n",
      "Trained batch 536 batch loss 2502.43408 epoch total loss 2544.0105\n",
      "Trained batch 537 batch loss 2462.54248 epoch total loss 2543.85864\n",
      "Trained batch 538 batch loss 2502.37695 epoch total loss 2543.78149\n",
      "Trained batch 539 batch loss 2501.83423 epoch total loss 2543.70386\n",
      "Trained batch 540 batch loss 2502.72778 epoch total loss 2543.62793\n",
      "Trained batch 541 batch loss 2503.57104 epoch total loss 2543.55396\n",
      "Trained batch 542 batch loss 2488.39087 epoch total loss 2543.45215\n",
      "Trained batch 543 batch loss 2499.96509 epoch total loss 2543.37231\n",
      "Trained batch 544 batch loss 2398.27075 epoch total loss 2543.10547\n",
      "Trained batch 545 batch loss 2504.90869 epoch total loss 2543.0354\n",
      "Trained batch 546 batch loss 2493.24658 epoch total loss 2542.94409\n",
      "Trained batch 547 batch loss 2489.75879 epoch total loss 2542.84692\n",
      "Trained batch 548 batch loss 2494.26587 epoch total loss 2542.7583\n",
      "Trained batch 549 batch loss 2516.90503 epoch total loss 2542.71118\n",
      "Trained batch 550 batch loss 2508.78711 epoch total loss 2542.64941\n",
      "Trained batch 551 batch loss 2509.98853 epoch total loss 2542.59\n",
      "Trained batch 552 batch loss 2520.58887 epoch total loss 2542.55029\n",
      "Trained batch 553 batch loss 2516.8269 epoch total loss 2542.50391\n",
      "Trained batch 554 batch loss 2514.91699 epoch total loss 2542.45386\n",
      "Trained batch 555 batch loss 2518.17188 epoch total loss 2542.41016\n",
      "Trained batch 556 batch loss 2511.75464 epoch total loss 2542.35498\n",
      "Trained batch 557 batch loss 2504.47314 epoch total loss 2542.28711\n",
      "Trained batch 558 batch loss 2471.69678 epoch total loss 2542.16064\n",
      "Trained batch 559 batch loss 2522.10254 epoch total loss 2542.12476\n",
      "Trained batch 560 batch loss 2479.75854 epoch total loss 2542.01343\n",
      "Trained batch 561 batch loss 2527.54907 epoch total loss 2541.98755\n",
      "Trained batch 562 batch loss 2527.59961 epoch total loss 2541.96191\n",
      "Trained batch 563 batch loss 2525.54419 epoch total loss 2541.93262\n",
      "Trained batch 564 batch loss 2491.56104 epoch total loss 2541.84326\n",
      "Trained batch 565 batch loss 2508.85498 epoch total loss 2541.78491\n",
      "Trained batch 566 batch loss 2514.54028 epoch total loss 2541.73682\n",
      "Trained batch 567 batch loss 2500.34692 epoch total loss 2541.66382\n",
      "Trained batch 568 batch loss 2475.97314 epoch total loss 2541.5481\n",
      "Trained batch 569 batch loss 2451.92163 epoch total loss 2541.39062\n",
      "Trained batch 570 batch loss 2524.10059 epoch total loss 2541.36035\n",
      "Trained batch 571 batch loss 2492.47168 epoch total loss 2541.27466\n",
      "Trained batch 572 batch loss 2494.30835 epoch total loss 2541.19263\n",
      "Trained batch 573 batch loss 2524.35229 epoch total loss 2541.16309\n",
      "Trained batch 574 batch loss 2513.49194 epoch total loss 2541.11499\n",
      "Trained batch 575 batch loss 2512.80737 epoch total loss 2541.06567\n",
      "Trained batch 576 batch loss 2515.94287 epoch total loss 2541.02222\n",
      "Trained batch 577 batch loss 2481.38403 epoch total loss 2540.9187\n",
      "Trained batch 578 batch loss 2507.57056 epoch total loss 2540.86108\n",
      "Trained batch 579 batch loss 2511.36621 epoch total loss 2540.8103\n",
      "Trained batch 580 batch loss 2521.01636 epoch total loss 2540.77612\n",
      "Trained batch 581 batch loss 2513.41455 epoch total loss 2540.729\n",
      "Trained batch 582 batch loss 2513.20776 epoch total loss 2540.68164\n",
      "Trained batch 583 batch loss 2520.08 epoch total loss 2540.64648\n",
      "Trained batch 584 batch loss 2519.68945 epoch total loss 2540.6106\n",
      "Trained batch 585 batch loss 2496.27637 epoch total loss 2540.53491\n",
      "Trained batch 586 batch loss 2507.51807 epoch total loss 2540.47852\n",
      "Trained batch 587 batch loss 2508.5708 epoch total loss 2540.42407\n",
      "Trained batch 588 batch loss 2482.39429 epoch total loss 2540.32544\n",
      "Trained batch 589 batch loss 2504.40869 epoch total loss 2540.2644\n",
      "Trained batch 590 batch loss 2490.3335 epoch total loss 2540.18\n",
      "Trained batch 591 batch loss 2509.59741 epoch total loss 2540.12817\n",
      "Trained batch 592 batch loss 2500.19556 epoch total loss 2540.06079\n",
      "Trained batch 593 batch loss 2504.56836 epoch total loss 2540.00098\n",
      "Trained batch 594 batch loss 2502.62402 epoch total loss 2539.93823\n",
      "Trained batch 595 batch loss 2488.56934 epoch total loss 2539.85181\n",
      "Trained batch 596 batch loss 2494.05811 epoch total loss 2539.7749\n",
      "Trained batch 597 batch loss 2469.17188 epoch total loss 2539.65674\n",
      "Trained batch 598 batch loss 2395.24512 epoch total loss 2539.41504\n",
      "Trained batch 599 batch loss 2492.12744 epoch total loss 2539.33618\n",
      "Trained batch 600 batch loss 2494.89 epoch total loss 2539.26196\n",
      "Trained batch 601 batch loss 2491.54736 epoch total loss 2539.18262\n",
      "Trained batch 602 batch loss 2426.22 epoch total loss 2538.99512\n",
      "Trained batch 603 batch loss 2450.07202 epoch total loss 2538.84766\n",
      "Trained batch 604 batch loss 2477.31885 epoch total loss 2538.74585\n",
      "Trained batch 605 batch loss 2487.80029 epoch total loss 2538.66162\n",
      "Trained batch 606 batch loss 2486.02783 epoch total loss 2538.57471\n",
      "Trained batch 607 batch loss 2485.2395 epoch total loss 2538.48682\n",
      "Trained batch 608 batch loss 2481.65576 epoch total loss 2538.39331\n",
      "Trained batch 609 batch loss 2482.81226 epoch total loss 2538.302\n",
      "Trained batch 610 batch loss 2480.05566 epoch total loss 2538.2063\n",
      "Trained batch 611 batch loss 2480.38037 epoch total loss 2538.11182\n",
      "Trained batch 612 batch loss 2455.72656 epoch total loss 2537.97705\n",
      "Trained batch 613 batch loss 2472.0603 epoch total loss 2537.86938\n",
      "Trained batch 614 batch loss 2452.42676 epoch total loss 2537.73022\n",
      "Trained batch 615 batch loss 2451.76025 epoch total loss 2537.59033\n",
      "Trained batch 616 batch loss 2463.61597 epoch total loss 2537.47046\n",
      "Trained batch 617 batch loss 2432.38843 epoch total loss 2537.3\n",
      "Trained batch 618 batch loss 2464.13745 epoch total loss 2537.18164\n",
      "Trained batch 619 batch loss 2435.5144 epoch total loss 2537.01733\n",
      "Trained batch 620 batch loss 2464.8457 epoch total loss 2536.90112\n",
      "Trained batch 621 batch loss 2422.13916 epoch total loss 2536.71606\n",
      "Trained batch 622 batch loss 2459.69214 epoch total loss 2536.59253\n",
      "Trained batch 623 batch loss 2465.10498 epoch total loss 2536.47778\n",
      "Trained batch 624 batch loss 2456.72949 epoch total loss 2536.34985\n",
      "Trained batch 625 batch loss 2462.00903 epoch total loss 2536.23096\n",
      "Trained batch 626 batch loss 2461.33496 epoch total loss 2536.11133\n",
      "Trained batch 627 batch loss 2440.60181 epoch total loss 2535.95923\n",
      "Trained batch 628 batch loss 2442.70874 epoch total loss 2535.81079\n",
      "Trained batch 629 batch loss 2455.52515 epoch total loss 2535.68311\n",
      "Trained batch 630 batch loss 2443.36963 epoch total loss 2535.53662\n",
      "Trained batch 631 batch loss 2462.79956 epoch total loss 2535.42114\n",
      "Trained batch 632 batch loss 2450.62378 epoch total loss 2535.28687\n",
      "Trained batch 633 batch loss 2405.75635 epoch total loss 2535.08228\n",
      "Trained batch 634 batch loss 2428.76196 epoch total loss 2534.91455\n",
      "Trained batch 635 batch loss 2394.42822 epoch total loss 2534.69336\n",
      "Trained batch 636 batch loss 2442.00488 epoch total loss 2534.54761\n",
      "Trained batch 637 batch loss 2412.08398 epoch total loss 2534.35547\n",
      "Trained batch 638 batch loss 2413.10498 epoch total loss 2534.16528\n",
      "Trained batch 639 batch loss 2419.89453 epoch total loss 2533.98657\n",
      "Trained batch 640 batch loss 2406.04492 epoch total loss 2533.78662\n",
      "Trained batch 641 batch loss 2470.9729 epoch total loss 2533.68848\n",
      "Trained batch 642 batch loss 2434.56763 epoch total loss 2533.53418\n",
      "Trained batch 643 batch loss 2469.49072 epoch total loss 2533.43457\n",
      "Trained batch 644 batch loss 2372.75415 epoch total loss 2533.18506\n",
      "Trained batch 645 batch loss 2193.73682 epoch total loss 2532.65894\n",
      "Trained batch 646 batch loss 2286.38379 epoch total loss 2532.27759\n",
      "Trained batch 647 batch loss 2417.1709 epoch total loss 2532.09961\n",
      "Trained batch 648 batch loss 2467.16675 epoch total loss 2531.99951\n",
      "Trained batch 649 batch loss 2466.4707 epoch total loss 2531.89844\n",
      "Trained batch 650 batch loss 2459.08179 epoch total loss 2531.78662\n",
      "Trained batch 651 batch loss 2456.82593 epoch total loss 2531.67139\n",
      "Trained batch 652 batch loss 2465.10132 epoch total loss 2531.56934\n",
      "Trained batch 653 batch loss 2469.97241 epoch total loss 2531.4751\n",
      "Trained batch 654 batch loss 2471.18555 epoch total loss 2531.38281\n",
      "Trained batch 655 batch loss 2470.15283 epoch total loss 2531.28931\n",
      "Trained batch 656 batch loss 2470.64624 epoch total loss 2531.19678\n",
      "Trained batch 657 batch loss 2422.68091 epoch total loss 2531.03149\n",
      "Trained batch 658 batch loss 2469.8479 epoch total loss 2530.93872\n",
      "Trained batch 659 batch loss 2469.75781 epoch total loss 2530.8457\n",
      "Trained batch 660 batch loss 2472.19336 epoch total loss 2530.75708\n",
      "Trained batch 661 batch loss 2472.12036 epoch total loss 2530.66821\n",
      "Trained batch 662 batch loss 2467.35376 epoch total loss 2530.57275\n",
      "Trained batch 663 batch loss 2416.73096 epoch total loss 2530.40112\n",
      "Trained batch 664 batch loss 2453.44873 epoch total loss 2530.28516\n",
      "Trained batch 665 batch loss 2429.96753 epoch total loss 2530.13428\n",
      "Trained batch 666 batch loss 2442.73682 epoch total loss 2530.00317\n",
      "Trained batch 667 batch loss 2447.04956 epoch total loss 2529.87866\n",
      "Trained batch 668 batch loss 2420.48047 epoch total loss 2529.71509\n",
      "Trained batch 669 batch loss 2432.33887 epoch total loss 2529.56958\n",
      "Trained batch 670 batch loss 2406.34985 epoch total loss 2529.38574\n",
      "Trained batch 671 batch loss 2446.75562 epoch total loss 2529.26245\n",
      "Trained batch 672 batch loss 2368.89233 epoch total loss 2529.02393\n",
      "Trained batch 673 batch loss 2472.56543 epoch total loss 2528.94\n",
      "Trained batch 674 batch loss 2475.8186 epoch total loss 2528.86133\n",
      "Trained batch 675 batch loss 2479.54712 epoch total loss 2528.78809\n",
      "Trained batch 676 batch loss 2482.27783 epoch total loss 2528.71924\n",
      "Trained batch 677 batch loss 2482.03271 epoch total loss 2528.65039\n",
      "Trained batch 678 batch loss 2472.98096 epoch total loss 2528.56812\n",
      "Trained batch 679 batch loss 2481.50024 epoch total loss 2528.49878\n",
      "Trained batch 680 batch loss 2478.58496 epoch total loss 2528.42554\n",
      "Trained batch 681 batch loss 2489.44043 epoch total loss 2528.36841\n",
      "Trained batch 682 batch loss 2488.60498 epoch total loss 2528.31\n",
      "Trained batch 683 batch loss 2488.86157 epoch total loss 2528.25244\n",
      "Trained batch 684 batch loss 2453.92969 epoch total loss 2528.14355\n",
      "Trained batch 685 batch loss 2487.6687 epoch total loss 2528.08447\n",
      "Trained batch 686 batch loss 2449.31738 epoch total loss 2527.96973\n",
      "Trained batch 687 batch loss 2497.77563 epoch total loss 2527.92578\n",
      "Trained batch 688 batch loss 2491.09521 epoch total loss 2527.87231\n",
      "Trained batch 689 batch loss 2460.17505 epoch total loss 2527.77393\n",
      "Trained batch 690 batch loss 2476.65894 epoch total loss 2527.69971\n",
      "Trained batch 691 batch loss 2504.85327 epoch total loss 2527.66675\n",
      "Trained batch 692 batch loss 2501.32349 epoch total loss 2527.62891\n",
      "Trained batch 693 batch loss 2482.05322 epoch total loss 2527.56299\n",
      "Trained batch 694 batch loss 2484.85474 epoch total loss 2527.50146\n",
      "Trained batch 695 batch loss 2498.47705 epoch total loss 2527.45972\n",
      "Trained batch 696 batch loss 2495.83081 epoch total loss 2527.41431\n",
      "Trained batch 697 batch loss 2492.33252 epoch total loss 2527.36401\n",
      "Trained batch 698 batch loss 2485.94556 epoch total loss 2527.30469\n",
      "Trained batch 699 batch loss 2473.31421 epoch total loss 2527.22754\n",
      "Trained batch 700 batch loss 2488.65894 epoch total loss 2527.17261\n",
      "Trained batch 701 batch loss 2520.03271 epoch total loss 2527.16235\n",
      "Trained batch 702 batch loss 2518.62085 epoch total loss 2527.15015\n",
      "Trained batch 703 batch loss 2523.7002 epoch total loss 2527.14526\n",
      "Trained batch 704 batch loss 2469.15332 epoch total loss 2527.06274\n",
      "Trained batch 705 batch loss 2482.4436 epoch total loss 2526.99976\n",
      "Trained batch 706 batch loss 2531.70752 epoch total loss 2527.00635\n",
      "Trained batch 707 batch loss 2516.21362 epoch total loss 2526.99121\n",
      "Trained batch 708 batch loss 2535.40942 epoch total loss 2527.00293\n",
      "Trained batch 709 batch loss 2512.104 epoch total loss 2526.98193\n",
      "Trained batch 710 batch loss 2410.94922 epoch total loss 2526.8186\n",
      "Trained batch 711 batch loss 2515.76636 epoch total loss 2526.80298\n",
      "Trained batch 712 batch loss 2515.60327 epoch total loss 2526.78735\n",
      "Trained batch 713 batch loss 2515.64624 epoch total loss 2526.77173\n",
      "Trained batch 714 batch loss 2503.71582 epoch total loss 2526.7395\n",
      "Trained batch 715 batch loss 2496.41675 epoch total loss 2526.69702\n",
      "Trained batch 716 batch loss 2462.05884 epoch total loss 2526.60669\n",
      "Trained batch 717 batch loss 2467.99146 epoch total loss 2526.5249\n",
      "Trained batch 718 batch loss 2495.44141 epoch total loss 2526.48169\n",
      "Trained batch 719 batch loss 2550.10864 epoch total loss 2526.51465\n",
      "Trained batch 720 batch loss 2524.06128 epoch total loss 2526.51123\n",
      "Trained batch 721 batch loss 2516.76855 epoch total loss 2526.49756\n",
      "Trained batch 722 batch loss 2513.94873 epoch total loss 2526.48022\n",
      "Trained batch 723 batch loss 2503.68066 epoch total loss 2526.44873\n",
      "Trained batch 724 batch loss 2556.80298 epoch total loss 2526.49048\n",
      "Trained batch 725 batch loss 2553.08936 epoch total loss 2526.52734\n",
      "Trained batch 726 batch loss 2533.13013 epoch total loss 2526.53638\n",
      "Trained batch 727 batch loss 2561.69214 epoch total loss 2526.58472\n",
      "Trained batch 728 batch loss 2563.16602 epoch total loss 2526.63501\n",
      "Trained batch 729 batch loss 2564.28101 epoch total loss 2526.68652\n",
      "Trained batch 730 batch loss 2562.3418 epoch total loss 2526.73535\n",
      "Trained batch 731 batch loss 2498.0979 epoch total loss 2526.69629\n",
      "Trained batch 732 batch loss 2565.68481 epoch total loss 2526.74951\n",
      "Trained batch 733 batch loss 2567.20361 epoch total loss 2526.80469\n",
      "Trained batch 734 batch loss 2568.10718 epoch total loss 2526.86108\n",
      "Trained batch 735 batch loss 2570.85791 epoch total loss 2526.9209\n",
      "Trained batch 736 batch loss 2569.32812 epoch total loss 2526.97852\n",
      "Trained batch 737 batch loss 2470.03833 epoch total loss 2526.90137\n",
      "Trained batch 738 batch loss 2550.59961 epoch total loss 2526.93335\n",
      "Trained batch 739 batch loss 2575.5686 epoch total loss 2526.99927\n",
      "Trained batch 740 batch loss 2571.87622 epoch total loss 2527.06\n",
      "Trained batch 741 batch loss 2589.40894 epoch total loss 2527.14404\n",
      "Trained batch 742 batch loss 2592.03613 epoch total loss 2527.23145\n",
      "Trained batch 743 batch loss 2566.5127 epoch total loss 2527.28442\n",
      "Trained batch 744 batch loss 2580.99219 epoch total loss 2527.35645\n",
      "Trained batch 745 batch loss 2596.80688 epoch total loss 2527.44971\n",
      "Trained batch 746 batch loss 2595.18799 epoch total loss 2527.54053\n",
      "Trained batch 747 batch loss 2602.53735 epoch total loss 2527.64087\n",
      "Trained batch 748 batch loss 2603.80615 epoch total loss 2527.74268\n",
      "Trained batch 749 batch loss 2576.44824 epoch total loss 2527.80786\n",
      "Trained batch 750 batch loss 2570.59351 epoch total loss 2527.86475\n",
      "Trained batch 751 batch loss 2607.3689 epoch total loss 2527.9707\n",
      "Trained batch 752 batch loss 2581.75537 epoch total loss 2528.04224\n",
      "Trained batch 753 batch loss 2606.91357 epoch total loss 2528.14697\n",
      "Trained batch 754 batch loss 2597.20044 epoch total loss 2528.23853\n",
      "Trained batch 755 batch loss 2552.73145 epoch total loss 2528.271\n",
      "Trained batch 756 batch loss 2607.10303 epoch total loss 2528.37524\n",
      "Trained batch 757 batch loss 2585.15674 epoch total loss 2528.4502\n",
      "Trained batch 758 batch loss 2616.5708 epoch total loss 2528.56665\n",
      "Trained batch 759 batch loss 2479.83618 epoch total loss 2528.50244\n",
      "Trained batch 760 batch loss 2543.68921 epoch total loss 2528.52246\n",
      "Trained batch 761 batch loss 2591.23804 epoch total loss 2528.60498\n",
      "Trained batch 762 batch loss 2594.66284 epoch total loss 2528.69165\n",
      "Trained batch 763 batch loss 2588.75 epoch total loss 2528.77026\n",
      "Trained batch 764 batch loss 2597.35 epoch total loss 2528.86\n",
      "Trained batch 765 batch loss 2611.92798 epoch total loss 2528.96851\n",
      "Trained batch 766 batch loss 2626.56055 epoch total loss 2529.09595\n",
      "Trained batch 767 batch loss 2629.60278 epoch total loss 2529.22705\n",
      "Trained batch 768 batch loss 2630.41406 epoch total loss 2529.35864\n",
      "Trained batch 769 batch loss 2602.82959 epoch total loss 2529.45435\n",
      "Trained batch 770 batch loss 2602.65283 epoch total loss 2529.54932\n",
      "Trained batch 771 batch loss 2635.16919 epoch total loss 2529.68628\n",
      "Trained batch 772 batch loss 2635.646 epoch total loss 2529.82349\n",
      "Trained batch 773 batch loss 2500.76392 epoch total loss 2529.78589\n",
      "Trained batch 774 batch loss 2584.51147 epoch total loss 2529.85669\n",
      "Trained batch 775 batch loss 2615.55762 epoch total loss 2529.96704\n",
      "Trained batch 776 batch loss 2645.56519 epoch total loss 2530.11621\n",
      "Trained batch 777 batch loss 2606.8418 epoch total loss 2530.21484\n",
      "Trained batch 778 batch loss 2556.6355 epoch total loss 2530.24878\n",
      "Trained batch 779 batch loss 2548.45386 epoch total loss 2530.27222\n",
      "Trained batch 780 batch loss 2618.71704 epoch total loss 2530.38574\n",
      "Trained batch 781 batch loss 2566.80688 epoch total loss 2530.43237\n",
      "Trained batch 782 batch loss 2588.92627 epoch total loss 2530.50708\n",
      "Trained batch 783 batch loss 2659.90186 epoch total loss 2530.67236\n",
      "Trained batch 784 batch loss 2620.604 epoch total loss 2530.78711\n",
      "Trained batch 785 batch loss 2665.86963 epoch total loss 2530.95898\n",
      "Trained batch 786 batch loss 2644.08423 epoch total loss 2531.10303\n",
      "Trained batch 787 batch loss 2652.15308 epoch total loss 2531.25684\n",
      "Trained batch 788 batch loss 2654.83105 epoch total loss 2531.41382\n",
      "Trained batch 789 batch loss 2671.67188 epoch total loss 2531.59131\n",
      "Trained batch 790 batch loss 2666.31421 epoch total loss 2531.76196\n",
      "Trained batch 791 batch loss 2679.39209 epoch total loss 2531.94873\n",
      "Trained batch 792 batch loss 2657.33276 epoch total loss 2532.10693\n",
      "Trained batch 793 batch loss 2647.24634 epoch total loss 2532.2522\n",
      "Trained batch 794 batch loss 2655.48975 epoch total loss 2532.40747\n",
      "Trained batch 795 batch loss 2664.40723 epoch total loss 2532.57349\n",
      "Trained batch 796 batch loss 2658.59399 epoch total loss 2532.73169\n",
      "Trained batch 797 batch loss 2694.20776 epoch total loss 2532.93433\n",
      "Trained batch 798 batch loss 2685.33594 epoch total loss 2533.12549\n",
      "Trained batch 799 batch loss 2633.40845 epoch total loss 2533.25098\n",
      "Trained batch 800 batch loss 2703.60107 epoch total loss 2533.46387\n",
      "Trained batch 801 batch loss 2707.12891 epoch total loss 2533.68066\n",
      "Trained batch 802 batch loss 2707.18555 epoch total loss 2533.89697\n",
      "Trained batch 803 batch loss 2682.77661 epoch total loss 2534.08228\n",
      "Trained batch 804 batch loss 2685.94482 epoch total loss 2534.27124\n",
      "Trained batch 805 batch loss 2682.62 epoch total loss 2534.45557\n",
      "Trained batch 806 batch loss 2718.24341 epoch total loss 2534.68359\n",
      "Trained batch 807 batch loss 2720.15771 epoch total loss 2534.91333\n",
      "Trained batch 808 batch loss 2721.61963 epoch total loss 2535.14453\n",
      "Trained batch 809 batch loss 2696.11108 epoch total loss 2535.34351\n",
      "Trained batch 810 batch loss 2691.79468 epoch total loss 2535.53662\n",
      "Trained batch 811 batch loss 2662.92432 epoch total loss 2535.6936\n",
      "Trained batch 812 batch loss 2690.53149 epoch total loss 2535.88428\n",
      "Trained batch 813 batch loss 2713.39111 epoch total loss 2536.10254\n",
      "Trained batch 814 batch loss 2743.35986 epoch total loss 2536.35718\n",
      "Trained batch 815 batch loss 2746.05591 epoch total loss 2536.6145\n",
      "Trained batch 816 batch loss 2748.3252 epoch total loss 2536.87402\n",
      "Trained batch 817 batch loss 2753.43921 epoch total loss 2537.13916\n",
      "Trained batch 818 batch loss 2757.57251 epoch total loss 2537.40869\n",
      "Trained batch 819 batch loss 2753.29468 epoch total loss 2537.67212\n",
      "Trained batch 820 batch loss 2758.54346 epoch total loss 2537.94141\n",
      "Trained batch 821 batch loss 2676.36987 epoch total loss 2538.11\n",
      "Trained batch 822 batch loss 2763.4187 epoch total loss 2538.38403\n",
      "Trained batch 823 batch loss 2767.29517 epoch total loss 2538.66211\n",
      "Trained batch 824 batch loss 2758.02271 epoch total loss 2538.92847\n",
      "Trained batch 825 batch loss 2768.34033 epoch total loss 2539.20654\n",
      "Trained batch 826 batch loss 2782.77979 epoch total loss 2539.50146\n",
      "Trained batch 827 batch loss 2767.71802 epoch total loss 2539.77759\n",
      "Trained batch 828 batch loss 2685.59814 epoch total loss 2539.95361\n",
      "Trained batch 829 batch loss 2791.17847 epoch total loss 2540.25659\n",
      "Trained batch 830 batch loss 2792.67285 epoch total loss 2540.56079\n",
      "Trained batch 831 batch loss 2783.84473 epoch total loss 2540.85352\n",
      "Trained batch 832 batch loss 2795.75342 epoch total loss 2541.16\n",
      "Trained batch 833 batch loss 2740.29297 epoch total loss 2541.39893\n",
      "Trained batch 834 batch loss 2683.52881 epoch total loss 2541.56934\n",
      "Trained batch 835 batch loss 2797.9917 epoch total loss 2541.87646\n",
      "Trained batch 836 batch loss 2776.9646 epoch total loss 2542.15771\n",
      "Trained batch 837 batch loss 2804.59131 epoch total loss 2542.47095\n",
      "Trained batch 838 batch loss 2804.66504 epoch total loss 2542.78394\n",
      "Trained batch 839 batch loss 2741.94165 epoch total loss 2543.02148\n",
      "Trained batch 840 batch loss 2758.14 epoch total loss 2543.27759\n",
      "Trained batch 841 batch loss 2711.53735 epoch total loss 2543.47778\n",
      "Trained batch 842 batch loss 2706.87915 epoch total loss 2543.67188\n",
      "Trained batch 843 batch loss 2683.61743 epoch total loss 2543.83789\n",
      "Trained batch 844 batch loss 2761.4519 epoch total loss 2544.0957\n",
      "Trained batch 845 batch loss 2812.77051 epoch total loss 2544.41357\n",
      "Trained batch 846 batch loss 2759.63965 epoch total loss 2544.66821\n",
      "Trained batch 847 batch loss 2744.28784 epoch total loss 2544.90381\n",
      "Trained batch 848 batch loss 2798.84229 epoch total loss 2545.20312\n",
      "Trained batch 849 batch loss 2747.677 epoch total loss 2545.44165\n",
      "Trained batch 850 batch loss 2805.28882 epoch total loss 2545.74731\n",
      "Trained batch 851 batch loss 2764.0293 epoch total loss 2546.00391\n",
      "Trained batch 852 batch loss 2814.70215 epoch total loss 2546.31934\n",
      "Trained batch 853 batch loss 2731.2937 epoch total loss 2546.53613\n",
      "Trained batch 854 batch loss 2777.82422 epoch total loss 2546.80688\n",
      "Trained batch 855 batch loss 2751.73022 epoch total loss 2547.04639\n",
      "Trained batch 856 batch loss 2825.11548 epoch total loss 2547.37109\n",
      "Trained batch 857 batch loss 2840.09912 epoch total loss 2547.71265\n",
      "Trained batch 858 batch loss 2808.73608 epoch total loss 2548.01685\n",
      "Trained batch 859 batch loss 2797.64014 epoch total loss 2548.30762\n",
      "Trained batch 860 batch loss 2844.42456 epoch total loss 2548.6521\n",
      "Trained batch 861 batch loss 2847.80688 epoch total loss 2548.99951\n",
      "Trained batch 862 batch loss 2815.3894 epoch total loss 2549.30859\n",
      "Trained batch 863 batch loss 2851.81543 epoch total loss 2549.65894\n",
      "Trained batch 864 batch loss 2853.57837 epoch total loss 2550.01074\n",
      "Trained batch 865 batch loss 2855.71606 epoch total loss 2550.36426\n",
      "Trained batch 866 batch loss 2858.95361 epoch total loss 2550.72046\n",
      "Trained batch 867 batch loss 2861.2605 epoch total loss 2551.07861\n",
      "Trained batch 868 batch loss 2845.04834 epoch total loss 2551.41724\n",
      "Trained batch 869 batch loss 2796.93091 epoch total loss 2551.7\n",
      "Trained batch 870 batch loss 2859.4895 epoch total loss 2552.05371\n",
      "Trained batch 871 batch loss 2853.36572 epoch total loss 2552.39966\n",
      "Trained batch 872 batch loss 2840.57617 epoch total loss 2552.73\n",
      "Trained batch 873 batch loss 2865.49414 epoch total loss 2553.08813\n",
      "Trained batch 874 batch loss 2863.91846 epoch total loss 2553.44385\n",
      "Trained batch 875 batch loss 2861.91016 epoch total loss 2553.79663\n",
      "Trained batch 876 batch loss 2864.01221 epoch total loss 2554.15063\n",
      "Trained batch 877 batch loss 2831.7002 epoch total loss 2554.46729\n",
      "Trained batch 878 batch loss 2808.93262 epoch total loss 2554.75708\n",
      "Trained batch 879 batch loss 2866.45898 epoch total loss 2555.11182\n",
      "Trained batch 880 batch loss 2854.68872 epoch total loss 2555.45239\n",
      "Trained batch 881 batch loss 2844.91357 epoch total loss 2555.78101\n",
      "Trained batch 882 batch loss 2859.35303 epoch total loss 2556.125\n",
      "Trained batch 883 batch loss 2836.82422 epoch total loss 2556.44287\n",
      "Trained batch 884 batch loss 2868.3772 epoch total loss 2556.7959\n",
      "Trained batch 885 batch loss 2735.93 epoch total loss 2556.99829\n",
      "Trained batch 886 batch loss 2826.19116 epoch total loss 2557.30225\n",
      "Trained batch 887 batch loss 2802.8645 epoch total loss 2557.57886\n",
      "Trained batch 888 batch loss 2860.4668 epoch total loss 2557.92017\n",
      "Trained batch 889 batch loss 2865.82324 epoch total loss 2558.26636\n",
      "Trained batch 890 batch loss 2831.47119 epoch total loss 2558.57324\n",
      "Trained batch 891 batch loss 2847.39771 epoch total loss 2558.89771\n",
      "Trained batch 892 batch loss 2880.63672 epoch total loss 2559.2583\n",
      "Trained batch 893 batch loss 2867.75366 epoch total loss 2559.60376\n",
      "Trained batch 894 batch loss 2882.91528 epoch total loss 2559.96558\n",
      "Trained batch 895 batch loss 2884.03271 epoch total loss 2560.32764\n",
      "Trained batch 896 batch loss 2848.98389 epoch total loss 2560.65\n",
      "Trained batch 897 batch loss 2885.07129 epoch total loss 2561.01147\n",
      "Trained batch 898 batch loss 2860.31519 epoch total loss 2561.34473\n",
      "Trained batch 899 batch loss 2889.89575 epoch total loss 2561.71021\n",
      "Trained batch 900 batch loss 2890.72119 epoch total loss 2562.07593\n",
      "Trained batch 901 batch loss 2879.1897 epoch total loss 2562.42798\n",
      "Trained batch 902 batch loss 2889.81543 epoch total loss 2562.79077\n",
      "Trained batch 903 batch loss 2881.58887 epoch total loss 2563.1438\n",
      "Trained batch 904 batch loss 2880.1123 epoch total loss 2563.49414\n",
      "Trained batch 905 batch loss 2861.43042 epoch total loss 2563.82349\n",
      "Trained batch 906 batch loss 2867.88159 epoch total loss 2564.15918\n",
      "Trained batch 907 batch loss 2900.98462 epoch total loss 2564.53052\n",
      "Trained batch 908 batch loss 2903.31323 epoch total loss 2564.90356\n",
      "Trained batch 909 batch loss 2903.31519 epoch total loss 2565.27588\n",
      "Trained batch 910 batch loss 2884.24194 epoch total loss 2565.62646\n",
      "Trained batch 911 batch loss 2877.91528 epoch total loss 2565.96924\n",
      "Trained batch 912 batch loss 2909.50562 epoch total loss 2566.34595\n",
      "Trained batch 913 batch loss 2911.43945 epoch total loss 2566.72388\n",
      "Trained batch 914 batch loss 2912.96338 epoch total loss 2567.10278\n",
      "Trained batch 915 batch loss 2916.76733 epoch total loss 2567.48486\n",
      "Trained batch 916 batch loss 2919.15845 epoch total loss 2567.8689\n",
      "Trained batch 917 batch loss 2875.02588 epoch total loss 2568.20386\n",
      "Trained batch 918 batch loss 2888.22876 epoch total loss 2568.55249\n",
      "Trained batch 919 batch loss 2882.76904 epoch total loss 2568.89453\n",
      "Trained batch 920 batch loss 2858.99243 epoch total loss 2569.20972\n",
      "Trained batch 921 batch loss 2899.92358 epoch total loss 2569.56885\n",
      "Trained batch 922 batch loss 2910.24707 epoch total loss 2569.93848\n",
      "Trained batch 923 batch loss 2824.19971 epoch total loss 2570.21387\n",
      "Trained batch 924 batch loss 2823.42847 epoch total loss 2570.48804\n",
      "Trained batch 925 batch loss 2926.15601 epoch total loss 2570.8728\n",
      "Trained batch 926 batch loss 2896.76074 epoch total loss 2571.22461\n",
      "Trained batch 927 batch loss 2931.59 epoch total loss 2571.61328\n",
      "Trained batch 928 batch loss 2934.56421 epoch total loss 2572.00439\n",
      "Trained batch 929 batch loss 2937.60449 epoch total loss 2572.39771\n",
      "Trained batch 930 batch loss 2939.71094 epoch total loss 2572.79272\n",
      "Trained batch 931 batch loss 2938.36719 epoch total loss 2573.1853\n",
      "Trained batch 932 batch loss 2915.71143 epoch total loss 2573.55273\n",
      "Trained batch 933 batch loss 2908.29175 epoch total loss 2573.91162\n",
      "Trained batch 934 batch loss 2887.29614 epoch total loss 2574.24707\n",
      "Trained batch 935 batch loss 2840.68213 epoch total loss 2574.53198\n",
      "Trained batch 936 batch loss 2860.15723 epoch total loss 2574.8374\n",
      "Trained batch 937 batch loss 2910.75562 epoch total loss 2575.1958\n",
      "Trained batch 938 batch loss 2848.14673 epoch total loss 2575.48706\n",
      "Trained batch 939 batch loss 2897.51587 epoch total loss 2575.82983\n",
      "Trained batch 940 batch loss 2921.55566 epoch total loss 2576.19751\n",
      "Trained batch 941 batch loss 2900.12622 epoch total loss 2576.54199\n",
      "Trained batch 942 batch loss 2931.29907 epoch total loss 2576.91846\n",
      "Trained batch 943 batch loss 2899.24292 epoch total loss 2577.26025\n",
      "Trained batch 944 batch loss 2929.94336 epoch total loss 2577.63403\n",
      "Trained batch 945 batch loss 2929.2522 epoch total loss 2578.0061\n",
      "Trained batch 946 batch loss 2902.78296 epoch total loss 2578.34937\n",
      "Trained batch 947 batch loss 2921.02734 epoch total loss 2578.71118\n",
      "Trained batch 948 batch loss 2920.16382 epoch total loss 2579.07153\n",
      "Trained batch 949 batch loss 2924.25073 epoch total loss 2579.4353\n",
      "Trained batch 950 batch loss 2926.48145 epoch total loss 2579.80054\n",
      "Trained batch 951 batch loss 2924.62329 epoch total loss 2580.16309\n",
      "Trained batch 952 batch loss 2925.02026 epoch total loss 2580.52515\n",
      "Trained batch 953 batch loss 2927.54126 epoch total loss 2580.8894\n",
      "Trained batch 954 batch loss 2926.86816 epoch total loss 2581.25195\n",
      "Trained batch 955 batch loss 2927.53491 epoch total loss 2581.6145\n",
      "Trained batch 956 batch loss 2895.91699 epoch total loss 2581.94336\n",
      "Trained batch 957 batch loss 2926.7002 epoch total loss 2582.30347\n",
      "Trained batch 958 batch loss 2892.74683 epoch total loss 2582.62769\n",
      "Trained batch 959 batch loss 2889.72412 epoch total loss 2582.94775\n",
      "Trained batch 960 batch loss 2887.9751 epoch total loss 2583.26562\n",
      "Trained batch 961 batch loss 2921.54028 epoch total loss 2583.61768\n",
      "Trained batch 962 batch loss 2920.90332 epoch total loss 2583.96826\n",
      "Trained batch 963 batch loss 2918.35938 epoch total loss 2584.31543\n",
      "Trained batch 964 batch loss 2920.28882 epoch total loss 2584.66382\n",
      "Trained batch 965 batch loss 2890.01733 epoch total loss 2584.98022\n",
      "Trained batch 966 batch loss 2895.05835 epoch total loss 2585.30127\n",
      "Trained batch 967 batch loss 2918.13354 epoch total loss 2585.64551\n",
      "Trained batch 968 batch loss 2916.53711 epoch total loss 2585.9873\n",
      "Trained batch 969 batch loss 2916.04321 epoch total loss 2586.32788\n",
      "Trained batch 970 batch loss 2915.60742 epoch total loss 2586.66724\n",
      "Trained batch 971 batch loss 2802.77539 epoch total loss 2586.89\n",
      "Trained batch 972 batch loss 2867.81323 epoch total loss 2587.17871\n",
      "Trained batch 973 batch loss 2907.71606 epoch total loss 2587.5083\n",
      "Trained batch 974 batch loss 2855.67798 epoch total loss 2587.78369\n",
      "Trained batch 975 batch loss 2898.18848 epoch total loss 2588.10205\n",
      "Trained batch 976 batch loss 2906.96362 epoch total loss 2588.42871\n",
      "Trained batch 977 batch loss 2831.31274 epoch total loss 2588.67725\n",
      "Trained batch 978 batch loss 2896.72925 epoch total loss 2588.99243\n",
      "Trained batch 979 batch loss 2908.3623 epoch total loss 2589.31836\n",
      "Trained batch 980 batch loss 2908.35059 epoch total loss 2589.6438\n",
      "Trained batch 981 batch loss 2909.41724 epoch total loss 2589.97\n",
      "Trained batch 982 batch loss 2828.47046 epoch total loss 2590.21289\n",
      "Trained batch 983 batch loss 2910.76685 epoch total loss 2590.53882\n",
      "Trained batch 984 batch loss 2879.4563 epoch total loss 2590.83252\n",
      "Trained batch 985 batch loss 2844.73804 epoch total loss 2591.09033\n",
      "Trained batch 986 batch loss 2846.79297 epoch total loss 2591.34961\n",
      "Trained batch 987 batch loss 2857.71558 epoch total loss 2591.61963\n",
      "Trained batch 988 batch loss 2916.06982 epoch total loss 2591.94775\n",
      "Trained batch 989 batch loss 2916.04028 epoch total loss 2592.27563\n",
      "Trained batch 990 batch loss 2917.31787 epoch total loss 2592.60376\n",
      "Trained batch 1125 batch loss 2972.28979 epoch total loss 2633.23193\n",
      "Trained batch 1126 batch loss 3030.07056 epoch total loss 2633.58447\n",
      "Trained batch 1127 batch loss 2990.85059 epoch total loss 2633.90137\n",
      "Trained batch 1128 batch loss 2955.76855 epoch total loss 2634.18652\n",
      "Trained batch 1129 batch loss 3032.27368 epoch total loss 2634.53931\n",
      "Trained batch 1130 batch loss 3036.43384 epoch total loss 2634.89502\n",
      "Trained batch 1131 batch loss 3030.54346 epoch total loss 2635.24463\n",
      "Trained batch 1132 batch loss 2913.13135 epoch total loss 2635.49023\n",
      "Trained batch 1133 batch loss 2979.68848 epoch total loss 2635.79419\n",
      "Trained batch 1134 batch loss 3010.23315 epoch total loss 2636.12427\n",
      "Trained batch 1135 batch loss 3014.63745 epoch total loss 2636.45801\n",
      "Trained batch 1136 batch loss 3000.57153 epoch total loss 2636.77832\n",
      "Trained batch 1137 batch loss 3047.02 epoch total loss 2637.13916\n",
      "Trained batch 1138 batch loss 3048.56079 epoch total loss 2637.50073\n",
      "Trained batch 1139 batch loss 3042.72852 epoch total loss 2637.85645\n",
      "Trained batch 1140 batch loss 3052.95215 epoch total loss 2638.2207\n",
      "Trained batch 1141 batch loss 3052.70483 epoch total loss 2638.58398\n",
      "Trained batch 1142 batch loss 3050.69165 epoch total loss 2638.94482\n",
      "Trained batch 1143 batch loss 3006.16309 epoch total loss 2639.26611\n",
      "Trained batch 1144 batch loss 3001.3728 epoch total loss 2639.58252\n",
      "Trained batch 1145 batch loss 3018.20068 epoch total loss 2639.91333\n",
      "Trained batch 1146 batch loss 3026.16187 epoch total loss 2640.25049\n",
      "Trained batch 1147 batch loss 3013.64014 epoch total loss 2640.57617\n",
      "Trained batch 1148 batch loss 3026.08936 epoch total loss 2640.91187\n",
      "Trained batch 1149 batch loss 3014.52808 epoch total loss 2641.23706\n",
      "Trained batch 1150 batch loss 3057.17944 epoch total loss 2641.59863\n",
      "Trained batch 1151 batch loss 3031.15063 epoch total loss 2641.93726\n",
      "Trained batch 1152 batch loss 3066.65 epoch total loss 2642.30591\n",
      "Trained batch 1153 batch loss 3059.66528 epoch total loss 2642.66797\n",
      "Trained batch 1154 batch loss 3024.10522 epoch total loss 2642.99854\n",
      "Trained batch 1155 batch loss 2977.43237 epoch total loss 2643.28809\n",
      "Trained batch 1156 batch loss 3070.7627 epoch total loss 2643.65796\n",
      "Trained batch 1157 batch loss 3034.93164 epoch total loss 2643.99609\n",
      "Trained batch 1158 batch loss 3079.84229 epoch total loss 2644.37231\n",
      "Trained batch 1159 batch loss 3083.80542 epoch total loss 2644.75146\n",
      "Trained batch 1160 batch loss 3086.10913 epoch total loss 2645.13184\n",
      "Trained batch 1161 batch loss 3088.22363 epoch total loss 2645.51367\n",
      "Trained batch 1162 batch loss 3091.38428 epoch total loss 2645.89746\n",
      "Trained batch 1163 batch loss 3094.00928 epoch total loss 2646.28271\n",
      "Trained batch 1164 batch loss 3045.46411 epoch total loss 2646.62573\n",
      "Trained batch 1165 batch loss 3084.54175 epoch total loss 2647.00146\n",
      "Trained batch 1166 batch loss 3097.44507 epoch total loss 2647.38794\n",
      "Trained batch 1167 batch loss 3089.36279 epoch total loss 2647.7666\n",
      "Trained batch 1168 batch loss 3035.65039 epoch total loss 2648.09863\n",
      "Trained batch 1169 batch loss 3106.82324 epoch total loss 2648.49097\n",
      "Trained batch 1170 batch loss 3108.09888 epoch total loss 2648.88379\n",
      "Trained batch 1171 batch loss 3109.47485 epoch total loss 2649.2771\n",
      "Trained batch 1172 batch loss 3111.20068 epoch total loss 2649.67139\n",
      "Trained batch 1173 batch loss 3119.18213 epoch total loss 2650.07153\n",
      "Trained batch 1174 batch loss 3088.52222 epoch total loss 2650.44507\n",
      "Trained batch 1175 batch loss 3113.24585 epoch total loss 2650.83887\n",
      "Trained batch 1176 batch loss 3129.08765 epoch total loss 2651.24561\n",
      "Trained batch 1177 batch loss 3087.20068 epoch total loss 2651.61597\n",
      "Trained batch 1178 batch loss 3129.76538 epoch total loss 2652.02197\n",
      "Trained batch 1179 batch loss 3091.63745 epoch total loss 2652.39478\n",
      "Trained batch 1180 batch loss 3123.52686 epoch total loss 2652.79419\n",
      "Trained batch 1181 batch loss 3121.56958 epoch total loss 2653.19092\n",
      "Trained batch 1182 batch loss 3055.85693 epoch total loss 2653.53149\n",
      "Trained batch 1183 batch loss 3128.33154 epoch total loss 2653.93286\n",
      "Trained batch 1184 batch loss 3160.51807 epoch total loss 2654.3606\n",
      "Trained batch 1185 batch loss 3141.4314 epoch total loss 2654.77173\n",
      "Trained batch 1186 batch loss 3134.8 epoch total loss 2655.17651\n",
      "Trained batch 1187 batch loss 3175.35034 epoch total loss 2655.6145\n",
      "Trained batch 1188 batch loss 3182.46436 epoch total loss 2656.05811\n",
      "Trained batch 1189 batch loss 3171.77783 epoch total loss 2656.4917\n",
      "Trained batch 1190 batch loss 3049.1731 epoch total loss 2656.82178\n",
      "Trained batch 1191 batch loss 3115.04297 epoch total loss 2657.20654\n",
      "Trained batch 1192 batch loss 3125.55322 epoch total loss 2657.59937\n",
      "Trained batch 1193 batch loss 3162.51123 epoch total loss 2658.02271\n",
      "Trained batch 1194 batch loss 3101.12427 epoch total loss 2658.39355\n",
      "Trained batch 1195 batch loss 3038.08643 epoch total loss 2658.71118\n",
      "Trained batch 1196 batch loss 3196.79199 epoch total loss 2659.16113\n",
      "Trained batch 1197 batch loss 3198.13257 epoch total loss 2659.61157\n",
      "Trained batch 1198 batch loss 3220.49341 epoch total loss 2660.07983\n",
      "Trained batch 1199 batch loss 3217.37744 epoch total loss 2660.54468\n",
      "Trained batch 1200 batch loss 3139.9104 epoch total loss 2660.94409\n",
      "Trained batch 1201 batch loss 3169.32764 epoch total loss 2661.36743\n",
      "Trained batch 1202 batch loss 3230.16675 epoch total loss 2661.84058\n",
      "Trained batch 1203 batch loss 3193.48926 epoch total loss 2662.28271\n",
      "Trained batch 1204 batch loss 3239.83276 epoch total loss 2662.76221\n",
      "Trained batch 1205 batch loss 3245.64624 epoch total loss 2663.24609\n",
      "Trained batch 1206 batch loss 3249.33984 epoch total loss 2663.73193\n",
      "Trained batch 1207 batch loss 3160.92822 epoch total loss 2664.14404\n",
      "Trained batch 1208 batch loss 3255.06885 epoch total loss 2664.63306\n",
      "Trained batch 1209 batch loss 3262.43506 epoch total loss 2665.12769\n",
      "Trained batch 1210 batch loss 3212.02173 epoch total loss 2665.57959\n",
      "Trained batch 1211 batch loss 3265.05273 epoch total loss 2666.07446\n",
      "Trained batch 1212 batch loss 3271.67896 epoch total loss 2666.57422\n",
      "Trained batch 1213 batch loss 3180.81543 epoch total loss 2666.99805\n",
      "Trained batch 1214 batch loss 3124.85278 epoch total loss 2667.37524\n",
      "Trained batch 1215 batch loss 3171.58667 epoch total loss 2667.79\n",
      "Trained batch 1216 batch loss 3189.78 epoch total loss 2668.21948\n",
      "Trained batch 1217 batch loss 3267.94165 epoch total loss 2668.71216\n",
      "Trained batch 1218 batch loss 3284.70532 epoch total loss 2669.21802\n",
      "Trained batch 1219 batch loss 3284.25708 epoch total loss 2669.72241\n",
      "Trained batch 1220 batch loss 3244.10962 epoch total loss 2670.19336\n",
      "Trained batch 1221 batch loss 3284.82935 epoch total loss 2670.69653\n",
      "Trained batch 1222 batch loss 3277.78223 epoch total loss 2671.19336\n",
      "Trained batch 1223 batch loss 3129.27393 epoch total loss 2671.56787\n",
      "Trained batch 1224 batch loss 3092.05615 epoch total loss 2671.91138\n",
      "Trained batch 1225 batch loss 3253.11426 epoch total loss 2672.38574\n",
      "Trained batch 1226 batch loss 3277.00757 epoch total loss 2672.87891\n",
      "Trained batch 1227 batch loss 3260.80298 epoch total loss 2673.35791\n",
      "Trained batch 1228 batch loss 3293.5708 epoch total loss 2673.86304\n",
      "Trained batch 1229 batch loss 3262.88086 epoch total loss 2674.34229\n",
      "Trained batch 1230 batch loss 3309.73364 epoch total loss 2674.85889\n",
      "Trained batch 1231 batch loss 3286.85425 epoch total loss 2675.35596\n",
      "Trained batch 1232 batch loss 3286.07056 epoch total loss 2675.85156\n",
      "Trained batch 1233 batch loss 3316.21436 epoch total loss 2676.37109\n",
      "Trained batch 1234 batch loss 3252.24072 epoch total loss 2676.83765\n",
      "Trained batch 1235 batch loss 3328.22168 epoch total loss 2677.36523\n",
      "Trained batch 1236 batch loss 3333.9707 epoch total loss 2677.89648\n",
      "Trained batch 1237 batch loss 3337.07593 epoch total loss 2678.4292\n",
      "Trained batch 1238 batch loss 3322.79663 epoch total loss 2678.94971\n",
      "Trained batch 1239 batch loss 3330.3606 epoch total loss 2679.47534\n",
      "Trained batch 1240 batch loss 3334.81836 epoch total loss 2680.00391\n",
      "Trained batch 1241 batch loss 3287.81616 epoch total loss 2680.49365\n",
      "Trained batch 1242 batch loss 3298.79517 epoch total loss 2680.99146\n",
      "Trained batch 1243 batch loss 3325.6084 epoch total loss 2681.50977\n",
      "Trained batch 1244 batch loss 3362.69434 epoch total loss 2682.05737\n",
      "Trained batch 1245 batch loss 3352.52466 epoch total loss 2682.59595\n",
      "Trained batch 1246 batch loss 3336.08911 epoch total loss 2683.12036\n",
      "Trained batch 1247 batch loss 3355.2749 epoch total loss 2683.65942\n",
      "Trained batch 1248 batch loss 3361.32837 epoch total loss 2684.20239\n",
      "Trained batch 1249 batch loss 3377.98389 epoch total loss 2684.75781\n",
      "Trained batch 1250 batch loss 3376.35522 epoch total loss 2685.31104\n",
      "Trained batch 1251 batch loss 3384.64673 epoch total loss 2685.87\n",
      "Trained batch 1252 batch loss 3388.10083 epoch total loss 2686.43091\n",
      "Trained batch 1253 batch loss 3391.03101 epoch total loss 2686.99316\n",
      "Trained batch 1254 batch loss 3393.14282 epoch total loss 2687.5564\n",
      "Trained batch 1255 batch loss 3393.33691 epoch total loss 2688.11865\n",
      "Trained batch 1256 batch loss 3394.52588 epoch total loss 2688.68115\n",
      "Trained batch 1257 batch loss 3395.6 epoch total loss 2689.24341\n",
      "Trained batch 1258 batch loss 3402.56396 epoch total loss 2689.8103\n",
      "Trained batch 1259 batch loss 3354.41504 epoch total loss 2690.33838\n",
      "Trained batch 1260 batch loss 3402.87427 epoch total loss 2690.90381\n",
      "Trained batch 1261 batch loss 3404.18091 epoch total loss 2691.46948\n",
      "Trained batch 1262 batch loss 3406.03345 epoch total loss 2692.03564\n",
      "Trained batch 1263 batch loss 3369.12671 epoch total loss 2692.57178\n",
      "Trained batch 1264 batch loss 3390.92334 epoch total loss 2693.12451\n",
      "Trained batch 1265 batch loss 3420.76245 epoch total loss 2693.69971\n",
      "Trained batch 1266 batch loss 3421.9104 epoch total loss 2694.2749\n",
      "Trained batch 1267 batch loss 3418.2373 epoch total loss 2694.84619\n",
      "Trained batch 1268 batch loss 3422.35645 epoch total loss 2695.42\n",
      "Trained batch 1269 batch loss 3381.03882 epoch total loss 2695.96021\n",
      "Trained batch 1270 batch loss 3421.67822 epoch total loss 2696.53174\n",
      "Trained batch 1271 batch loss 3326.07715 epoch total loss 2697.02686\n",
      "Trained batch 1272 batch loss 3366.58911 epoch total loss 2697.55322\n",
      "Trained batch 1273 batch loss 3431.91626 epoch total loss 2698.13013\n",
      "Trained batch 1274 batch loss 3292.50928 epoch total loss 2698.59668\n",
      "Trained batch 1275 batch loss 3429.15161 epoch total loss 2699.17\n",
      "Trained batch 1276 batch loss 3454.10132 epoch total loss 2699.76147\n",
      "Trained batch 1277 batch loss 3432.23218 epoch total loss 2700.33496\n",
      "Trained batch 1278 batch loss 3432.09814 epoch total loss 2700.90747\n",
      "Trained batch 1279 batch loss 3338.53027 epoch total loss 2701.40601\n",
      "Trained batch 1280 batch loss 3467.91211 epoch total loss 2702.00488\n",
      "Trained batch 1281 batch loss 3472.07129 epoch total loss 2702.60596\n",
      "Trained batch 1282 batch loss 3475.76758 epoch total loss 2703.20898\n",
      "Trained batch 1283 batch loss 3479.22607 epoch total loss 2703.81396\n",
      "Trained batch 1284 batch loss 3477.52979 epoch total loss 2704.4165\n",
      "Trained batch 1285 batch loss 3484.12402 epoch total loss 2705.02319\n",
      "Trained batch 1286 batch loss 3489.57812 epoch total loss 2705.63306\n",
      "Trained batch 1287 batch loss 3443.61548 epoch total loss 2706.20654\n",
      "Trained batch 1288 batch loss 3446.0332 epoch total loss 2706.78076\n",
      "Trained batch 1289 batch loss 3500.72 epoch total loss 2707.39673\n",
      "Trained batch 1290 batch loss 3438.08105 epoch total loss 2707.96313\n",
      "Trained batch 1291 batch loss 3503.41431 epoch total loss 2708.57935\n",
      "Trained batch 1292 batch loss 3510.61743 epoch total loss 2709.2002\n",
      "Trained batch 1293 batch loss 3503.55347 epoch total loss 2709.81445\n",
      "Trained batch 1294 batch loss 3509.89966 epoch total loss 2710.43286\n",
      "Trained batch 1295 batch loss 3510.92261 epoch total loss 2711.05103\n",
      "Trained batch 1296 batch loss 3468.60522 epoch total loss 2711.6355\n",
      "Trained batch 1297 batch loss 3471.72705 epoch total loss 2712.22144\n",
      "Trained batch 1298 batch loss 3526.85815 epoch total loss 2712.84888\n",
      "Trained batch 1299 batch loss 3514.39819 epoch total loss 2713.46606\n",
      "Trained batch 1300 batch loss 3530.63574 epoch total loss 2714.09473\n",
      "Trained batch 1301 batch loss 3429.08032 epoch total loss 2714.64429\n",
      "Trained batch 1302 batch loss 3526.15088 epoch total loss 2715.26758\n",
      "Trained batch 1303 batch loss 3533.62402 epoch total loss 2715.89551\n",
      "Trained batch 1304 batch loss 3536.69922 epoch total loss 2716.52515\n",
      "Trained batch 1305 batch loss 3534.52563 epoch total loss 2717.15186\n",
      "Trained batch 1306 batch loss 3494.61914 epoch total loss 2717.74707\n",
      "Trained batch 1307 batch loss 3539.29541 epoch total loss 2718.37573\n",
      "Trained batch 1308 batch loss 3468.08545 epoch total loss 2718.94873\n",
      "Trained batch 1309 batch loss 3408.00073 epoch total loss 2719.4751\n",
      "Trained batch 1310 batch loss 3232.81494 epoch total loss 2719.86694\n",
      "Trained batch 1311 batch loss 3295.55566 epoch total loss 2720.30615\n",
      "Trained batch 1312 batch loss 3374.45044 epoch total loss 2720.80469\n",
      "Trained batch 1313 batch loss 3498.72485 epoch total loss 2721.39722\n",
      "Trained batch 1314 batch loss 3499.34521 epoch total loss 2721.98926\n",
      "Trained batch 1315 batch loss 3370.94629 epoch total loss 2722.48267\n",
      "Trained batch 1316 batch loss 3527.0708 epoch total loss 2723.09399\n",
      "Trained batch 1317 batch loss 3569.38867 epoch total loss 2723.73682\n",
      "Trained batch 1318 batch loss 3556.82 epoch total loss 2724.36865\n",
      "Trained batch 1319 batch loss 3545.49878 epoch total loss 2724.99121\n",
      "Trained batch 1320 batch loss 3501.0105 epoch total loss 2725.5791\n",
      "Trained batch 1321 batch loss 3551.18164 epoch total loss 2726.2041\n",
      "Trained batch 1322 batch loss 3467.88403 epoch total loss 2726.76538\n",
      "Trained batch 1323 batch loss 3563.10376 epoch total loss 2727.39746\n",
      "Trained batch 1324 batch loss 3573.12427 epoch total loss 2728.03613\n",
      "Trained batch 1325 batch loss 3535.42 epoch total loss 2728.64551\n",
      "Trained batch 1326 batch loss 3578.19019 epoch total loss 2729.28613\n",
      "Trained batch 1327 batch loss 3502.81909 epoch total loss 2729.86914\n",
      "Trained batch 1328 batch loss 3507.71313 epoch total loss 2730.45483\n",
      "Trained batch 1329 batch loss 3580.61646 epoch total loss 2731.09448\n",
      "Trained batch 1330 batch loss 3576.80298 epoch total loss 2731.73022\n",
      "Trained batch 1331 batch loss 3582.18408 epoch total loss 2732.36938\n",
      "Trained batch 1332 batch loss 3580.55688 epoch total loss 2733.0061\n",
      "Trained batch 1333 batch loss 3577.90039 epoch total loss 2733.64\n",
      "Trained batch 1334 batch loss 3583.4292 epoch total loss 2734.2771\n",
      "Trained batch 1335 batch loss 3568.68433 epoch total loss 2734.9021\n",
      "Trained batch 1336 batch loss 3554.94702 epoch total loss 2735.51587\n",
      "Trained batch 1337 batch loss 3588.02979 epoch total loss 2736.15356\n",
      "Trained batch 1338 batch loss 3592.10742 epoch total loss 2736.79321\n",
      "Trained batch 1339 batch loss 3590.354 epoch total loss 2737.43066\n",
      "Trained batch 1340 batch loss 3599.00342 epoch total loss 2738.07349\n",
      "Trained batch 1341 batch loss 3542.84595 epoch total loss 2738.67358\n",
      "Trained batch 1342 batch loss 3583.46655 epoch total loss 2739.30298\n",
      "Trained batch 1343 batch loss 3598.79785 epoch total loss 2739.94312\n",
      "Trained batch 1344 batch loss 3598.29199 epoch total loss 2740.58154\n",
      "Trained batch 1345 batch loss 3602.2417 epoch total loss 2741.22241\n",
      "Trained batch 1346 batch loss 3608.50928 epoch total loss 2741.8667\n",
      "Trained batch 1347 batch loss 3568.44434 epoch total loss 2742.48022\n",
      "Trained batch 1348 batch loss 3605.52979 epoch total loss 2743.12061\n",
      "Trained batch 1349 batch loss 3615.12939 epoch total loss 2743.76709\n",
      "Trained batch 1350 batch loss 3613.59741 epoch total loss 2744.41138\n",
      "Trained batch 1351 batch loss 3604.24536 epoch total loss 2745.04785\n",
      "Trained batch 1352 batch loss 3603.8457 epoch total loss 2745.68286\n",
      "Trained batch 1353 batch loss 3556.08765 epoch total loss 2746.28174\n",
      "Trained batch 1354 batch loss 3614.79 epoch total loss 2746.9231\n",
      "Trained batch 1355 batch loss 3615.85913 epoch total loss 2747.56445\n",
      "Trained batch 1356 batch loss 3580.44238 epoch total loss 2748.17871\n",
      "Trained batch 1357 batch loss 3502.21191 epoch total loss 2748.73438\n",
      "Trained batch 1358 batch loss 3618.23511 epoch total loss 2749.37451\n",
      "Trained batch 1359 batch loss 3611.74927 epoch total loss 2750.00928\n",
      "Trained batch 1360 batch loss 3616.91748 epoch total loss 2750.64673\n",
      "Trained batch 1361 batch loss 3618.38428 epoch total loss 2751.28442\n",
      "Trained batch 1362 batch loss 3615.36865 epoch total loss 2751.9187\n",
      "Trained batch 1363 batch loss 3611.77563 epoch total loss 2752.54956\n",
      "Trained batch 1364 batch loss 3609.7688 epoch total loss 2753.17798\n",
      "Trained batch 1365 batch loss 3612.1123 epoch total loss 2753.80713\n",
      "Trained batch 1366 batch loss 3618.21216 epoch total loss 2754.44\n",
      "Trained batch 1367 batch loss 3618.76904 epoch total loss 2755.07227\n",
      "Trained batch 1368 batch loss 3607.79053 epoch total loss 2755.69556\n",
      "Trained batch 1369 batch loss 3574.53467 epoch total loss 2756.2937\n",
      "Trained batch 1370 batch loss 3616.51025 epoch total loss 2756.92163\n",
      "Trained batch 1371 batch loss 3615.08618 epoch total loss 2757.54736\n",
      "Trained batch 1372 batch loss 3492.40771 epoch total loss 2758.08301\n",
      "Trained batch 1373 batch loss 3617.73877 epoch total loss 2758.70923\n",
      "Trained batch 1374 batch loss 3618.37939 epoch total loss 2759.33496\n",
      "Trained batch 1375 batch loss 3615.78442 epoch total loss 2759.95776\n",
      "Trained batch 1376 batch loss 3618.65112 epoch total loss 2760.58203\n",
      "Trained batch 1377 batch loss 3619.11572 epoch total loss 2761.20532\n",
      "Trained batch 1378 batch loss 3615.43506 epoch total loss 2761.8252\n",
      "Trained batch 1379 batch loss 3603.62231 epoch total loss 2762.43555\n",
      "Trained batch 1380 batch loss 3621.05957 epoch total loss 2763.05786\n",
      "Trained batch 1381 batch loss 3614.07153 epoch total loss 2763.67407\n",
      "Trained batch 1382 batch loss 3621.40234 epoch total loss 2764.29468\n",
      "Trained batch 1383 batch loss 3596.34082 epoch total loss 2764.89624\n",
      "Trained batch 1384 batch loss 3625.66699 epoch total loss 2765.51831\n",
      "Trained batch 1385 batch loss 3553.1604 epoch total loss 2766.08691\n",
      "Trained batch 1386 batch loss 3625.38965 epoch total loss 2766.70703\n",
      "Trained batch 1387 batch loss 3628.23657 epoch total loss 2767.32812\n",
      "Trained batch 1388 batch loss 3629.97705 epoch total loss 2767.94971\n",
      "Epoch 7 train loss 2767.94970703125\n",
      "Validated batch 1 batch loss 1.33249779e+26\n",
      "Validated batch 2 batch loss 1.34563492e+26\n",
      "Validated batch 3 batch loss 1.34132825e+26\n",
      "Validated batch 4 batch loss 1.35997588e+26\n",
      "Validated batch 5 batch loss 1.38584356e+26\n",
      "Validated batch 6 batch loss 1.38736376e+26\n",
      "Validated batch 7 batch loss 1.36074769e+26\n",
      "Validated batch 8 batch loss 1.34923757e+26\n",
      "Validated batch 9 batch loss 1.36856745e+26\n",
      "Validated batch 10 batch loss 1.36879407e+26\n",
      "Validated batch 11 batch loss 1.36030773e+26\n",
      "Validated batch 12 batch loss 1.36990991e+26\n",
      "Validated batch 13 batch loss 1.32592005e+26\n",
      "Validated batch 14 batch loss 1.39021507e+26\n",
      "Validated batch 15 batch loss 1.37531508e+26\n",
      "Validated batch 16 batch loss 1.38297731e+26\n",
      "Validated batch 17 batch loss 1.332664e+26\n",
      "Validated batch 18 batch loss 1.30774945e+26\n",
      "Validated batch 19 batch loss 1.34688644e+26\n",
      "Validated batch 20 batch loss 1.37170303e+26\n",
      "Validated batch 21 batch loss 1.3670609e+26\n",
      "Validated batch 22 batch loss 1.37718697e+26\n",
      "Validated batch 23 batch loss 1.37366142e+26\n",
      "Validated batch 24 batch loss 1.38781644e+26\n",
      "Validated batch 25 batch loss 1.34275944e+26\n",
      "Validated batch 26 batch loss 1.35490155e+26\n",
      "Validated batch 27 batch loss 1.35771274e+26\n",
      "Validated batch 28 batch loss 1.31653804e+26\n",
      "Validated batch 29 batch loss 1.35002155e+26\n",
      "Validated batch 30 batch loss 1.35472492e+26\n",
      "Validated batch 31 batch loss 1.35771726e+26\n",
      "Validated batch 32 batch loss 1.35858416e+26\n",
      "Validated batch 33 batch loss 1.33767118e+26\n",
      "Validated batch 34 batch loss 1.33985657e+26\n",
      "Validated batch 35 batch loss 1.31412096e+26\n",
      "Validated batch 36 batch loss 1.34847913e+26\n",
      "Validated batch 37 batch loss 1.32540428e+26\n",
      "Validated batch 38 batch loss 1.38106032e+26\n",
      "Validated batch 39 batch loss 1.3632594e+26\n",
      "Validated batch 40 batch loss 1.31925358e+26\n",
      "Validated batch 41 batch loss 1.32029167e+26\n",
      "Validated batch 42 batch loss 1.31401028e+26\n",
      "Validated batch 43 batch loss 1.37301911e+26\n",
      "Validated batch 44 batch loss 1.33383389e+26\n",
      "Validated batch 45 batch loss 1.35942745e+26\n",
      "Validated batch 46 batch loss 1.33513328e+26\n",
      "Validated batch 47 batch loss 1.368e+26\n",
      "Validated batch 48 batch loss 1.36224925e+26\n",
      "Validated batch 49 batch loss 1.36055879e+26\n",
      "Validated batch 50 batch loss 1.29428739e+26\n",
      "Validated batch 51 batch loss 1.33097151e+26\n",
      "Validated batch 52 batch loss 1.37095907e+26\n",
      "Validated batch 53 batch loss 1.33045546e+26\n",
      "Validated batch 54 batch loss 1.32260554e+26\n",
      "Validated batch 55 batch loss 1.30536097e+26\n",
      "Validated batch 56 batch loss 1.34568897e+26\n",
      "Validated batch 57 batch loss 1.34174828e+26\n",
      "Validated batch 58 batch loss 1.33715661e+26\n",
      "Validated batch 59 batch loss 1.34163908e+26\n",
      "Validated batch 60 batch loss 1.34368334e+26\n",
      "Validated batch 61 batch loss 1.37218679e+26\n",
      "Validated batch 62 batch loss 1.35127289e+26\n",
      "Validated batch 63 batch loss 1.38001891e+26\n",
      "Validated batch 64 batch loss 1.36842845e+26\n",
      "Validated batch 65 batch loss 1.35543632e+26\n",
      "Validated batch 66 batch loss 1.33631479e+26\n",
      "Validated batch 67 batch loss 1.33500858e+26\n",
      "Validated batch 68 batch loss 1.29009361e+26\n",
      "Validated batch 69 batch loss 1.40954431e+26\n",
      "Validated batch 70 batch loss 1.3365094e+26\n",
      "Validated batch 71 batch loss 1.38808346e+26\n",
      "Validated batch 72 batch loss 1.28436166e+26\n",
      "Validated batch 73 batch loss 1.28142503e+26\n",
      "Validated batch 74 batch loss 1.34248689e+26\n",
      "Validated batch 75 batch loss 1.35456711e+26\n",
      "Validated batch 76 batch loss 1.33661649e+26\n",
      "Validated batch 77 batch loss 1.37607177e+26\n",
      "Validated batch 78 batch loss 1.38646024e+26\n",
      "Validated batch 79 batch loss 1.34305486e+26\n",
      "Validated batch 80 batch loss 1.3644862e+26\n",
      "Validated batch 81 batch loss 1.32035448e+26\n",
      "Validated batch 82 batch loss 1.37953127e+26\n",
      "Validated batch 83 batch loss 1.35060788e+26\n",
      "Validated batch 84 batch loss 1.34723213e+26\n",
      "Validated batch 85 batch loss 1.380918e+26\n",
      "Validated batch 86 batch loss 1.34016536e+26\n",
      "Validated batch 87 batch loss 1.36072408e+26\n",
      "Validated batch 88 batch loss 1.39120225e+26\n",
      "Validated batch 89 batch loss 1.33654888e+26\n",
      "Validated batch 90 batch loss 1.36277572e+26\n",
      "Validated batch 91 batch loss 1.38510984e+26\n",
      "Validated batch 92 batch loss 1.35724982e+26\n",
      "Validated batch 93 batch loss 1.33836994e+26\n",
      "Validated batch 94 batch loss 1.32990307e+26\n",
      "Validated batch 95 batch loss 1.33901549e+26\n",
      "Validated batch 96 batch loss 1.37028761e+26\n",
      "Validated batch 97 batch loss 1.34516425e+26\n",
      "Validated batch 98 batch loss 1.34488496e+26\n",
      "Validated batch 99 batch loss 1.34400976e+26\n",
      "Validated batch 100 batch loss 1.34746345e+26\n",
      "Validated batch 101 batch loss 1.33618714e+26\n",
      "Validated batch 102 batch loss 1.37277773e+26\n",
      "Validated batch 103 batch loss 1.34237787e+26\n",
      "Validated batch 104 batch loss 1.32950794e+26\n",
      "Validated batch 105 batch loss 1.3533784e+26\n",
      "Validated batch 106 batch loss 1.34958464e+26\n",
      "Validated batch 107 batch loss 1.34163409e+26\n",
      "Validated batch 108 batch loss 1.34443108e+26\n",
      "Validated batch 109 batch loss 1.35704875e+26\n",
      "Validated batch 110 batch loss 1.34167071e+26\n",
      "Validated batch 111 batch loss 1.40861994e+26\n",
      "Validated batch 112 batch loss 1.40181733e+26\n",
      "Validated batch 113 batch loss 1.37379332e+26\n",
      "Validated batch 114 batch loss 1.36649403e+26\n",
      "Validated batch 115 batch loss 1.326258e+26\n",
      "Validated batch 116 batch loss 1.37169436e+26\n",
      "Validated batch 117 batch loss 1.28540768e+26\n",
      "Validated batch 118 batch loss 1.3309538e+26\n",
      "Validated batch 119 batch loss 1.34675196e+26\n",
      "Validated batch 120 batch loss 1.35847302e+26\n",
      "Validated batch 121 batch loss 1.35989914e+26\n",
      "Validated batch 122 batch loss 1.3087061e+26\n",
      "Validated batch 123 batch loss 1.36662436e+26\n",
      "Validated batch 124 batch loss 1.3426067e+26\n",
      "Validated batch 125 batch loss 1.37013634e+26\n",
      "Validated batch 126 batch loss 1.38257185e+26\n",
      "Validated batch 127 batch loss 1.3766141e+26\n",
      "Validated batch 128 batch loss 1.34882888e+26\n",
      "Validated batch 129 batch loss 1.36655362e+26\n",
      "Validated batch 130 batch loss 1.3662997e+26\n",
      "Validated batch 131 batch loss 1.33883563e+26\n",
      "Validated batch 132 batch loss 1.37624978e+26\n",
      "Validated batch 133 batch loss 1.31079437e+26\n",
      "Validated batch 134 batch loss 1.34942701e+26\n",
      "Validated batch 135 batch loss 1.31325867e+26\n",
      "Validated batch 136 batch loss 1.36887671e+26\n",
      "Validated batch 137 batch loss 1.33996771e+26\n",
      "Validated batch 138 batch loss 1.36554347e+26\n",
      "Validated batch 139 batch loss 1.4259062e+26\n",
      "Validated batch 140 batch loss 1.35241806e+26\n",
      "Validated batch 141 batch loss 1.38262424e+26\n",
      "Validated batch 142 batch loss 1.37911705e+26\n",
      "Validated batch 143 batch loss 1.37546256e+26\n",
      "Validated batch 144 batch loss 1.34912808e+26\n",
      "Validated batch 145 batch loss 1.3498096e+26\n",
      "Validated batch 146 batch loss 1.35216165e+26\n",
      "Validated batch 147 batch loss 1.34040517e+26\n",
      "Validated batch 148 batch loss 1.34884437e+26\n",
      "Validated batch 149 batch loss 1.34630287e+26\n",
      "Validated batch 150 batch loss 1.35461507e+26\n",
      "Validated batch 151 batch loss 1.35371754e+26\n",
      "Validated batch 152 batch loss 1.32983703e+26\n",
      "Validated batch 153 batch loss 1.3297828e+26\n",
      "Validated batch 154 batch loss 1.35629031e+26\n",
      "Validated batch 155 batch loss 1.38123335e+26\n",
      "Validated batch 156 batch loss 1.36121522e+26\n",
      "Validated batch 157 batch loss 1.36697706e+26\n",
      "Validated batch 158 batch loss 1.3395854e+26\n",
      "Validated batch 159 batch loss 1.33373483e+26\n",
      "Validated batch 160 batch loss 1.3440877e+26\n",
      "Validated batch 161 batch loss 1.31802152e+26\n",
      "Validated batch 162 batch loss 1.36042026e+26\n",
      "Validated batch 163 batch loss 1.32837485e+26\n",
      "Validated batch 164 batch loss 1.33657784e+26\n",
      "Validated batch 165 batch loss 1.36592717e+26\n",
      "Validated batch 166 batch loss 1.37128834e+26\n",
      "Validated batch 167 batch loss 1.35945014e+26\n",
      "Validated batch 168 batch loss 1.33858577e+26\n",
      "Validated batch 169 batch loss 1.36644699e+26\n",
      "Validated batch 170 batch loss 1.38709739e+26\n",
      "Validated batch 171 batch loss 1.33932115e+26\n",
      "Validated batch 172 batch loss 1.35308473e+26\n",
      "Validated batch 173 batch loss 1.33709767e+26\n",
      "Validated batch 174 batch loss 1.25725481e+26\n",
      "Validated batch 175 batch loss 1.34800043e+26\n",
      "Validated batch 176 batch loss 1.38175023e+26\n",
      "Validated batch 177 batch loss 1.37509114e+26\n",
      "Validated batch 178 batch loss 1.34133793e+26\n",
      "Validated batch 179 batch loss 1.34185232e+26\n",
      "Validated batch 180 batch loss 1.36449717e+26\n",
      "Validated batch 181 batch loss 1.38209122e+26\n",
      "Validated batch 182 batch loss 1.36969777e+26\n",
      "Validated batch 183 batch loss 1.3535162e+26\n",
      "Validated batch 184 batch loss 1.32729286e+26\n",
      "Validated batch 185 batch loss 1.36480911e+26\n",
      "Epoch 7 val loss 1.3516714291767325e+26\n",
      "Start epoch 8 with learning rate 0.5\n",
      "Start distributed traininng...\n",
      "Trained batch 1 batch loss 3636.55835 epoch total loss 3636.55835\n",
      "Trained batch 2 batch loss 3638.16772 epoch total loss 3637.36304\n",
      "Trained batch 3 batch loss 3565.55127 epoch total loss 3613.42578\n",
      "Trained batch 4 batch loss 3633.04541 epoch total loss 3618.33057\n",
      "Trained batch 5 batch loss 3552.40186 epoch total loss 3605.14502\n",
      "Trained batch 6 batch loss 3643.86108 epoch total loss 3611.59766\n",
      "Trained batch 7 batch loss 3635.66821 epoch total loss 3615.03638\n",
      "Trained batch 8 batch loss 3643.75781 epoch total loss 3618.62646\n",
      "Trained batch 9 batch loss 3645.43115 epoch total loss 3621.60474\n",
      "Trained batch 10 batch loss 3651.15918 epoch total loss 3624.56\n",
      "Trained batch 11 batch loss 3568.0127 epoch total loss 3619.41943\n",
      "Trained batch 12 batch loss 3658.16797 epoch total loss 3622.64844\n",
      "Trained batch 13 batch loss 3655.79688 epoch total loss 3625.19824\n",
      "Trained batch 14 batch loss 3659.80273 epoch total loss 3627.67017\n",
      "Trained batch 15 batch loss 3665.05566 epoch total loss 3630.1626\n",
      "Trained batch 16 batch loss 3591.02563 epoch total loss 3627.71655\n",
      "Trained batch 17 batch loss 3616.72437 epoch total loss 3627.06982\n",
      "Trained batch 18 batch loss 3554.91138 epoch total loss 3623.06104\n",
      "Trained batch 19 batch loss 3612.89014 epoch total loss 3622.52539\n",
      "Trained batch 20 batch loss 3670.42505 epoch total loss 3624.92041\n",
      "Trained batch 21 batch loss 3648.7356 epoch total loss 3626.0542\n",
      "Trained batch 22 batch loss 3615.63232 epoch total loss 3625.58057\n",
      "Trained batch 23 batch loss 3632.15454 epoch total loss 3625.86646\n",
      "Trained batch 24 batch loss 3631.14697 epoch total loss 3626.08667\n",
      "Trained batch 25 batch loss 3688.7251 epoch total loss 3628.59229\n",
      "Trained batch 26 batch loss 3686.40332 epoch total loss 3630.81592\n",
      "Trained batch 27 batch loss 3697.02319 epoch total loss 3633.26782\n",
      "Trained batch 28 batch loss 3584.81616 epoch total loss 3631.53735\n",
      "Trained batch 29 batch loss 3678.08545 epoch total loss 3633.14258\n",
      "Trained batch 30 batch loss 3580.57031 epoch total loss 3631.39014\n",
      "Trained batch 31 batch loss 3617.67236 epoch total loss 3630.94751\n",
      "Trained batch 32 batch loss 3702.08887 epoch total loss 3633.17065\n",
      "Trained batch 33 batch loss 3708.71265 epoch total loss 3635.45972\n",
      "Trained batch 34 batch loss 3717.68042 epoch total loss 3637.87793\n",
      "Trained batch 35 batch loss 3712.7522 epoch total loss 3640.01709\n",
      "Trained batch 36 batch loss 3721.73462 epoch total loss 3642.28735\n",
      "Trained batch 37 batch loss 3720.72754 epoch total loss 3644.40747\n",
      "Trained batch 38 batch loss 3718.14575 epoch total loss 3646.3479\n",
      "Trained batch 39 batch loss 3677.09961 epoch total loss 3647.13623\n",
      "Trained batch 40 batch loss 3633.68579 epoch total loss 3646.8\n",
      "Trained batch 41 batch loss 3687.43677 epoch total loss 3647.79126\n",
      "Trained batch 42 batch loss 3677.78271 epoch total loss 3648.50513\n",
      "Trained batch 43 batch loss 3712.11133 epoch total loss 3649.98438\n",
      "Trained batch 44 batch loss 3716.52612 epoch total loss 3651.49683\n",
      "Trained batch 45 batch loss 3719.22314 epoch total loss 3653.00171\n",
      "Trained batch 46 batch loss 3668.89038 epoch total loss 3653.34717\n",
      "Trained batch 47 batch loss 3673.79688 epoch total loss 3653.78223\n",
      "Trained batch 48 batch loss 3741.65259 epoch total loss 3655.61304\n",
      "Trained batch 49 batch loss 3743.28296 epoch total loss 3657.4021\n",
      "Trained batch 50 batch loss 3745.08032 epoch total loss 3659.15552\n",
      "Trained batch 51 batch loss 3741.35352 epoch total loss 3660.76758\n",
      "Trained batch 52 batch loss 3733.54053 epoch total loss 3662.16699\n",
      "Trained batch 53 batch loss 3734.63721 epoch total loss 3663.53442\n",
      "Trained batch 54 batch loss 3683.26416 epoch total loss 3663.9\n",
      "Trained batch 55 batch loss 3752.21729 epoch total loss 3665.50562\n",
      "Trained batch 56 batch loss 3723.67749 epoch total loss 3666.54443\n",
      "Trained batch 57 batch loss 3574.7749 epoch total loss 3664.93457\n",
      "Trained batch 58 batch loss 3735.20483 epoch total loss 3666.146\n",
      "Trained batch 59 batch loss 3768.15576 epoch total loss 3667.875\n",
      "Trained batch 60 batch loss 3740.81421 epoch total loss 3669.09058\n",
      "Trained batch 61 batch loss 3690.83521 epoch total loss 3669.44702\n",
      "Trained batch 62 batch loss 3765.5481 epoch total loss 3670.99707\n",
      "Trained batch 63 batch loss 3776.76123 epoch total loss 3672.67578\n",
      "Trained batch 64 batch loss 3784.00049 epoch total loss 3674.41528\n",
      "Trained batch 65 batch loss 3787.25757 epoch total loss 3676.15112\n",
      "Trained batch 66 batch loss 3789.89648 epoch total loss 3677.87451\n",
      "Trained batch 67 batch loss 3788.2146 epoch total loss 3679.52148\n",
      "Trained batch 68 batch loss 3784.80054 epoch total loss 3681.06958\n",
      "Trained batch 69 batch loss 3740.28906 epoch total loss 3681.92798\n",
      "Trained batch 70 batch loss 3733.97827 epoch total loss 3682.67163\n",
      "Trained batch 71 batch loss 3802.73755 epoch total loss 3684.36279\n",
      "Trained batch 72 batch loss 3724.47412 epoch total loss 3684.91968\n",
      "Trained batch 73 batch loss 3805.54761 epoch total loss 3686.57227\n",
      "Trained batch 74 batch loss 3796.40259 epoch total loss 3688.05664\n",
      "Trained batch 75 batch loss 3631.98145 epoch total loss 3687.30884\n",
      "Trained batch 76 batch loss 3817.72729 epoch total loss 3689.02466\n",
      "Trained batch 77 batch loss 3821.69849 epoch total loss 3690.74756\n",
      "Trained batch 78 batch loss 3824.65674 epoch total loss 3692.46436\n",
      "Trained batch 79 batch loss 3827.65503 epoch total loss 3694.17554\n",
      "Trained batch 80 batch loss 3814.37354 epoch total loss 3695.67822\n",
      "Trained batch 81 batch loss 3790.75342 epoch total loss 3696.85181\n",
      "Trained batch 82 batch loss 3631.80029 epoch total loss 3696.05859\n",
      "Trained batch 83 batch loss 3523.35864 epoch total loss 3693.97778\n",
      "Trained batch 84 batch loss 3709.34692 epoch total loss 3694.16064\n",
      "Trained batch 85 batch loss 3802.97583 epoch total loss 3695.44092\n",
      "Trained batch 86 batch loss 3853.89233 epoch total loss 3697.28345\n",
      "Trained batch 87 batch loss 3723.34546 epoch total loss 3697.58301\n",
      "Trained batch 88 batch loss 3838.76367 epoch total loss 3699.18726\n",
      "Trained batch 89 batch loss 3775.65137 epoch total loss 3700.04639\n",
      "Trained batch 90 batch loss 3861.17798 epoch total loss 3701.83691\n",
      "Trained batch 91 batch loss 3853.51514 epoch total loss 3703.50342\n",
      "Trained batch 92 batch loss 3866.39502 epoch total loss 3705.27417\n",
      "Trained batch 93 batch loss 3863.71143 epoch total loss 3706.97778\n",
      "Trained batch 94 batch loss 3810.87573 epoch total loss 3708.08301\n",
      "Trained batch 95 batch loss 3878.49268 epoch total loss 3709.87695\n",
      "Trained batch 96 batch loss 3755.36475 epoch total loss 3710.35083\n",
      "Trained batch 97 batch loss 3882.25635 epoch total loss 3712.12305\n",
      "Trained batch 98 batch loss 3878.93921 epoch total loss 3713.8252\n",
      "Trained batch 99 batch loss 3896.82715 epoch total loss 3715.67358\n",
      "Trained batch 100 batch loss 3900.48853 epoch total loss 3717.52197\n",
      "Trained batch 101 batch loss 3908.27612 epoch total loss 3719.41064\n",
      "Trained batch 102 batch loss 3893.77783 epoch total loss 3721.12\n",
      "Trained batch 103 batch loss 3911.42017 epoch total loss 3722.96753\n",
      "Trained batch 104 batch loss 3907.24756 epoch total loss 3724.7395\n",
      "Trained batch 105 batch loss 3908.74268 epoch total loss 3726.49194\n",
      "Trained batch 106 batch loss 3922.03955 epoch total loss 3728.33667\n",
      "Trained batch 107 batch loss 3829.88135 epoch total loss 3729.28564\n",
      "Trained batch 108 batch loss 3854.47729 epoch total loss 3730.44482\n",
      "Trained batch 109 batch loss 3917.48633 epoch total loss 3732.16089\n",
      "Trained batch 110 batch loss 3913.14526 epoch total loss 3733.80615\n",
      "Trained batch 111 batch loss 3831.93457 epoch total loss 3734.69043\n",
      "Trained batch 112 batch loss 3927.41016 epoch total loss 3736.41089\n",
      "Trained batch 113 batch loss 3813.63647 epoch total loss 3737.09424\n",
      "Trained batch 114 batch loss 3874.85815 epoch total loss 3738.30273\n",
      "Trained batch 115 batch loss 3868.07227 epoch total loss 3739.43091\n",
      "Trained batch 116 batch loss 3908.57788 epoch total loss 3740.88892\n",
      "Trained batch 117 batch loss 3949.26074 epoch total loss 3742.67\n",
      "Trained batch 118 batch loss 3948.8772 epoch total loss 3744.41748\n",
      "Trained batch 119 batch loss 3940.55127 epoch total loss 3746.06567\n",
      "Trained batch 120 batch loss 3908.14429 epoch total loss 3747.4165\n",
      "Trained batch 121 batch loss 3890.46777 epoch total loss 3748.59863\n",
      "Trained batch 122 batch loss 3930.5708 epoch total loss 3750.09\n",
      "Trained batch 123 batch loss 3899.3833 epoch total loss 3751.30396\n",
      "Trained batch 124 batch loss 3918.47949 epoch total loss 3752.65186\n",
      "Trained batch 125 batch loss 3956.28784 epoch total loss 3754.28101\n",
      "Trained batch 126 batch loss 3907.20972 epoch total loss 3755.49487\n",
      "Trained batch 127 batch loss 3967.60303 epoch total loss 3757.16479\n",
      "Trained batch 128 batch loss 3911.16626 epoch total loss 3758.36792\n",
      "Trained batch 129 batch loss 3945.45483 epoch total loss 3759.81836\n",
      "Trained batch 130 batch loss 3981.71167 epoch total loss 3761.52515\n",
      "Trained batch 131 batch loss 3960.1521 epoch total loss 3763.0415\n",
      "Trained batch 132 batch loss 3967.19287 epoch total loss 3764.58813\n",
      "Trained batch 133 batch loss 3953.68823 epoch total loss 3766.00977\n",
      "Trained batch 134 batch loss 4002.3772 epoch total loss 3767.77368\n",
      "Trained batch 135 batch loss 4003.02393 epoch total loss 3769.51636\n",
      "Trained batch 136 batch loss 3999.68677 epoch total loss 3771.20898\n",
      "Trained batch 137 batch loss 3938.80249 epoch total loss 3772.43237\n",
      "Trained batch 138 batch loss 3822.40039 epoch total loss 3772.79443\n",
      "Trained batch 139 batch loss 3897.86108 epoch total loss 3773.69434\n",
      "Trained batch 140 batch loss 3853.65063 epoch total loss 3774.26514\n",
      "Trained batch 141 batch loss 4022.69214 epoch total loss 3776.0271\n",
      "Trained batch 142 batch loss 3887.3418 epoch total loss 3776.81079\n",
      "Trained batch 143 batch loss 4041.12305 epoch total loss 3778.65918\n",
      "Trained batch 144 batch loss 4045.87256 epoch total loss 3780.51465\n",
      "Trained batch 145 batch loss 4051.35645 epoch total loss 3782.38281\n",
      "Trained batch 146 batch loss 4056.22046 epoch total loss 3784.25854\n",
      "Trained batch 147 batch loss 4060.18384 epoch total loss 3786.13574\n",
      "Trained batch 148 batch loss 4064.75806 epoch total loss 3788.01807\n",
      "Trained batch 149 batch loss 4069.05615 epoch total loss 3789.9043\n",
      "Trained batch 150 batch loss 4072.83081 epoch total loss 3791.79053\n",
      "Trained batch 151 batch loss 4075.44165 epoch total loss 3793.66895\n",
      "Trained batch 152 batch loss 4068.98315 epoch total loss 3795.48022\n",
      "Trained batch 153 batch loss 3993.42798 epoch total loss 3796.77417\n",
      "Trained batch 154 batch loss 4064.1416 epoch total loss 3798.51025\n",
      "Trained batch 155 batch loss 4084.73315 epoch total loss 3800.35693\n",
      "Trained batch 156 batch loss 4042.90308 epoch total loss 3801.91138\n",
      "Trained batch 157 batch loss 4087.67969 epoch total loss 3803.73169\n",
      "Trained batch 158 batch loss 4065.88574 epoch total loss 3805.39087\n",
      "Trained batch 159 batch loss 4102.95557 epoch total loss 3807.26221\n",
      "Trained batch 160 batch loss 4112.50635 epoch total loss 3809.17\n",
      "Trained batch 161 batch loss 4081.26099 epoch total loss 3810.85986\n",
      "Trained batch 162 batch loss 4088.34253 epoch total loss 3812.57251\n",
      "Trained batch 163 batch loss 4128.76855 epoch total loss 3814.51221\n",
      "Trained batch 164 batch loss 4101.44043 epoch total loss 3816.26172\n",
      "Trained batch 165 batch loss 4121.3252 epoch total loss 3818.1106\n",
      "Trained batch 166 batch loss 4140.61084 epoch total loss 3820.05347\n",
      "Trained batch 167 batch loss 4144.45215 epoch total loss 3821.99585\n",
      "Trained batch 168 batch loss 4146.18555 epoch total loss 3823.92554\n",
      "Trained batch 169 batch loss 4149.59277 epoch total loss 3825.85254\n",
      "Trained batch 170 batch loss 4155.62402 epoch total loss 3827.79224\n",
      "Trained batch 171 batch loss 4156.69531 epoch total loss 3829.71558\n",
      "Trained batch 172 batch loss 4141.75439 epoch total loss 3831.52979\n",
      "Trained batch 173 batch loss 4168.57031 epoch total loss 3833.47803\n",
      "Trained batch 174 batch loss 4093.7417 epoch total loss 3834.97388\n",
      "Trained batch 175 batch loss 4168.53613 epoch total loss 3836.88\n",
      "Trained batch 176 batch loss 4114.49561 epoch total loss 3838.45728\n",
      "Trained batch 177 batch loss 4080.8855 epoch total loss 3839.8269\n",
      "Trained batch 178 batch loss 4101.02441 epoch total loss 3841.29419\n",
      "Trained batch 179 batch loss 4042.98413 epoch total loss 3842.42114\n",
      "Trained batch 180 batch loss 4109.1958 epoch total loss 3843.90308\n",
      "Trained batch 181 batch loss 4194.41113 epoch total loss 3845.83984\n",
      "Trained batch 182 batch loss 4143.20459 epoch total loss 3847.47363\n",
      "Trained batch 183 batch loss 4136.63428 epoch total loss 3849.05371\n",
      "Trained batch 184 batch loss 4127.82471 epoch total loss 3850.5686\n",
      "Trained batch 185 batch loss 4199.61816 epoch total loss 3852.45532\n",
      "Trained batch 186 batch loss 4216.06592 epoch total loss 3854.4104\n",
      "Trained batch 187 batch loss 4218.9834 epoch total loss 3856.35986\n",
      "Trained batch 188 batch loss 4146.1084 epoch total loss 3857.90137\n",
      "Trained batch 189 batch loss 4091.09082 epoch total loss 3859.13501\n",
      "Trained batch 190 batch loss 4095.11206 epoch total loss 3860.37695\n",
      "Trained batch 191 batch loss 4173.97412 epoch total loss 3862.01904\n",
      "Trained batch 192 batch loss 4100.47461 epoch total loss 3863.26099\n",
      "Trained batch 193 batch loss 4187.00635 epoch total loss 3864.93848\n",
      "Trained batch 194 batch loss 4201.95752 epoch total loss 3866.67554\n",
      "Trained batch 195 batch loss 3974.7124 epoch total loss 3867.22949\n",
      "Trained batch 196 batch loss 3937.3606 epoch total loss 3867.5874\n",
      "Trained batch 197 batch loss 4078.24878 epoch total loss 3868.65674\n",
      "Trained batch 198 batch loss 4256.12451 epoch total loss 3870.61353\n",
      "Trained batch 199 batch loss 4269.51709 epoch total loss 3872.61816\n",
      "Trained batch 200 batch loss 4185.29199 epoch total loss 3874.18164\n",
      "Trained batch 201 batch loss 4270.92383 epoch total loss 3876.15552\n",
      "Trained batch 202 batch loss 4190.75098 epoch total loss 3877.71289\n",
      "Trained batch 203 batch loss 4279.78223 epoch total loss 3879.6936\n",
      "Trained batch 204 batch loss 4281.55 epoch total loss 3881.66357\n",
      "Trained batch 205 batch loss 4275.41797 epoch total loss 3883.58447\n",
      "Trained batch 206 batch loss 4212.81104 epoch total loss 3885.18262\n",
      "Trained batch 207 batch loss 4229.59 epoch total loss 3886.84644\n",
      "Trained batch 208 batch loss 4158.09961 epoch total loss 3888.15063\n",
      "Trained batch 209 batch loss 4181.42773 epoch total loss 3889.55371\n",
      "Trained batch 210 batch loss 4184.42334 epoch total loss 3890.95801\n",
      "Trained batch 211 batch loss 4247.09961 epoch total loss 3892.646\n",
      "Trained batch 212 batch loss 4303.53027 epoch total loss 3894.58398\n",
      "Trained batch 213 batch loss 4155.31055 epoch total loss 3895.80811\n",
      "Trained batch 214 batch loss 4099.97559 epoch total loss 3896.76221\n",
      "Trained batch 215 batch loss 3950.59204 epoch total loss 3897.01245\n",
      "Trained batch 216 batch loss 4299.29736 epoch total loss 3898.875\n",
      "Trained batch 217 batch loss 4239.20166 epoch total loss 3900.44336\n",
      "Trained batch 218 batch loss 4308.33447 epoch total loss 3902.31421\n",
      "Trained batch 219 batch loss 4319.31738 epoch total loss 3904.21826\n",
      "Trained batch 220 batch loss 4333.87402 epoch total loss 3906.17139\n",
      "Trained batch 221 batch loss 4332.34717 epoch total loss 3908.09985\n",
      "Trained batch 222 batch loss 4258.48682 epoch total loss 3909.67822\n",
      "Trained batch 223 batch loss 4337.55273 epoch total loss 3911.59692\n",
      "Trained batch 224 batch loss 4320.2251 epoch total loss 3913.42139\n",
      "Trained batch 225 batch loss 4265.36963 epoch total loss 3914.9856\n",
      "Trained batch 226 batch loss 4338.0918 epoch total loss 3916.85767\n",
      "Trained batch 227 batch loss 4322.41064 epoch total loss 3918.64429\n",
      "Trained batch 228 batch loss 4354.85498 epoch total loss 3920.55762\n",
      "Trained batch 229 batch loss 4359.11816 epoch total loss 3922.47266\n",
      "Trained batch 230 batch loss 4355.73486 epoch total loss 3924.35645\n",
      "Trained batch 231 batch loss 4294.6084 epoch total loss 3925.95947\n",
      "Trained batch 232 batch loss 4217.95898 epoch total loss 3927.21802\n",
      "Trained batch 233 batch loss 4208.49854 epoch total loss 3928.42505\n",
      "Trained batch 234 batch loss 4375.11328 epoch total loss 3930.33423\n",
      "Trained batch 235 batch loss 4378.47363 epoch total loss 3932.24121\n",
      "Trained batch 236 batch loss 4298.71631 epoch total loss 3933.79395\n",
      "Trained batch 237 batch loss 4361.23291 epoch total loss 3935.59766\n",
      "Trained batch 238 batch loss 4291.65234 epoch total loss 3937.09351\n",
      "Trained batch 239 batch loss 4307.76465 epoch total loss 3938.64429\n",
      "Trained batch 240 batch loss 4360.28174 epoch total loss 3940.40137\n",
      "Trained batch 241 batch loss 4396.9834 epoch total loss 3942.2959\n",
      "Trained batch 242 batch loss 4394.26074 epoch total loss 3944.16357\n",
      "Trained batch 243 batch loss 4400.24951 epoch total loss 3946.04028\n",
      "Trained batch 244 batch loss 4397.54639 epoch total loss 3947.89087\n",
      "Trained batch 245 batch loss 4402.52441 epoch total loss 3949.74634\n",
      "Trained batch 246 batch loss 4286.51465 epoch total loss 3951.11523\n",
      "Trained batch 247 batch loss 4410.84668 epoch total loss 3952.97681\n",
      "Trained batch 248 batch loss 4415.34521 epoch total loss 3954.84131\n",
      "Trained batch 249 batch loss 4413.17822 epoch total loss 3956.68188\n",
      "Trained batch 250 batch loss 4383.62305 epoch total loss 3958.38965\n",
      "Trained batch 251 batch loss 4418.01562 epoch total loss 3960.22095\n",
      "Trained batch 252 batch loss 4357.11475 epoch total loss 3961.7959\n",
      "Trained batch 253 batch loss 4434.05225 epoch total loss 3963.6626\n",
      "Trained batch 254 batch loss 4432.02783 epoch total loss 3965.50635\n",
      "Trained batch 255 batch loss 4439.01416 epoch total loss 3967.36328\n",
      "Trained batch 256 batch loss 4441.69873 epoch total loss 3969.21606\n",
      "Trained batch 257 batch loss 4439.15186 epoch total loss 3971.04443\n",
      "Trained batch 258 batch loss 4437.80127 epoch total loss 3972.85376\n",
      "Trained batch 259 batch loss 4449.98486 epoch total loss 3974.69604\n",
      "Trained batch 260 batch loss 4453.88135 epoch total loss 3976.53906\n",
      "Trained batch 261 batch loss 4453.76123 epoch total loss 3978.36743\n",
      "Trained batch 262 batch loss 4452.09131 epoch total loss 3980.17529\n",
      "Trained batch 263 batch loss 4468.59082 epoch total loss 3982.03223\n",
      "Trained batch 264 batch loss 4472.67969 epoch total loss 3983.89062\n",
      "Trained batch 265 batch loss 4471.19531 epoch total loss 3985.72974\n",
      "Trained batch 266 batch loss 4474.90381 epoch total loss 3987.5686\n",
      "Trained batch 267 batch loss 4389.96582 epoch total loss 3989.07593\n",
      "Trained batch 268 batch loss 4485.2 epoch total loss 3990.92725\n",
      "Trained batch 269 batch loss 4488.25391 epoch total loss 3992.77612\n",
      "Trained batch 270 batch loss 4493.125 epoch total loss 3994.62915\n",
      "Trained batch 271 batch loss 4495.7373 epoch total loss 3996.47827\n",
      "Trained batch 272 batch loss 4490.52 epoch total loss 3998.29468\n",
      "Trained batch 273 batch loss 4403.79834 epoch total loss 3999.77979\n",
      "Trained batch 274 batch loss 4392.07861 epoch total loss 4001.21167\n",
      "Trained batch 275 batch loss 4435.45215 epoch total loss 4002.79102\n",
      "Trained batch 276 batch loss 4434.12891 epoch total loss 4004.35376\n",
      "Trained batch 277 batch loss 4312.7124 epoch total loss 4005.46704\n",
      "Trained batch 278 batch loss 4431.21387 epoch total loss 4006.99854\n",
      "Trained batch 279 batch loss 4511.22412 epoch total loss 4008.80591\n",
      "Trained batch 280 batch loss 4376.59863 epoch total loss 4010.11963\n",
      "Trained batch 281 batch loss 4445.88428 epoch total loss 4011.67041\n",
      "Trained batch 282 batch loss 4442.38623 epoch total loss 4013.19775\n",
      "Trained batch 283 batch loss 4333.79639 epoch total loss 4014.33032\n",
      "Trained batch 284 batch loss 4379.6 epoch total loss 4015.6167\n",
      "Trained batch 285 batch loss 4435.18262 epoch total loss 4017.08862\n",
      "Trained batch 286 batch loss 4374.57324 epoch total loss 4018.33862\n",
      "Trained batch 287 batch loss 4356.16113 epoch total loss 4019.51562\n",
      "Trained batch 288 batch loss 4415.51221 epoch total loss 4020.89062\n",
      "Trained batch 289 batch loss 4443.91357 epoch total loss 4022.35425\n",
      "Trained batch 290 batch loss 4568.28857 epoch total loss 4024.23657\n",
      "Trained batch 291 batch loss 4535.0332 epoch total loss 4025.99194\n",
      "Trained batch 292 batch loss 4583.82471 epoch total loss 4027.90234\n",
      "Trained batch 293 batch loss 4580.61963 epoch total loss 4029.78882\n",
      "Trained batch 294 batch loss 4590.24316 epoch total loss 4031.69507\n",
      "Trained batch 295 batch loss 4593.11865 epoch total loss 4033.59839\n",
      "Trained batch 296 batch loss 4556.32 epoch total loss 4035.3645\n",
      "Trained batch 297 batch loss 4563.30957 epoch total loss 4037.14185\n",
      "Trained batch 298 batch loss 4576.22217 epoch total loss 4038.95093\n",
      "Trained batch 299 batch loss 4604.21924 epoch total loss 4040.84155\n",
      "Trained batch 300 batch loss 4602.50635 epoch total loss 4042.71387\n",
      "Trained batch 301 batch loss 4607.07617 epoch total loss 4044.58887\n",
      "Trained batch 302 batch loss 4617.59131 epoch total loss 4046.48633\n",
      "Trained batch 303 batch loss 4619.94971 epoch total loss 4048.37915\n",
      "Trained batch 304 batch loss 4591.02539 epoch total loss 4050.16406\n",
      "Trained batch 305 batch loss 4431.46631 epoch total loss 4051.41431\n",
      "Trained batch 306 batch loss 4505.88672 epoch total loss 4052.89941\n",
      "Trained batch 307 batch loss 4465.45801 epoch total loss 4054.24341\n",
      "Trained batch 308 batch loss 4619.67871 epoch total loss 4056.0791\n",
      "Trained batch 309 batch loss 4639.54346 epoch total loss 4057.96729\n",
      "Trained batch 310 batch loss 4551.2124 epoch total loss 4059.55835\n",
      "Trained batch 311 batch loss 4543.65479 epoch total loss 4061.11499\n",
      "Trained batch 312 batch loss 4563.72754 epoch total loss 4062.72607\n",
      "Trained batch 313 batch loss 4619.25049 epoch total loss 4064.50391\n",
      "Trained batch 314 batch loss 4663.62451 epoch total loss 4066.41211\n",
      "Trained batch 315 batch loss 4667.46826 epoch total loss 4068.32031\n",
      "Trained batch 316 batch loss 4668.62207 epoch total loss 4070.22\n",
      "Trained batch 317 batch loss 4673.3877 epoch total loss 4072.12256\n",
      "Trained batch 318 batch loss 4670.48047 epoch total loss 4074.00439\n",
      "Trained batch 319 batch loss 4602.94385 epoch total loss 4075.6626\n",
      "Trained batch 320 batch loss 4595.46924 epoch total loss 4077.28711\n",
      "Trained batch 321 batch loss 4673.97217 epoch total loss 4079.146\n",
      "Trained batch 322 batch loss 4521.65527 epoch total loss 4080.52026\n",
      "Trained batch 323 batch loss 4677.25293 epoch total loss 4082.36768\n",
      "Trained batch 324 batch loss 4605.42871 epoch total loss 4083.98193\n",
      "Trained batch 325 batch loss 4099.27441 epoch total loss 4084.02881\n",
      "Trained batch 326 batch loss 4534.40771 epoch total loss 4085.41016\n",
      "Trained batch 327 batch loss 4529.81445 epoch total loss 4086.76953\n",
      "Trained batch 328 batch loss 4630.26367 epoch total loss 4088.42651\n",
      "Trained batch 329 batch loss 4573.39648 epoch total loss 4089.90039\n",
      "Trained batch 330 batch loss 4668.13379 epoch total loss 4091.65259\n",
      "Trained batch 331 batch loss 4614.59082 epoch total loss 4093.23267\n",
      "Trained batch 332 batch loss 4565.42432 epoch total loss 4094.65479\n",
      "Trained batch 333 batch loss 4597.48682 epoch total loss 4096.16455\n",
      "Trained batch 334 batch loss 4664.24512 epoch total loss 4097.86572\n",
      "Trained batch 335 batch loss 4694.5459 epoch total loss 4099.64648\n",
      "Trained batch 336 batch loss 4703.51025 epoch total loss 4101.44385\n",
      "Trained batch 337 batch loss 4706.30078 epoch total loss 4103.23828\n",
      "Trained batch 338 batch loss 4688.49658 epoch total loss 4104.97\n",
      "Trained batch 339 batch loss 4247.54834 epoch total loss 4105.39062\n",
      "Trained batch 340 batch loss 4688.60352 epoch total loss 4107.10596\n",
      "Trained batch 341 batch loss 4714.88379 epoch total loss 4108.88818\n",
      "Trained batch 342 batch loss 4631.36133 epoch total loss 4110.41602\n",
      "Trained batch 343 batch loss 4717.51855 epoch total loss 4112.18604\n",
      "Trained batch 344 batch loss 4710.66699 epoch total loss 4113.92529\n",
      "Trained batch 345 batch loss 4717.48 epoch total loss 4115.6748\n",
      "Trained batch 346 batch loss 4721.5376 epoch total loss 4117.42578\n",
      "Trained batch 347 batch loss 4714.09375 epoch total loss 4119.14551\n",
      "Trained batch 348 batch loss 4637.16748 epoch total loss 4120.63379\n",
      "Trained batch 349 batch loss 4720.14111 epoch total loss 4122.35156\n",
      "Trained batch 350 batch loss 4732.75342 epoch total loss 4124.0957\n",
      "Trained batch 351 batch loss 4731.01904 epoch total loss 4125.82471\n",
      "Trained batch 352 batch loss 4734.51514 epoch total loss 4127.5542\n",
      "Trained batch 353 batch loss 4734.65625 epoch total loss 4129.27393\n",
      "Trained batch 354 batch loss 4731.55029 epoch total loss 4130.9751\n",
      "Trained batch 355 batch loss 4657.48926 epoch total loss 4132.45801\n",
      "Trained batch 356 batch loss 4736.08252 epoch total loss 4134.15381\n",
      "Trained batch 357 batch loss 4736.52197 epoch total loss 4135.84082\n",
      "Trained batch 358 batch loss 4673.92529 epoch total loss 4137.34375\n",
      "Trained batch 359 batch loss 4715.35693 epoch total loss 4138.9541\n",
      "Trained batch 360 batch loss 4722.07 epoch total loss 4140.57373\n",
      "Trained batch 361 batch loss 4710.47314 epoch total loss 4142.15283\n",
      "Trained batch 362 batch loss 4710.32422 epoch total loss 4143.72217\n",
      "Trained batch 363 batch loss 4687.76562 epoch total loss 4145.22119\n",
      "Trained batch 364 batch loss 4655.88232 epoch total loss 4146.62402\n",
      "Trained batch 365 batch loss 4753.94873 epoch total loss 4148.28809\n",
      "Trained batch 366 batch loss 4762.0376 epoch total loss 4149.96484\n",
      "Trained batch 367 batch loss 4744.92578 epoch total loss 4151.58594\n",
      "Trained batch 368 batch loss 4637.93213 epoch total loss 4152.90723\n",
      "Trained batch 369 batch loss 4757.11621 epoch total loss 4154.54492\n",
      "Trained batch 370 batch loss 4762.7417 epoch total loss 4156.18848\n",
      "Trained batch 371 batch loss 4743.36816 epoch total loss 4157.771\n",
      "Trained batch 372 batch loss 4749 epoch total loss 4159.36035\n",
      "Trained batch 373 batch loss 4723.80078 epoch total loss 4160.87354\n",
      "Trained batch 374 batch loss 4749.01709 epoch total loss 4162.44629\n",
      "Trained batch 375 batch loss 4673.01855 epoch total loss 4163.80762\n",
      "Trained batch 376 batch loss 4736.15918 epoch total loss 4165.32959\n",
      "Trained batch 377 batch loss 4721.47949 epoch total loss 4166.80518\n",
      "Trained batch 378 batch loss 4690.82178 epoch total loss 4168.19141\n",
      "Trained batch 379 batch loss 4764.30029 epoch total loss 4169.76416\n",
      "Trained batch 380 batch loss 4763.65 epoch total loss 4171.32715\n",
      "Trained batch 381 batch loss 4745.97266 epoch total loss 4172.83545\n",
      "Trained batch 382 batch loss 4790.56934 epoch total loss 4174.45264\n",
      "Trained batch 383 batch loss 4792.58545 epoch total loss 4176.06641\n",
      "Trained batch 384 batch loss 4790.59521 epoch total loss 4177.66699\n",
      "Trained batch 385 batch loss 4799.42139 epoch total loss 4179.28174\n",
      "Trained batch 386 batch loss 4799.73633 epoch total loss 4180.88916\n",
      "Trained batch 387 batch loss 4803.90088 epoch total loss 4182.49902\n",
      "Trained batch 388 batch loss 4800.54395 epoch total loss 4184.0918\n",
      "Trained batch 389 batch loss 4804.43652 epoch total loss 4185.68652\n",
      "Trained batch 390 batch loss 4790.25635 epoch total loss 4187.23633\n",
      "Trained batch 391 batch loss 4799.70801 epoch total loss 4188.80322\n",
      "Trained batch 392 batch loss 4686.15625 epoch total loss 4190.07178\n",
      "Trained batch 393 batch loss 4790.46582 epoch total loss 4191.59961\n",
      "Trained batch 394 batch loss 4798.21729 epoch total loss 4193.13916\n",
      "Trained batch 395 batch loss 4822.05664 epoch total loss 4194.73145\n",
      "Trained batch 396 batch loss 4812.5459 epoch total loss 4196.2915\n",
      "Trained batch 397 batch loss 4821.25049 epoch total loss 4197.86572\n",
      "Trained batch 398 batch loss 4809.62207 epoch total loss 4199.40283\n",
      "Trained batch 399 batch loss 4818.84229 epoch total loss 4200.95508\n",
      "Trained batch 400 batch loss 4600.6543 epoch total loss 4201.95459\n",
      "Trained batch 401 batch loss 4819.62402 epoch total loss 4203.49463\n",
      "Trained batch 402 batch loss 4833.82812 epoch total loss 4205.06299\n",
      "Trained batch 403 batch loss 4824.26709 epoch total loss 4206.59912\n",
      "Trained batch 404 batch loss 4811.63525 epoch total loss 4208.09668\n",
      "Trained batch 405 batch loss 4729.7085 epoch total loss 4209.38477\n",
      "Trained batch 406 batch loss 4837.48291 epoch total loss 4210.93213\n",
      "Trained batch 407 batch loss 4758.80078 epoch total loss 4212.27783\n",
      "Trained batch 408 batch loss 4661.72803 epoch total loss 4213.37939\n",
      "Trained batch 409 batch loss 4718.16748 epoch total loss 4214.61377\n",
      "Trained batch 410 batch loss 4774.71338 epoch total loss 4215.98\n",
      "Trained batch 411 batch loss 4704.94092 epoch total loss 4217.17\n",
      "Trained batch 412 batch loss 4847.34424 epoch total loss 4218.69922\n",
      "Trained batch 413 batch loss 4720.7124 epoch total loss 4219.91504\n",
      "Trained batch 414 batch loss 4855.24365 epoch total loss 4221.44971\n",
      "Trained batch 415 batch loss 4770.64502 epoch total loss 4222.77295\n",
      "Trained batch 416 batch loss 4765.84863 epoch total loss 4224.07861\n",
      "Trained batch 417 batch loss 4771.47656 epoch total loss 4225.39111\n",
      "Trained batch 418 batch loss 4741.99316 epoch total loss 4226.62695\n",
      "Trained batch 419 batch loss 4860.60205 epoch total loss 4228.14\n",
      "Trained batch 420 batch loss 4731.3208 epoch total loss 4229.33838\n",
      "Trained batch 421 batch loss 4878.54102 epoch total loss 4230.88037\n",
      "Trained batch 422 batch loss 4878.34033 epoch total loss 4232.41455\n",
      "Trained batch 423 batch loss 4853.92285 epoch total loss 4233.88379\n",
      "Trained batch 424 batch loss 4873.81299 epoch total loss 4235.39307\n",
      "Trained batch 425 batch loss 4857.59717 epoch total loss 4236.85742\n",
      "Trained batch 426 batch loss 4886.88623 epoch total loss 4238.3833\n",
      "Trained batch 427 batch loss 4851.62646 epoch total loss 4239.81934\n",
      "Trained batch 428 batch loss 4883.22803 epoch total loss 4241.32275\n",
      "Trained batch 429 batch loss 4910.51318 epoch total loss 4242.88281\n",
      "Trained batch 430 batch loss 4833.99609 epoch total loss 4244.25732\n",
      "Trained batch 431 batch loss 4895.23975 epoch total loss 4245.76758\n",
      "Trained batch 432 batch loss 4721.68555 epoch total loss 4246.86914\n",
      "Trained batch 433 batch loss 4798.12256 epoch total loss 4248.14209\n",
      "Trained batch 434 batch loss 4883.08057 epoch total loss 4249.60547\n",
      "Trained batch 435 batch loss 4844.55029 epoch total loss 4250.97314\n",
      "Trained batch 436 batch loss 4911.26025 epoch total loss 4252.4873\n",
      "Trained batch 437 batch loss 4818.28955 epoch total loss 4253.78223\n",
      "Trained batch 438 batch loss 4813.73779 epoch total loss 4255.06055\n",
      "Trained batch 439 batch loss 4765.69873 epoch total loss 4256.22363\n",
      "Trained batch 440 batch loss 4573.48389 epoch total loss 4256.94482\n",
      "Trained batch 441 batch loss 4839.97754 epoch total loss 4258.26709\n",
      "Trained batch 442 batch loss 4757.25049 epoch total loss 4259.396\n",
      "Trained batch 443 batch loss 4872.94043 epoch total loss 4260.78125\n",
      "Trained batch 444 batch loss 4764.50244 epoch total loss 4261.91553\n",
      "Trained batch 445 batch loss 4891.73779 epoch total loss 4263.33105\n",
      "Trained batch 446 batch loss 4981.37549 epoch total loss 4264.94092\n",
      "Trained batch 447 batch loss 4949.89893 epoch total loss 4266.47314\n",
      "Trained batch 448 batch loss 4818.60352 epoch total loss 4267.70557\n",
      "Trained batch 449 batch loss 4478.63281 epoch total loss 4268.17529\n",
      "Trained batch 450 batch loss 4060.50342 epoch total loss 4267.71387\n",
      "Trained batch 451 batch loss 4908.04932 epoch total loss 4269.13379\n",
      "Trained batch 452 batch loss 4653.33936 epoch total loss 4269.98389\n",
      "Trained batch 453 batch loss 4842.86963 epoch total loss 4271.24854\n",
      "Trained batch 454 batch loss 5042.90625 epoch total loss 4272.94775\n",
      "Trained batch 455 batch loss 5042.63086 epoch total loss 4274.63965\n",
      "Trained batch 456 batch loss 5043.15869 epoch total loss 4276.32471\n",
      "Trained batch 457 batch loss 4960.82 epoch total loss 4277.82275\n",
      "Trained batch 458 batch loss 4896.22168 epoch total loss 4279.17285\n",
      "Trained batch 459 batch loss 4778.34912 epoch total loss 4280.26074\n",
      "Trained batch 460 batch loss 4986.21289 epoch total loss 4281.79541\n",
      "Trained batch 461 batch loss 5063.771 epoch total loss 4283.4917\n",
      "Trained batch 462 batch loss 5058.44336 epoch total loss 4285.16895\n",
      "Trained batch 463 batch loss 4981.39258 epoch total loss 4286.67285\n",
      "Trained batch 464 batch loss 5104.49756 epoch total loss 4288.43555\n",
      "Trained batch 465 batch loss 5108.94971 epoch total loss 4290.2\n",
      "Trained batch 466 batch loss 5121.02832 epoch total loss 4291.98291\n",
      "Trained batch 467 batch loss 5030.71631 epoch total loss 4293.56494\n",
      "Trained batch 468 batch loss 5048.0957 epoch total loss 4295.17725\n",
      "Trained batch 469 batch loss 5140.23389 epoch total loss 4296.979\n",
      "Trained batch 470 batch loss 5145.46338 epoch total loss 4298.78418\n",
      "Trained batch 471 batch loss 5137.03467 epoch total loss 4300.56396\n",
      "Trained batch 472 batch loss 5162.17773 epoch total loss 4302.38916\n",
      "Trained batch 473 batch loss 5170.11084 epoch total loss 4304.22363\n",
      "Trained batch 474 batch loss 5185.16602 epoch total loss 4306.08252\n",
      "Trained batch 475 batch loss 5072.75879 epoch total loss 4307.69629\n",
      "Trained batch 476 batch loss 5163.40674 epoch total loss 4309.49414\n",
      "Trained batch 477 batch loss 5198.57471 epoch total loss 4311.35791\n",
      "Trained batch 478 batch loss 5106.79443 epoch total loss 4313.02197\n",
      "Trained batch 479 batch loss 5182.68701 epoch total loss 4314.8374\n",
      "Trained batch 480 batch loss 5114.58838 epoch total loss 4316.50342\n",
      "Trained batch 481 batch loss 5047.43652 epoch total loss 4318.02295\n",
      "Trained batch 482 batch loss 5082.90576 epoch total loss 4319.61\n",
      "Trained batch 483 batch loss 5042.94531 epoch total loss 4321.10742\n",
      "Trained batch 484 batch loss 5212.25879 epoch total loss 4322.94873\n",
      "Trained batch 485 batch loss 5257.60205 epoch total loss 4324.87598\n",
      "Trained batch 486 batch loss 5241.8 epoch total loss 4326.76221\n",
      "Trained batch 487 batch loss 5220.8877 epoch total loss 4328.59863\n",
      "Trained batch 488 batch loss 5178.47 epoch total loss 4330.34033\n",
      "Trained batch 489 batch loss 5255.65576 epoch total loss 4332.23242\n",
      "Trained batch 490 batch loss 5258.35596 epoch total loss 4334.12256\n",
      "Trained batch 491 batch loss 5192.4834 epoch total loss 4335.87061\n",
      "Trained batch 492 batch loss 5306.36182 epoch total loss 4337.84277\n",
      "Trained batch 493 batch loss 5315.42285 epoch total loss 4339.82617\n",
      "Trained batch 494 batch loss 5315.6333 epoch total loss 4341.80176\n",
      "Trained batch 495 batch loss 5246.92285 epoch total loss 4343.63037\n",
      "Trained batch 496 batch loss 5321.81348 epoch total loss 4345.60254\n",
      "Trained batch 497 batch loss 5302.13623 epoch total loss 4347.52734\n",
      "Trained batch 498 batch loss 5132.80029 epoch total loss 4349.104\n",
      "Trained batch 499 batch loss 4882.14209 epoch total loss 4350.17236\n",
      "Trained batch 500 batch loss 4798.6875 epoch total loss 4351.06934\n",
      "Trained batch 501 batch loss 5030.51904 epoch total loss 4352.42578\n",
      "Trained batch 502 batch loss 5137.38037 epoch total loss 4353.98975\n",
      "Trained batch 503 batch loss 5283.88428 epoch total loss 4355.83838\n",
      "Trained batch 504 batch loss 5374.82568 epoch total loss 4357.86035\n",
      "Trained batch 505 batch loss 5409.54053 epoch total loss 4359.94238\n",
      "Trained batch 506 batch loss 5293.26807 epoch total loss 4361.78711\n",
      "Trained batch 507 batch loss 5424.95068 epoch total loss 4363.88428\n",
      "Trained batch 508 batch loss 5426.89648 epoch total loss 4365.97705\n",
      "Trained batch 509 batch loss 5323.43 epoch total loss 4367.85791\n",
      "Trained batch 510 batch loss 5401.54785 epoch total loss 4369.88477\n",
      "Trained batch 511 batch loss 5339.11621 epoch total loss 4371.78125\n",
      "Trained batch 512 batch loss 5453.91553 epoch total loss 4373.89502\n",
      "Trained batch 513 batch loss 5217.74658 epoch total loss 4375.54\n",
      "Trained batch 514 batch loss 5463.90088 epoch total loss 4377.65771\n",
      "Trained batch 515 batch loss 5175.75195 epoch total loss 4379.20752\n",
      "Trained batch 516 batch loss 5488.19482 epoch total loss 4381.35645\n",
      "Trained batch 517 batch loss 5478.88867 epoch total loss 4383.47949\n",
      "Trained batch 518 batch loss 5266.04395 epoch total loss 4385.18359\n",
      "Trained batch 519 batch loss 5195.01172 epoch total loss 4386.74365\n",
      "Trained batch 520 batch loss 5228.99756 epoch total loss 4388.36328\n",
      "Trained batch 521 batch loss 5511.30811 epoch total loss 4390.51855\n",
      "Trained batch 522 batch loss 5367.64697 epoch total loss 4392.39062\n",
      "Trained batch 523 batch loss 5465.83838 epoch total loss 4394.44336\n",
      "Trained batch 524 batch loss 5542.05811 epoch total loss 4396.6333\n",
      "Trained batch 525 batch loss 5557.86523 epoch total loss 4398.84473\n",
      "Trained batch 526 batch loss 5565.09717 epoch total loss 4401.06201\n",
      "Trained batch 527 batch loss 5563.73242 epoch total loss 4403.26807\n",
      "Trained batch 528 batch loss 5560.15381 epoch total loss 4405.45947\n",
      "Trained batch 529 batch loss 5452.68604 epoch total loss 4407.43896\n",
      "Trained batch 530 batch loss 5329.31689 epoch total loss 4409.17822\n",
      "Trained batch 531 batch loss 5448.04 epoch total loss 4411.13477\n",
      "Trained batch 532 batch loss 5579.50488 epoch total loss 4413.33105\n",
      "Trained batch 533 batch loss 5388.91 epoch total loss 4415.16113\n",
      "Trained batch 534 batch loss 5369.6416 epoch total loss 4416.94873\n",
      "Trained batch 535 batch loss 5184.95654 epoch total loss 4418.38477\n",
      "Trained batch 536 batch loss 5396.271 epoch total loss 4420.20898\n",
      "Trained batch 537 batch loss 5622.07422 epoch total loss 4422.44678\n",
      "Trained batch 538 batch loss 5629.67676 epoch total loss 4424.69092\n",
      "Trained batch 539 batch loss 5638.8623 epoch total loss 4426.94336\n",
      "Trained batch 540 batch loss 5645.40869 epoch total loss 4429.2\n",
      "Trained batch 541 batch loss 5522.34521 epoch total loss 4431.22\n",
      "Trained batch 542 batch loss 5512.96094 epoch total loss 4433.21631\n",
      "Trained batch 543 batch loss 5636.15 epoch total loss 4435.43164\n",
      "Trained batch 544 batch loss 5633.44824 epoch total loss 4437.63428\n",
      "Trained batch 545 batch loss 5666.854 epoch total loss 4439.88965\n",
      "Trained batch 546 batch loss 5674.27393 epoch total loss 4442.15039\n",
      "Trained batch 547 batch loss 5681.03613 epoch total loss 4444.41504\n",
      "Trained batch 548 batch loss 5689.49805 epoch total loss 4446.68701\n",
      "Trained batch 549 batch loss 5675.79053 epoch total loss 4448.92578\n",
      "Trained batch 550 batch loss 5465.82275 epoch total loss 4450.77441\n",
      "Trained batch 551 batch loss 5502.68945 epoch total loss 4452.68359\n",
      "Trained batch 552 batch loss 5523.36328 epoch total loss 4454.62305\n",
      "Trained batch 553 batch loss 5573.40332 epoch total loss 4456.64648\n",
      "Trained batch 554 batch loss 5489.24072 epoch total loss 4458.51025\n",
      "Trained batch 555 batch loss 5123.08301 epoch total loss 4459.70752\n",
      "Trained batch 556 batch loss 5670.20703 epoch total loss 4461.88477\n",
      "Trained batch 557 batch loss 5689.93115 epoch total loss 4464.09\n",
      "Trained batch 558 batch loss 5705.35596 epoch total loss 4466.31396\n",
      "Trained batch 559 batch loss 5737.45898 epoch total loss 4468.58789\n",
      "Trained batch 560 batch loss 5761.10107 epoch total loss 4470.896\n",
      "Trained batch 561 batch loss 5767.47168 epoch total loss 4473.20703\n",
      "Trained batch 562 batch loss 5777.74854 epoch total loss 4475.52832\n",
      "Trained batch 563 batch loss 5787.71826 epoch total loss 4477.85938\n",
      "Trained batch 564 batch loss 5760.14795 epoch total loss 4480.13281\n",
      "Trained batch 565 batch loss 5801.01807 epoch total loss 4482.4707\n",
      "Trained batch 566 batch loss 5769.20752 epoch total loss 4484.74414\n",
      "Trained batch 567 batch loss 5800.90332 epoch total loss 4487.06592\n",
      "Trained batch 568 batch loss 5813.13037 epoch total loss 4489.40039\n",
      "Trained batch 569 batch loss 5760.09912 epoch total loss 4491.63379\n",
      "Trained batch 570 batch loss 5707.20752 epoch total loss 4493.76611\n",
      "Trained batch 571 batch loss 5813.2207 epoch total loss 4496.07715\n",
      "Trained batch 572 batch loss 5671.9751 epoch total loss 4498.13281\n",
      "Trained batch 573 batch loss 5507.36182 epoch total loss 4499.89404\n",
      "Trained batch 574 batch loss 5435.83496 epoch total loss 4501.52441\n",
      "Trained batch 575 batch loss 5808.9873 epoch total loss 4503.79834\n",
      "Trained batch 576 batch loss 5837.58057 epoch total loss 4506.11377\n",
      "Trained batch 577 batch loss 5639.53955 epoch total loss 4508.07812\n",
      "Trained batch 578 batch loss 5858.4292 epoch total loss 4510.41455\n",
      "Trained batch 579 batch loss 5850.99756 epoch total loss 4512.72949\n",
      "Trained batch 580 batch loss 5715.72168 epoch total loss 4514.80371\n",
      "Trained batch 581 batch loss 5842.78857 epoch total loss 4517.08936\n",
      "Trained batch 582 batch loss 5744.78076 epoch total loss 4519.19873\n",
      "Trained batch 583 batch loss 5856.86621 epoch total loss 4521.49316\n",
      "Trained batch 584 batch loss 5868.90332 epoch total loss 4523.80029\n",
      "Trained batch 585 batch loss 5891.79883 epoch total loss 4526.13867\n",
      "Trained batch 586 batch loss 5891.43604 epoch total loss 4528.46875\n",
      "Trained batch 587 batch loss 5909.35303 epoch total loss 4530.82129\n",
      "Trained batch 588 batch loss 5915.98438 epoch total loss 4533.17676\n",
      "Trained batch 589 batch loss 5928.87305 epoch total loss 4535.54639\n",
      "Trained batch 590 batch loss 5935.73682 epoch total loss 4537.91943\n",
      "Trained batch 591 batch loss 5951.45264 epoch total loss 4540.31152\n",
      "Trained batch 592 batch loss 5936.33936 epoch total loss 4542.66943\n",
      "Trained batch 593 batch loss 5964.8042 epoch total loss 4545.06738\n",
      "Trained batch 594 batch loss 5970.48242 epoch total loss 4547.46729\n",
      "Trained batch 595 batch loss 5976.64844 epoch total loss 4549.86914\n",
      "Trained batch 596 batch loss 5980.44385 epoch total loss 4552.26953\n",
      "Trained batch 597 batch loss 5898.27734 epoch total loss 4554.52441\n",
      "Trained batch 598 batch loss 5857.73193 epoch total loss 4556.70361\n",
      "Trained batch 599 batch loss 6009.4668 epoch total loss 4559.12891\n",
      "Trained batch 600 batch loss 6005.58447 epoch total loss 4561.53955\n",
      "Trained batch 601 batch loss 6011.46 epoch total loss 4563.95215\n",
      "Trained batch 602 batch loss 5884.34766 epoch total loss 4566.14551\n",
      "Trained batch 603 batch loss 5897.16553 epoch total loss 4568.35303\n",
      "Trained batch 604 batch loss 6040.24756 epoch total loss 4570.78955\n",
      "Trained batch 605 batch loss 6026.43945 epoch total loss 4573.1958\n",
      "Trained batch 606 batch loss 6034.04053 epoch total loss 4575.60645\n",
      "Trained batch 607 batch loss 6055.17676 epoch total loss 4578.04395\n",
      "Trained batch 608 batch loss 6055.94727 epoch total loss 4580.4751\n",
      "Trained batch 609 batch loss 6060.08252 epoch total loss 4582.9043\n",
      "Trained batch 610 batch loss 6075.15234 epoch total loss 4585.35059\n",
      "Trained batch 611 batch loss 6074.7251 epoch total loss 4587.78857\n",
      "Trained batch 612 batch loss 6051.21484 epoch total loss 4590.17969\n",
      "Trained batch 613 batch loss 6050.04053 epoch total loss 4592.56104\n",
      "Trained batch 614 batch loss 6052.96875 epoch total loss 4594.94\n",
      "Trained batch 615 batch loss 6052.18506 epoch total loss 4597.30957\n",
      "Trained batch 616 batch loss 6071.16699 epoch total loss 4599.70215\n",
      "Trained batch 617 batch loss 5960.68799 epoch total loss 4601.9082\n",
      "Trained batch 618 batch loss 6100.55713 epoch total loss 4604.33301\n",
      "Trained batch 619 batch loss 6048.81787 epoch total loss 4606.6665\n",
      "Trained batch 620 batch loss 6106.7583 epoch total loss 4609.08594\n",
      "Trained batch 621 batch loss 5993.85645 epoch total loss 4611.31543\n",
      "Trained batch 622 batch loss 6070.58154 epoch total loss 4613.66162\n",
      "Trained batch 623 batch loss 6104.09863 epoch total loss 4616.05371\n",
      "Trained batch 624 batch loss 6103.83936 epoch total loss 4618.43799\n",
      "Trained batch 625 batch loss 6098.51318 epoch total loss 4620.80615\n",
      "Trained batch 626 batch loss 6121.19531 epoch total loss 4623.20264\n",
      "Trained batch 627 batch loss 6005.62061 epoch total loss 4625.40771\n",
      "Trained batch 628 batch loss 6051.4873 epoch total loss 4627.67822\n",
      "Trained batch 629 batch loss 6098.90381 epoch total loss 4630.01758\n",
      "Trained batch 630 batch loss 6126.17529 epoch total loss 4632.39258\n",
      "Trained batch 631 batch loss 6137.79736 epoch total loss 4634.77832\n",
      "Trained batch 632 batch loss 6145.36279 epoch total loss 4637.16797\n",
      "Trained batch 633 batch loss 6148.67041 epoch total loss 4639.55615\n",
      "Trained batch 634 batch loss 6144.04541 epoch total loss 4641.9292\n",
      "Trained batch 635 batch loss 6136.05762 epoch total loss 4644.28174\n",
      "Trained batch 636 batch loss 6133.81836 epoch total loss 4646.62402\n",
      "Trained batch 637 batch loss 6135.91357 epoch total loss 4648.96191\n",
      "Trained batch 638 batch loss 6085.89062 epoch total loss 4651.21436\n",
      "Trained batch 639 batch loss 6135.51221 epoch total loss 4653.53711\n",
      "Trained batch 640 batch loss 6090.99268 epoch total loss 4655.7832\n",
      "Trained batch 641 batch loss 5921.53906 epoch total loss 4657.75781\n",
      "Trained batch 642 batch loss 6161.39355 epoch total loss 4660.1\n",
      "Trained batch 643 batch loss 6185.46143 epoch total loss 4662.47217\n",
      "Trained batch 644 batch loss 6175.20068 epoch total loss 4664.82129\n",
      "Trained batch 645 batch loss 6150.52051 epoch total loss 4667.125\n",
      "Trained batch 646 batch loss 6023.12744 epoch total loss 4669.22412\n",
      "Trained batch 647 batch loss 6148.07129 epoch total loss 4671.51\n",
      "Trained batch 648 batch loss 6224.78076 epoch total loss 4673.90674\n",
      "Trained batch 649 batch loss 5993.50586 epoch total loss 4675.94\n",
      "Trained batch 650 batch loss 6135.29443 epoch total loss 4678.18506\n",
      "Trained batch 651 batch loss 6271.86 epoch total loss 4680.63281\n",
      "Trained batch 652 batch loss 6273.42578 epoch total loss 4683.07568\n",
      "Trained batch 653 batch loss 6240.50195 epoch total loss 4685.46094\n",
      "Trained batch 654 batch loss 6180.45801 epoch total loss 4687.74707\n",
      "Trained batch 655 batch loss 6148.84912 epoch total loss 4689.97754\n",
      "Trained batch 656 batch loss 6218.44287 epoch total loss 4692.30762\n",
      "Trained batch 657 batch loss 6268.72217 epoch total loss 4694.70703\n",
      "Trained batch 658 batch loss 6187.40479 epoch total loss 4696.97559\n",
      "Trained batch 659 batch loss 6274.24316 epoch total loss 4699.36914\n",
      "Trained batch 660 batch loss 6321.79199 epoch total loss 4701.82715\n",
      "Trained batch 661 batch loss 6329.22 epoch total loss 4704.28955\n",
      "Trained batch 662 batch loss 6348.92725 epoch total loss 4706.77393\n",
      "Trained batch 663 batch loss 6346.49951 epoch total loss 4709.24707\n",
      "Trained batch 664 batch loss 6367.0918 epoch total loss 4711.74365\n",
      "Trained batch 665 batch loss 6370.33105 epoch total loss 4714.23779\n",
      "Trained batch 666 batch loss 6381.80615 epoch total loss 4716.74121\n",
      "Trained batch 667 batch loss 6286.73389 epoch total loss 4719.09521\n",
      "Trained batch 668 batch loss 6340.896 epoch total loss 4721.52344\n",
      "Trained batch 669 batch loss 6402.72803 epoch total loss 4724.03613\n",
      "Trained batch 670 batch loss 6404.64355 epoch total loss 4726.54492\n",
      "Trained batch 671 batch loss 6417.04102 epoch total loss 4729.06396\n",
      "Trained batch 672 batch loss 6239.03125 epoch total loss 4731.31104\n",
      "Trained batch 673 batch loss 6368.57861 epoch total loss 4733.74365\n",
      "Trained batch 674 batch loss 6427.86035 epoch total loss 4736.25684\n",
      "Trained batch 675 batch loss 6306.95752 epoch total loss 4738.58398\n",
      "Trained batch 676 batch loss 6383.24365 epoch total loss 4741.01709\n",
      "Trained batch 677 batch loss 6446.67139 epoch total loss 4743.53662\n",
      "Trained batch 678 batch loss 6451.38477 epoch total loss 4746.05566\n",
      "Trained batch 679 batch loss 6476.73291 epoch total loss 4748.60449\n",
      "Trained batch 680 batch loss 6431.36279 epoch total loss 4751.0791\n",
      "Trained batch 681 batch loss 6484.91943 epoch total loss 4753.625\n",
      "Trained batch 682 batch loss 6487.20654 epoch total loss 4756.16699\n",
      "Trained batch 683 batch loss 6416.32178 epoch total loss 4758.59766\n",
      "Trained batch 684 batch loss 6295.54688 epoch total loss 4760.84473\n",
      "Trained batch 685 batch loss 6223.42383 epoch total loss 4762.98\n",
      "Trained batch 686 batch loss 6327.71436 epoch total loss 4765.26074\n",
      "Trained batch 687 batch loss 6400.47656 epoch total loss 4767.64111\n",
      "Trained batch 688 batch loss 6375.44531 epoch total loss 4769.97803\n",
      "Trained batch 689 batch loss 6483.64893 epoch total loss 4772.46533\n",
      "Trained batch 690 batch loss 6488.06 epoch total loss 4774.95166\n",
      "Trained batch 691 batch loss 6373.87061 epoch total loss 4777.26562\n",
      "Trained batch 692 batch loss 6532.09229 epoch total loss 4779.80127\n",
      "Trained batch 693 batch loss 6422.10498 epoch total loss 4782.1709\n",
      "Trained batch 694 batch loss 6491.91309 epoch total loss 4784.63477\n",
      "Trained batch 695 batch loss 6413.28369 epoch total loss 4786.97803\n",
      "Trained batch 696 batch loss 6150.27783 epoch total loss 4788.93701\n",
      "Trained batch 697 batch loss 6420.51562 epoch total loss 4791.27783\n",
      "Trained batch 698 batch loss 6443.06348 epoch total loss 4793.64404\n",
      "Trained batch 699 batch loss 6401.96 epoch total loss 4795.94482\n",
      "Trained batch 700 batch loss 6235.35693 epoch total loss 4798.00098\n",
      "Trained batch 701 batch loss 6334.90869 epoch total loss 4800.19385\n",
      "Trained batch 702 batch loss 6602.56 epoch total loss 4802.76123\n",
      "Trained batch 703 batch loss 6620.42871 epoch total loss 4805.34668\n",
      "Trained batch 704 batch loss 6630.20703 epoch total loss 4807.93896\n",
      "Trained batch 705 batch loss 6641.01221 epoch total loss 4810.53906\n",
      "Trained batch 706 batch loss 6645.47656 epoch total loss 4813.13818\n",
      "Trained batch 707 batch loss 6650.35303 epoch total loss 4815.73633\n",
      "Trained batch 708 batch loss 6607.5874 epoch total loss 4818.26709\n",
      "Trained batch 709 batch loss 6638.36377 epoch total loss 4820.83447\n",
      "Trained batch 710 batch loss 6503.39111 epoch total loss 4823.2041\n",
      "Trained batch 711 batch loss 6635.31055 epoch total loss 4825.75293\n",
      "Trained batch 712 batch loss 6625.54395 epoch total loss 4828.28076\n",
      "Trained batch 713 batch loss 6639.96094 epoch total loss 4830.82129\n",
      "Trained batch 714 batch loss 6599.55811 epoch total loss 4833.29883\n",
      "Trained batch 715 batch loss 6417.28174 epoch total loss 4835.51416\n",
      "Trained batch 716 batch loss 6362.12695 epoch total loss 4837.64648\n",
      "Trained batch 717 batch loss 6184.34717 epoch total loss 4839.52441\n",
      "Trained batch 718 batch loss 6483.77783 epoch total loss 4841.81445\n",
      "Trained batch 719 batch loss 6434.94531 epoch total loss 4844.03027\n",
      "Trained batch 720 batch loss 6603.75879 epoch total loss 4846.47412\n",
      "Trained batch 721 batch loss 6481.94775 epoch total loss 4848.74268\n",
      "Trained batch 722 batch loss 6640.24268 epoch total loss 4851.22412\n",
      "Trained batch 723 batch loss 6506.39062 epoch total loss 4853.51367\n",
      "Trained batch 724 batch loss 6521.05371 epoch total loss 4855.81641\n",
      "Trained batch 725 batch loss 6501.21191 epoch total loss 4858.08643\n",
      "Trained batch 726 batch loss 6391.92188 epoch total loss 4860.19922\n",
      "Trained batch 727 batch loss 6549.17432 epoch total loss 4862.52246\n",
      "Trained batch 728 batch loss 6316.20361 epoch total loss 4864.51904\n",
      "Trained batch 729 batch loss 6473.90137 epoch total loss 4866.72705\n",
      "Trained batch 730 batch loss 6513.68896 epoch total loss 4868.9834\n",
      "Trained batch 731 batch loss 6635.13135 epoch total loss 4871.39941\n",
      "Trained batch 732 batch loss 6573.7627 epoch total loss 4873.7251\n",
      "Trained batch 733 batch loss 6640.25 epoch total loss 4876.13525\n",
      "Trained batch 734 batch loss 6200.03369 epoch total loss 4877.93848\n",
      "Trained batch 735 batch loss 6615.14062 epoch total loss 4880.30225\n",
      "Trained batch 736 batch loss 6590.53271 epoch total loss 4882.62598\n",
      "Trained batch 737 batch loss 6657.85547 epoch total loss 4885.03467\n",
      "Trained batch 738 batch loss 6641.12744 epoch total loss 4887.41406\n",
      "Trained batch 739 batch loss 6459.52148 epoch total loss 4889.5415\n",
      "Trained batch 740 batch loss 6622.31055 epoch total loss 4891.8833\n",
      "Trained batch 741 batch loss 6583.62158 epoch total loss 4894.16602\n",
      "Trained batch 742 batch loss 6631.57568 epoch total loss 4896.50732\n",
      "Trained batch 743 batch loss 6591.63184 epoch total loss 4898.78906\n",
      "Trained batch 744 batch loss 6618.97803 epoch total loss 4901.10107\n",
      "Trained batch 745 batch loss 6589.34082 epoch total loss 4903.36719\n",
      "Trained batch 746 batch loss 6610.93848 epoch total loss 4905.65625\n",
      "Trained batch 747 batch loss 6569.15674 epoch total loss 4907.8833\n",
      "Trained batch 748 batch loss 6587.98975 epoch total loss 4910.12939\n",
      "Trained batch 749 batch loss 6535.62158 epoch total loss 4912.29932\n",
      "Trained batch 750 batch loss 6155.42285 epoch total loss 4913.95703\n",
      "Trained batch 751 batch loss 6275.89502 epoch total loss 4915.77051\n",
      "Trained batch 752 batch loss 6554.42529 epoch total loss 4917.94971\n",
      "Trained batch 753 batch loss 6551.11 epoch total loss 4920.11865\n",
      "Trained batch 754 batch loss 6336.34619 epoch total loss 4921.99658\n",
      "Trained batch 755 batch loss 6562.19336 epoch total loss 4924.16943\n",
      "Trained batch 756 batch loss 6555.68652 epoch total loss 4926.32715\n",
      "Trained batch 757 batch loss 6197.92969 epoch total loss 4928.00732\n",
      "Trained batch 758 batch loss 6551.44141 epoch total loss 4930.14893\n",
      "Trained batch 759 batch loss 6553.4209 epoch total loss 4932.28809\n",
      "Trained batch 760 batch loss 6343.88867 epoch total loss 4934.14551\n",
      "Trained batch 761 batch loss 6448.39453 epoch total loss 4936.13525\n",
      "Trained batch 762 batch loss 6590.09082 epoch total loss 4938.30566\n",
      "Trained batch 763 batch loss 6407.50879 epoch total loss 4940.23145\n",
      "Trained batch 764 batch loss 6451.71045 epoch total loss 4942.21\n",
      "Trained batch 765 batch loss 6558.94922 epoch total loss 4944.32324\n",
      "Trained batch 766 batch loss 6418.0918 epoch total loss 4946.24707\n",
      "Trained batch 767 batch loss 6380.6543 epoch total loss 4948.11719\n",
      "Trained batch 768 batch loss 6332.9292 epoch total loss 4949.92041\n",
      "Trained batch 769 batch loss 6255.26758 epoch total loss 4951.61816\n",
      "Trained batch 770 batch loss 6448.55322 epoch total loss 4953.56201\n",
      "Trained batch 771 batch loss 6503.71484 epoch total loss 4955.57275\n",
      "Trained batch 772 batch loss 6570.90332 epoch total loss 4957.66504\n",
      "Trained batch 773 batch loss 6576.63965 epoch total loss 4959.76\n",
      "Trained batch 774 batch loss 6617.71924 epoch total loss 4961.90186\n",
      "Trained batch 775 batch loss 6523.80078 epoch total loss 4963.91699\n",
      "Trained batch 776 batch loss 6566.47949 epoch total loss 4965.98242\n",
      "Trained batch 777 batch loss 6560.56396 epoch total loss 4968.03467\n",
      "Trained batch 778 batch loss 6646.21094 epoch total loss 4970.19141\n",
      "Trained batch 779 batch loss 6587.96 epoch total loss 4972.26807\n",
      "Trained batch 780 batch loss 6615.95068 epoch total loss 4974.37549\n",
      "Trained batch 781 batch loss 6644.28467 epoch total loss 4976.51367\n",
      "Trained batch 782 batch loss 6646.81348 epoch total loss 4978.64941\n",
      "Trained batch 783 batch loss 6355.15137 epoch total loss 4980.40771\n",
      "Trained batch 784 batch loss 6573.47705 epoch total loss 4982.44\n",
      "Trained batch 785 batch loss 6622.1377 epoch total loss 4984.52881\n",
      "Trained batch 786 batch loss 6647.31689 epoch total loss 4986.64404\n",
      "Trained batch 787 batch loss 6641.93506 epoch total loss 4988.74756\n",
      "Trained batch 788 batch loss 6633.4751 epoch total loss 4990.83447\n",
      "Trained batch 789 batch loss 6605.36914 epoch total loss 4992.88086\n",
      "Trained batch 790 batch loss 6434.38477 epoch total loss 4994.70557\n",
      "Trained batch 791 batch loss 6407.00537 epoch total loss 4996.49121\n",
      "Trained batch 792 batch loss 6576.43848 epoch total loss 4998.48633\n",
      "Trained batch 793 batch loss 6518.125 epoch total loss 5000.40234\n",
      "Trained batch 794 batch loss 6511.79053 epoch total loss 5002.30566\n",
      "Trained batch 795 batch loss 6520.08301 epoch total loss 5004.21484\n",
      "Trained batch 796 batch loss 6406.76855 epoch total loss 5005.97656\n",
      "Trained batch 797 batch loss 6078.58545 epoch total loss 5007.32227\n",
      "Trained batch 798 batch loss 6515.74707 epoch total loss 5009.21289\n",
      "Trained batch 799 batch loss 6563.34863 epoch total loss 5011.15771\n",
      "Trained batch 800 batch loss 6677.26807 epoch total loss 5013.24\n",
      "Trained batch 801 batch loss 6598.13477 epoch total loss 5015.21924\n",
      "Trained batch 802 batch loss 6658.93359 epoch total loss 5017.26855\n",
      "Trained batch 803 batch loss 6692.58057 epoch total loss 5019.35498\n",
      "Trained batch 804 batch loss 6650.22 epoch total loss 5021.3833\n",
      "Trained batch 805 batch loss 6559.18359 epoch total loss 5023.29395\n",
      "Trained batch 806 batch loss 6422.69043 epoch total loss 5025.03027\n",
      "Trained batch 807 batch loss 6723.46973 epoch total loss 5027.13477\n",
      "Trained batch 808 batch loss 6724.75342 epoch total loss 5029.23584\n",
      "Trained batch 809 batch loss 6741.85596 epoch total loss 5031.35254\n",
      "Trained batch 810 batch loss 6729.59717 epoch total loss 5033.44922\n",
      "Trained batch 811 batch loss 6737.06 epoch total loss 5035.55\n",
      "Trained batch 812 batch loss 6561.87646 epoch total loss 5037.42969\n",
      "Trained batch 813 batch loss 6727.78857 epoch total loss 5039.50879\n",
      "Trained batch 814 batch loss 6709.70361 epoch total loss 5041.56055\n",
      "Trained batch 815 batch loss 6335.46 epoch total loss 5043.14795\n",
      "Trained batch 816 batch loss 6743.1543 epoch total loss 5045.23145\n",
      "Trained batch 817 batch loss 6425.80469 epoch total loss 5046.92139\n",
      "Trained batch 818 batch loss 6770.6416 epoch total loss 5049.02881\n",
      "Trained batch 819 batch loss 6793.66357 epoch total loss 5051.15918\n",
      "Trained batch 820 batch loss 6671.11182 epoch total loss 5053.13428\n",
      "Trained batch 821 batch loss 6804.48389 epoch total loss 5055.26758\n",
      "Trained batch 822 batch loss 6803.26172 epoch total loss 5057.39404\n",
      "Trained batch 823 batch loss 6789.13037 epoch total loss 5059.49854\n",
      "Trained batch 824 batch loss 6721.3042 epoch total loss 5061.51514\n",
      "Trained batch 825 batch loss 6434.4 epoch total loss 5063.1792\n",
      "Trained batch 826 batch loss 6691.34033 epoch total loss 5065.15039\n",
      "Trained batch 827 batch loss 6765.31445 epoch total loss 5067.20605\n",
      "Trained batch 828 batch loss 6799.82227 epoch total loss 5069.29883\n",
      "Trained batch 829 batch loss 6807.01318 epoch total loss 5071.39502\n",
      "Trained batch 830 batch loss 6838.15674 epoch total loss 5073.52344\n",
      "Trained batch 831 batch loss 6855.15283 epoch total loss 5075.66748\n",
      "Trained batch 832 batch loss 6869.75879 epoch total loss 5077.82373\n",
      "Trained batch 833 batch loss 6856.09863 epoch total loss 5079.9585\n",
      "Trained batch 834 batch loss 6868.41895 epoch total loss 5082.10303\n",
      "Trained batch 835 batch loss 6883.16113 epoch total loss 5084.26\n",
      "Trained batch 836 batch loss 6874.16211 epoch total loss 5086.40088\n",
      "Trained batch 837 batch loss 6758.64404 epoch total loss 5088.39844\n",
      "Trained batch 838 batch loss 6877.85498 epoch total loss 5090.53418\n",
      "Trained batch 839 batch loss 6635.02783 epoch total loss 5092.375\n",
      "Trained batch 840 batch loss 6386.29785 epoch total loss 5093.91553\n",
      "Trained batch 841 batch loss 6916.94922 epoch total loss 5096.08301\n",
      "Trained batch 842 batch loss 6855.12158 epoch total loss 5098.17236\n",
      "Trained batch 843 batch loss 6796.34961 epoch total loss 5100.18701\n",
      "Trained batch 844 batch loss 6902.55615 epoch total loss 5102.32227\n",
      "Trained batch 845 batch loss 6743.36182 epoch total loss 5104.26465\n",
      "Trained batch 846 batch loss 6894.68457 epoch total loss 5106.38037\n",
      "Trained batch 847 batch loss 6729.68213 epoch total loss 5108.29688\n",
      "Trained batch 848 batch loss 6773.1416 epoch total loss 5110.26025\n",
      "Trained batch 849 batch loss 6759.29443 epoch total loss 5112.20264\n",
      "Trained batch 850 batch loss 6875.87061 epoch total loss 5114.27783\n",
      "Trained batch 851 batch loss 6583.00244 epoch total loss 5116.00342\n",
      "Trained batch 852 batch loss 6669.43945 epoch total loss 5117.82666\n",
      "Trained batch 853 batch loss 6849.98975 epoch total loss 5119.85742\n",
      "Trained batch 854 batch loss 6550.97705 epoch total loss 5121.5332\n",
      "Trained batch 855 batch loss 6648.69629 epoch total loss 5123.31934\n",
      "Trained batch 856 batch loss 6558.50098 epoch total loss 5124.99609\n",
      "Trained batch 857 batch loss 6596.99512 epoch total loss 5126.71338\n",
      "Trained batch 858 batch loss 6724.93311 epoch total loss 5128.57617\n",
      "Trained batch 859 batch loss 6733.36768 epoch total loss 5130.44482\n",
      "Trained batch 860 batch loss 6635.78711 epoch total loss 5132.19531\n",
      "Trained batch 861 batch loss 6887.14111 epoch total loss 5134.2334\n",
      "Trained batch 862 batch loss 6906.57959 epoch total loss 5136.28955\n",
      "Trained batch 863 batch loss 6928.59424 epoch total loss 5138.36621\n",
      "Trained batch 864 batch loss 6944.43066 epoch total loss 5140.45654\n",
      "Trained batch 865 batch loss 6968.1709 epoch total loss 5142.56934\n",
      "Trained batch 866 batch loss 6959.87598 epoch total loss 5144.66797\n",
      "Trained batch 999 batch loss 7856.69482 epoch total loss 5425.14941\n",
      "Trained batch 1000 batch loss 7948.5835 epoch total loss 5427.67285\n",
      "Trained batch 1001 batch loss 7932.91748 epoch total loss 5430.17578\n",
      "Trained batch 1002 batch loss 7820.87402 epoch total loss 5432.56201\n",
      "Trained batch 1003 batch loss 7950.06152 epoch total loss 5435.07178\n",
      "Trained batch 1004 batch loss 7820.38232 epoch total loss 5437.44775\n",
      "Trained batch 1005 batch loss 7909.48242 epoch total loss 5439.90723\n",
      "Trained batch 1006 batch loss 7799.81689 epoch total loss 5442.25342\n",
      "Trained batch 1007 batch loss 7902.96191 epoch total loss 5444.69727\n",
      "Trained batch 1008 batch loss 7918.13135 epoch total loss 5447.15088\n",
      "Trained batch 1009 batch loss 7908.80762 epoch total loss 5449.59082\n",
      "Trained batch 1010 batch loss 7830.79346 epoch total loss 5451.94873\n",
      "Trained batch 1011 batch loss 7884.74756 epoch total loss 5454.35449\n",
      "Trained batch 1012 batch loss 7902.75928 epoch total loss 5456.77441\n",
      "Trained batch 1013 batch loss 7639.32422 epoch total loss 5458.92871\n",
      "Trained batch 1014 batch loss 7652.30469 epoch total loss 5461.09229\n",
      "Trained batch 1015 batch loss 7974.37305 epoch total loss 5463.56836\n",
      "Trained batch 1016 batch loss 7947.99512 epoch total loss 5466.01367\n",
      "Trained batch 1017 batch loss 7741.43262 epoch total loss 5468.25146\n",
      "Trained batch 1018 batch loss 7999.21826 epoch total loss 5470.7373\n",
      "Trained batch 1019 batch loss 7994.66309 epoch total loss 5473.21387\n",
      "Trained batch 1020 batch loss 7984.70508 epoch total loss 5475.67578\n",
      "Trained batch 1021 batch loss 7990.06055 epoch total loss 5478.13867\n",
      "Trained batch 1022 batch loss 7992.97852 epoch total loss 5480.59912\n",
      "Trained batch 1023 batch loss 7939.77539 epoch total loss 5483.00342\n",
      "Trained batch 1024 batch loss 7961.96338 epoch total loss 5485.42432\n",
      "Trained batch 1025 batch loss 7806.67822 epoch total loss 5487.68896\n",
      "Trained batch 1026 batch loss 8001.83838 epoch total loss 5490.13916\n",
      "Trained batch 1027 batch loss 7827.62256 epoch total loss 5492.41553\n",
      "Trained batch 1028 batch loss 7793.10352 epoch total loss 5494.65332\n",
      "Trained batch 1029 batch loss 7980.66113 epoch total loss 5497.06885\n",
      "Trained batch 1030 batch loss 7985.84 epoch total loss 5499.48535\n",
      "Trained batch 1031 batch loss 7995.77441 epoch total loss 5501.90674\n",
      "Trained batch 1032 batch loss 7978.72266 epoch total loss 5504.30664\n",
      "Trained batch 1033 batch loss 7966.1543 epoch total loss 5506.69\n",
      "Trained batch 1034 batch loss 7903.57129 epoch total loss 5509.00781\n",
      "Trained batch 1035 batch loss 7570.6875 epoch total loss 5510.99951\n",
      "Trained batch 1036 batch loss 7889.00586 epoch total loss 5513.29492\n",
      "Trained batch 1037 batch loss 7965.125 epoch total loss 5515.65918\n",
      "Trained batch 1038 batch loss 7979.94482 epoch total loss 5518.0332\n",
      "Trained batch 1039 batch loss 7786.80078 epoch total loss 5520.2168\n",
      "Trained batch 1040 batch loss 7928.24902 epoch total loss 5522.53223\n",
      "Trained batch 1041 batch loss 7736.93164 epoch total loss 5524.65967\n",
      "Trained batch 1042 batch loss 7897.27051 epoch total loss 5526.93652\n",
      "Trained batch 1043 batch loss 7928.12549 epoch total loss 5529.23877\n",
      "Trained batch 1044 batch loss 7976.91211 epoch total loss 5531.5835\n",
      "Trained batch 1045 batch loss 7987.13721 epoch total loss 5533.93311\n",
      "Trained batch 1046 batch loss 7956.81396 epoch total loss 5536.24951\n",
      "Trained batch 1047 batch loss 7932.771 epoch total loss 5538.53857\n",
      "Trained batch 1048 batch loss 7676.33398 epoch total loss 5540.57861\n",
      "Trained batch 1049 batch loss 7515.62 epoch total loss 5542.46143\n",
      "Trained batch 1050 batch loss 7796.3 epoch total loss 5544.60791\n",
      "Trained batch 1051 batch loss 7797.6958 epoch total loss 5546.75146\n",
      "Trained batch 1052 batch loss 7760.01123 epoch total loss 5548.85547\n",
      "Trained batch 1053 batch loss 7579.01416 epoch total loss 5550.78369\n",
      "Trained batch 1054 batch loss 7872.38477 epoch total loss 5552.98633\n",
      "Trained batch 1055 batch loss 7910.53125 epoch total loss 5555.2207\n",
      "Trained batch 1056 batch loss 7943.30273 epoch total loss 5557.48242\n",
      "Trained batch 1057 batch loss 7893.77539 epoch total loss 5559.69287\n",
      "Trained batch 1058 batch loss 7935.19434 epoch total loss 5561.93799\n",
      "Trained batch 1059 batch loss 7881.78955 epoch total loss 5564.12891\n",
      "Trained batch 1060 batch loss 7956.38184 epoch total loss 5566.38574\n",
      "Trained batch 1061 batch loss 7995.33301 epoch total loss 5568.67529\n",
      "Trained batch 1062 batch loss 7693.00342 epoch total loss 5570.67578\n",
      "Trained batch 1063 batch loss 7938.2 epoch total loss 5572.90283\n",
      "Trained batch 1064 batch loss 8022.39111 epoch total loss 5575.20508\n",
      "Trained batch 1065 batch loss 7735.70264 epoch total loss 5577.2334\n",
      "Trained batch 1066 batch loss 7845.26855 epoch total loss 5579.36133\n",
      "Trained batch 1067 batch loss 7809.63867 epoch total loss 5581.45117\n",
      "Trained batch 1068 batch loss 7918.83936 epoch total loss 5583.64\n",
      "Trained batch 1069 batch loss 8084.62158 epoch total loss 5585.97949\n",
      "Trained batch 1070 batch loss 7990.44434 epoch total loss 5588.22656\n",
      "Trained batch 1071 batch loss 8018.63281 epoch total loss 5590.49561\n",
      "Trained batch 1072 batch loss 8069.48047 epoch total loss 5592.80811\n",
      "Trained batch 1073 batch loss 8078.06299 epoch total loss 5595.12451\n",
      "Trained batch 1074 batch loss 7831.16699 epoch total loss 5597.20605\n",
      "Trained batch 1075 batch loss 7744.41699 epoch total loss 5599.20361\n",
      "Trained batch 1076 batch loss 7945.79541 epoch total loss 5601.38477\n",
      "Trained batch 1077 batch loss 8078.76611 epoch total loss 5603.68506\n",
      "Trained batch 1078 batch loss 8142.70312 epoch total loss 5606.04053\n",
      "Trained batch 1079 batch loss 8154.81152 epoch total loss 5608.40283\n",
      "Trained batch 1080 batch loss 8034.62354 epoch total loss 5610.64893\n",
      "Trained batch 1081 batch loss 8049.96436 epoch total loss 5612.90576\n",
      "Trained batch 1082 batch loss 8097.11865 epoch total loss 5615.20166\n",
      "Trained batch 1083 batch loss 8112.31885 epoch total loss 5617.50732\n",
      "Trained batch 1084 batch loss 8122.76318 epoch total loss 5619.81885\n",
      "Trained batch 1085 batch loss 8102.82275 epoch total loss 5622.10742\n",
      "Trained batch 1086 batch loss 8137.1167 epoch total loss 5624.42334\n",
      "Trained batch 1087 batch loss 8053.88477 epoch total loss 5626.6582\n",
      "Trained batch 1088 batch loss 7691.47607 epoch total loss 5628.55615\n",
      "Trained batch 1089 batch loss 7985.55811 epoch total loss 5630.72\n",
      "Trained batch 1090 batch loss 8145.70215 epoch total loss 5633.02734\n",
      "Trained batch 1091 batch loss 8135.52441 epoch total loss 5635.32129\n",
      "Trained batch 1092 batch loss 8104.74609 epoch total loss 5637.58252\n",
      "Trained batch 1093 batch loss 8140.98682 epoch total loss 5639.87305\n",
      "Trained batch 1094 batch loss 8094.60791 epoch total loss 5642.1167\n",
      "Trained batch 1095 batch loss 8025.79834 epoch total loss 5644.29346\n",
      "Trained batch 1096 batch loss 7946.28516 epoch total loss 5646.39404\n",
      "Trained batch 1097 batch loss 8093.46045 epoch total loss 5648.625\n",
      "Trained batch 1098 batch loss 8072.13477 epoch total loss 5650.83203\n",
      "Trained batch 1099 batch loss 8126.07422 epoch total loss 5653.08398\n",
      "Trained batch 1100 batch loss 8141.80713 epoch total loss 5655.34668\n",
      "Trained batch 1101 batch loss 8154.62891 epoch total loss 5657.6167\n",
      "Trained batch 1102 batch loss 7979.89941 epoch total loss 5659.72412\n",
      "Trained batch 1103 batch loss 8059.55762 epoch total loss 5661.9\n",
      "Trained batch 1104 batch loss 8052.74561 epoch total loss 5664.06543\n",
      "Trained batch 1105 batch loss 8064.05762 epoch total loss 5666.2373\n",
      "Trained batch 1106 batch loss 7929.11377 epoch total loss 5668.2832\n",
      "Trained batch 1107 batch loss 8059.26074 epoch total loss 5670.44287\n",
      "Trained batch 1108 batch loss 8117.68799 epoch total loss 5672.65186\n",
      "Trained batch 1109 batch loss 8131.979 epoch total loss 5674.86914\n",
      "Trained batch 1110 batch loss 7894.80273 epoch total loss 5676.86914\n",
      "Trained batch 1111 batch loss 7785.50049 epoch total loss 5678.76709\n",
      "Trained batch 1112 batch loss 7733.72363 epoch total loss 5680.61523\n",
      "Trained batch 1113 batch loss 8038.62451 epoch total loss 5682.7334\n",
      "Trained batch 1114 batch loss 8155.51 epoch total loss 5684.95312\n",
      "Trained batch 1115 batch loss 8138.33496 epoch total loss 5687.15381\n",
      "Trained batch 1116 batch loss 8147.70605 epoch total loss 5689.3584\n",
      "Trained batch 1117 batch loss 8142.84521 epoch total loss 5691.55518\n",
      "Trained batch 1118 batch loss 8151.01953 epoch total loss 5693.75488\n",
      "Trained batch 1119 batch loss 8164.26904 epoch total loss 5695.96289\n",
      "Trained batch 1120 batch loss 8160.3335 epoch total loss 5698.16357\n",
      "Trained batch 1121 batch loss 8068.64746 epoch total loss 5700.27783\n",
      "Trained batch 1122 batch loss 8103.7 epoch total loss 5702.42\n",
      "Trained batch 1123 batch loss 7970.02051 epoch total loss 5704.43896\n",
      "Trained batch 1124 batch loss 8088.48389 epoch total loss 5706.56\n",
      "Trained batch 1125 batch loss 8154.51465 epoch total loss 5708.73584\n",
      "Trained batch 1126 batch loss 8118.1748 epoch total loss 5710.87549\n",
      "Trained batch 1127 batch loss 8100.59814 epoch total loss 5712.99609\n",
      "Trained batch 1128 batch loss 8099.60596 epoch total loss 5715.11182\n",
      "Trained batch 1129 batch loss 7895.21191 epoch total loss 5717.04248\n",
      "Trained batch 1130 batch loss 8015.60596 epoch total loss 5719.07666\n",
      "Trained batch 1131 batch loss 8022.67432 epoch total loss 5721.11328\n",
      "Trained batch 1132 batch loss 7804.84717 epoch total loss 5722.9541\n",
      "Trained batch 1133 batch loss 7661.43555 epoch total loss 5724.66504\n",
      "Trained batch 1134 batch loss 7829.72949 epoch total loss 5726.521\n",
      "Trained batch 1135 batch loss 7787.07568 epoch total loss 5728.33643\n",
      "Trained batch 1136 batch loss 7275.53711 epoch total loss 5729.69873\n",
      "Trained batch 1137 batch loss 8044.14111 epoch total loss 5731.73389\n",
      "Trained batch 1138 batch loss 8051.94385 epoch total loss 5733.77295\n",
      "Trained batch 1139 batch loss 8136.0415 epoch total loss 5735.88184\n",
      "Trained batch 1140 batch loss 7881.95312 epoch total loss 5737.76465\n",
      "Trained batch 1141 batch loss 7914.7041 epoch total loss 5739.67236\n",
      "Trained batch 1142 batch loss 8065.06348 epoch total loss 5741.7085\n",
      "Trained batch 1143 batch loss 8061.27393 epoch total loss 5743.73779\n",
      "Trained batch 1144 batch loss 7910.34766 epoch total loss 5745.63184\n",
      "Trained batch 1145 batch loss 8098.49854 epoch total loss 5747.68701\n",
      "Trained batch 1146 batch loss 8070.271 epoch total loss 5749.71387\n",
      "Trained batch 1147 batch loss 8147.25781 epoch total loss 5751.8042\n",
      "Trained batch 1148 batch loss 8126.80078 epoch total loss 5753.87305\n",
      "Trained batch 1149 batch loss 8155.97363 epoch total loss 5755.96387\n",
      "Trained batch 1150 batch loss 8137.5249 epoch total loss 5758.03467\n",
      "Trained batch 1151 batch loss 8115.50684 epoch total loss 5760.08301\n",
      "Trained batch 1152 batch loss 8131.77295 epoch total loss 5762.14209\n",
      "Trained batch 1153 batch loss 7916.74658 epoch total loss 5764.01025\n",
      "Trained batch 1154 batch loss 8147.44141 epoch total loss 5766.07568\n",
      "Trained batch 1155 batch loss 8191.10449 epoch total loss 5768.17529\n",
      "Trained batch 1156 batch loss 8166.43896 epoch total loss 5770.25\n",
      "Trained batch 1157 batch loss 8076.06152 epoch total loss 5772.24268\n",
      "Trained batch 1158 batch loss 8142.40918 epoch total loss 5774.28955\n",
      "Trained batch 1159 batch loss 8182.33105 epoch total loss 5776.36768\n",
      "Trained batch 1160 batch loss 8178.9585 epoch total loss 5778.43896\n",
      "Trained batch 1161 batch loss 8197.7 epoch total loss 5780.52246\n",
      "Trained batch 1162 batch loss 8133.03076 epoch total loss 5782.54688\n",
      "Trained batch 1163 batch loss 8190.96191 epoch total loss 5784.61768\n",
      "Trained batch 1164 batch loss 8177.92969 epoch total loss 5786.67383\n",
      "Trained batch 1165 batch loss 8033.7207 epoch total loss 5788.60254\n",
      "Trained batch 1166 batch loss 8133.17236 epoch total loss 5790.61328\n",
      "Trained batch 1167 batch loss 7974.0459 epoch total loss 5792.48438\n",
      "Trained batch 1168 batch loss 7931.31689 epoch total loss 5794.31543\n",
      "Trained batch 1169 batch loss 8026.11133 epoch total loss 5796.22461\n",
      "Trained batch 1170 batch loss 8002.17627 epoch total loss 5798.11\n",
      "Trained batch 1171 batch loss 8033.74805 epoch total loss 5800.01855\n",
      "Trained batch 1172 batch loss 8192.11719 epoch total loss 5802.05957\n",
      "Trained batch 1173 batch loss 8185.98682 epoch total loss 5804.09229\n",
      "Trained batch 1174 batch loss 8089.77 epoch total loss 5806.03906\n",
      "Trained batch 1175 batch loss 8110.68604 epoch total loss 5808.00049\n",
      "Trained batch 1176 batch loss 8165.47607 epoch total loss 5810.00488\n",
      "Trained batch 1177 batch loss 8174.61768 epoch total loss 5812.01416\n",
      "Trained batch 1178 batch loss 7684.62354 epoch total loss 5813.60352\n",
      "Trained batch 1179 batch loss 8131.87256 epoch total loss 5815.57\n",
      "Trained batch 1180 batch loss 8051.9043 epoch total loss 5817.46533\n",
      "Trained batch 1181 batch loss 7883.8291 epoch total loss 5819.21484\n",
      "Trained batch 1182 batch loss 8064.88086 epoch total loss 5821.11523\n",
      "Trained batch 1183 batch loss 8292.56836 epoch total loss 5823.2041\n",
      "Trained batch 1184 batch loss 8293.05469 epoch total loss 5825.29\n",
      "Trained batch 1185 batch loss 8296.26465 epoch total loss 5827.37549\n",
      "Trained batch 1186 batch loss 8248.45215 epoch total loss 5829.41699\n",
      "Trained batch 1187 batch loss 8320.94141 epoch total loss 5831.51611\n",
      "Trained batch 1188 batch loss 8305.91602 epoch total loss 5833.59912\n",
      "Trained batch 1189 batch loss 8157.52783 epoch total loss 5835.55322\n",
      "Trained batch 1190 batch loss 8246.62695 epoch total loss 5837.57959\n",
      "Trained batch 1191 batch loss 8338.51074 epoch total loss 5839.6792\n",
      "Trained batch 1192 batch loss 8299.15234 epoch total loss 5841.74268\n",
      "Trained batch 1193 batch loss 8115.9458 epoch total loss 5843.64893\n",
      "Trained batch 1194 batch loss 8261.01 epoch total loss 5845.67334\n",
      "Trained batch 1195 batch loss 8266.60645 epoch total loss 5847.69922\n",
      "Trained batch 1196 batch loss 8245.0332 epoch total loss 5849.70361\n",
      "Trained batch 1197 batch loss 7921.64111 epoch total loss 5851.43457\n",
      "Trained batch 1198 batch loss 8005.79199 epoch total loss 5853.23291\n",
      "Trained batch 1199 batch loss 8436.64551 epoch total loss 5855.38721\n",
      "Trained batch 1200 batch loss 8397.57227 epoch total loss 5857.50586\n",
      "Trained batch 1201 batch loss 8417.13086 epoch total loss 5859.63721\n",
      "Trained batch 1202 batch loss 8528.44141 epoch total loss 5861.85742\n",
      "Trained batch 1203 batch loss 8503.41211 epoch total loss 5864.05322\n",
      "Trained batch 1204 batch loss 8475.77 epoch total loss 5866.22266\n",
      "Trained batch 1205 batch loss 8484.80469 epoch total loss 5868.396\n",
      "Trained batch 1206 batch loss 8242.16 epoch total loss 5870.36377\n",
      "Trained batch 1207 batch loss 8497.76562 epoch total loss 5872.54102\n",
      "Trained batch 1208 batch loss 8540.35938 epoch total loss 5874.74951\n",
      "Trained batch 1209 batch loss 8576.23438 epoch total loss 5876.98389\n",
      "Trained batch 1210 batch loss 8579.46484 epoch total loss 5879.21729\n",
      "Trained batch 1211 batch loss 8591.28809 epoch total loss 5881.45703\n",
      "Trained batch 1212 batch loss 8608.53906 epoch total loss 5883.70703\n",
      "Trained batch 1213 batch loss 8636.86523 epoch total loss 5885.97705\n",
      "Trained batch 1214 batch loss 8611.80176 epoch total loss 5888.22217\n",
      "Trained batch 1215 batch loss 8581.27246 epoch total loss 5890.43896\n",
      "Trained batch 1216 batch loss 8574.45117 epoch total loss 5892.64648\n",
      "Trained batch 1217 batch loss 8591.30371 epoch total loss 5894.86377\n",
      "Trained batch 1218 batch loss 8638.57812 epoch total loss 5897.1167\n",
      "Trained batch 1219 batch loss 8662.44238 epoch total loss 5899.38525\n",
      "Trained batch 1220 batch loss 8678.29297 epoch total loss 5901.66309\n",
      "Trained batch 1221 batch loss 8697.04883 epoch total loss 5903.95264\n",
      "Trained batch 1222 batch loss 8620.5127 epoch total loss 5906.17529\n",
      "Trained batch 1223 batch loss 8626.7832 epoch total loss 5908.40039\n",
      "Trained batch 1224 batch loss 8528.04883 epoch total loss 5910.54053\n",
      "Trained batch 1225 batch loss 8645.64258 epoch total loss 5912.77295\n",
      "Trained batch 1226 batch loss 8739.19824 epoch total loss 5915.07812\n",
      "Trained batch 1227 batch loss 8753.75781 epoch total loss 5917.39209\n",
      "Trained batch 1228 batch loss 8761.86328 epoch total loss 5919.7085\n",
      "Trained batch 1229 batch loss 8764.71582 epoch total loss 5922.02295\n",
      "Trained batch 1230 batch loss 8794.6416 epoch total loss 5924.3584\n",
      "Trained batch 1231 batch loss 8792.30273 epoch total loss 5926.68848\n",
      "Trained batch 1232 batch loss 8818.66309 epoch total loss 5929.03564\n",
      "Trained batch 1233 batch loss 8842.62305 epoch total loss 5931.39844\n",
      "Trained batch 1234 batch loss 8837.13 epoch total loss 5933.75342\n",
      "Trained batch 1235 batch loss 8862.65527 epoch total loss 5936.12451\n",
      "Trained batch 1236 batch loss 8804.47656 epoch total loss 5938.44531\n",
      "Trained batch 1237 batch loss 8883.51367 epoch total loss 5940.82617\n",
      "Trained batch 1238 batch loss 8859.83691 epoch total loss 5943.18408\n",
      "Trained batch 1239 batch loss 8864.16406 epoch total loss 5945.5415\n",
      "Trained batch 1240 batch loss 8522.94824 epoch total loss 5947.62\n",
      "Trained batch 1241 batch loss 8379.34766 epoch total loss 5949.57959\n",
      "Trained batch 1242 batch loss 8678.67285 epoch total loss 5951.77686\n",
      "Trained batch 1243 batch loss 8689.92578 epoch total loss 5953.98\n",
      "Trained batch 1244 batch loss 8814.12793 epoch total loss 5956.27881\n",
      "Trained batch 1245 batch loss 8960.40332 epoch total loss 5958.69189\n",
      "Trained batch 1246 batch loss 8962.21191 epoch total loss 5961.10254\n",
      "Trained batch 1247 batch loss 8973.72266 epoch total loss 5963.51807\n",
      "Trained batch 1248 batch loss 8997.10254 epoch total loss 5965.94873\n",
      "Trained batch 1249 batch loss 9016.32812 epoch total loss 5968.39111\n",
      "Trained batch 1250 batch loss 8979.27246 epoch total loss 5970.8\n",
      "Trained batch 1251 batch loss 8914.86328 epoch total loss 5973.15332\n",
      "Trained batch 1252 batch loss 8648.07324 epoch total loss 5975.29\n",
      "Trained batch 1253 batch loss 8989.90234 epoch total loss 5977.6958\n",
      "Trained batch 1254 batch loss 8718.45312 epoch total loss 5979.88135\n",
      "Trained batch 1255 batch loss 8784.7666 epoch total loss 5982.1167\n",
      "Trained batch 1256 batch loss 8970.09082 epoch total loss 5984.49561\n",
      "Trained batch 1257 batch loss 8948.16211 epoch total loss 5986.85303\n",
      "Trained batch 1258 batch loss 8649.09277 epoch total loss 5988.96924\n",
      "Trained batch 1259 batch loss 8893.43652 epoch total loss 5991.27637\n",
      "Trained batch 1260 batch loss 8716.06934 epoch total loss 5993.43896\n",
      "Trained batch 1261 batch loss 8881.10156 epoch total loss 5995.729\n",
      "Trained batch 1262 batch loss 9048.44 epoch total loss 5998.14795\n",
      "Trained batch 1263 batch loss 9076.42188 epoch total loss 6000.58496\n",
      "Trained batch 1264 batch loss 8928.7207 epoch total loss 6002.90137\n",
      "Trained batch 1265 batch loss 8976.2168 epoch total loss 6005.25195\n",
      "Trained batch 1266 batch loss 9034.14 epoch total loss 6007.64404\n",
      "Trained batch 1267 batch loss 8996.21484 epoch total loss 6010.00293\n",
      "Trained batch 1268 batch loss 9002.16895 epoch total loss 6012.3623\n",
      "Trained batch 1269 batch loss 8941.52246 epoch total loss 6014.67041\n",
      "Trained batch 1270 batch loss 8751.22559 epoch total loss 6016.8252\n",
      "Trained batch 1271 batch loss 8730.87402 epoch total loss 6018.96045\n",
      "Trained batch 1272 batch loss 8950.64453 epoch total loss 6021.26514\n",
      "Trained batch 1273 batch loss 9013.05762 epoch total loss 6023.61523\n",
      "Trained batch 1274 batch loss 8947.51 epoch total loss 6025.91064\n",
      "Trained batch 1275 batch loss 8693.38867 epoch total loss 6028.00293\n",
      "Trained batch 1276 batch loss 9052.69141 epoch total loss 6030.37305\n",
      "Trained batch 1277 batch loss 9105.53906 epoch total loss 6032.78125\n",
      "Trained batch 1278 batch loss 9100.1543 epoch total loss 6035.18115\n",
      "Trained batch 1279 batch loss 9112.26367 epoch total loss 6037.5874\n",
      "Trained batch 1280 batch loss 9092.75 epoch total loss 6039.97412\n",
      "Trained batch 1281 batch loss 8885.90625 epoch total loss 6042.1958\n",
      "Trained batch 1282 batch loss 9075.9668 epoch total loss 6044.5625\n",
      "Trained batch 1283 batch loss 9091.72168 epoch total loss 6046.9375\n",
      "Trained batch 1284 batch loss 8928.82715 epoch total loss 6049.18164\n",
      "Trained batch 1285 batch loss 9141.14258 epoch total loss 6051.58789\n",
      "Trained batch 1286 batch loss 8906.80566 epoch total loss 6053.80811\n",
      "Trained batch 1287 batch loss 9134.60156 epoch total loss 6056.20215\n",
      "Trained batch 1288 batch loss 8934.78223 epoch total loss 6058.43701\n",
      "Trained batch 1289 batch loss 8827.62402 epoch total loss 6060.58545\n",
      "Trained batch 1290 batch loss 9147.38672 epoch total loss 6062.97852\n",
      "Trained batch 1291 batch loss 9012.82812 epoch total loss 6065.26318\n",
      "Trained batch 1292 batch loss 9157.06348 epoch total loss 6067.65625\n",
      "Trained batch 1293 batch loss 9155.68555 epoch total loss 6070.04443\n",
      "Trained batch 1294 batch loss 9131.92773 epoch total loss 6072.41064\n",
      "Trained batch 1295 batch loss 9153.20703 epoch total loss 6074.78955\n",
      "Trained batch 1296 batch loss 9152.53516 epoch total loss 6077.16455\n",
      "Trained batch 1297 batch loss 8931.07715 epoch total loss 6079.36475\n",
      "Trained batch 1298 batch loss 8900.0625 epoch total loss 6081.5376\n",
      "Trained batch 1299 batch loss 8930.63867 epoch total loss 6083.73096\n",
      "Trained batch 1300 batch loss 8894.26074 epoch total loss 6085.89307\n",
      "Trained batch 1301 batch loss 8741.05 epoch total loss 6087.93408\n",
      "Trained batch 1302 batch loss 9156.25293 epoch total loss 6090.29053\n",
      "Trained batch 1303 batch loss 9092.54785 epoch total loss 6092.59473\n",
      "Trained batch 1304 batch loss 9001.26855 epoch total loss 6094.82568\n",
      "Trained batch 1305 batch loss 9152.1416 epoch total loss 6097.16797\n",
      "Trained batch 1306 batch loss 8975.7666 epoch total loss 6099.37256\n",
      "Trained batch 1307 batch loss 9148.04395 epoch total loss 6101.70508\n",
      "Trained batch 1308 batch loss 8975.99121 epoch total loss 6103.90234\n",
      "Trained batch 1309 batch loss 9174.81348 epoch total loss 6106.24854\n",
      "Trained batch 1310 batch loss 9176.82227 epoch total loss 6108.59277\n",
      "Trained batch 1311 batch loss 9157.73145 epoch total loss 6110.91846\n",
      "Trained batch 1312 batch loss 9156.92871 epoch total loss 6113.24\n",
      "Trained batch 1313 batch loss 9189.08496 epoch total loss 6115.58252\n",
      "Trained batch 1314 batch loss 9183.74609 epoch total loss 6117.91748\n",
      "Trained batch 1315 batch loss 9184.45801 epoch total loss 6120.24951\n",
      "Trained batch 1316 batch loss 9167.73 epoch total loss 6122.56494\n",
      "Trained batch 1317 batch loss 9165.43555 epoch total loss 6124.87549\n",
      "Trained batch 1318 batch loss 9146.12695 epoch total loss 6127.16748\n",
      "Trained batch 1319 batch loss 9157.46582 epoch total loss 6129.46533\n",
      "Trained batch 1320 batch loss 9188.63281 epoch total loss 6131.78271\n",
      "Trained batch 1321 batch loss 9195.01465 epoch total loss 6134.10156\n",
      "Trained batch 1322 batch loss 9169.58496 epoch total loss 6136.39746\n",
      "Trained batch 1323 batch loss 8969.3252 epoch total loss 6138.53906\n",
      "Trained batch 1324 batch loss 9151.59668 epoch total loss 6140.81445\n",
      "Trained batch 1325 batch loss 9224.375 epoch total loss 6143.14209\n",
      "Trained batch 1326 batch loss 9175.8916 epoch total loss 6145.4292\n",
      "Trained batch 1327 batch loss 9216.51465 epoch total loss 6147.74316\n",
      "Trained batch 1328 batch loss 9220.99316 epoch total loss 6150.05762\n",
      "Trained batch 1329 batch loss 9237.40332 epoch total loss 6152.38086\n",
      "Trained batch 1330 batch loss 9230.99805 epoch total loss 6154.69531\n",
      "Trained batch 1331 batch loss 9224.24316 epoch total loss 6157.00146\n",
      "Trained batch 1332 batch loss 9239.72559 epoch total loss 6159.31592\n",
      "Trained batch 1333 batch loss 9239.58887 epoch total loss 6161.62646\n",
      "Trained batch 1334 batch loss 9236.54492 epoch total loss 6163.93164\n",
      "Trained batch 1335 batch loss 9074.56543 epoch total loss 6166.11182\n",
      "Trained batch 1336 batch loss 9233.38574 epoch total loss 6168.40771\n",
      "Trained batch 1337 batch loss 9229.04785 epoch total loss 6170.69678\n",
      "Trained batch 1338 batch loss 9261.91797 epoch total loss 6173.00732\n",
      "Trained batch 1339 batch loss 9198.1748 epoch total loss 6175.26611\n",
      "Trained batch 1340 batch loss 9222.23633 epoch total loss 6177.54\n",
      "Trained batch 1341 batch loss 9204.41797 epoch total loss 6179.79736\n",
      "Trained batch 1342 batch loss 9268.52148 epoch total loss 6182.09863\n",
      "Trained batch 1343 batch loss 8985.70117 epoch total loss 6184.18604\n",
      "Trained batch 1344 batch loss 8842.16113 epoch total loss 6186.16357\n",
      "Trained batch 1345 batch loss 9259.99805 epoch total loss 6188.44922\n",
      "Trained batch 1346 batch loss 9210.9082 epoch total loss 6190.69482\n",
      "Trained batch 1347 batch loss 9226.14746 epoch total loss 6192.94824\n",
      "Trained batch 1348 batch loss 9242.8252 epoch total loss 6195.21045\n",
      "Trained batch 1349 batch loss 9268.27246 epoch total loss 6197.48877\n",
      "Trained batch 1350 batch loss 9215.81445 epoch total loss 6199.72461\n",
      "Trained batch 1351 batch loss 9204.52734 epoch total loss 6201.94873\n",
      "Trained batch 1352 batch loss 9280.38672 epoch total loss 6204.22607\n",
      "Trained batch 1353 batch loss 9080.06934 epoch total loss 6206.35205\n",
      "Trained batch 1354 batch loss 9251.15723 epoch total loss 6208.60059\n",
      "Trained batch 1355 batch loss 9314.48242 epoch total loss 6210.89209\n",
      "Trained batch 1356 batch loss 9283.02734 epoch total loss 6213.15771\n",
      "Trained batch 1357 batch loss 9290.33105 epoch total loss 6215.42529\n",
      "Trained batch 1358 batch loss 9112.18555 epoch total loss 6217.55811\n",
      "Trained batch 1359 batch loss 9310.82 epoch total loss 6219.83447\n",
      "Trained batch 1360 batch loss 8942.60254 epoch total loss 6221.83691\n",
      "Trained batch 1361 batch loss 9216.93555 epoch total loss 6224.0376\n",
      "Trained batch 1362 batch loss 9204.14 epoch total loss 6226.22559\n",
      "Trained batch 1363 batch loss 9283.25391 epoch total loss 6228.46826\n",
      "Trained batch 1364 batch loss 9226.46094 epoch total loss 6230.66553\n",
      "Trained batch 1365 batch loss 9292.04102 epoch total loss 6232.9082\n",
      "Trained batch 1366 batch loss 9232.0918 epoch total loss 6235.104\n",
      "Trained batch 1367 batch loss 8666.30762 epoch total loss 6236.88232\n",
      "Trained batch 1368 batch loss 9096.2041 epoch total loss 6238.97217\n",
      "Trained batch 1369 batch loss 9131.54102 epoch total loss 6241.08545\n",
      "Trained batch 1370 batch loss 9101.54 epoch total loss 6243.17383\n",
      "Trained batch 1371 batch loss 9285.92 epoch total loss 6245.39307\n",
      "Trained batch 1372 batch loss 9275.06738 epoch total loss 6247.60107\n",
      "Trained batch 1373 batch loss 9099.06641 epoch total loss 6249.67822\n",
      "Trained batch 1374 batch loss 9054.17578 epoch total loss 6251.71924\n",
      "Trained batch 1375 batch loss 9237.80273 epoch total loss 6253.89111\n",
      "Trained batch 1376 batch loss 9219.13184 epoch total loss 6256.0459\n",
      "Trained batch 1377 batch loss 9216.53613 epoch total loss 6258.19629\n",
      "Trained batch 1378 batch loss 9311.92578 epoch total loss 6260.41211\n",
      "Trained batch 1379 batch loss 9319.34668 epoch total loss 6262.63037\n",
      "Trained batch 1380 batch loss 9238.39 epoch total loss 6264.78613\n",
      "Trained batch 1381 batch loss 9299.36523 epoch total loss 6266.9834\n",
      "Trained batch 1382 batch loss 9308.12598 epoch total loss 6269.18359\n",
      "Trained batch 1383 batch loss 9317.55566 epoch total loss 6271.38818\n",
      "Trained batch 1384 batch loss 9318.20801 epoch total loss 6273.58936\n",
      "Trained batch 1385 batch loss 9250.4834 epoch total loss 6275.73877\n",
      "Trained batch 1386 batch loss 9231.89258 epoch total loss 6277.87158\n",
      "Trained batch 1387 batch loss 9306.35449 epoch total loss 6280.05469\n",
      "Trained batch 1388 batch loss 9307.24707 epoch total loss 6282.23535\n",
      "Epoch 8 train loss 6282.2353515625\n",
      "Validated batch 1 batch loss 7.17659879e+25\n",
      "Validated batch 2 batch loss 7.24376e+25\n",
      "Validated batch 3 batch loss 7.26808081e+25\n",
      "Validated batch 4 batch loss 7.27044245e+25\n",
      "Validated batch 5 batch loss 7.34964539e+25\n",
      "Validated batch 6 batch loss 7.33337721e+25\n",
      "Validated batch 7 batch loss 7.24746841e+25\n",
      "Validated batch 8 batch loss 7.28669542e+25\n",
      "Validated batch 9 batch loss 7.45525669e+25\n",
      "Validated batch 10 batch loss 7.23568463e+25\n",
      "Validated batch 11 batch loss 7.24172963e+25\n",
      "Validated batch 12 batch loss 7.11214356e+25\n",
      "Validated batch 13 batch loss 6.97518294e+25\n",
      "Validated batch 14 batch loss 7.24772805e+25\n",
      "Validated batch 15 batch loss 7.47106601e+25\n",
      "Validated batch 16 batch loss 7.29542165e+25\n",
      "Validated batch 17 batch loss 7.29006748e+25\n",
      "Validated batch 18 batch loss 7.28346862e+25\n",
      "Validated batch 19 batch loss 7.33397949e+25\n",
      "Validated batch 20 batch loss 7.36989023e+25\n",
      "Validated batch 21 batch loss 7.2831707e+25\n",
      "Validated batch 22 batch loss 7.1976e+25\n",
      "Validated batch 23 batch loss 7.35221917e+25\n",
      "Validated batch 24 batch loss 7.1696107e+25\n",
      "Validated batch 25 batch loss 7.04807848e+25\n",
      "Validated batch 26 batch loss 7.24718895e+25\n",
      "Validated batch 27 batch loss 7.29468655e+25\n",
      "Validated batch 28 batch loss 7.34077389e+25\n",
      "Validated batch 29 batch loss 7.308214e+25\n",
      "Validated batch 30 batch loss 7.01117116e+25\n",
      "Validated batch 31 batch loss 7.25637727e+25\n",
      "Validated batch 32 batch loss 7.30117842e+25\n",
      "Validated batch 33 batch loss 7.40893738e+25\n",
      "Validated batch 34 batch loss 7.38241096e+25\n",
      "Validated batch 35 batch loss 7.35672848e+25\n",
      "Validated batch 36 batch loss 7.2717886e+25\n",
      "Validated batch 37 batch loss 7.37270105e+25\n",
      "Validated batch 38 batch loss 7.24338846e+25\n",
      "Validated batch 39 batch loss 7.29721237e+25\n",
      "Validated batch 40 batch loss 7.29463213e+25\n",
      "Validated batch 41 batch loss 7.0231062e+25\n",
      "Validated batch 42 batch loss 7.2378429e+25\n",
      "Validated batch 43 batch loss 7.22234579e+25\n",
      "Validated batch 44 batch loss 7.35755397e+25\n",
      "Validated batch 45 batch loss 7.24031e+25\n",
      "Validated batch 46 batch loss 7.3254036e+25\n",
      "Validated batch 47 batch loss 7.28751814e+25\n",
      "Validated batch 48 batch loss 7.43009349e+25\n",
      "Validated batch 49 batch loss 6.9282512e+25\n",
      "Validated batch 50 batch loss 6.93836647e+25\n",
      "Validated batch 51 batch loss 7.22902674e+25\n",
      "Validated batch 52 batch loss 7.21142163e+25\n",
      "Validated batch 53 batch loss 7.29064486e+25\n",
      "Validated batch 54 batch loss 7.39954752e+25\n",
      "Validated batch 55 batch loss 7.38519365e+25\n",
      "Validated batch 56 batch loss 7.17549568e+25\n",
      "Validated batch 57 batch loss 7.33511305e+25\n",
      "Validated batch 58 batch loss 7.10801841e+25\n",
      "Validated batch 59 batch loss 7.50880713e+25\n",
      "Validated batch 60 batch loss 7.24639574e+25\n",
      "Validated batch 61 batch loss 7.24469633e+25\n",
      "Validated batch 62 batch loss 7.46899675e+25\n",
      "Validated batch 63 batch loss 7.2428309e+25\n",
      "Validated batch 64 batch loss 7.19664579e+25\n",
      "Validated batch 65 batch loss 7.33516793e+25\n",
      "Validated batch 66 batch loss 7.2627188e+25\n",
      "Validated batch 67 batch loss 7.33311342e+25\n",
      "Validated batch 68 batch loss 7.4566425e+25\n",
      "Validated batch 69 batch loss 7.21739653e+25\n",
      "Validated batch 70 batch loss 7.2701616e+25\n",
      "Validated batch 71 batch loss 7.32568076e+25\n",
      "Validated batch 72 batch loss 7.29780036e+25\n",
      "Validated batch 73 batch loss 6.98204559e+25\n",
      "Validated batch 74 batch loss 7.17550859e+25\n",
      "Validated batch 75 batch loss 7.39181695e+25\n",
      "Validated batch 76 batch loss 7.20222409e+25\n",
      "Validated batch 77 batch loss 7.08803643e+25\n",
      "Validated batch 78 batch loss 7.03776813e+25\n",
      "Validated batch 79 batch loss 7.22846135e+25\n",
      "Validated batch 80 batch loss 7.26152437e+25\n",
      "Validated batch 81 batch loss 7.24036181e+25\n",
      "Validated batch 82 batch loss 7.202136e+25\n",
      "Validated batch 83 batch loss 7.26327866e+25\n",
      "Validated batch 84 batch loss 7.36359067e+25\n",
      "Validated batch 85 batch loss 7.28282529e+25\n",
      "Validated batch 86 batch loss 7.45252796e+25\n",
      "Validated batch 87 batch loss 7.33992442e+25\n",
      "Validated batch 88 batch loss 7.32970723e+25\n",
      "Validated batch 89 batch loss 7.28073435e+25\n",
      "Validated batch 90 batch loss 7.12762776e+25\n",
      "Validated batch 91 batch loss 7.07526898e+25\n",
      "Validated batch 92 batch loss 7.51928949e+25\n",
      "Validated batch 93 batch loss 7.23772484e+25\n",
      "Validated batch 94 batch loss 7.17083188e+25\n",
      "Validated batch 95 batch loss 7.20528348e+25\n",
      "Validated batch 96 batch loss 7.3318203e+25\n",
      "Validated batch 97 batch loss 7.31205093e+25\n",
      "Validated batch 98 batch loss 7.20563304e+25\n",
      "Validated batch 99 batch loss 7.21115508e+25\n",
      "Validated batch 100 batch loss 7.22593876e+25\n",
      "Validated batch 101 batch loss 7.21369704e+25\n",
      "Validated batch 102 batch loss 7.38717621e+25\n",
      "Validated batch 103 batch loss 7.22307859e+25\n",
      "Validated batch 104 batch loss 7.15093614e+25\n",
      "Validated batch 105 batch loss 7.30965239e+25\n",
      "Validated batch 106 batch loss 7.23323214e+25\n",
      "Validated batch 107 batch loss 7.18885204e+25\n",
      "Validated batch 108 batch loss 7.23928452e+25\n",
      "Validated batch 109 batch loss 7.29350964e+25\n",
      "Validated batch 110 batch loss 7.26807804e+25\n",
      "Validated batch 111 batch loss 7.54806457e+25\n",
      "Validated batch 112 batch loss 7.47851388e+25\n",
      "Validated batch 113 batch loss 7.38300863e+25\n",
      "Validated batch 114 batch loss 7.27843727e+25\n",
      "Validated batch 115 batch loss 7.14427e+25\n",
      "Validated batch 116 batch loss 7.16899781e+25\n",
      "Validated batch 117 batch loss 7.27922033e+25\n",
      "Validated batch 118 batch loss 7.1168544e+25\n",
      "Validated batch 119 batch loss 7.32691439e+25\n",
      "Validated batch 120 batch loss 7.39274114e+25\n",
      "Validated batch 121 batch loss 7.40192485e+25\n",
      "Validated batch 122 batch loss 7.39279463e+25\n",
      "Validated batch 123 batch loss 7.12282607e+25\n",
      "Validated batch 124 batch loss 7.36442815e+25\n",
      "Validated batch 125 batch loss 7.41995885e+25\n",
      "Validated batch 126 batch loss 7.26525661e+25\n",
      "Validated batch 127 batch loss 7.40446911e+25\n",
      "Validated batch 128 batch loss 7.03359133e+25\n",
      "Validated batch 129 batch loss 7.47958e+25\n",
      "Validated batch 130 batch loss 7.22317866e+25\n",
      "Validated batch 131 batch loss 7.62664447e+25\n",
      "Validated batch 132 batch loss 7.18478915e+25\n",
      "Validated batch 133 batch loss 7.25324086e+25\n",
      "Validated batch 134 batch loss 7.1597749e+25\n",
      "Validated batch 135 batch loss 7.3191128e+25\n",
      "Validated batch 136 batch loss 7.29673506e+25\n",
      "Validated batch 137 batch loss 7.35014484e+25\n",
      "Validated batch 138 batch loss 7.48923375e+25\n",
      "Validated batch 139 batch loss 7.48280091e+25\n",
      "Validated batch 140 batch loss 7.22786229e+25\n",
      "Validated batch 141 batch loss 7.24529539e+25\n",
      "Validated batch 142 batch loss 7.27798579e+25\n",
      "Validated batch 143 batch loss 7.2120027e+25\n",
      "Validated batch 144 batch loss 7.30286537e+25\n",
      "Validated batch 145 batch loss 7.15188108e+25\n",
      "Validated batch 146 batch loss 7.38379862e+25\n",
      "Validated batch 147 batch loss 7.29853407e+25\n",
      "Validated batch 148 batch loss 7.07132737e+25\n",
      "Validated batch 149 batch loss 7.26455609e+25\n",
      "Validated batch 150 batch loss 7.24234668e+25\n",
      "Validated batch 151 batch loss 7.11429722e+25\n",
      "Validated batch 152 batch loss 7.28821727e+25\n",
      "Validated batch 153 batch loss 7.34183135e+25\n",
      "Validated batch 154 batch loss 7.28714044e+25\n",
      "Validated batch 155 batch loss 7.10960483e+25\n",
      "Validated batch 156 batch loss 7.22077183e+25\n",
      "Validated batch 157 batch loss 7.09739124e+25\n",
      "Validated batch 158 batch loss 7.31262739e+25\n",
      "Validated batch 159 batch loss 7.19579632e+25\n",
      "Validated batch 160 batch loss 7.22781479e+25\n",
      "Validated batch 161 batch loss 7.28336024e+25\n",
      "Validated batch 162 batch loss 7.6242344e+25\n",
      "Validated batch 163 batch loss 7.25759429e+25\n",
      "Validated batch 164 batch loss 7.44053434e+25\n",
      "Validated batch 165 batch loss 7.37181515e+25\n",
      "Validated batch 166 batch loss 7.45872329e+25\n",
      "Validated batch 167 batch loss 7.18276784e+25\n",
      "Validated batch 168 batch loss 7.27949842e+25\n",
      "Validated batch 169 batch loss 7.30744524e+25\n",
      "Validated batch 170 batch loss 7.2003213e+25\n",
      "Validated batch 171 batch loss 7.22260497e+25\n",
      "Validated batch 172 batch loss 7.24238726e+25\n",
      "Validated batch 173 batch loss 7.31672625e+25\n",
      "Validated batch 174 batch loss 7.30701312e+25\n",
      "Validated batch 175 batch loss 7.20126301e+25\n",
      "Validated batch 176 batch loss 7.22434358e+25\n",
      "Validated batch 177 batch loss 7.27697859e+25\n",
      "Validated batch 178 batch loss 7.38800355e+25\n",
      "Validated batch 179 batch loss 7.28584687e+25\n",
      "Validated batch 180 batch loss 7.35308801e+25\n",
      "Validated batch 181 batch loss 7.15064e+25\n",
      "Validated batch 182 batch loss 7.1102643e+25\n",
      "Validated batch 183 batch loss 7.28919449e+25\n",
      "Validated batch 184 batch loss 7.0793328e+25\n",
      "Validated batch 185 batch loss 7.50078233e+25\n",
      "Epoch 8 val loss 7.272035788238021e+25\n",
      "Start epoch 9 with learning rate 0.5\n",
      "Start distributed traininng...\n",
      "Trained batch 1 batch loss 9302.72754 epoch total loss 9302.72754\n",
      "Trained batch 2 batch loss 9333.93457 epoch total loss 9318.33105\n",
      "Trained batch 3 batch loss 9324.24 epoch total loss 9320.30078\n",
      "Trained batch 4 batch loss 9152.17773 epoch total loss 9278.27\n",
      "Trained batch 5 batch loss 9324.4668 epoch total loss 9287.51\n",
      "Trained batch 6 batch loss 8969.54 epoch total loss 9234.51465\n",
      "Trained batch 7 batch loss 9343.68164 epoch total loss 9250.10938\n",
      "Trained batch 8 batch loss 9343.37793 epoch total loss 9261.76758\n",
      "Trained batch 9 batch loss 9332.05469 epoch total loss 9269.57715\n",
      "Trained batch 10 batch loss 9315.9375 epoch total loss 9274.21289\n",
      "Trained batch 11 batch loss 9355.83691 epoch total loss 9281.63379\n",
      "Trained batch 12 batch loss 9351.98242 epoch total loss 9287.49609\n",
      "Trained batch 13 batch loss 9356.6543 epoch total loss 9292.81641\n",
      "Trained batch 14 batch loss 9340.19629 epoch total loss 9296.2\n",
      "Trained batch 15 batch loss 9360.95605 epoch total loss 9300.51758\n",
      "Trained batch 16 batch loss 9337.48 epoch total loss 9302.82812\n",
      "Trained batch 17 batch loss 9328.54297 epoch total loss 9304.34082\n",
      "Trained batch 18 batch loss 9357.80371 epoch total loss 9307.31055\n",
      "Trained batch 19 batch loss 9148.73 epoch total loss 9298.96484\n",
      "Trained batch 20 batch loss 9373.13 epoch total loss 9302.67285\n",
      "Trained batch 21 batch loss 9355.32129 epoch total loss 9305.18\n",
      "Trained batch 22 batch loss 9376.16 epoch total loss 9308.40625\n",
      "Trained batch 23 batch loss 9379.11816 epoch total loss 9311.48145\n",
      "Trained batch 24 batch loss 9358.91113 epoch total loss 9313.45703\n",
      "Trained batch 25 batch loss 9229.53711 epoch total loss 9310.1\n",
      "Trained batch 26 batch loss 8938.2959 epoch total loss 9295.8\n",
      "Trained batch 27 batch loss 8937.19141 epoch total loss 9282.51758\n",
      "Trained batch 28 batch loss 9271.96484 epoch total loss 9282.1416\n",
      "Trained batch 29 batch loss 9292.8877 epoch total loss 9282.51172\n",
      "Trained batch 30 batch loss 9397.14355 epoch total loss 9286.33301\n",
      "Trained batch 31 batch loss 9327.8252 epoch total loss 9287.6709\n",
      "Trained batch 32 batch loss 9321.6377 epoch total loss 9288.73242\n",
      "Trained batch 33 batch loss 9425.49512 epoch total loss 9292.87695\n",
      "Trained batch 34 batch loss 9442.41309 epoch total loss 9297.27441\n",
      "Trained batch 35 batch loss 9432.86621 epoch total loss 9301.14941\n",
      "Trained batch 36 batch loss 9364.03809 epoch total loss 9302.89551\n",
      "Trained batch 37 batch loss 9414.80078 epoch total loss 9305.9209\n",
      "Trained batch 38 batch loss 8916.01855 epoch total loss 9295.66\n",
      "Trained batch 39 batch loss 9319.48242 epoch total loss 9296.27051\n",
      "Trained batch 40 batch loss 9427.90234 epoch total loss 9299.56152\n",
      "Trained batch 41 batch loss 9223.30273 epoch total loss 9297.70215\n",
      "Trained batch 42 batch loss 9432.91699 epoch total loss 9300.9209\n",
      "Trained batch 43 batch loss 9450.35 epoch total loss 9304.39648\n",
      "Trained batch 44 batch loss 9508.19336 epoch total loss 9309.02734\n",
      "Trained batch 45 batch loss 9109.95703 epoch total loss 9304.60449\n",
      "Trained batch 46 batch loss 9320.94629 epoch total loss 9304.95898\n",
      "Trained batch 47 batch loss 9488.02441 epoch total loss 9308.85449\n",
      "Trained batch 48 batch loss 9566.95312 epoch total loss 9314.23145\n",
      "Trained batch 49 batch loss 9568.66699 epoch total loss 9319.42383\n",
      "Trained batch 50 batch loss 9582.73242 epoch total loss 9324.69\n",
      "Trained batch 51 batch loss 9582.20508 epoch total loss 9329.73926\n",
      "Trained batch 52 batch loss 9363.96191 epoch total loss 9330.39746\n",
      "Trained batch 53 batch loss 9463.6543 epoch total loss 9332.91211\n",
      "Trained batch 54 batch loss 9362.61523 epoch total loss 9333.46191\n",
      "Trained batch 55 batch loss 9369.63477 epoch total loss 9334.12\n",
      "Trained batch 56 batch loss 9486.00391 epoch total loss 9336.83203\n",
      "Trained batch 57 batch loss 9515.60449 epoch total loss 9339.96777\n",
      "Trained batch 58 batch loss 9396.54199 epoch total loss 9340.94434\n",
      "Trained batch 59 batch loss 9333.48145 epoch total loss 9340.81738\n",
      "Trained batch 60 batch loss 9056.39844 epoch total loss 9336.07715\n",
      "Trained batch 61 batch loss 9629.68164 epoch total loss 9340.89062\n",
      "Trained batch 62 batch loss 9491.18457 epoch total loss 9343.31445\n",
      "Trained batch 63 batch loss 9644.91895 epoch total loss 9348.10254\n",
      "Trained batch 64 batch loss 9671.29785 epoch total loss 9353.15234\n",
      "Trained batch 65 batch loss 9497.44531 epoch total loss 9355.37207\n",
      "Trained batch 66 batch loss 9192.05859 epoch total loss 9352.89746\n",
      "Trained batch 67 batch loss 9447.52832 epoch total loss 9354.31\n",
      "Trained batch 68 batch loss 9673.29 epoch total loss 9359.00098\n",
      "Trained batch 69 batch loss 9659.95 epoch total loss 9363.3623\n",
      "Trained batch 70 batch loss 9622.67188 epoch total loss 9367.06738\n",
      "Trained batch 71 batch loss 9621.66211 epoch total loss 9370.65332\n",
      "Trained batch 72 batch loss 9643.88281 epoch total loss 9374.44824\n",
      "Trained batch 73 batch loss 9604.72559 epoch total loss 9377.60254\n",
      "Trained batch 74 batch loss 9676.24512 epoch total loss 9381.63867\n",
      "Trained batch 75 batch loss 9642.27441 epoch total loss 9385.11328\n",
      "Trained batch 76 batch loss 9605.34473 epoch total loss 9388.01172\n",
      "Trained batch 77 batch loss 9363.62207 epoch total loss 9387.69434\n",
      "Trained batch 78 batch loss 9461.61816 epoch total loss 9388.64258\n",
      "Trained batch 79 batch loss 9613.70898 epoch total loss 9391.49121\n",
      "Trained batch 80 batch loss 9596.26855 epoch total loss 9394.05078\n",
      "Trained batch 81 batch loss 9598.14258 epoch total loss 9396.57\n",
      "Trained batch 82 batch loss 9408.125 epoch total loss 9396.71094\n",
      "Trained batch 83 batch loss 9690.21387 epoch total loss 9400.24707\n",
      "Trained batch 84 batch loss 9710.82227 epoch total loss 9403.94434\n",
      "Trained batch 85 batch loss 9633.20117 epoch total loss 9406.6416\n",
      "Trained batch 86 batch loss 9722.59668 epoch total loss 9410.31543\n",
      "Trained batch 87 batch loss 9526.46484 epoch total loss 9411.65\n",
      "Trained batch 88 batch loss 9738.04395 epoch total loss 9415.35938\n",
      "Trained batch 89 batch loss 9687.36523 epoch total loss 9418.41602\n",
      "Trained batch 90 batch loss 9693.72754 epoch total loss 9421.47461\n",
      "Trained batch 91 batch loss 9734.03711 epoch total loss 9424.91\n",
      "Trained batch 92 batch loss 9745.53906 epoch total loss 9428.39551\n",
      "Trained batch 93 batch loss 9728.4209 epoch total loss 9431.62207\n",
      "Trained batch 94 batch loss 9646.44434 epoch total loss 9433.90723\n",
      "Trained batch 95 batch loss 9739.51074 epoch total loss 9437.12402\n",
      "Trained batch 96 batch loss 9557.45898 epoch total loss 9438.37695\n",
      "Trained batch 97 batch loss 9760.35547 epoch total loss 9441.69629\n",
      "Trained batch 98 batch loss 9645.58398 epoch total loss 9443.77637\n",
      "Trained batch 99 batch loss 9698.73828 epoch total loss 9446.35254\n",
      "Trained batch 100 batch loss 9778.89844 epoch total loss 9449.67773\n",
      "Trained batch 101 batch loss 9787.29297 epoch total loss 9453.02051\n",
      "Trained batch 102 batch loss 9766.92871 epoch total loss 9456.09766\n",
      "Trained batch 103 batch loss 9778.43945 epoch total loss 9459.22754\n",
      "Trained batch 104 batch loss 9776.25781 epoch total loss 9462.27539\n",
      "Trained batch 105 batch loss 9776.55762 epoch total loss 9465.27\n",
      "Trained batch 106 batch loss 9750.70898 epoch total loss 9467.96191\n",
      "Trained batch 107 batch loss 9795.50586 epoch total loss 9471.02246\n",
      "Trained batch 108 batch loss 9612.41895 epoch total loss 9472.33203\n",
      "Trained batch 109 batch loss 9507.27832 epoch total loss 9472.65234\n",
      "Trained batch 110 batch loss 9363.60449 epoch total loss 9471.66113\n",
      "Trained batch 111 batch loss 9591.84473 epoch total loss 9472.74414\n",
      "Trained batch 112 batch loss 9818.97168 epoch total loss 9475.83594\n",
      "Trained batch 113 batch loss 9768.54 epoch total loss 9478.42578\n",
      "Trained batch 114 batch loss 9798.45312 epoch total loss 9481.2334\n",
      "Trained batch 115 batch loss 9800.66113 epoch total loss 9484.01074\n",
      "Trained batch 116 batch loss 9819.09277 epoch total loss 9486.89941\n",
      "Trained batch 117 batch loss 9815.52734 epoch total loss 9489.70801\n",
      "Trained batch 118 batch loss 9830.16699 epoch total loss 9492.59277\n",
      "Trained batch 119 batch loss 9855.75195 epoch total loss 9495.64453\n",
      "Trained batch 120 batch loss 9837.65332 epoch total loss 9498.49512\n",
      "Trained batch 121 batch loss 9842.71 epoch total loss 9501.34\n",
      "Trained batch 122 batch loss 9390.09473 epoch total loss 9500.42871\n",
      "Trained batch 123 batch loss 9626.95215 epoch total loss 9501.45703\n",
      "Trained batch 124 batch loss 9628.7666 epoch total loss 9502.4834\n",
      "Trained batch 125 batch loss 9746.53223 epoch total loss 9504.43555\n",
      "Trained batch 126 batch loss 9845.48535 epoch total loss 9507.14258\n",
      "Trained batch 127 batch loss 9807.81 epoch total loss 9509.51\n",
      "Trained batch 128 batch loss 9875.01 epoch total loss 9512.36523\n",
      "Trained batch 129 batch loss 9743.04688 epoch total loss 9514.15332\n",
      "Trained batch 130 batch loss 9792.35547 epoch total loss 9516.29297\n",
      "Trained batch 131 batch loss 9655.8 epoch total loss 9517.35742\n",
      "Trained batch 132 batch loss 9301.90332 epoch total loss 9515.72559\n",
      "Trained batch 133 batch loss 9509.02246 epoch total loss 9515.6748\n",
      "Trained batch 134 batch loss 9890.30176 epoch total loss 9518.47\n",
      "Trained batch 135 batch loss 9661.77637 epoch total loss 9519.53125\n",
      "Trained batch 136 batch loss 9572.85059 epoch total loss 9519.92383\n",
      "Trained batch 137 batch loss 9310.09473 epoch total loss 9518.39258\n",
      "Trained batch 138 batch loss 9890.77734 epoch total loss 9521.09082\n",
      "Trained batch 139 batch loss 9909.95 epoch total loss 9523.88867\n",
      "Trained batch 140 batch loss 9920.32715 epoch total loss 9526.7207\n",
      "Trained batch 141 batch loss 9911.69922 epoch total loss 9529.45117\n",
      "Trained batch 142 batch loss 9920.69824 epoch total loss 9532.20703\n",
      "Trained batch 143 batch loss 9920.52344 epoch total loss 9534.92188\n",
      "Trained batch 144 batch loss 9920.38 epoch total loss 9537.59863\n",
      "Trained batch 145 batch loss 9839.63 epoch total loss 9539.68164\n",
      "Trained batch 146 batch loss 9905.95117 epoch total loss 9542.19141\n",
      "Trained batch 147 batch loss 9877.68457 epoch total loss 9544.47266\n",
      "Trained batch 148 batch loss 9882.44 epoch total loss 9546.75684\n",
      "Trained batch 149 batch loss 9887.80469 epoch total loss 9549.04492\n",
      "Trained batch 150 batch loss 9579.85547 epoch total loss 9549.25098\n",
      "Trained batch 151 batch loss 9425.4834 epoch total loss 9548.43164\n",
      "Trained batch 152 batch loss 9760.17285 epoch total loss 9549.82422\n",
      "Trained batch 153 batch loss 9879.64844 epoch total loss 9551.97949\n",
      "Trained batch 154 batch loss 9360.77832 epoch total loss 9550.73828\n",
      "Trained batch 155 batch loss 9660.12598 epoch total loss 9551.44336\n",
      "Trained batch 156 batch loss 9606.90137 epoch total loss 9551.79883\n",
      "Trained batch 157 batch loss 9592.18848 epoch total loss 9552.05664\n",
      "Trained batch 158 batch loss 9871.98535 epoch total loss 9554.08105\n",
      "Trained batch 159 batch loss 9722.25098 epoch total loss 9555.13867\n",
      "Trained batch 160 batch loss 9901.66 epoch total loss 9557.30469\n",
      "Trained batch 161 batch loss 9893.25 epoch total loss 9559.3916\n",
      "Trained batch 162 batch loss 9915.75488 epoch total loss 9561.59082\n",
      "Trained batch 163 batch loss 9913.99512 epoch total loss 9563.75293\n",
      "Trained batch 164 batch loss 9750.27832 epoch total loss 9564.89062\n",
      "Trained batch 165 batch loss 9893.55176 epoch total loss 9566.88184\n",
      "Trained batch 166 batch loss 9719.23926 epoch total loss 9567.8\n",
      "Trained batch 167 batch loss 9856.27344 epoch total loss 9569.52734\n",
      "Trained batch 168 batch loss 9910.24316 epoch total loss 9571.55469\n",
      "Trained batch 169 batch loss 9916.50586 epoch total loss 9573.5957\n",
      "Trained batch 170 batch loss 9924.18164 epoch total loss 9575.6582\n",
      "Trained batch 171 batch loss 9710.11133 epoch total loss 9576.44434\n",
      "Trained batch 172 batch loss 9921.20312 epoch total loss 9578.44922\n",
      "Trained batch 173 batch loss 9864.26562 epoch total loss 9580.10156\n",
      "Trained batch 174 batch loss 9913.91895 epoch total loss 9582.02\n",
      "Trained batch 175 batch loss 9929.34 epoch total loss 9584.00391\n",
      "Trained batch 176 batch loss 9913.07812 epoch total loss 9585.87402\n",
      "Trained batch 177 batch loss 9937.35156 epoch total loss 9587.86\n",
      "Trained batch 178 batch loss 9932.31934 epoch total loss 9589.7959\n",
      "Trained batch 179 batch loss 9942.53223 epoch total loss 9591.76562\n",
      "Trained batch 180 batch loss 9882.05469 epoch total loss 9593.37891\n",
      "Trained batch 181 batch loss 9938.99414 epoch total loss 9595.28809\n",
      "Trained batch 182 batch loss 9934.95703 epoch total loss 9597.1543\n",
      "Trained batch 183 batch loss 9933.34863 epoch total loss 9598.99219\n",
      "Trained batch 184 batch loss 9921.22656 epoch total loss 9600.74316\n",
      "Trained batch 185 batch loss 9796.21582 epoch total loss 9601.8\n",
      "Trained batch 186 batch loss 9832.59 epoch total loss 9603.04102\n",
      "Trained batch 187 batch loss 9740.89453 epoch total loss 9603.77832\n",
      "Trained batch 188 batch loss 9934.49414 epoch total loss 9605.53711\n",
      "Trained batch 189 batch loss 9919.81738 epoch total loss 9607.2\n",
      "Trained batch 190 batch loss 9527.36 epoch total loss 9606.78\n",
      "Trained batch 191 batch loss 9903.48926 epoch total loss 9608.33398\n",
      "Trained batch 192 batch loss 9945.45605 epoch total loss 9610.09\n",
      "Trained batch 193 batch loss 9910.86621 epoch total loss 9611.64844\n",
      "Trained batch 194 batch loss 9888.51465 epoch total loss 9613.0752\n",
      "Trained batch 195 batch loss 9719.50195 epoch total loss 9613.62109\n",
      "Trained batch 196 batch loss 9907.25 epoch total loss 9615.11914\n",
      "Trained batch 197 batch loss 9856.98926 epoch total loss 9616.34668\n",
      "Trained batch 198 batch loss 9928.85 epoch total loss 9617.92578\n",
      "Trained batch 199 batch loss 9686.76465 epoch total loss 9618.27148\n",
      "Trained batch 200 batch loss 9550.35742 epoch total loss 9617.93164\n",
      "Trained batch 201 batch loss 9495.32324 epoch total loss 9617.32227\n",
      "Trained batch 202 batch loss 9868.80371 epoch total loss 9618.56641\n",
      "Trained batch 203 batch loss 9936.67578 epoch total loss 9620.13379\n",
      "Trained batch 204 batch loss 9946.05566 epoch total loss 9621.73145\n",
      "Trained batch 205 batch loss 9942.58789 epoch total loss 9623.2959\n",
      "Trained batch 206 batch loss 9922.14746 epoch total loss 9624.74707\n",
      "Trained batch 207 batch loss 9906.25195 epoch total loss 9626.10645\n",
      "Trained batch 208 batch loss 9774.69629 epoch total loss 9626.82129\n",
      "Trained batch 209 batch loss 9842.23535 epoch total loss 9627.85254\n",
      "Trained batch 210 batch loss 9900.80859 epoch total loss 9629.15137\n",
      "Trained batch 211 batch loss 9935.91113 epoch total loss 9630.60547\n",
      "Trained batch 212 batch loss 9932.65625 epoch total loss 9632.03\n",
      "Trained batch 213 batch loss 9936.66406 epoch total loss 9633.46\n",
      "Trained batch 214 batch loss 9956.7373 epoch total loss 9634.9707\n",
      "Trained batch 215 batch loss 9953.80762 epoch total loss 9636.45312\n",
      "Trained batch 216 batch loss 9927.93555 epoch total loss 9637.80273\n",
      "Trained batch 217 batch loss 9665.08398 epoch total loss 9637.92871\n",
      "Trained batch 218 batch loss 9481.22363 epoch total loss 9637.21\n",
      "Trained batch 219 batch loss 9476.23 epoch total loss 9636.47461\n",
      "Trained batch 220 batch loss 9783.81 epoch total loss 9637.14453\n",
      "Trained batch 221 batch loss 9926.86328 epoch total loss 9638.45508\n",
      "Trained batch 222 batch loss 9667.12793 epoch total loss 9638.58398\n",
      "Trained batch 223 batch loss 9944.38086 epoch total loss 9639.95605\n",
      "Trained batch 224 batch loss 9720.22168 epoch total loss 9640.31445\n",
      "Trained batch 225 batch loss 9771.18164 epoch total loss 9640.89648\n",
      "Trained batch 226 batch loss 9713.85059 epoch total loss 9641.21875\n",
      "Trained batch 227 batch loss 9471.76367 epoch total loss 9640.47266\n",
      "Trained batch 228 batch loss 9638.45801 epoch total loss 9640.46387\n",
      "Trained batch 229 batch loss 9891.44629 epoch total loss 9641.56\n",
      "Trained batch 230 batch loss 9699.03418 epoch total loss 9641.81\n",
      "Trained batch 231 batch loss 9807.88184 epoch total loss 9642.5293\n",
      "Trained batch 232 batch loss 9900.40137 epoch total loss 9643.6416\n",
      "Trained batch 233 batch loss 9740.8877 epoch total loss 9644.05859\n",
      "Trained batch 234 batch loss 9944.24707 epoch total loss 9645.3418\n",
      "Trained batch 235 batch loss 9509.43945 epoch total loss 9644.76367\n",
      "Trained batch 236 batch loss 9945.33 epoch total loss 9646.03711\n",
      "Trained batch 237 batch loss 9943.50293 epoch total loss 9647.29199\n",
      "Trained batch 238 batch loss 9942.60156 epoch total loss 9648.53223\n",
      "Trained batch 239 batch loss 9973.60254 epoch total loss 9649.89258\n",
      "Trained batch 240 batch loss 9988.49512 epoch total loss 9651.30273\n",
      "Trained batch 241 batch loss 9990.03125 epoch total loss 9652.70898\n",
      "Trained batch 242 batch loss 9989.83203 epoch total loss 9654.10156\n",
      "Trained batch 243 batch loss 9978.50195 epoch total loss 9655.43652\n",
      "Trained batch 244 batch loss 9984.0752 epoch total loss 9656.7832\n",
      "Trained batch 245 batch loss 9969.7373 epoch total loss 9658.06055\n",
      "Trained batch 246 batch loss 9850.35352 epoch total loss 9658.8418\n",
      "Trained batch 247 batch loss 9935.06738 epoch total loss 9659.96\n",
      "Trained batch 248 batch loss 9906.61 epoch total loss 9660.9541\n",
      "Trained batch 249 batch loss 9940 epoch total loss 9662.07422\n",
      "Trained batch 250 batch loss 9833.69727 epoch total loss 9662.76074\n",
      "Trained batch 251 batch loss 9581.77246 epoch total loss 9662.43848\n",
      "Trained batch 252 batch loss 9945.41504 epoch total loss 9663.56152\n",
      "Trained batch 253 batch loss 9946.92383 epoch total loss 9664.68164\n",
      "Trained batch 254 batch loss 9929.69 epoch total loss 9665.72559\n",
      "Trained batch 255 batch loss 9779.41309 epoch total loss 9666.17188\n",
      "Trained batch 256 batch loss 9738.74707 epoch total loss 9666.45508\n",
      "Trained batch 257 batch loss 9979.87793 epoch total loss 9667.6748\n",
      "Trained batch 258 batch loss 9742.31348 epoch total loss 9667.96387\n",
      "Trained batch 259 batch loss 9959.39941 epoch total loss 9669.09\n",
      "Trained batch 260 batch loss 9508.30176 epoch total loss 9668.4707\n",
      "Trained batch 261 batch loss 9924.49609 epoch total loss 9669.45215\n",
      "Trained batch 262 batch loss 9685.43 epoch total loss 9669.51367\n",
      "Trained batch 263 batch loss 9958.64941 epoch total loss 9670.61328\n",
      "Trained batch 264 batch loss 9968.83398 epoch total loss 9671.74219\n",
      "Trained batch 265 batch loss 9822.35059 epoch total loss 9672.31055\n",
      "Trained batch 266 batch loss 9983.60938 epoch total loss 9673.48\n",
      "Trained batch 267 batch loss 9949.69434 epoch total loss 9674.51465\n",
      "Trained batch 268 batch loss 9911.01758 epoch total loss 9675.39746\n",
      "Trained batch 269 batch loss 9912.7168 epoch total loss 9676.2793\n",
      "Trained batch 270 batch loss 9924.84668 epoch total loss 9677.2\n",
      "Trained batch 271 batch loss 9979.03 epoch total loss 9678.31348\n",
      "Trained batch 272 batch loss 9929.07422 epoch total loss 9679.23535\n",
      "Trained batch 273 batch loss 9986.75684 epoch total loss 9680.36133\n",
      "Trained batch 274 batch loss 9972.5293 epoch total loss 9681.42773\n",
      "Trained batch 275 batch loss 9986.77148 epoch total loss 9682.53809\n",
      "Trained batch 276 batch loss 9990.36426 epoch total loss 9683.65332\n",
      "Trained batch 277 batch loss 9992.4248 epoch total loss 9684.76758\n",
      "Trained batch 278 batch loss 9994.16211 epoch total loss 9685.88086\n",
      "Trained batch 279 batch loss 9987.70801 epoch total loss 9686.96289\n",
      "Trained batch 280 batch loss 9956.84277 epoch total loss 9687.92676\n",
      "Trained batch 281 batch loss 10000.9 epoch total loss 9689.04102\n",
      "Trained batch 282 batch loss 9967.12109 epoch total loss 9690.02637\n",
      "Trained batch 283 batch loss 9894.65 epoch total loss 9690.75\n",
      "Trained batch 284 batch loss 9762.56055 epoch total loss 9691.00293\n",
      "Trained batch 285 batch loss 9405.54492 epoch total loss 9690.00098\n",
      "Trained batch 286 batch loss 9524.73535 epoch total loss 9689.42285\n",
      "Trained batch 287 batch loss 9725.41504 epoch total loss 9689.54883\n",
      "Trained batch 288 batch loss 9436.91113 epoch total loss 9688.67188\n",
      "Trained batch 289 batch loss 9872.3877 epoch total loss 9689.30762\n",
      "Trained batch 290 batch loss 9874.35 epoch total loss 9689.94531\n",
      "Trained batch 291 batch loss 9978.37891 epoch total loss 9690.9375\n",
      "Trained batch 292 batch loss 9990.93945 epoch total loss 9691.96484\n",
      "Trained batch 293 batch loss 9980.77344 epoch total loss 9692.95\n",
      "Trained batch 294 batch loss 9989.83301 epoch total loss 9693.96\n",
      "Trained batch 295 batch loss 9982.4707 epoch total loss 9694.93848\n",
      "Trained batch 296 batch loss 9893.32324 epoch total loss 9695.6084\n",
      "Trained batch 297 batch loss 9897.72949 epoch total loss 9696.28906\n",
      "Trained batch 298 batch loss 9924.7334 epoch total loss 9697.05566\n",
      "Trained batch 299 batch loss 9973.4 epoch total loss 9697.97949\n",
      "Trained batch 300 batch loss 9969.27637 epoch total loss 9698.88379\n",
      "Trained batch 301 batch loss 9983.33691 epoch total loss 9699.8291\n",
      "Trained batch 302 batch loss 9973.99805 epoch total loss 9700.73633\n",
      "Trained batch 303 batch loss 9978.6543 epoch total loss 9701.6543\n",
      "Trained batch 304 batch loss 9985.47754 epoch total loss 9702.58789\n",
      "Trained batch 305 batch loss 9983.53223 epoch total loss 9703.50879\n",
      "Trained batch 306 batch loss 9984.00488 epoch total loss 9704.42578\n",
      "Trained batch 307 batch loss 9983.4082 epoch total loss 9705.33496\n",
      "Trained batch 308 batch loss 9964.85547 epoch total loss 9706.17676\n",
      "Trained batch 309 batch loss 9983.77734 epoch total loss 9707.0752\n",
      "Trained batch 310 batch loss 9798.64062 epoch total loss 9707.37109\n",
      "Trained batch 311 batch loss 9797.53 epoch total loss 9707.66113\n",
      "Trained batch 312 batch loss 9993.25098 epoch total loss 9708.57617\n",
      "Trained batch 313 batch loss 9989.93652 epoch total loss 9709.47559\n",
      "Trained batch 314 batch loss 9784.49316 epoch total loss 9709.71387\n",
      "Trained batch 315 batch loss 9994.14062 epoch total loss 9710.61719\n",
      "Trained batch 316 batch loss 10001.4551 epoch total loss 9711.53809\n",
      "Trained batch 317 batch loss 9993.79297 epoch total loss 9712.42871\n",
      "Trained batch 318 batch loss 9785.00586 epoch total loss 9712.65625\n",
      "Trained batch 319 batch loss 9791.53613 epoch total loss 9712.90332\n",
      "Trained batch 320 batch loss 9998.40625 epoch total loss 9713.7959\n",
      "Trained batch 321 batch loss 10006.7236 epoch total loss 9714.70898\n",
      "Trained batch 322 batch loss 9972.74121 epoch total loss 9715.51\n",
      "Trained batch 323 batch loss 10008.6279 epoch total loss 9716.41797\n",
      "Trained batch 324 batch loss 9807.43066 epoch total loss 9716.69922\n",
      "Trained batch 325 batch loss 10011.1982 epoch total loss 9717.60547\n",
      "Trained batch 326 batch loss 9807.42676 epoch total loss 9717.88086\n",
      "Trained batch 327 batch loss 9934.78 epoch total loss 9718.54395\n",
      "Trained batch 328 batch loss 9891.52 epoch total loss 9719.07129\n",
      "Trained batch 329 batch loss 9989.68652 epoch total loss 9719.89453\n",
      "Trained batch 330 batch loss 10004.668 epoch total loss 9720.75781\n",
      "Trained batch 331 batch loss 9976.45312 epoch total loss 9721.53\n",
      "Trained batch 332 batch loss 10013.2432 epoch total loss 9722.40918\n",
      "Trained batch 333 batch loss 9756.71777 epoch total loss 9722.51172\n",
      "Trained batch 334 batch loss 9310.0293 epoch total loss 9721.27734\n",
      "Trained batch 335 batch loss 9417.24316 epoch total loss 9720.36914\n",
      "Trained batch 336 batch loss 9985.27148 epoch total loss 9721.1582\n",
      "Trained batch 337 batch loss 9627.42188 epoch total loss 9720.88\n",
      "Trained batch 338 batch loss 10001.5615 epoch total loss 9721.71\n",
      "Trained batch 339 batch loss 10021.7305 epoch total loss 9722.59473\n",
      "Trained batch 340 batch loss 10023.0068 epoch total loss 9723.47852\n",
      "Trained batch 341 batch loss 10023.751 epoch total loss 9724.35938\n",
      "Trained batch 342 batch loss 10024.3496 epoch total loss 9725.23633\n",
      "Trained batch 343 batch loss 10022.3887 epoch total loss 9726.10254\n",
      "Trained batch 344 batch loss 10023.4961 epoch total loss 9726.96777\n",
      "Trained batch 345 batch loss 10027.5957 epoch total loss 9727.83887\n",
      "Trained batch 346 batch loss 10021.9844 epoch total loss 9728.68848\n",
      "Trained batch 347 batch loss 10022.5898 epoch total loss 9729.53516\n",
      "Trained batch 348 batch loss 9815.1123 epoch total loss 9729.78125\n",
      "Trained batch 349 batch loss 9793.26 epoch total loss 9729.96289\n",
      "Trained batch 350 batch loss 9614.28418 epoch total loss 9729.63184\n",
      "Trained batch 351 batch loss 9640.06 epoch total loss 9729.37695\n",
      "Trained batch 352 batch loss 10041.4434 epoch total loss 9730.26367\n",
      "Trained batch 353 batch loss 10044.9268 epoch total loss 9731.15527\n",
      "Trained batch 354 batch loss 9828.83496 epoch total loss 9731.43066\n",
      "Trained batch 355 batch loss 10033.418 epoch total loss 9732.28125\n",
      "Trained batch 356 batch loss 9772.55664 epoch total loss 9732.39453\n",
      "Trained batch 357 batch loss 10019.2412 epoch total loss 9733.19824\n",
      "Trained batch 358 batch loss 10037.6748 epoch total loss 9734.04883\n",
      "Trained batch 359 batch loss 10054.3447 epoch total loss 9734.94\n",
      "Trained batch 360 batch loss 10043.0059 epoch total loss 9735.79688\n",
      "Trained batch 361 batch loss 10081.5225 epoch total loss 9736.75391\n",
      "Trained batch 362 batch loss 10081.1338 epoch total loss 9737.70605\n",
      "Trained batch 363 batch loss 10047.502 epoch total loss 9738.56\n",
      "Trained batch 364 batch loss 10086.9561 epoch total loss 9739.5166\n",
      "Trained batch 365 batch loss 10074.498 epoch total loss 9740.43457\n",
      "Trained batch 366 batch loss 9980.52344 epoch total loss 9741.09\n",
      "Trained batch 367 batch loss 10096.5664 epoch total loss 9742.05859\n",
      "Trained batch 368 batch loss 10067.5566 epoch total loss 9742.94336\n",
      "Trained batch 369 batch loss 9943.86523 epoch total loss 9743.4873\n",
      "Trained batch 370 batch loss 10100.0703 epoch total loss 9744.45\n",
      "Trained batch 371 batch loss 10057.9492 epoch total loss 9745.2959\n",
      "Trained batch 372 batch loss 10126.0215 epoch total loss 9746.31934\n",
      "Trained batch 373 batch loss 9921.14941 epoch total loss 9746.78809\n",
      "Trained batch 374 batch loss 10080.3574 epoch total loss 9747.68\n",
      "Trained batch 375 batch loss 10135.8398 epoch total loss 9748.71484\n",
      "Trained batch 376 batch loss 10130.6475 epoch total loss 9749.73\n",
      "Trained batch 377 batch loss 9939.8916 epoch total loss 9750.23535\n",
      "Trained batch 378 batch loss 10145.5703 epoch total loss 9751.28125\n",
      "Trained batch 379 batch loss 10150.4434 epoch total loss 9752.33398\n",
      "Trained batch 380 batch loss 10137.1133 epoch total loss 9753.34668\n",
      "Trained batch 381 batch loss 10146.1006 epoch total loss 9754.37695\n",
      "Trained batch 382 batch loss 10156.5078 epoch total loss 9755.43\n",
      "Trained batch 383 batch loss 9967.875 epoch total loss 9755.98438\n",
      "Trained batch 384 batch loss 10142.751 epoch total loss 9756.99121\n",
      "Trained batch 385 batch loss 9973.86133 epoch total loss 9757.55469\n",
      "Trained batch 386 batch loss 10165.6689 epoch total loss 9758.6123\n",
      "Trained batch 387 batch loss 10172.8877 epoch total loss 9759.68262\n",
      "Trained batch 388 batch loss 10195.915 epoch total loss 9760.80762\n",
      "Trained batch 389 batch loss 10008.334 epoch total loss 9761.44336\n",
      "Trained batch 390 batch loss 10183.0146 epoch total loss 9762.52441\n",
      "Trained batch 391 batch loss 10185.3613 epoch total loss 9763.60547\n",
      "Trained batch 392 batch loss 10138.916 epoch total loss 9764.56348\n",
      "Trained batch 393 batch loss 9981.46777 epoch total loss 9765.11523\n",
      "Trained batch 394 batch loss 9981.71582 epoch total loss 9765.66504\n",
      "Trained batch 395 batch loss 10194.1348 epoch total loss 9766.75\n",
      "Trained batch 396 batch loss 10200.6885 epoch total loss 9767.8457\n",
      "Trained batch 397 batch loss 10129.374 epoch total loss 9768.75586\n",
      "Trained batch 398 batch loss 10157.2236 epoch total loss 9769.73242\n",
      "Trained batch 399 batch loss 10217.1016 epoch total loss 9770.85352\n",
      "Trained batch 400 batch loss 10149.7031 epoch total loss 9771.80078\n",
      "Trained batch 401 batch loss 10185.1699 epoch total loss 9772.83203\n",
      "Trained batch 402 batch loss 9761.3877 epoch total loss 9772.80371\n",
      "Trained batch 403 batch loss 10018.416 epoch total loss 9773.41309\n",
      "Trained batch 404 batch loss 10203.3799 epoch total loss 9774.47754\n",
      "Trained batch 405 batch loss 9892.91211 epoch total loss 9774.77051\n",
      "Trained batch 406 batch loss 10165.0303 epoch total loss 9775.73145\n",
      "Trained batch 407 batch loss 10013.6523 epoch total loss 9776.31641\n",
      "Trained batch 408 batch loss 10195.7627 epoch total loss 9777.34473\n",
      "Trained batch 409 batch loss 9985.96289 epoch total loss 9777.85449\n",
      "Trained batch 410 batch loss 10016.0439 epoch total loss 9778.43555\n",
      "Trained batch 411 batch loss 10206.6045 epoch total loss 9779.47656\n",
      "Trained batch 412 batch loss 10200.9512 epoch total loss 9780.5\n",
      "Trained batch 413 batch loss 10162.6982 epoch total loss 9781.42578\n",
      "Trained batch 414 batch loss 10167.7012 epoch total loss 9782.3584\n",
      "Trained batch 415 batch loss 10171.0029 epoch total loss 9783.29492\n",
      "Trained batch 416 batch loss 10193.2031 epoch total loss 9784.28\n",
      "Trained batch 417 batch loss 10182.5205 epoch total loss 9785.23535\n",
      "Trained batch 418 batch loss 10163.5059 epoch total loss 9786.14062\n",
      "Trained batch 419 batch loss 10169.5244 epoch total loss 9787.05566\n",
      "Trained batch 420 batch loss 9990.49121 epoch total loss 9787.54\n",
      "Trained batch 421 batch loss 10186.416 epoch total loss 9788.4873\n",
      "Trained batch 422 batch loss 10168.5273 epoch total loss 9789.3877\n",
      "Trained batch 423 batch loss 10169.9434 epoch total loss 9790.28809\n",
      "Trained batch 424 batch loss 10174.2422 epoch total loss 9791.19336\n",
      "Trained batch 425 batch loss 9976.09766 epoch total loss 9791.62793\n",
      "Trained batch 426 batch loss 10166.4131 epoch total loss 9792.50781\n",
      "Trained batch 427 batch loss 9973.2959 epoch total loss 9792.93164\n",
      "Trained batch 428 batch loss 9935.46094 epoch total loss 9793.26465\n",
      "Trained batch 429 batch loss 10150.8662 epoch total loss 9794.09766\n",
      "Trained batch 430 batch loss 10124.6777 epoch total loss 9794.86621\n",
      "Trained batch 431 batch loss 10143.8613 epoch total loss 9795.67676\n",
      "Trained batch 432 batch loss 10082.3184 epoch total loss 9796.34\n",
      "Trained batch 433 batch loss 10087.9746 epoch total loss 9797.01367\n",
      "Trained batch 434 batch loss 10163.5254 epoch total loss 9797.8584\n",
      "Trained batch 435 batch loss 10110.3057 epoch total loss 9798.57715\n",
      "Trained batch 436 batch loss 10171.2539 epoch total loss 9799.43262\n",
      "Trained batch 437 batch loss 9975.71191 epoch total loss 9799.83496\n",
      "Trained batch 438 batch loss 10143.3945 epoch total loss 9800.62\n",
      "Trained batch 439 batch loss 9936.04883 epoch total loss 9800.92871\n",
      "Trained batch 440 batch loss 10174.7754 epoch total loss 9801.77832\n",
      "Trained batch 441 batch loss 10172.9639 epoch total loss 9802.62\n",
      "Trained batch 442 batch loss 10179.6992 epoch total loss 9803.47266\n",
      "Trained batch 443 batch loss 10181.2627 epoch total loss 9804.32617\n",
      "Trained batch 444 batch loss 10003.0107 epoch total loss 9804.77344\n",
      "Trained batch 445 batch loss 10186.7393 epoch total loss 9805.63184\n",
      "Trained batch 446 batch loss 10054.3936 epoch total loss 9806.18945\n",
      "Trained batch 447 batch loss 10183.6299 epoch total loss 9807.0332\n",
      "Trained batch 448 batch loss 10187.4717 epoch total loss 9807.88281\n",
      "Trained batch 449 batch loss 10178.0693 epoch total loss 9808.70703\n",
      "Trained batch 450 batch loss 9705.58 epoch total loss 9808.47754\n",
      "Trained batch 451 batch loss 9941.9707 epoch total loss 9808.77344\n",
      "Trained batch 452 batch loss 9969.05078 epoch total loss 9809.12793\n",
      "Trained batch 453 batch loss 10133.1396 epoch total loss 9809.84375\n",
      "Trained batch 454 batch loss 10017.2842 epoch total loss 9810.30078\n",
      "Trained batch 455 batch loss 10147.8955 epoch total loss 9811.04297\n",
      "Trained batch 456 batch loss 10114.29 epoch total loss 9811.70801\n",
      "Trained batch 457 batch loss 9984.66211 epoch total loss 9812.08691\n",
      "Trained batch 458 batch loss 10187.0303 epoch total loss 9812.90527\n",
      "Trained batch 459 batch loss 10204.3359 epoch total loss 9813.75781\n",
      "Trained batch 460 batch loss 10117.3145 epoch total loss 9814.41895\n",
      "Trained batch 461 batch loss 10018.5557 epoch total loss 9814.86133\n",
      "Trained batch 462 batch loss 10038.8594 epoch total loss 9815.34668\n",
      "Trained batch 463 batch loss 10206.7949 epoch total loss 9816.19238\n",
      "Trained batch 464 batch loss 10218.501 epoch total loss 9817.06\n",
      "Trained batch 465 batch loss 9900.97559 epoch total loss 9817.24\n",
      "Trained batch 466 batch loss 10212.6855 epoch total loss 9818.08789\n",
      "Trained batch 467 batch loss 10213.457 epoch total loss 9818.93457\n",
      "Trained batch 468 batch loss 9922.52246 epoch total loss 9819.15625\n",
      "Trained batch 469 batch loss 10098.0127 epoch total loss 9819.75098\n",
      "Trained batch 470 batch loss 10228.4189 epoch total loss 9820.62\n",
      "Trained batch 471 batch loss 10188.6592 epoch total loss 9821.40137\n",
      "Trained batch 472 batch loss 10235.1055 epoch total loss 9822.27734\n",
      "Trained batch 473 batch loss 10140.9092 epoch total loss 9822.95117\n",
      "Trained batch 474 batch loss 10197.8691 epoch total loss 9823.74219\n",
      "Trained batch 475 batch loss 10237.4619 epoch total loss 9824.61328\n",
      "Trained batch 476 batch loss 10245.4033 epoch total loss 9825.49805\n",
      "Trained batch 477 batch loss 10227.3047 epoch total loss 9826.34082\n",
      "Trained batch 478 batch loss 10085.7676 epoch total loss 9826.88379\n",
      "Trained batch 479 batch loss 10197.4775 epoch total loss 9827.65723\n",
      "Trained batch 480 batch loss 10054.9814 epoch total loss 9828.13086\n",
      "Trained batch 481 batch loss 10186.5273 epoch total loss 9828.87598\n",
      "Trained batch 482 batch loss 10246.5254 epoch total loss 9829.74316\n",
      "Trained batch 483 batch loss 10264.4355 epoch total loss 9830.64258\n",
      "Trained batch 484 batch loss 10267.0508 epoch total loss 9831.54395\n",
      "Trained batch 485 batch loss 10271.4316 epoch total loss 9832.45117\n",
      "Trained batch 486 batch loss 10274.46 epoch total loss 9833.36133\n",
      "Trained batch 487 batch loss 10274.8643 epoch total loss 9834.26758\n",
      "Trained batch 488 batch loss 10274.7969 epoch total loss 9835.1709\n",
      "Trained batch 489 batch loss 10266.5625 epoch total loss 9836.05273\n",
      "Trained batch 490 batch loss 10289.0732 epoch total loss 9836.97754\n",
      "Trained batch 491 batch loss 10275.3789 epoch total loss 9837.87109\n",
      "Trained batch 492 batch loss 10270.3857 epoch total loss 9838.75\n",
      "Trained batch 493 batch loss 9688.67871 epoch total loss 9838.44531\n",
      "Trained batch 494 batch loss 10094.2314 epoch total loss 9838.96289\n",
      "Trained batch 495 batch loss 10051.1865 epoch total loss 9839.39062\n",
      "Trained batch 496 batch loss 10299.8867 epoch total loss 9840.31934\n",
      "Trained batch 497 batch loss 10244.9443 epoch total loss 9841.13379\n",
      "Trained batch 498 batch loss 10268.5391 epoch total loss 9841.99219\n",
      "Trained batch 499 batch loss 10219.5459 epoch total loss 9842.74805\n",
      "Trained batch 500 batch loss 10314.9775 epoch total loss 9843.69336\n",
      "Trained batch 501 batch loss 10321.4443 epoch total loss 9844.64648\n",
      "Trained batch 502 batch loss 10185.5811 epoch total loss 9845.32617\n",
      "Trained batch 503 batch loss 10320.2725 epoch total loss 9846.27051\n",
      "Trained batch 504 batch loss 10290.8125 epoch total loss 9847.15234\n",
      "Trained batch 505 batch loss 10116.2129 epoch total loss 9847.68555\n",
      "Trained batch 506 batch loss 10335.2432 epoch total loss 9848.64844\n",
      "Trained batch 507 batch loss 10109.1699 epoch total loss 9849.16211\n",
      "Trained batch 508 batch loss 10117.3096 epoch total loss 9849.69\n",
      "Trained batch 509 batch loss 10268.1494 epoch total loss 9850.51172\n",
      "Trained batch 510 batch loss 10199.8906 epoch total loss 9851.19727\n",
      "Trained batch 511 batch loss 10321.0781 epoch total loss 9852.11621\n",
      "Trained batch 512 batch loss 10345.1699 epoch total loss 9853.0791\n",
      "Trained batch 513 batch loss 10343.4658 epoch total loss 9854.03516\n",
      "Trained batch 514 batch loss 10202.9453 epoch total loss 9854.71387\n",
      "Trained batch 515 batch loss 10350.7363 epoch total loss 9855.67676\n",
      "Trained batch 516 batch loss 10346.7578 epoch total loss 9856.62891\n",
      "Trained batch 517 batch loss 10275.6758 epoch total loss 9857.43945\n",
      "Trained batch 518 batch loss 9958.33691 epoch total loss 9857.63379\n",
      "Trained batch 519 batch loss 9931.28125 epoch total loss 9857.77637\n",
      "Trained batch 520 batch loss 10074.5361 epoch total loss 9858.19336\n",
      "Trained batch 521 batch loss 10354.3994 epoch total loss 9859.14551\n",
      "Trained batch 522 batch loss 10064.0684 epoch total loss 9859.53809\n",
      "Trained batch 523 batch loss 10355.6523 epoch total loss 9860.48633\n",
      "Trained batch 524 batch loss 10334.7842 epoch total loss 9861.39258\n",
      "Trained batch 525 batch loss 10305.127 epoch total loss 9862.2373\n",
      "Trained batch 526 batch loss 9912.35059 epoch total loss 9862.33301\n",
      "Trained batch 527 batch loss 10171.5176 epoch total loss 9862.91895\n",
      "Trained batch 528 batch loss 10156.3154 epoch total loss 9863.47559\n",
      "Trained batch 529 batch loss 10157.3877 epoch total loss 9864.03125\n",
      "Trained batch 530 batch loss 10311.8018 epoch total loss 9864.87598\n",
      "Trained batch 531 batch loss 10366.1182 epoch total loss 9865.82\n",
      "Trained batch 532 batch loss 10367.2578 epoch total loss 9866.7627\n",
      "Trained batch 533 batch loss 10344.0654 epoch total loss 9867.6582\n",
      "Trained batch 534 batch loss 10359.6406 epoch total loss 9868.5791\n",
      "Trained batch 535 batch loss 10310.3184 epoch total loss 9869.40527\n",
      "Trained batch 536 batch loss 10320.4043 epoch total loss 9870.24707\n",
      "Trained batch 537 batch loss 10189.5898 epoch total loss 9870.8418\n",
      "Trained batch 538 batch loss 10374.0664 epoch total loss 9871.77734\n",
      "Trained batch 539 batch loss 10338.8125 epoch total loss 9872.64355\n",
      "Trained batch 540 batch loss 10170.5811 epoch total loss 9873.19531\n",
      "Trained batch 541 batch loss 9747.79395 epoch total loss 9872.96387\n",
      "Trained batch 542 batch loss 10101.0742 epoch total loss 9873.38477\n",
      "Trained batch 543 batch loss 9207.90918 epoch total loss 9872.15918\n",
      "Trained batch 544 batch loss 10084.7 epoch total loss 9872.55\n",
      "Trained batch 545 batch loss 9801.83691 epoch total loss 9872.42\n",
      "Trained batch 546 batch loss 10370.0684 epoch total loss 9873.33105\n",
      "Trained batch 547 batch loss 10196.0547 epoch total loss 9873.92188\n",
      "Trained batch 548 batch loss 9956.48828 epoch total loss 9874.07227\n",
      "Trained batch 549 batch loss 10212.9941 epoch total loss 9874.68945\n",
      "Trained batch 550 batch loss 10370.9189 epoch total loss 9875.5918\n",
      "Trained batch 551 batch loss 10388.5869 epoch total loss 9876.52246\n",
      "Trained batch 552 batch loss 10391.1855 epoch total loss 9877.45508\n",
      "Trained batch 553 batch loss 10010.4854 epoch total loss 9877.69531\n",
      "Trained batch 554 batch loss 10382.7812 epoch total loss 9878.60742\n",
      "Trained batch 555 batch loss 10377.9766 epoch total loss 9879.50684\n",
      "Trained batch 556 batch loss 9985.68945 epoch total loss 9879.69824\n",
      "Trained batch 557 batch loss 10202.6328 epoch total loss 9880.27734\n",
      "Trained batch 558 batch loss 10232.2891 epoch total loss 9880.9082\n",
      "Trained batch 559 batch loss 10235.5342 epoch total loss 9881.54297\n",
      "Trained batch 560 batch loss 10346.3652 epoch total loss 9882.37305\n",
      "Trained batch 561 batch loss 10072.2764 epoch total loss 9882.71191\n",
      "Trained batch 562 batch loss 9789.35938 epoch total loss 9882.5459\n",
      "Trained batch 563 batch loss 10186.9014 epoch total loss 9883.08691\n",
      "Trained batch 564 batch loss 10016.3213 epoch total loss 9883.32324\n",
      "Trained batch 565 batch loss 10451.8486 epoch total loss 9884.33\n",
      "Trained batch 566 batch loss 10039.708 epoch total loss 9884.60449\n",
      "Trained batch 567 batch loss 10263.2559 epoch total loss 9885.27246\n",
      "Trained batch 568 batch loss 10425.543 epoch total loss 9886.22363\n",
      "Trained batch 569 batch loss 9983.42 epoch total loss 9886.39453\n",
      "Trained batch 570 batch loss 10365.1748 epoch total loss 9887.23438\n",
      "Trained batch 571 batch loss 10272.8105 epoch total loss 9887.91\n",
      "Trained batch 572 batch loss 10243.6396 epoch total loss 9888.53125\n",
      "Trained batch 573 batch loss 10392.8857 epoch total loss 9889.41211\n",
      "Trained batch 574 batch loss 10132.1201 epoch total loss 9889.83496\n",
      "Trained batch 575 batch loss 9968.70898 epoch total loss 9889.97168\n",
      "Trained batch 576 batch loss 10325.5791 epoch total loss 9890.72754\n",
      "Trained batch 577 batch loss 10393.2607 epoch total loss 9891.59863\n",
      "Trained batch 578 batch loss 10249.2 epoch total loss 9892.2168\n",
      "Trained batch 579 batch loss 10344.4961 epoch total loss 9892.99805\n",
      "Trained batch 580 batch loss 10441.041 epoch total loss 9893.94336\n",
      "Trained batch 581 batch loss 10494.9697 epoch total loss 9894.97754\n",
      "Trained batch 582 batch loss 10499.7412 epoch total loss 9896.0166\n",
      "Trained batch 583 batch loss 10477.0068 epoch total loss 9897.0127\n",
      "Trained batch 584 batch loss 10499.4844 epoch total loss 9898.04492\n",
      "Trained batch 585 batch loss 10486.0859 epoch total loss 9899.05\n",
      "Trained batch 586 batch loss 10273.1934 epoch total loss 9899.6875\n",
      "Trained batch 587 batch loss 10443.4062 epoch total loss 9900.61426\n",
      "Trained batch 588 batch loss 9851.89453 epoch total loss 9900.53125\n",
      "Trained batch 589 batch loss 10268.3418 epoch total loss 9901.15625\n",
      "Trained batch 590 batch loss 10498.5879 epoch total loss 9902.16895\n",
      "Trained batch 591 batch loss 10491.3135 epoch total loss 9903.16602\n",
      "Trained batch 592 batch loss 10297.3242 epoch total loss 9903.83203\n",
      "Trained batch 593 batch loss 10270.4062 epoch total loss 9904.45\n",
      "Trained batch 594 batch loss 10427.71 epoch total loss 9905.33105\n",
      "Trained batch 595 batch loss 10507.6348 epoch total loss 9906.34277\n",
      "Trained batch 596 batch loss 10318.3936 epoch total loss 9907.03418\n",
      "Trained batch 597 batch loss 10512.7402 epoch total loss 9908.04883\n",
      "Trained batch 598 batch loss 10521.7344 epoch total loss 9909.07422\n",
      "Trained batch 599 batch loss 10524.1377 epoch total loss 9910.10059\n",
      "Trained batch 600 batch loss 10323.46 epoch total loss 9910.79\n",
      "Trained batch 601 batch loss 10338.5283 epoch total loss 9911.50195\n",
      "Trained batch 602 batch loss 10533.8525 epoch total loss 9912.53613\n",
      "Trained batch 603 batch loss 10125.6465 epoch total loss 9912.88867\n",
      "Trained batch 604 batch loss 10539.1123 epoch total loss 9913.92578\n",
      "Trained batch 605 batch loss 10538.8096 epoch total loss 9914.95898\n",
      "Trained batch 606 batch loss 10340.3564 epoch total loss 9915.66113\n",
      "Trained batch 607 batch loss 9941.59766 epoch total loss 9915.70312\n",
      "Trained batch 608 batch loss 9918.46777 epoch total loss 9915.70801\n",
      "Trained batch 609 batch loss 9959.42676 epoch total loss 9915.78\n",
      "Trained batch 610 batch loss 10548.9277 epoch total loss 9916.81836\n",
      "Trained batch 611 batch loss 10466.4248 epoch total loss 9917.71777\n",
      "Trained batch 612 batch loss 9646.17578 epoch total loss 9917.27344\n",
      "Trained batch 613 batch loss 10556.0049 epoch total loss 9918.31543\n",
      "Trained batch 614 batch loss 10559.4033 epoch total loss 9919.36\n",
      "Trained batch 615 batch loss 10335.8799 epoch total loss 9920.03711\n",
      "Trained batch 616 batch loss 10331.6738 epoch total loss 9920.70508\n",
      "Trained batch 617 batch loss 10157.0088 epoch total loss 9921.08789\n",
      "Trained batch 618 batch loss 10548.5449 epoch total loss 9922.10352\n",
      "Trained batch 619 batch loss 10536.3057 epoch total loss 9923.0957\n",
      "Trained batch 620 batch loss 10572.291 epoch total loss 9924.14355\n",
      "Trained batch 621 batch loss 10384.4502 epoch total loss 9924.88477\n",
      "Trained batch 622 batch loss 10582.2705 epoch total loss 9925.94238\n",
      "Trained batch 623 batch loss 10432.4287 epoch total loss 9926.75488\n",
      "Trained batch 624 batch loss 10581.4902 epoch total loss 9927.80469\n",
      "Trained batch 625 batch loss 10557.0293 epoch total loss 9928.81152\n",
      "Trained batch 626 batch loss 10605.0361 epoch total loss 9929.8916\n",
      "Trained batch 627 batch loss 10613.0576 epoch total loss 9930.98\n",
      "Trained batch 628 batch loss 10618.334 epoch total loss 9932.0752\n",
      "Trained batch 629 batch loss 10599.2744 epoch total loss 9933.13672\n",
      "Trained batch 630 batch loss 10628.9023 epoch total loss 9934.24121\n",
      "Trained batch 631 batch loss 10632.96 epoch total loss 9935.34863\n",
      "Trained batch 632 batch loss 10637.625 epoch total loss 9936.46\n",
      "Trained batch 633 batch loss 10641.3613 epoch total loss 9937.57324\n",
      "Trained batch 634 batch loss 10643.8809 epoch total loss 9938.6875\n",
      "Trained batch 635 batch loss 10601.8662 epoch total loss 9939.73242\n",
      "Trained batch 636 batch loss 10652.7334 epoch total loss 9940.85254\n",
      "Trained batch 637 batch loss 10654.9639 epoch total loss 9941.97363\n",
      "Trained batch 638 batch loss 10656.665 epoch total loss 9943.09375\n",
      "Trained batch 639 batch loss 10661.1143 epoch total loss 9944.21777\n",
      "Trained batch 640 batch loss 10663.5029 epoch total loss 9945.3418\n",
      "Trained batch 641 batch loss 10470.6035 epoch total loss 9946.16113\n",
      "Trained batch 642 batch loss 10673.6777 epoch total loss 9947.29395\n",
      "Trained batch 643 batch loss 10667.5107 epoch total loss 9948.41406\n",
      "Trained batch 644 batch loss 10673.9971 epoch total loss 9949.54\n",
      "Trained batch 645 batch loss 10642.8389 epoch total loss 9950.61523\n",
      "Trained batch 646 batch loss 10682.3408 epoch total loss 9951.74805\n",
      "Trained batch 647 batch loss 10640.0762 epoch total loss 9952.8125\n",
      "Trained batch 648 batch loss 10620.125 epoch total loss 9953.8418\n",
      "Trained batch 649 batch loss 10601.3555 epoch total loss 9954.84\n",
      "Trained batch 650 batch loss 10244.0195 epoch total loss 9955.28418\n",
      "Trained batch 651 batch loss 10243.084 epoch total loss 9955.72656\n",
      "Trained batch 652 batch loss 10083.5576 epoch total loss 9955.92285\n",
      "Trained batch 653 batch loss 10480.3643 epoch total loss 9956.72559\n",
      "Trained batch 654 batch loss 10710.6631 epoch total loss 9957.87891\n",
      "Trained batch 655 batch loss 10651.3994 epoch total loss 9958.9375\n",
      "Trained batch 656 batch loss 10661.2139 epoch total loss 9960.00781\n",
      "Trained batch 657 batch loss 10413.9629 epoch total loss 9960.69824\n",
      "Trained batch 658 batch loss 10699.3643 epoch total loss 9961.82129\n",
      "Trained batch 659 batch loss 10500.9834 epoch total loss 9962.64\n",
      "Trained batch 660 batch loss 10736.1885 epoch total loss 9963.81152\n",
      "Trained batch 661 batch loss 10529.502 epoch total loss 9964.66699\n",
      "Trained batch 662 batch loss 10736.8145 epoch total loss 9965.83398\n",
      "Trained batch 663 batch loss 10490.1357 epoch total loss 9966.62402\n",
      "Trained batch 664 batch loss 10479.624 epoch total loss 9967.39648\n",
      "Trained batch 665 batch loss 10451.5078 epoch total loss 9968.125\n",
      "Trained batch 666 batch loss 10205.8213 epoch total loss 9968.48242\n",
      "Trained batch 667 batch loss 10748.2373 epoch total loss 9969.65\n",
      "Trained batch 668 batch loss 10746.8809 epoch total loss 9970.81445\n",
      "Trained batch 669 batch loss 10742.6 epoch total loss 9971.96777\n",
      "Trained batch 670 batch loss 10746.0615 epoch total loss 9973.12305\n",
      "Trained batch 671 batch loss 10744.9854 epoch total loss 9974.27344\n",
      "Trained batch 672 batch loss 10553.375 epoch total loss 9975.13574\n",
      "Trained batch 673 batch loss 10742.3633 epoch total loss 9976.27539\n",
      "Trained batch 674 batch loss 10752.125 epoch total loss 9977.42676\n",
      "Trained batch 675 batch loss 10697.4805 epoch total loss 9978.49316\n",
      "Trained batch 676 batch loss 10745.8896 epoch total loss 9979.62891\n",
      "Trained batch 677 batch loss 10738.1953 epoch total loss 9980.74902\n",
      "Trained batch 678 batch loss 10543.7256 epoch total loss 9981.5791\n",
      "Trained batch 679 batch loss 10480.5576 epoch total loss 9982.31348\n",
      "Trained batch 680 batch loss 10484.2402 epoch total loss 9983.05176\n",
      "Trained batch 681 batch loss 10473.4287 epoch total loss 9983.77148\n",
      "Trained batch 682 batch loss 10591.3447 epoch total loss 9984.66309\n",
      "Trained batch 683 batch loss 10741.3135 epoch total loss 9985.77051\n",
      "Trained batch 684 batch loss 10550.3584 epoch total loss 9986.59668\n",
      "Trained batch 685 batch loss 10752.0479 epoch total loss 9987.71387\n",
      "Trained batch 686 batch loss 10751.3057 epoch total loss 9988.82715\n",
      "Trained batch 687 batch loss 10751.1084 epoch total loss 9989.93652\n",
      "Trained batch 688 batch loss 10745.4131 epoch total loss 9991.03516\n",
      "Trained batch 689 batch loss 10679.7441 epoch total loss 9992.03418\n",
      "Trained batch 690 batch loss 10697.5732 epoch total loss 9993.05664\n",
      "Trained batch 691 batch loss 10464.3789 epoch total loss 9993.73926\n",
      "Trained batch 692 batch loss 10673.4043 epoch total loss 9994.7207\n",
      "Trained batch 693 batch loss 10666.377 epoch total loss 9995.69\n",
      "Trained batch 694 batch loss 10710.4111 epoch total loss 9996.7207\n",
      "Trained batch 695 batch loss 10720.5049 epoch total loss 9997.76172\n",
      "Trained batch 696 batch loss 10727.2031 epoch total loss 9998.81\n",
      "Trained batch 697 batch loss 10712.0869 epoch total loss 9999.83301\n",
      "Trained batch 698 batch loss 10734.9404 epoch total loss 10000.8857\n",
      "Trained batch 699 batch loss 10733.4346 epoch total loss 10001.9346\n",
      "Trained batch 700 batch loss 10504.7402 epoch total loss 10002.6523\n",
      "Trained batch 701 batch loss 10728.8408 epoch total loss 10003.6885\n",
      "Trained batch 702 batch loss 10718.3584 epoch total loss 10004.707\n",
      "Trained batch 703 batch loss 10717.7207 epoch total loss 10005.7207\n",
      "Trained batch 704 batch loss 10715.3965 epoch total loss 10006.7285\n",
      "Trained batch 705 batch loss 10514.6035 epoch total loss 10007.4492\n",
      "Trained batch 706 batch loss 10718.3848 epoch total loss 10008.4561\n",
      "Trained batch 707 batch loss 10511.4707 epoch total loss 10009.168\n",
      "Trained batch 708 batch loss 10673.2412 epoch total loss 10010.1055\n",
      "Trained batch 709 batch loss 10689.6719 epoch total loss 10011.0635\n",
      "Trained batch 710 batch loss 10699.8906 epoch total loss 10012.0342\n",
      "Trained batch 711 batch loss 10426.6025 epoch total loss 10012.6172\n",
      "Trained batch 712 batch loss 10490.0361 epoch total loss 10013.2871\n",
      "Trained batch 713 batch loss 10677.5479 epoch total loss 10014.2188\n",
      "Trained batch 714 batch loss 10703.666 epoch total loss 10015.1846\n",
      "Trained batch 715 batch loss 10484.8516 epoch total loss 10015.8408\n",
      "Trained batch 716 batch loss 10671.1221 epoch total loss 10016.7559\n",
      "Trained batch 717 batch loss 10673.3408 epoch total loss 10017.6719\n",
      "Trained batch 718 batch loss 10653.4463 epoch total loss 10018.5576\n",
      "Trained batch 719 batch loss 10643.9707 epoch total loss 10019.4277\n",
      "Trained batch 720 batch loss 10639.9531 epoch total loss 10020.29\n",
      "Trained batch 721 batch loss 10621.2705 epoch total loss 10021.123\n",
      "Trained batch 722 batch loss 10590.7256 epoch total loss 10021.9121\n",
      "Trained batch 723 batch loss 10474.1738 epoch total loss 10022.5371\n",
      "Trained batch 724 batch loss 10681.4102 epoch total loss 10023.4473\n",
      "Trained batch 725 batch loss 10677.6387 epoch total loss 10024.3496\n",
      "Trained batch 726 batch loss 10688.916 epoch total loss 10025.2656\n",
      "Trained batch 727 batch loss 10691.6523 epoch total loss 10026.1816\n",
      "Trained batch 728 batch loss 10683.8213 epoch total loss 10027.085\n",
      "Trained batch 729 batch loss 10689.8115 epoch total loss 10027.9941\n",
      "Trained batch 730 batch loss 10622.3857 epoch total loss 10028.8086\n",
      "Trained batch 731 batch loss 10688.4443 epoch total loss 10029.7109\n",
      "Trained batch 732 batch loss 10684.6406 epoch total loss 10030.6055\n",
      "Trained batch 733 batch loss 10554.1465 epoch total loss 10031.3203\n",
      "Trained batch 734 batch loss 10682.166 epoch total loss 10032.2061\n",
      "Trained batch 735 batch loss 10659.2861 epoch total loss 10033.0596\n",
      "Trained batch 736 batch loss 10618.9551 epoch total loss 10033.8564\n",
      "Trained batch 737 batch loss 10619.4648 epoch total loss 10034.6504\n",
      "Trained batch 738 batch loss 10664.0439 epoch total loss 10035.5029\n",
      "Trained batch 739 batch loss 10384.0215 epoch total loss 10035.9746\n",
      "Trained batch 740 batch loss 10176.0498 epoch total loss 10036.1641\n",
      "Trained batch 741 batch loss 10644.832 epoch total loss 10036.9854\n",
      "Trained batch 742 batch loss 10579.3379 epoch total loss 10037.7168\n",
      "Trained batch 743 batch loss 10646.8623 epoch total loss 10038.5371\n",
      "Trained batch 744 batch loss 10051.0273 epoch total loss 10038.5537\n",
      "Trained batch 745 batch loss 9965.7041 epoch total loss 10038.4561\n",
      "Trained batch 746 batch loss 10383.2129 epoch total loss 10038.918\n",
      "Trained batch 747 batch loss 10667.5244 epoch total loss 10039.7588\n",
      "Trained batch 748 batch loss 10674.3379 epoch total loss 10040.6074\n",
      "Trained batch 749 batch loss 10676.1143 epoch total loss 10041.4561\n",
      "Trained batch 750 batch loss 10426.1406 epoch total loss 10041.9688\n",
      "Trained batch 751 batch loss 10374.3662 epoch total loss 10042.4111\n",
      "Trained batch 752 batch loss 10670.958 epoch total loss 10043.2471\n",
      "Trained batch 753 batch loss 10462.9492 epoch total loss 10043.8047\n",
      "Trained batch 754 batch loss 10671.5732 epoch total loss 10044.6377\n",
      "Trained batch 755 batch loss 10655.9922 epoch total loss 10045.4473\n",
      "Trained batch 756 batch loss 10678.0635 epoch total loss 10046.2842\n",
      "Trained batch 757 batch loss 10683.0361 epoch total loss 10047.125\n",
      "Trained batch 758 batch loss 10690.6318 epoch total loss 10047.9736\n",
      "Trained batch 759 batch loss 10690.2744 epoch total loss 10048.8203\n",
      "Trained batch 760 batch loss 10691.3809 epoch total loss 10049.666\n",
      "Trained batch 761 batch loss 10687.6016 epoch total loss 10050.5039\n",
      "Trained batch 762 batch loss 10673.1426 epoch total loss 10051.3213\n",
      "Trained batch 763 batch loss 10479.6143 epoch total loss 10051.8818\n",
      "Trained batch 764 batch loss 10534.998 epoch total loss 10052.5146\n",
      "Trained batch 765 batch loss 10688.6855 epoch total loss 10053.3457\n",
      "Trained batch 766 batch loss 10692.2295 epoch total loss 10054.1797\n",
      "Trained batch 767 batch loss 10675.6865 epoch total loss 10054.9893\n",
      "Trained batch 768 batch loss 10676.1318 epoch total loss 10055.7979\n",
      "Trained batch 769 batch loss 10124.25 epoch total loss 10055.8867\n",
      "Trained batch 770 batch loss 10472.2891 epoch total loss 10056.4277\n",
      "Trained batch 771 batch loss 10662.3555 epoch total loss 10057.2139\n",
      "Trained batch 772 batch loss 10683.8428 epoch total loss 10058.0264\n",
      "Trained batch 773 batch loss 10684.4766 epoch total loss 10058.8359\n",
      "Trained batch 774 batch loss 10658.3818 epoch total loss 10059.6113\n",
      "Trained batch 775 batch loss 10262.4971 epoch total loss 10059.873\n",
      "Trained batch 776 batch loss 9961.30469 epoch total loss 10059.7461\n",
      "Trained batch 777 batch loss 9810.59277 epoch total loss 10059.4258\n",
      "Trained batch 778 batch loss 10640.1191 epoch total loss 10060.1719\n",
      "Trained batch 779 batch loss 10458.6592 epoch total loss 10060.6826\n",
      "Trained batch 780 batch loss 10457.8076 epoch total loss 10061.1924\n",
      "Trained batch 781 batch loss 10456.6709 epoch total loss 10061.6982\n",
      "Trained batch 782 batch loss 10552.1064 epoch total loss 10062.3252\n",
      "Trained batch 783 batch loss 10605.6904 epoch total loss 10063.0195\n",
      "Trained batch 784 batch loss 10439.8877 epoch total loss 10063.5\n",
      "Trained batch 785 batch loss 10647.0117 epoch total loss 10064.2432\n",
      "Trained batch 786 batch loss 10643.2686 epoch total loss 10064.9805\n",
      "Trained batch 787 batch loss 10626.8838 epoch total loss 10065.6943\n",
      "Trained batch 788 batch loss 10583.1074 epoch total loss 10066.3506\n",
      "Trained batch 789 batch loss 10620.8896 epoch total loss 10067.0537\n",
      "Trained batch 790 batch loss 10621.6641 epoch total loss 10067.7559\n",
      "Trained batch 791 batch loss 10617.5908 epoch total loss 10068.4512\n",
      "Trained batch 792 batch loss 10617.2236 epoch total loss 10069.1436\n",
      "Trained batch 793 batch loss 10543.0449 epoch total loss 10069.7412\n",
      "Trained batch 794 batch loss 10616.9141 epoch total loss 10070.4297\n",
      "Trained batch 795 batch loss 10619.5508 epoch total loss 10071.1211\n",
      "Trained batch 796 batch loss 10609.9062 epoch total loss 10071.7979\n",
      "Trained batch 797 batch loss 10245.4463 epoch total loss 10072.0156\n",
      "Trained batch 798 batch loss 10401.126 epoch total loss 10072.4277\n",
      "Trained batch 799 batch loss 10549.8428 epoch total loss 10073.0254\n",
      "Trained batch 800 batch loss 10526.3418 epoch total loss 10073.5928\n",
      "Trained batch 801 batch loss 10500.8926 epoch total loss 10074.126\n",
      "Trained batch 802 batch loss 10429.2822 epoch total loss 10074.5693\n",
      "Trained batch 803 batch loss 10562.2852 epoch total loss 10075.1768\n",
      "Trained batch 804 batch loss 10575.2793 epoch total loss 10075.7988\n",
      "Trained batch 805 batch loss 10584.8887 epoch total loss 10076.4316\n",
      "Trained batch 806 batch loss 10585.8799 epoch total loss 10077.0635\n",
      "Trained batch 807 batch loss 10589.2637 epoch total loss 10077.6992\n",
      "Trained batch 808 batch loss 10572.9238 epoch total loss 10078.3115\n",
      "Trained batch 809 batch loss 10369.1113 epoch total loss 10078.6709\n",
      "Trained batch 810 batch loss 10580.3037 epoch total loss 10079.291\n",
      "Trained batch 811 batch loss 10580.2812 epoch total loss 10079.9092\n",
      "Trained batch 812 batch loss 10575.8564 epoch total loss 10080.5195\n",
      "Trained batch 813 batch loss 10577.1064 epoch total loss 10081.1309\n",
      "Trained batch 814 batch loss 10548.2119 epoch total loss 10081.7041\n",
      "Trained batch 815 batch loss 10324.1709 epoch total loss 10082.001\n",
      "Trained batch 816 batch loss 10313.6553 epoch total loss 10082.2852\n",
      "Trained batch 817 batch loss 10274.959 epoch total loss 10082.5205\n",
      "Trained batch 818 batch loss 10526.7461 epoch total loss 10083.0635\n",
      "Trained batch 819 batch loss 9826.64844 epoch total loss 10082.75\n",
      "Trained batch 820 batch loss 10537.8779 epoch total loss 10083.3057\n",
      "Trained batch 821 batch loss 10333.8193 epoch total loss 10083.6113\n",
      "Trained batch 822 batch loss 10347.1826 epoch total loss 10083.9316\n",
      "Trained batch 823 batch loss 10148.2383 epoch total loss 10084.0088\n",
      "Trained batch 824 batch loss 10330.7021 epoch total loss 10084.3086\n",
      "Trained batch 825 batch loss 10534.0908 epoch total loss 10084.8535\n",
      "Trained batch 826 batch loss 10508.2637 epoch total loss 10085.3662\n",
      "Trained batch 827 batch loss 10549.7256 epoch total loss 10085.9277\n",
      "Trained batch 828 batch loss 10330.5977 epoch total loss 10086.2227\n",
      "Trained batch 829 batch loss 10546.2969 epoch total loss 10086.7783\n",
      "Trained batch 830 batch loss 10540.9707 epoch total loss 10087.3252\n",
      "Trained batch 831 batch loss 10237.666 epoch total loss 10087.5059\n",
      "Trained batch 832 batch loss 9878.6582 epoch total loss 10087.2549\n",
      "Trained batch 833 batch loss 10535.124 epoch total loss 10087.792\n",
      "Trained batch 834 batch loss 10396.0166 epoch total loss 10088.1621\n",
      "Trained batch 835 batch loss 10136.4 epoch total loss 10088.2188\n",
      "Trained batch 836 batch loss 10341.2988 epoch total loss 10088.5215\n",
      "Trained batch 837 batch loss 10512.3672 epoch total loss 10089.0273\n",
      "Trained batch 838 batch loss 10453.165 epoch total loss 10089.4619\n",
      "Trained batch 839 batch loss 10508.5186 epoch total loss 10089.9619\n",
      "Trained batch 840 batch loss 10506.0215 epoch total loss 10090.457\n",
      "Trained batch 841 batch loss 10525.5928 epoch total loss 10090.9746\n",
      "Trained batch 842 batch loss 10408.4209 epoch total loss 10091.3516\n",
      "Trained batch 843 batch loss 10521.1338 epoch total loss 10091.8613\n",
      "Trained batch 844 batch loss 10517.2021 epoch total loss 10092.3652\n",
      "Trained batch 845 batch loss 10125.8857 epoch total loss 10092.4043\n",
      "Trained batch 846 batch loss 10377.3369 epoch total loss 10092.7412\n",
      "Trained batch 847 batch loss 10319.5264 epoch total loss 10093.0098\n",
      "Trained batch 848 batch loss 10506.7344 epoch total loss 10093.498\n",
      "Trained batch 849 batch loss 10504.8047 epoch total loss 10093.9824\n",
      "Trained batch 850 batch loss 10433.3672 epoch total loss 10094.3809\n",
      "Trained batch 851 batch loss 10467.6416 epoch total loss 10094.8203\n",
      "Trained batch 852 batch loss 10517.5596 epoch total loss 10095.3174\n",
      "Trained batch 853 batch loss 10510.4902 epoch total loss 10095.8027\n",
      "Trained batch 854 batch loss 10494.126 epoch total loss 10096.2695\n",
      "Trained batch 855 batch loss 10500.6553 epoch total loss 10096.7432\n",
      "Trained batch 856 batch loss 10484.457 epoch total loss 10097.1953\n",
      "Trained batch 857 batch loss 10301.7178 epoch total loss 10097.4336\n",
      "Trained batch 858 batch loss 10480.5156 epoch total loss 10097.8809\n",
      "Trained batch 859 batch loss 9922.71191 epoch total loss 10097.6777\n",
      "Trained batch 860 batch loss 10461.2754 epoch total loss 10098.1\n",
      "Trained batch 861 batch loss 10262.6035 epoch total loss 10098.292\n",
      "Trained batch 862 batch loss 10094.6074 epoch total loss 10098.2881\n",
      "Trained batch 863 batch loss 10486.6084 epoch total loss 10098.7383\n",
      "Trained batch 864 batch loss 10492.2598 epoch total loss 10099.1934\n",
      "Trained batch 865 batch loss 10460.1484 epoch total loss 10099.6104\n",
      "Trained batch 866 batch loss 10112.8135 epoch total loss 10099.626\n",
      "Trained batch 867 batch loss 10230.918 epoch total loss 10099.7773\n",
      "Trained batch 868 batch loss 10419.627 epoch total loss 10100.1465\n",
      "Trained batch 869 batch loss 10217.2275 epoch total loss 10100.2812\n",
      "Trained batch 870 batch loss 10485.3945 epoch total loss 10100.7227\n",
      "Trained batch 871 batch loss 10335.3652 epoch total loss 10100.9922\n",
      "Trained batch 872 batch loss 10465.1719 epoch total loss 10101.4092\n",
      "Trained batch 873 batch loss 10131.4922 epoch total loss 10101.4434\n",
      "Trained batch 874 batch loss 10331.4443 epoch total loss 10101.7061\n",
      "Trained batch 875 batch loss 10458.5566 epoch total loss 10102.1143\n",
      "Trained batch 876 batch loss 10467.2188 epoch total loss 10102.5312\n",
      "Trained batch 877 batch loss 10267.4609 epoch total loss 10102.7188\n",
      "Trained batch 878 batch loss 10380.1797 epoch total loss 10103.0342\n",
      "Trained batch 879 batch loss 10270.0381 epoch total loss 10103.2236\n",
      "Trained batch 880 batch loss 10458.9121 epoch total loss 10103.6279\n",
      "Trained batch 881 batch loss 10449.7783 epoch total loss 10104.0215\n",
      "Trained batch 882 batch loss 10466.0488 epoch total loss 10104.4316\n",
      "Trained batch 883 batch loss 9877.69238 epoch total loss 10104.1758\n",
      "Trained batch 884 batch loss 10274.8311 epoch total loss 10104.3691\n",
      "Trained batch 885 batch loss 9878.74121 epoch total loss 10104.1143\n",
      "Trained batch 886 batch loss 10460.4521 epoch total loss 10104.5156\n",
      "Trained batch 887 batch loss 10453.9883 epoch total loss 10104.9102\n",
      "Trained batch 888 batch loss 9863.87 epoch total loss 10104.6387\n",
      "Trained batch 889 batch loss 10118.1055 epoch total loss 10104.6533\n",
      "Trained batch 890 batch loss 10252.1055 epoch total loss 10104.8193\n",
      "Trained batch 891 batch loss 10287.7051 epoch total loss 10105.0244\n",
      "Trained batch 892 batch loss 10429.7393 epoch total loss 10105.3887\n",
      "Trained batch 893 batch loss 9961.04297 epoch total loss 10105.2275\n",
      "Trained batch 894 batch loss 10458.7314 epoch total loss 10105.623\n",
      "Trained batch 895 batch loss 10457.3887 epoch total loss 10106.0156\n",
      "Trained batch 896 batch loss 10455.9912 epoch total loss 10106.4062\n",
      "Trained batch 897 batch loss 10453.1035 epoch total loss 10106.793\n",
      "Trained batch 898 batch loss 10450.3652 epoch total loss 10107.1748\n",
      "Trained batch 899 batch loss 10215.6064 epoch total loss 10107.2959\n",
      "Trained batch 900 batch loss 10335.5547 epoch total loss 10107.5498\n",
      "Trained batch 901 batch loss 10254.4883 epoch total loss 10107.7129\n",
      "Trained batch 902 batch loss 9893.31738 epoch total loss 10107.4746\n",
      "Trained batch 903 batch loss 10444.793 epoch total loss 10107.8486\n",
      "Trained batch 904 batch loss 10051.9473 epoch total loss 10107.7861\n",
      "Trained batch 905 batch loss 10443.458 epoch total loss 10108.1572\n",
      "Trained batch 906 batch loss 10439.8848 epoch total loss 10108.5234\n",
      "Trained batch 907 batch loss 10249.7158 epoch total loss 10108.6787\n",
      "Trained batch 908 batch loss 10439.709 epoch total loss 10109.0439\n",
      "Trained batch 909 batch loss 10439.168 epoch total loss 10109.4072\n",
      "Trained batch 910 batch loss 10429.7783 epoch total loss 10109.7598\n",
      "Trained batch 911 batch loss 10424.1016 epoch total loss 10110.1045\n",
      "Trained batch 912 batch loss 10427.2021 epoch total loss 10110.4521\n",
      "Trained batch 913 batch loss 10425.6875 epoch total loss 10110.7979\n",
      "Trained batch 914 batch loss 10370.9023 epoch total loss 10111.082\n",
      "Trained batch 915 batch loss 10364.3848 epoch total loss 10111.3584\n",
      "Trained batch 916 batch loss 10422.9277 epoch total loss 10111.6982\n",
      "Trained batch 917 batch loss 10207.4824 epoch total loss 10111.8027\n",
      "Trained batch 918 batch loss 10212.0576 epoch total loss 10111.9121\n",
      "Trained batch 919 batch loss 9722.4834 epoch total loss 10111.4873\n",
      "Trained batch 920 batch loss 9764.28 epoch total loss 10111.1094\n",
      "Trained batch 921 batch loss 9158.68359 epoch total loss 10110.0762\n",
      "Trained batch 922 batch loss 10154.1035 epoch total loss 10110.124\n",
      "Trained batch 923 batch loss 10215.2949 epoch total loss 10110.2373\n",
      "Trained batch 924 batch loss 10416.499 epoch total loss 10110.5684\n",
      "Trained batch 925 batch loss 10021.7422 epoch total loss 10110.4727\n",
      "Trained batch 926 batch loss 10128.7441 epoch total loss 10110.4922\n",
      "Trained batch 927 batch loss 10395.6338 epoch total loss 10110.8008\n",
      "Trained batch 928 batch loss 10380.2607 epoch total loss 10111.0908\n",
      "Trained batch 929 batch loss 10186.5029 epoch total loss 10111.1719\n",
      "Trained batch 930 batch loss 10370.7842 epoch total loss 10111.4512\n",
      "Trained batch 931 batch loss 10337.5879 epoch total loss 10111.6953\n",
      "Trained batch 932 batch loss 10373.1689 epoch total loss 10111.9756\n",
      "Trained batch 933 batch loss 10185.1016 epoch total loss 10112.0537\n",
      "Trained batch 934 batch loss 10397.9395 epoch total loss 10112.3594\n",
      "Trained batch 935 batch loss 10435.6592 epoch total loss 10112.7061\n",
      "Trained batch 936 batch loss 10310.7812 epoch total loss 10112.918\n",
      "Trained batch 937 batch loss 10423.251 epoch total loss 10113.249\n",
      "Trained batch 938 batch loss 10285.9316 epoch total loss 10113.4326\n",
      "Trained batch 939 batch loss 10036.5088 epoch total loss 10113.3516\n",
      "Trained batch 940 batch loss 9826.625 epoch total loss 10113.0469\n",
      "Trained batch 941 batch loss 9921.39648 epoch total loss 10112.8428\n",
      "Trained batch 942 batch loss 10094.4287 epoch total loss 10112.8223\n",
      "Trained batch 943 batch loss 9903.80469 epoch total loss 10112.6016\n",
      "Trained batch 944 batch loss 9926.02148 epoch total loss 10112.4033\n",
      "Trained batch 945 batch loss 9864.93945 epoch total loss 10112.1416\n",
      "Trained batch 946 batch loss 9769.56836 epoch total loss 10111.7803\n",
      "Trained batch 947 batch loss 9707.86719 epoch total loss 10111.3535\n",
      "Trained batch 948 batch loss 9586.92188 epoch total loss 10110.8008\n",
      "Trained batch 949 batch loss 9352.89844 epoch total loss 10110.002\n",
      "Trained batch 950 batch loss 9327.98926 epoch total loss 10109.1787\n",
      "Trained batch 951 batch loss 9007.93066 epoch total loss 10108.0215\n",
      "Trained batch 952 batch loss 9022.33789 epoch total loss 10106.8799\n",
      "Trained batch 953 batch loss 8660.38867 epoch total loss 10105.3623\n",
      "Trained batch 954 batch loss 7638.14062 epoch total loss 10102.7754\n",
      "Trained batch 955 batch loss 6878.09814 epoch total loss 10099.3994\n",
      "Trained batch 956 batch loss 4853.38623 epoch total loss 10093.9111\n",
      "Trained batch 957 batch loss 3387.98511 epoch total loss 10086.9043\n",
      "Trained batch 958 batch loss 2207.61084 epoch total loss 10078.6797\n",
      "Trained batch 959 batch loss 1202.13806 epoch total loss 10069.4238\n",
      "Trained batch 960 batch loss 648.84314 epoch total loss 10059.6104\n",
      "Trained batch 961 batch loss 615.700073 epoch total loss 10049.7832\n",
      "Trained batch 962 batch loss 380.110413 epoch total loss 10039.7314\n",
      "Trained batch 963 batch loss 603.164307 epoch total loss 10029.9326\n",
      "Trained batch 964 batch loss 197.953766 epoch total loss 10019.7334\n",
      "Trained batch 965 batch loss 247.279541 epoch total loss 10009.6064\n",
      "Trained batch 966 batch loss 380.475311 epoch total loss 9999.6377\n",
      "Trained batch 967 batch loss 231.631363 epoch total loss 9989.53711\n",
      "Trained batch 968 batch loss 193.644714 epoch total loss 9979.41699\n",
      "Trained batch 969 batch loss 197.395676 epoch total loss 9969.32227\n",
      "Trained batch 970 batch loss 249.853317 epoch total loss 9959.30176\n",
      "Trained batch 971 batch loss 147.378143 epoch total loss 9949.19629\n",
      "Trained batch 972 batch loss 162.057632 epoch total loss 9939.12793\n",
      "Trained batch 973 batch loss 85.6937256 epoch total loss 9929.00098\n",
      "Trained batch 974 batch loss 89.2066498 epoch total loss 9918.89844\n",
      "Trained batch 975 batch loss 215.027832 epoch total loss 9908.94531\n",
      "Trained batch 976 batch loss 77.6306458 epoch total loss 9898.87305\n",
      "Trained batch 977 batch loss 92.0055542 epoch total loss 9888.83496\n",
      "Trained batch 978 batch loss 55.1109238 epoch total loss 9878.78\n",
      "Trained batch 979 batch loss 49.1165314 epoch total loss 9868.73926\n",
      "Trained batch 980 batch loss 41.2197647 epoch total loss 9858.71094\n",
      "Trained batch 981 batch loss 47.0620956 epoch total loss 9848.71\n",
      "Trained batch 982 batch loss 43.7971344 epoch total loss 9838.72461\n",
      "Trained batch 983 batch loss 40.0507774 epoch total loss 9828.75684\n",
      "Trained batch 984 batch loss 24.4489079 epoch total loss 9818.79297\n",
      "Trained batch 985 batch loss 22.5174789 epoch total loss 9808.84766\n",
      "Trained batch 986 batch loss 21.4689846 epoch total loss 9798.9209\n",
      "Trained batch 987 batch loss 19.0681572 epoch total loss 9789.01172\n",
      "Trained batch 988 batch loss 14.8739748 epoch total loss 9779.11914\n",
      "Trained batch 989 batch loss 14.7189007 epoch total loss 9769.24707\n",
      "Trained batch 990 batch loss 15.8637276 epoch total loss 9759.39453\n",
      "Trained batch 1129 batch loss 20.9769936 epoch total loss 8559.53125\n",
      "Trained batch 1130 batch loss 21.0727577 epoch total loss 8551.97559\n",
      "Trained batch 1131 batch loss 21.1391068 epoch total loss 8544.43262\n",
      "Trained batch 1132 batch loss 21.2700253 epoch total loss 8536.90234\n",
      "Trained batch 1133 batch loss 21.5779209 epoch total loss 8529.3877\n",
      "Trained batch 1134 batch loss 21.4632893 epoch total loss 8521.88477\n",
      "Trained batch 1135 batch loss 21.0003815 epoch total loss 8514.39453\n",
      "Trained batch 1136 batch loss 20.9581795 epoch total loss 8506.91797\n",
      "Trained batch 1137 batch loss 20.6821594 epoch total loss 8499.45508\n",
      "Trained batch 1138 batch loss 19.9547882 epoch total loss 8492.00391\n",
      "Trained batch 1139 batch loss 20.5512962 epoch total loss 8484.56641\n",
      "Trained batch 1140 batch loss 20.2055874 epoch total loss 8477.1416\n",
      "Trained batch 1141 batch loss 19.3819523 epoch total loss 8469.72852\n",
      "Trained batch 1142 batch loss 19.7866573 epoch total loss 8462.3291\n",
      "Trained batch 1143 batch loss 19.1175175 epoch total loss 8454.94238\n",
      "Trained batch 1144 batch loss 19.4140453 epoch total loss 8447.56836\n",
      "Trained batch 1145 batch loss 19.2205696 epoch total loss 8440.20703\n",
      "Trained batch 1146 batch loss 24.5339527 epoch total loss 8432.86426\n",
      "Trained batch 1147 batch loss 20.778347 epoch total loss 8425.53\n",
      "Trained batch 1148 batch loss 19.3844013 epoch total loss 8418.20703\n",
      "Trained batch 1149 batch loss 19.679491 epoch total loss 8410.89844\n",
      "Trained batch 1150 batch loss 21.6703033 epoch total loss 8403.60352\n",
      "Trained batch 1151 batch loss 20.0629139 epoch total loss 8396.31934\n",
      "Trained batch 1152 batch loss 19.3912849 epoch total loss 8389.04785\n",
      "Trained batch 1153 batch loss 21.1048565 epoch total loss 8381.79\n",
      "Trained batch 1154 batch loss 19.0123119 epoch total loss 8374.54297\n",
      "Trained batch 1155 batch loss 19.2095013 epoch total loss 8367.31\n",
      "Trained batch 1156 batch loss 19.5936394 epoch total loss 8360.08789\n",
      "Trained batch 1157 batch loss 19.2808762 epoch total loss 8352.87891\n",
      "Trained batch 1158 batch loss 19.3289623 epoch total loss 8345.68262\n",
      "Trained batch 1159 batch loss 19.0234699 epoch total loss 8338.49805\n",
      "Trained batch 1160 batch loss 19.1281223 epoch total loss 8331.32617\n",
      "Trained batch 1161 batch loss 18.7353916 epoch total loss 8324.16602\n",
      "Trained batch 1162 batch loss 19.0683613 epoch total loss 8317.01855\n",
      "Trained batch 1163 batch loss 19.1509323 epoch total loss 8309.88379\n",
      "Trained batch 1164 batch loss 19.021244 epoch total loss 8302.76074\n",
      "Trained batch 1165 batch loss 19.079546 epoch total loss 8295.65\n",
      "Trained batch 1166 batch loss 19.4284649 epoch total loss 8288.55273\n",
      "Trained batch 1167 batch loss 19.5317936 epoch total loss 8281.4668\n",
      "Trained batch 1168 batch loss 19.032711 epoch total loss 8274.39258\n",
      "Trained batch 1169 batch loss 18.8652477 epoch total loss 8267.33105\n",
      "Trained batch 1170 batch loss 19.2347908 epoch total loss 8260.28125\n",
      "Trained batch 1171 batch loss 19.0329533 epoch total loss 8253.24316\n",
      "Trained batch 1172 batch loss 18.9513817 epoch total loss 8246.21777\n",
      "Trained batch 1173 batch loss 19.1049328 epoch total loss 8239.2041\n",
      "Trained batch 1174 batch loss 19.0770721 epoch total loss 8232.20215\n",
      "Trained batch 1175 batch loss 19.1417618 epoch total loss 8225.21191\n",
      "Trained batch 1176 batch loss 19.0338802 epoch total loss 8218.2334\n",
      "Trained batch 1177 batch loss 19.1670055 epoch total loss 8211.26758\n",
      "Trained batch 1178 batch loss 18.9962921 epoch total loss 8204.31348\n",
      "Trained batch 1179 batch loss 18.7183228 epoch total loss 8197.37109\n",
      "Trained batch 1180 batch loss 18.571146 epoch total loss 8190.44\n",
      "Trained batch 1181 batch loss 19.011694 epoch total loss 8183.52051\n",
      "Trained batch 1182 batch loss 19.0954037 epoch total loss 8176.61328\n",
      "Trained batch 1183 batch loss 19.2435131 epoch total loss 8169.71777\n",
      "Trained batch 1184 batch loss 18.9062023 epoch total loss 8162.8335\n",
      "Trained batch 1185 batch loss 19.0823689 epoch total loss 8155.96094\n",
      "Trained batch 1186 batch loss 18.4134808 epoch total loss 8149.09961\n",
      "Trained batch 1187 batch loss 19.1046238 epoch total loss 8142.25\n",
      "Trained batch 1188 batch loss 19.2504463 epoch total loss 8135.4126\n",
      "Trained batch 1189 batch loss 19.2596321 epoch total loss 8128.58643\n",
      "Trained batch 1190 batch loss 18.8517628 epoch total loss 8121.77148\n",
      "Trained batch 1191 batch loss 18.6716251 epoch total loss 8114.96826\n",
      "Trained batch 1192 batch loss 18.9327145 epoch total loss 8108.17627\n",
      "Trained batch 1193 batch loss 18.9856682 epoch total loss 8101.39551\n",
      "Trained batch 1194 batch loss 19.0143108 epoch total loss 8094.62646\n",
      "Trained batch 1195 batch loss 19.1618423 epoch total loss 8087.86865\n",
      "Trained batch 1196 batch loss 19.0577126 epoch total loss 8081.12207\n",
      "Trained batch 1197 batch loss 18.6512527 epoch total loss 8074.38672\n",
      "Trained batch 1198 batch loss 18.3865166 epoch total loss 8067.66211\n",
      "Trained batch 1199 batch loss 19.1349373 epoch total loss 8060.94922\n",
      "Trained batch 1200 batch loss 18.9888248 epoch total loss 8054.24756\n",
      "Trained batch 1201 batch loss 18.9409466 epoch total loss 8047.55713\n",
      "Trained batch 1202 batch loss 19.0066528 epoch total loss 8040.87793\n",
      "Trained batch 1203 batch loss 19.0180264 epoch total loss 8034.20947\n",
      "Trained batch 1204 batch loss 18.5685692 epoch total loss 8027.55225\n",
      "Trained batch 1205 batch loss 18.7566319 epoch total loss 8020.90625\n",
      "Trained batch 1206 batch loss 19.2659874 epoch total loss 8014.271\n",
      "Trained batch 1207 batch loss 19.3011189 epoch total loss 8007.64697\n",
      "Trained batch 1208 batch loss 19.4719791 epoch total loss 8001.03418\n",
      "Trained batch 1209 batch loss 19.5245266 epoch total loss 7994.43262\n",
      "Trained batch 1210 batch loss 19.6142712 epoch total loss 7987.84229\n",
      "Trained batch 1211 batch loss 19.5415459 epoch total loss 7981.2627\n",
      "Trained batch 1212 batch loss 19.5440464 epoch total loss 7974.69385\n",
      "Trained batch 1213 batch loss 19.5686493 epoch total loss 7968.13623\n",
      "Trained batch 1214 batch loss 19.6276455 epoch total loss 7961.58887\n",
      "Trained batch 1215 batch loss 19.8000107 epoch total loss 7955.05273\n",
      "Trained batch 1216 batch loss 19.6310673 epoch total loss 7948.52734\n",
      "Trained batch 1217 batch loss 18.8826027 epoch total loss 7942.01172\n",
      "Trained batch 1218 batch loss 18.575016 epoch total loss 7935.50635\n",
      "Trained batch 1219 batch loss 19.1347389 epoch total loss 7929.01221\n",
      "Trained batch 1220 batch loss 19.6011543 epoch total loss 7922.5293\n",
      "Trained batch 1221 batch loss 19.606945 epoch total loss 7916.05713\n",
      "Trained batch 1222 batch loss 19.5663834 epoch total loss 7909.5957\n",
      "Trained batch 1223 batch loss 19.8024616 epoch total loss 7903.14453\n",
      "Trained batch 1224 batch loss 19.4003563 epoch total loss 7896.70361\n",
      "Trained batch 1225 batch loss 19.9365387 epoch total loss 7890.27344\n",
      "Trained batch 1226 batch loss 19.5886459 epoch total loss 7883.854\n",
      "Trained batch 1227 batch loss 19.268919 epoch total loss 7877.44434\n",
      "Trained batch 1228 batch loss 19.4847946 epoch total loss 7871.04492\n",
      "Trained batch 1229 batch loss 19.8352947 epoch total loss 7864.65674\n",
      "Trained batch 1230 batch loss 19.6410389 epoch total loss 7858.27881\n",
      "Trained batch 1231 batch loss 19.8433819 epoch total loss 7851.91162\n",
      "Trained batch 1232 batch loss 19.7422409 epoch total loss 7845.5542\n",
      "Trained batch 1233 batch loss 19.7587452 epoch total loss 7839.20752\n",
      "Trained batch 1234 batch loss 20.0757866 epoch total loss 7832.87109\n",
      "Trained batch 1235 batch loss 20.1067314 epoch total loss 7826.54492\n",
      "Trained batch 1236 batch loss 19.9843102 epoch total loss 7820.229\n",
      "Trained batch 1237 batch loss 19.7360268 epoch total loss 7813.92334\n",
      "Trained batch 1238 batch loss 19.7244415 epoch total loss 7807.62744\n",
      "Trained batch 1239 batch loss 19.5892715 epoch total loss 7801.34229\n",
      "Trained batch 1240 batch loss 19.5620136 epoch total loss 7795.06689\n",
      "Trained batch 1241 batch loss 19.3579502 epoch total loss 7788.80078\n",
      "Trained batch 1242 batch loss 20.1884308 epoch total loss 7782.5459\n",
      "Trained batch 1243 batch loss 20.9498196 epoch total loss 7776.30176\n",
      "Trained batch 1244 batch loss 21.8512688 epoch total loss 7770.06836\n",
      "Trained batch 1245 batch loss 22.5391808 epoch total loss 7763.8457\n",
      "Trained batch 1246 batch loss 23.4103661 epoch total loss 7757.6333\n",
      "Trained batch 1247 batch loss 24.8598404 epoch total loss 7751.43213\n",
      "Trained batch 1248 batch loss 25.7731209 epoch total loss 7745.24219\n",
      "Trained batch 1249 batch loss 27.2326927 epoch total loss 7739.0625\n",
      "Trained batch 1250 batch loss 29.2723484 epoch total loss 7732.89453\n",
      "Trained batch 1251 batch loss 30.9980774 epoch total loss 7726.73779\n",
      "Trained batch 1252 batch loss 32.2951698 epoch total loss 7720.5918\n",
      "Trained batch 1253 batch loss 34.0999107 epoch total loss 7714.45752\n",
      "Trained batch 1254 batch loss 35.1616402 epoch total loss 7708.3335\n",
      "Trained batch 1255 batch loss 36.3517075 epoch total loss 7702.21973\n",
      "Trained batch 1256 batch loss 36.8579178 epoch total loss 7696.11719\n",
      "Trained batch 1257 batch loss 39.133934 epoch total loss 7690.02539\n",
      "Trained batch 1258 batch loss 40.1465263 epoch total loss 7683.94434\n",
      "Trained batch 1259 batch loss 42.178463 epoch total loss 7677.87451\n",
      "Trained batch 1260 batch loss 43.2422256 epoch total loss 7671.81494\n",
      "Trained batch 1261 batch loss 44.5395737 epoch total loss 7665.76709\n",
      "Trained batch 1262 batch loss 46.504 epoch total loss 7659.73\n",
      "Trained batch 1263 batch loss 48.5775146 epoch total loss 7653.7041\n",
      "Trained batch 1264 batch loss 50.3613548 epoch total loss 7647.68848\n",
      "Trained batch 1265 batch loss 51.6764 epoch total loss 7641.68359\n",
      "Trained batch 1266 batch loss 51.7598457 epoch total loss 7635.68896\n",
      "Trained batch 1267 batch loss 51.7735558 epoch total loss 7629.70312\n",
      "Trained batch 1268 batch loss 54.1563 epoch total loss 7623.72852\n",
      "Trained batch 1269 batch loss 54.6851616 epoch total loss 7617.76416\n",
      "Trained batch 1270 batch loss 56.5500259 epoch total loss 7611.81104\n",
      "Trained batch 1271 batch loss 59.1566162 epoch total loss 7605.86865\n",
      "Trained batch 1272 batch loss 60.0155716 epoch total loss 7599.93652\n",
      "Trained batch 1273 batch loss 61.8353577 epoch total loss 7594.01514\n",
      "Trained batch 1274 batch loss 63.2794609 epoch total loss 7588.10352\n",
      "Trained batch 1275 batch loss 64.7242584 epoch total loss 7582.20312\n",
      "Trained batch 1276 batch loss 66.9792 epoch total loss 7576.31348\n",
      "Trained batch 1277 batch loss 69.2985 epoch total loss 7570.43457\n",
      "Trained batch 1278 batch loss 71.2389145 epoch total loss 7564.56641\n",
      "Trained batch 1279 batch loss 72.3463821 epoch total loss 7558.7085\n",
      "Trained batch 1280 batch loss 74.6353073 epoch total loss 7552.86182\n",
      "Trained batch 1281 batch loss 78.3103714 epoch total loss 7547.02637\n",
      "Trained batch 1282 batch loss 81.1326294 epoch total loss 7541.20264\n",
      "Trained batch 1283 batch loss 83.1818314 epoch total loss 7535.38965\n",
      "Trained batch 1284 batch loss 88.8841171 epoch total loss 7529.59033\n",
      "Trained batch 1285 batch loss 91.764946 epoch total loss 7523.80225\n",
      "Trained batch 1286 batch loss 92.6803665 epoch total loss 7518.02393\n",
      "Trained batch 1287 batch loss 97.7318344 epoch total loss 7512.25879\n",
      "Trained batch 1288 batch loss 99.7080383 epoch total loss 7506.50391\n",
      "Trained batch 1289 batch loss 101.048714 epoch total loss 7500.75879\n",
      "Trained batch 1290 batch loss 106.556381 epoch total loss 7495.02734\n",
      "Trained batch 1291 batch loss 106.085724 epoch total loss 7489.30371\n",
      "Trained batch 1292 batch loss 112.258743 epoch total loss 7483.59375\n",
      "Trained batch 1293 batch loss 113.67054 epoch total loss 7477.89404\n",
      "Trained batch 1294 batch loss 117.243584 epoch total loss 7472.20557\n",
      "Trained batch 1295 batch loss 118.326004 epoch total loss 7466.52686\n",
      "Trained batch 1296 batch loss 122.157 epoch total loss 7460.85938\n",
      "Trained batch 1297 batch loss 124.78231 epoch total loss 7455.20361\n",
      "Trained batch 1298 batch loss 127.251213 epoch total loss 7449.55762\n",
      "Trained batch 1299 batch loss 130.147125 epoch total loss 7443.92285\n",
      "Trained batch 1300 batch loss 132.274841 epoch total loss 7438.29834\n",
      "Trained batch 1301 batch loss 135.00502 epoch total loss 7432.68506\n",
      "Trained batch 1302 batch loss 137.962357 epoch total loss 7427.08203\n",
      "Trained batch 1303 batch loss 141.117432 epoch total loss 7421.49\n",
      "Trained batch 1304 batch loss 145.63385 epoch total loss 7415.91113\n",
      "Trained batch 1305 batch loss 149.03833 epoch total loss 7410.34229\n",
      "Trained batch 1306 batch loss 151.350143 epoch total loss 7404.78418\n",
      "Trained batch 1307 batch loss 155.234192 epoch total loss 7399.2373\n",
      "Trained batch 1308 batch loss 156.839706 epoch total loss 7393.7\n",
      "Trained batch 1309 batch loss 158.250961 epoch total loss 7388.17285\n",
      "Trained batch 1310 batch loss 162.482513 epoch total loss 7382.65625\n",
      "Trained batch 1311 batch loss 163.786301 epoch total loss 7377.15039\n",
      "Trained batch 1312 batch loss 163.45253 epoch total loss 7371.65186\n",
      "Trained batch 1313 batch loss 164.808792 epoch total loss 7366.16309\n",
      "Trained batch 1314 batch loss 165.529419 epoch total loss 7360.68359\n",
      "Trained batch 1315 batch loss 166.81041 epoch total loss 7355.21289\n",
      "Trained batch 1316 batch loss 167.661667 epoch total loss 7349.75146\n",
      "Trained batch 1317 batch loss 175.326767 epoch total loss 7344.30371\n",
      "Trained batch 1318 batch loss 178.83 epoch total loss 7338.86719\n",
      "Trained batch 1319 batch loss 187.512405 epoch total loss 7333.4458\n",
      "Trained batch 1320 batch loss 191.882874 epoch total loss 7328.03564\n",
      "Trained batch 1321 batch loss 197.062897 epoch total loss 7322.63721\n",
      "Trained batch 1322 batch loss 204.947357 epoch total loss 7317.25342\n",
      "Trained batch 1323 batch loss 213.136627 epoch total loss 7311.88379\n",
      "Trained batch 1324 batch loss 223.973282 epoch total loss 7306.53027\n",
      "Trained batch 1325 batch loss 233.833557 epoch total loss 7301.19238\n",
      "Trained batch 1326 batch loss 242.546127 epoch total loss 7295.86963\n",
      "Trained batch 1327 batch loss 252.376297 epoch total loss 7290.56152\n",
      "Trained batch 1328 batch loss 261.292969 epoch total loss 7285.26807\n",
      "Trained batch 1329 batch loss 270.597839 epoch total loss 7279.99\n",
      "Trained batch 1330 batch loss 281.007172 epoch total loss 7274.72803\n",
      "Trained batch 1331 batch loss 289.875519 epoch total loss 7269.48\n",
      "Trained batch 1332 batch loss 293.56427 epoch total loss 7264.24316\n",
      "Trained batch 1333 batch loss 308.059937 epoch total loss 7259.0249\n",
      "Trained batch 1334 batch loss 317.976685 epoch total loss 7253.82178\n",
      "Trained batch 1335 batch loss 329.744904 epoch total loss 7248.63525\n",
      "Trained batch 1336 batch loss 340.194061 epoch total loss 7243.46387\n",
      "Trained batch 1337 batch loss 350.72641 epoch total loss 7238.30908\n",
      "Trained batch 1338 batch loss 352.057617 epoch total loss 7233.16211\n",
      "Trained batch 1339 batch loss 368.902069 epoch total loss 7228.03564\n",
      "Trained batch 1340 batch loss 373.989655 epoch total loss 7222.9209\n",
      "Trained batch 1341 batch loss 383.848206 epoch total loss 7217.8208\n",
      "Trained batch 1342 batch loss 396.386749 epoch total loss 7212.73779\n",
      "Trained batch 1343 batch loss 407.637299 epoch total loss 7207.6709\n",
      "Trained batch 1344 batch loss 421.782471 epoch total loss 7202.62207\n",
      "Trained batch 1345 batch loss 434.571411 epoch total loss 7197.59033\n",
      "Trained batch 1346 batch loss 444.393036 epoch total loss 7192.57275\n",
      "Trained batch 1347 batch loss 460.827881 epoch total loss 7187.5752\n",
      "Trained batch 1348 batch loss 474.684052 epoch total loss 7182.5957\n",
      "Trained batch 1349 batch loss 490.798584 epoch total loss 7177.63525\n",
      "Trained batch 1350 batch loss 506.075592 epoch total loss 7172.69336\n",
      "Trained batch 1351 batch loss 514.661804 epoch total loss 7167.76514\n",
      "Trained batch 1352 batch loss 524.809143 epoch total loss 7162.85205\n",
      "Trained batch 1353 batch loss 547.852539 epoch total loss 7157.96289\n",
      "Trained batch 1354 batch loss 561.39386 epoch total loss 7153.09082\n",
      "Trained batch 1355 batch loss 573.780457 epoch total loss 7148.23535\n",
      "Trained batch 1356 batch loss 602.682129 epoch total loss 7143.40869\n",
      "Trained batch 1357 batch loss 601.877197 epoch total loss 7138.58789\n",
      "Trained batch 1358 batch loss 629.307861 epoch total loss 7133.79443\n",
      "Trained batch 1359 batch loss 657.807373 epoch total loss 7129.0293\n",
      "Trained batch 1360 batch loss 681.82312 epoch total loss 7124.28906\n",
      "Trained batch 1361 batch loss 706.934631 epoch total loss 7119.57373\n",
      "Trained batch 1362 batch loss 725.44751 epoch total loss 7114.87891\n",
      "Trained batch 1363 batch loss 756.003 epoch total loss 7110.21338\n",
      "Trained batch 1364 batch loss 776.783 epoch total loss 7105.57031\n",
      "Trained batch 1365 batch loss 803.92627 epoch total loss 7100.95361\n",
      "Trained batch 1366 batch loss 824.634 epoch total loss 7096.35938\n",
      "Trained batch 1367 batch loss 838.030212 epoch total loss 7091.78125\n",
      "Trained batch 1368 batch loss 867.440308 epoch total loss 7087.23096\n",
      "Trained batch 1369 batch loss 898.292 epoch total loss 7082.71\n",
      "Trained batch 1370 batch loss 925.754578 epoch total loss 7078.21582\n",
      "Trained batch 1371 batch loss 954.843323 epoch total loss 7073.75\n",
      "Trained batch 1372 batch loss 979.118469 epoch total loss 7069.30762\n",
      "Trained batch 1373 batch loss 1008.16748 epoch total loss 7064.89307\n",
      "Trained batch 1374 batch loss 1040.90491 epoch total loss 7060.50879\n",
      "Trained batch 1375 batch loss 1067.66589 epoch total loss 7056.15039\n",
      "Trained batch 1376 batch loss 1090.00598 epoch total loss 7051.81445\n",
      "Trained batch 1377 batch loss 1119.09216 epoch total loss 7047.50635\n",
      "Trained batch 1378 batch loss 1156.21252 epoch total loss 7043.23096\n",
      "Trained batch 1379 batch loss 1173.92749 epoch total loss 7038.97461\n",
      "Trained batch 1380 batch loss 1213.04 epoch total loss 7034.75293\n",
      "Trained batch 1381 batch loss 1238.99939 epoch total loss 7030.55615\n",
      "Trained batch 1382 batch loss 1271.12402 epoch total loss 7026.38867\n",
      "Trained batch 1383 batch loss 1291.26367 epoch total loss 7022.2417\n",
      "Trained batch 1384 batch loss 1304.04102 epoch total loss 7018.11\n",
      "Trained batch 1385 batch loss 1360.15393 epoch total loss 7014.02441\n",
      "Trained batch 1386 batch loss 1380.65161 epoch total loss 7009.96045\n",
      "Trained batch 1387 batch loss 1428.89563 epoch total loss 7005.93652\n",
      "Trained batch 1388 batch loss 1456.96924 epoch total loss 7001.93896\n",
      "Epoch 9 train loss 7001.93896484375\n",
      "Validated batch 1 batch loss 7.43806042e+29\n",
      "Validated batch 2 batch loss 7.11251256e+29\n",
      "Validated batch 3 batch loss 7.1301455e+29\n",
      "Validated batch 4 batch loss 7.54473e+29\n",
      "Validated batch 5 batch loss 7.52107056e+29\n",
      "Validated batch 6 batch loss 7.39356666e+29\n",
      "Validated batch 7 batch loss 7.4876634e+29\n",
      "Validated batch 8 batch loss 7.62396677e+29\n",
      "Validated batch 9 batch loss 7.52048121e+29\n",
      "Validated batch 10 batch loss 7.2525772e+29\n",
      "Validated batch 11 batch loss 7.49607148e+29\n",
      "Validated batch 12 batch loss 7.29396704e+29\n",
      "Validated batch 13 batch loss 7.13894573e+29\n",
      "Validated batch 14 batch loss 7.3823025e+29\n",
      "Validated batch 15 batch loss 7.28296884e+29\n",
      "Validated batch 16 batch loss 7.33750726e+29\n",
      "Validated batch 17 batch loss 6.98883719e+29\n",
      "Validated batch 18 batch loss 7.42773846e+29\n",
      "Validated batch 19 batch loss 7.36626912e+29\n",
      "Validated batch 20 batch loss 7.38959458e+29\n",
      "Validated batch 21 batch loss 7.48166335e+29\n",
      "Validated batch 22 batch loss 7.51126e+29\n",
      "Validated batch 23 batch loss 7.49360905e+29\n",
      "Validated batch 24 batch loss 7.16815942e+29\n",
      "Validated batch 25 batch loss 7.36938134e+29\n",
      "Validated batch 26 batch loss 7.3728827e+29\n",
      "Validated batch 27 batch loss 7.39626105e+29\n",
      "Validated batch 28 batch loss 7.47676267e+29\n",
      "Validated batch 29 batch loss 7.73994582e+29\n",
      "Validated batch 30 batch loss 7.27491059e+29\n",
      "Validated batch 31 batch loss 7.09727557e+29\n",
      "Validated batch 32 batch loss 7.43101238e+29\n",
      "Validated batch 33 batch loss 7.27220411e+29\n",
      "Validated batch 34 batch loss 7.64715321e+29\n",
      "Validated batch 35 batch loss 7.27572435e+29\n",
      "Validated batch 36 batch loss 7.20978651e+29\n",
      "Validated batch 37 batch loss 7.59277648e+29\n",
      "Validated batch 38 batch loss 7.25851151e+29\n",
      "Validated batch 39 batch loss 7.47611816e+29\n",
      "Validated batch 40 batch loss 7.37226312e+29\n",
      "Validated batch 41 batch loss 7.25476233e+29\n",
      "Validated batch 42 batch loss 7.54771906e+29\n",
      "Validated batch 43 batch loss 7.34186922e+29\n",
      "Validated batch 44 batch loss 7.24910532e+29\n",
      "Validated batch 45 batch loss 7.53687726e+29\n",
      "Validated batch 46 batch loss 7.64636136e+29\n",
      "Validated batch 47 batch loss 7.40837e+29\n",
      "Validated batch 48 batch loss 7.68755853e+29\n",
      "Validated batch 49 batch loss 7.48072417e+29\n",
      "Validated batch 50 batch loss 7.23830054e+29\n",
      "Validated batch 51 batch loss 7.20589e+29\n",
      "Validated batch 52 batch loss 7.54326417e+29\n",
      "Validated batch 53 batch loss 7.36921663e+29\n",
      "Validated batch 54 batch loss 7.27969567e+29\n",
      "Validated batch 55 batch loss 7.33059145e+29\n",
      "Validated batch 56 batch loss 7.58124e+29\n",
      "Validated batch 57 batch loss 7.47618087e+29\n",
      "Validated batch 58 batch loss 7.00577046e+29\n",
      "Validated batch 59 batch loss 7.60165528e+29\n",
      "Validated batch 60 batch loss 7.33552462e+29\n",
      "Validated batch 61 batch loss 7.38984846e+29\n",
      "Validated batch 62 batch loss 7.051792e+29\n",
      "Validated batch 63 batch loss 7.57734756e+29\n",
      "Validated batch 64 batch loss 7.43610498e+29\n",
      "Validated batch 65 batch loss 7.4403e+29\n",
      "Validated batch 66 batch loss 7.41980564e+29\n",
      "Validated batch 67 batch loss 7.42123671e+29\n",
      "Validated batch 68 batch loss 7.16339172e+29\n",
      "Validated batch 69 batch loss 7.67497286e+29\n",
      "Validated batch 70 batch loss 7.27074131e+29\n",
      "Validated batch 71 batch loss 6.76515e+29\n",
      "Validated batch 72 batch loss 7.3408983e+29\n",
      "Validated batch 73 batch loss 7.41537342e+29\n",
      "Validated batch 74 batch loss 7.28750382e+29\n",
      "Validated batch 75 batch loss 7.49737334e+29\n",
      "Validated batch 76 batch loss 7.06157825e+29\n",
      "Validated batch 77 batch loss 7.40919505e+29\n",
      "Validated batch 78 batch loss 7.37232e+29\n",
      "Validated batch 79 batch loss 7.40549951e+29\n",
      "Validated batch 80 batch loss 7.39526067e+29\n",
      "Validated batch 81 batch loss 7.45993064e+29\n",
      "Validated batch 82 batch loss 7.52504414e+29\n",
      "Validated batch 83 batch loss 7.31590753e+29\n",
      "Validated batch 84 batch loss 7.46774862e+29\n",
      "Validated batch 85 batch loss 7.3439395e+29\n",
      "Validated batch 86 batch loss 7.4853808e+29\n",
      "Validated batch 87 batch loss 7.09645652e+29\n",
      "Validated batch 88 batch loss 7.22878176e+29\n",
      "Validated batch 89 batch loss 6.9391662e+29\n",
      "Validated batch 90 batch loss 7.53253571e+29\n",
      "Validated batch 91 batch loss 7.18833488e+29\n",
      "Validated batch 92 batch loss 7.51762512e+29\n",
      "Validated batch 93 batch loss 7.47303842e+29\n",
      "Validated batch 94 batch loss 7.25253715e+29\n",
      "Validated batch 95 batch loss 7.47913065e+29\n",
      "Validated batch 96 batch loss 7.14217432e+29\n",
      "Validated batch 97 batch loss 7.46091743e+29\n",
      "Validated batch 98 batch loss 7.11803207e+29\n",
      "Validated batch 99 batch loss 7.02318579e+29\n",
      "Validated batch 100 batch loss 7.2691153e+29\n",
      "Validated batch 101 batch loss 7.64760127e+29\n",
      "Validated batch 102 batch loss 7.26801594e+29\n",
      "Validated batch 103 batch loss 7.42345e+29\n",
      "Validated batch 104 batch loss 7.36112e+29\n",
      "Validated batch 105 batch loss 7.32108778e+29\n",
      "Validated batch 106 batch loss 7.47249441e+29\n",
      "Validated batch 107 batch loss 7.35613e+29\n",
      "Validated batch 108 batch loss 7.30438043e+29\n",
      "Validated batch 109 batch loss 7.01922e+29\n",
      "Validated batch 110 batch loss 7.34025152e+29\n",
      "Validated batch 111 batch loss 7.18179837e+29\n",
      "Validated batch 112 batch loss 7.35022365e+29\n",
      "Validated batch 113 batch loss 7.46974259e+29\n",
      "Validated batch 114 batch loss 7.43041925e+29\n",
      "Validated batch 115 batch loss 7.52082424e+29\n",
      "Validated batch 116 batch loss 7.88351181e+29\n",
      "Validated batch 117 batch loss 7.4108271e+29\n",
      "Validated batch 118 batch loss 7.51495e+29\n",
      "Validated batch 119 batch loss 7.48131e+29\n",
      "Validated batch 120 batch loss 7.14767115e+29\n",
      "Validated batch 121 batch loss 7.47097191e+29\n",
      "Validated batch 122 batch loss 7.32673875e+29\n",
      "Validated batch 123 batch loss 7.39881642e+29\n",
      "Validated batch 124 batch loss 7.28061068e+29\n",
      "Validated batch 125 batch loss 7.33092315e+29\n",
      "Validated batch 126 batch loss 7.12243558e+29\n",
      "Validated batch 127 batch loss 7.12199281e+29\n",
      "Validated batch 128 batch loss 7.21433888e+29\n",
      "Validated batch 129 batch loss 7.10247e+29\n",
      "Validated batch 130 batch loss 7.19705501e+29\n",
      "Validated batch 131 batch loss 7.18026e+29\n",
      "Validated batch 132 batch loss 7.50326081e+29\n",
      "Validated batch 133 batch loss 7.17751877e+29\n",
      "Validated batch 134 batch loss 7.31937564e+29\n",
      "Validated batch 135 batch loss 7.68637378e+29\n",
      "Validated batch 136 batch loss 7.48524857e+29\n",
      "Validated batch 137 batch loss 7.16404076e+29\n",
      "Validated batch 138 batch loss 7.08801142e+29\n",
      "Validated batch 139 batch loss 7.17699516e+29\n",
      "Validated batch 140 batch loss 7.29497121e+29\n",
      "Validated batch 141 batch loss 7.03593e+29\n",
      "Validated batch 142 batch loss 7.07690517e+29\n",
      "Validated batch 143 batch loss 7.43940157e+29\n",
      "Validated batch 144 batch loss 7.56921376e+29\n",
      "Validated batch 145 batch loss 7.13414327e+29\n",
      "Validated batch 146 batch loss 7.25881072e+29\n",
      "Validated batch 147 batch loss 7.34127382e+29\n",
      "Validated batch 148 batch loss 7.40736126e+29\n",
      "Validated batch 149 batch loss 7.08848e+29\n",
      "Validated batch 150 batch loss 7.16818e+29\n",
      "Validated batch 151 batch loss 7.11344268e+29\n",
      "Validated batch 152 batch loss 6.86582898e+29\n",
      "Validated batch 153 batch loss 7.40779118e+29\n",
      "Validated batch 154 batch loss 7.35142124e+29\n",
      "Validated batch 155 batch loss 7.28408936e+29\n",
      "Validated batch 156 batch loss 6.93006148e+29\n",
      "Validated batch 157 batch loss 7.33065416e+29\n",
      "Validated batch 158 batch loss 7.18081612e+29\n",
      "Validated batch 159 batch loss 7.35447227e+29\n",
      "Validated batch 160 batch loss 7.30363543e+29\n",
      "Validated batch 161 batch loss 7.18265595e+29\n",
      "Validated batch 162 batch loss 7.16013366e+29\n",
      "Validated batch 163 batch loss 7.17436877e+29\n",
      "Validated batch 164 batch loss 7.29545704e+29\n",
      "Validated batch 165 batch loss 7.31761e+29\n",
      "Validated batch 166 batch loss 7.21660561e+29\n",
      "Validated batch 167 batch loss 7.17267e+29\n",
      "Validated batch 168 batch loss 7.09550373e+29\n",
      "Validated batch 169 batch loss 7.38863349e+29\n",
      "Validated batch 170 batch loss 7.34814959e+29\n",
      "Validated batch 171 batch loss 7.39900607e+29\n",
      "Validated batch 172 batch loss 7.29837206e+29\n",
      "Validated batch 173 batch loss 7.43124e+29\n",
      "Validated batch 174 batch loss 7.51099416e+29\n",
      "Validated batch 175 batch loss 7.3668426e+29\n",
      "Validated batch 176 batch loss 7.22596345e+29\n",
      "Validated batch 177 batch loss 7.32649697e+29\n",
      "Validated batch 178 batch loss 7.51138253e+29\n",
      "Validated batch 179 batch loss 7.38433e+29\n",
      "Validated batch 180 batch loss 7.60057405e+29\n",
      "Validated batch 181 batch loss 7.59997866e+29\n",
      "Validated batch 182 batch loss 7.39138682e+29\n",
      "Validated batch 183 batch loss 7.38403277e+29\n",
      "Validated batch 184 batch loss 7.54861291e+29\n",
      "Validated batch 185 batch loss 7.42201042e+29\n",
      "Epoch 9 val loss 7.341753612015035e+29\n",
      "Start epoch 10 with learning rate 0.5\n",
      "Start distributed traininng...\n",
      "Trained batch 1 batch loss 1482.36194 epoch total loss 1482.36194\n",
      "Trained batch 2 batch loss 1518.64478 epoch total loss 1500.50342\n",
      "Trained batch 3 batch loss 1524.14307 epoch total loss 1508.3833\n",
      "Trained batch 4 batch loss 1564.0979 epoch total loss 1522.31201\n",
      "Trained batch 5 batch loss 1589.14062 epoch total loss 1535.67773\n",
      "Trained batch 6 batch loss 1622.87622 epoch total loss 1550.21082\n",
      "Trained batch 7 batch loss 1646.55 epoch total loss 1563.97351\n",
      "Trained batch 8 batch loss 1679.19934 epoch total loss 1578.37671\n",
      "Trained batch 9 batch loss 1724.60022 epoch total loss 1594.62378\n",
      "Trained batch 10 batch loss 1762.60083 epoch total loss 1611.42151\n",
      "Trained batch 11 batch loss 1788.3103 epoch total loss 1627.50232\n",
      "Trained batch 12 batch loss 1823.20984 epoch total loss 1643.81116\n",
      "Trained batch 13 batch loss 1885.57849 epoch total loss 1662.40869\n",
      "Trained batch 14 batch loss 1925.39819 epoch total loss 1681.1936\n",
      "Trained batch 15 batch loss 1967.01526 epoch total loss 1700.24841\n",
      "Trained batch 16 batch loss 2005.44421 epoch total loss 1719.32312\n",
      "Trained batch 17 batch loss 2040.63171 epoch total loss 1738.22363\n",
      "Trained batch 18 batch loss 2073.54248 epoch total loss 1756.85242\n",
      "Trained batch 19 batch loss 2121.70337 epoch total loss 1776.05505\n",
      "Trained batch 20 batch loss 2150.31616 epoch total loss 1794.76819\n",
      "Trained batch 21 batch loss 2194.24268 epoch total loss 1813.79077\n",
      "Trained batch 22 batch loss 2199.43945 epoch total loss 1831.32031\n",
      "Trained batch 23 batch loss 2242.76221 epoch total loss 1849.20911\n",
      "Trained batch 24 batch loss 2293.01685 epoch total loss 1867.70105\n",
      "Trained batch 25 batch loss 2326.00659 epoch total loss 1886.03333\n",
      "Trained batch 26 batch loss 2350.34937 epoch total loss 1903.89148\n",
      "Trained batch 27 batch loss 2381.94043 epoch total loss 1921.59705\n",
      "Trained batch 28 batch loss 2399.06445 epoch total loss 1938.64954\n",
      "Trained batch 29 batch loss 2451.72192 epoch total loss 1956.34167\n",
      "Trained batch 30 batch loss 2427.70215 epoch total loss 1972.05383\n",
      "Trained batch 31 batch loss 2511.99219 epoch total loss 1989.47119\n",
      "Trained batch 32 batch loss 2549.2063 epoch total loss 2006.96289\n",
      "Trained batch 33 batch loss 2557.01 epoch total loss 2023.63086\n",
      "Trained batch 34 batch loss 2550.29883 epoch total loss 2039.12109\n",
      "Trained batch 35 batch loss 2616.81274 epoch total loss 2055.62646\n",
      "Trained batch 36 batch loss 2637.48975 epoch total loss 2071.78955\n",
      "Trained batch 37 batch loss 2736.10742 epoch total loss 2089.74414\n",
      "Trained batch 38 batch loss 2779.11914 epoch total loss 2107.8855\n",
      "Trained batch 39 batch loss 2805.44897 epoch total loss 2125.77173\n",
      "Trained batch 40 batch loss 2796.93066 epoch total loss 2142.55054\n",
      "Trained batch 41 batch loss 2922.18335 epoch total loss 2161.56592\n",
      "Trained batch 42 batch loss 2986.54907 epoch total loss 2181.20825\n",
      "Trained batch 43 batch loss 3036.23486 epoch total loss 2201.09277\n",
      "Trained batch 44 batch loss 3089.90063 epoch total loss 2221.29272\n",
      "Trained batch 45 batch loss 3183.44702 epoch total loss 2242.67407\n",
      "Trained batch 46 batch loss 3184.39478 epoch total loss 2263.14624\n",
      "Trained batch 47 batch loss 3229.02832 epoch total loss 2283.69702\n",
      "Trained batch 48 batch loss 3299.02 epoch total loss 2304.84961\n",
      "Trained batch 49 batch loss 3371.16431 epoch total loss 2326.61108\n",
      "Trained batch 50 batch loss 3418.24561 epoch total loss 2348.44385\n",
      "Trained batch 51 batch loss 3463.75586 epoch total loss 2370.31274\n",
      "Trained batch 52 batch loss 3564.2417 epoch total loss 2393.27295\n",
      "Trained batch 53 batch loss 3623.41431 epoch total loss 2416.48315\n",
      "Trained batch 54 batch loss 3684.62305 epoch total loss 2439.96704\n",
      "Trained batch 55 batch loss 3789.37231 epoch total loss 2464.50171\n",
      "Trained batch 56 batch loss 3851.54321 epoch total loss 2489.27026\n",
      "Trained batch 57 batch loss 3871.54248 epoch total loss 2513.52075\n",
      "Trained batch 58 batch loss 3893.89673 epoch total loss 2537.32031\n",
      "Trained batch 59 batch loss 3821.92627 epoch total loss 2559.09326\n",
      "Trained batch 60 batch loss 3945.56274 epoch total loss 2582.20093\n",
      "Trained batch 61 batch loss 4099.58789 epoch total loss 2607.07642\n",
      "Trained batch 62 batch loss 4218.25684 epoch total loss 2633.06299\n",
      "Trained batch 63 batch loss 4350.01318 epoch total loss 2660.31616\n",
      "Trained batch 64 batch loss 4427.29297 epoch total loss 2687.92529\n",
      "Trained batch 65 batch loss 4481.99561 epoch total loss 2715.52637\n",
      "Trained batch 66 batch loss 4473.13232 epoch total loss 2742.15674\n",
      "Trained batch 67 batch loss 4581.15381 epoch total loss 2769.60449\n",
      "Trained batch 68 batch loss 4623.48535 epoch total loss 2796.86743\n",
      "Trained batch 69 batch loss 4646.24805 epoch total loss 2823.67017\n",
      "Trained batch 70 batch loss 4754.09717 epoch total loss 2851.24756\n",
      "Trained batch 71 batch loss 4814.11523 epoch total loss 2878.89355\n",
      "Trained batch 72 batch loss 4886.75928 epoch total loss 2906.78052\n",
      "Trained batch 73 batch loss 4952.88525 epoch total loss 2934.80957\n",
      "Trained batch 74 batch loss 4998.2 epoch total loss 2962.69312\n",
      "Trained batch 75 batch loss 5025.79102 epoch total loss 2990.20117\n",
      "Trained batch 76 batch loss 5215.70605 epoch total loss 3019.48413\n",
      "Trained batch 77 batch loss 5308.8623 epoch total loss 3049.21631\n",
      "Trained batch 78 batch loss 5256.91748 epoch total loss 3077.52026\n",
      "Trained batch 79 batch loss 5437.21631 epoch total loss 3107.39\n",
      "Trained batch 80 batch loss 5494.89111 epoch total loss 3137.23364\n",
      "Trained batch 81 batch loss 5535.78418 epoch total loss 3166.84521\n",
      "Trained batch 82 batch loss 5613.25928 epoch total loss 3196.67969\n",
      "Trained batch 83 batch loss 5636.06836 epoch total loss 3226.07\n",
      "Trained batch 84 batch loss 5689.52344 epoch total loss 3255.39697\n",
      "Trained batch 85 batch loss 5807.24854 epoch total loss 3285.4187\n",
      "Trained batch 86 batch loss 5798.9248 epoch total loss 3314.64575\n",
      "Trained batch 87 batch loss 5932.92627 epoch total loss 3344.74097\n",
      "Trained batch 88 batch loss 6030.24072 epoch total loss 3375.25806\n",
      "Trained batch 89 batch loss 5996.88672 epoch total loss 3404.7146\n",
      "Trained batch 90 batch loss 6095.94043 epoch total loss 3434.61694\n",
      "Trained batch 91 batch loss 6072.75732 epoch total loss 3463.60742\n",
      "Trained batch 92 batch loss 6193.66895 epoch total loss 3493.28198\n",
      "Trained batch 93 batch loss 6254.63916 epoch total loss 3522.97388\n",
      "Trained batch 94 batch loss 6300.32861 epoch total loss 3552.52026\n",
      "Trained batch 95 batch loss 6321.59521 epoch total loss 3581.66846\n",
      "Trained batch 96 batch loss 6518.20654 epoch total loss 3612.25757\n",
      "Trained batch 97 batch loss 6603.35693 epoch total loss 3643.09351\n",
      "Trained batch 98 batch loss 6689.34766 epoch total loss 3674.17773\n",
      "Trained batch 99 batch loss 6812.69824 epoch total loss 3705.87964\n",
      "Trained batch 100 batch loss 6866.521 epoch total loss 3737.48633\n",
      "Trained batch 101 batch loss 6958.09912 epoch total loss 3769.37354\n",
      "Trained batch 102 batch loss 7040.33252 epoch total loss 3801.44189\n",
      "Trained batch 103 batch loss 7032.13721 epoch total loss 3832.80762\n",
      "Trained batch 104 batch loss 7131.36426 epoch total loss 3864.52466\n",
      "Trained batch 105 batch loss 7217.38135 epoch total loss 3896.45654\n",
      "Trained batch 106 batch loss 7344.979 epoch total loss 3928.98975\n",
      "Trained batch 107 batch loss 7411.48438 epoch total loss 3961.53613\n",
      "Trained batch 108 batch loss 7491.7832 epoch total loss 3994.22363\n",
      "Trained batch 109 batch loss 7619.8584 epoch total loss 4027.48633\n",
      "Trained batch 110 batch loss 7537.59814 epoch total loss 4059.39624\n",
      "Trained batch 111 batch loss 7698.6416 epoch total loss 4092.18237\n",
      "Trained batch 112 batch loss 7640.40186 epoch total loss 4123.86279\n",
      "Trained batch 113 batch loss 7750.00879 epoch total loss 4155.95264\n",
      "Trained batch 114 batch loss 7948.16113 epoch total loss 4189.21777\n",
      "Trained batch 115 batch loss 7937.66064 epoch total loss 4221.81299\n",
      "Trained batch 116 batch loss 8006.95459 epoch total loss 4254.44336\n",
      "Trained batch 117 batch loss 8187.80518 epoch total loss 4288.06201\n",
      "Trained batch 118 batch loss 8260.69 epoch total loss 4321.72852\n",
      "Trained batch 119 batch loss 8371.30078 epoch total loss 4355.7583\n",
      "Trained batch 120 batch loss 8221.67383 epoch total loss 4387.97461\n",
      "Trained batch 121 batch loss 8420.27734 epoch total loss 4421.29883\n",
      "Trained batch 122 batch loss 8446.88086 epoch total loss 4454.29541\n",
      "Trained batch 123 batch loss 8601.05273 epoch total loss 4488.00928\n",
      "Trained batch 124 batch loss 8528.62207 epoch total loss 4520.59473\n",
      "Trained batch 125 batch loss 8717.41602 epoch total loss 4554.16943\n",
      "Trained batch 126 batch loss 8883.44727 epoch total loss 4588.52881\n",
      "Trained batch 127 batch loss 8927.09082 epoch total loss 4622.69043\n",
      "Trained batch 128 batch loss 9052.44434 epoch total loss 4657.29785\n",
      "Trained batch 129 batch loss 9267.69824 epoch total loss 4693.03711\n",
      "Trained batch 130 batch loss 9296.06 epoch total loss 4728.44531\n",
      "Trained batch 131 batch loss 9409.62402 epoch total loss 4764.1792\n",
      "Trained batch 132 batch loss 9613.97852 epoch total loss 4800.92041\n",
      "Trained batch 133 batch loss 9575.48633 epoch total loss 4836.81934\n",
      "Trained batch 134 batch loss 9538.46484 epoch total loss 4871.90625\n",
      "Trained batch 135 batch loss 9409.73828 epoch total loss 4905.52\n",
      "Trained batch 136 batch loss 9576.50488 epoch total loss 4939.86523\n",
      "Trained batch 137 batch loss 9942.06641 epoch total loss 4976.37793\n",
      "Trained batch 138 batch loss 10147.1738 epoch total loss 5013.84717\n",
      "Trained batch 139 batch loss 10253.4688 epoch total loss 5051.54248\n",
      "Trained batch 140 batch loss 10368.5107 epoch total loss 5089.52051\n",
      "Trained batch 141 batch loss 10460.2676 epoch total loss 5127.61084\n",
      "Trained batch 142 batch loss 10553.3223 epoch total loss 5165.82\n",
      "Trained batch 143 batch loss 10667.7607 epoch total loss 5204.29492\n",
      "Trained batch 144 batch loss 10730.2559 epoch total loss 5242.67\n",
      "Trained batch 145 batch loss 10605.46 epoch total loss 5279.6543\n",
      "Trained batch 146 batch loss 10711.8311 epoch total loss 5316.86084\n",
      "Trained batch 147 batch loss 10890.8818 epoch total loss 5354.7793\n",
      "Trained batch 148 batch loss 10959.2822 epoch total loss 5392.64795\n",
      "Trained batch 149 batch loss 11125.3613 epoch total loss 5431.12256\n",
      "Trained batch 150 batch loss 11231.9932 epoch total loss 5469.79492\n",
      "Trained batch 151 batch loss 11228.0439 epoch total loss 5507.9292\n",
      "Trained batch 152 batch loss 11277.5928 epoch total loss 5545.88721\n",
      "Trained batch 153 batch loss 11401.1299 epoch total loss 5584.15674\n",
      "Trained batch 154 batch loss 11460.0732 epoch total loss 5622.31201\n",
      "Trained batch 155 batch loss 11558.0928 epoch total loss 5660.60742\n",
      "Trained batch 156 batch loss 11498.5068 epoch total loss 5698.03\n",
      "Trained batch 157 batch loss 11873.1543 epoch total loss 5737.36133\n",
      "Trained batch 158 batch loss 11904.0088 epoch total loss 5776.39062\n",
      "Trained batch 159 batch loss 12091.5098 epoch total loss 5816.1084\n",
      "Trained batch 160 batch loss 11949.0381 epoch total loss 5854.43945\n",
      "Trained batch 161 batch loss 12195.2109 epoch total loss 5893.82275\n",
      "Trained batch 162 batch loss 12162.7646 epoch total loss 5932.52\n",
      "Trained batch 163 batch loss 12265.9307 epoch total loss 5971.37549\n",
      "Trained batch 164 batch loss 12208.0391 epoch total loss 6009.40381\n",
      "Trained batch 165 batch loss 12058.4707 epoch total loss 6046.06494\n",
      "Trained batch 166 batch loss 12316.9912 epoch total loss 6083.8418\n",
      "Trained batch 167 batch loss 12610.25 epoch total loss 6122.92236\n",
      "Trained batch 168 batch loss 12685.4404 epoch total loss 6161.98486\n",
      "Trained batch 169 batch loss 13033.6055 epoch total loss 6202.64551\n",
      "Trained batch 170 batch loss 12916.3379 epoch total loss 6242.1377\n",
      "Trained batch 171 batch loss 12887.0029 epoch total loss 6280.99658\n",
      "Trained batch 172 batch loss 12900.2832 epoch total loss 6319.48047\n",
      "Trained batch 173 batch loss 13331.4766 epoch total loss 6360.01221\n",
      "Trained batch 174 batch loss 13595.9795 epoch total loss 6401.59863\n",
      "Trained batch 175 batch loss 13695.0166 epoch total loss 6443.2749\n",
      "Trained batch 176 batch loss 13262.9414 epoch total loss 6482.02344\n",
      "Trained batch 177 batch loss 13233.9648 epoch total loss 6520.17041\n",
      "Trained batch 178 batch loss 13511.1816 epoch total loss 6559.44531\n",
      "Trained batch 179 batch loss 14066.8076 epoch total loss 6601.38525\n",
      "Trained batch 180 batch loss 14005.9941 epoch total loss 6642.52246\n",
      "Trained batch 181 batch loss 14348.7656 epoch total loss 6685.09814\n",
      "Trained batch 182 batch loss 14529.6855 epoch total loss 6728.19971\n",
      "Trained batch 183 batch loss 14547.4688 epoch total loss 6770.92822\n",
      "Trained batch 184 batch loss 14577.6318 epoch total loss 6813.35596\n",
      "Trained batch 185 batch loss 14564.3877 epoch total loss 6855.25342\n",
      "Trained batch 186 batch loss 14637.8604 epoch total loss 6897.09521\n",
      "Trained batch 187 batch loss 14772.3682 epoch total loss 6939.20898\n",
      "Trained batch 188 batch loss 15264.3008 epoch total loss 6983.49121\n",
      "Trained batch 189 batch loss 15168.5508 epoch total loss 7026.79834\n",
      "Trained batch 190 batch loss 15396.5342 epoch total loss 7070.84912\n",
      "Trained batch 191 batch loss 15561.2178 epoch total loss 7115.30176\n",
      "Trained batch 192 batch loss 15803.6191 epoch total loss 7160.55322\n",
      "Trained batch 193 batch loss 15865.1084 epoch total loss 7205.65479\n",
      "Trained batch 194 batch loss 16152.0586 epoch total loss 7251.77\n",
      "Trained batch 195 batch loss 16227.084 epoch total loss 7297.79736\n",
      "Trained batch 196 batch loss 16254.1182 epoch total loss 7343.49316\n",
      "Trained batch 197 batch loss 16299.8174 epoch total loss 7388.95703\n",
      "Trained batch 198 batch loss 16269.8555 epoch total loss 7433.81\n",
      "Trained batch 199 batch loss 16661.0879 epoch total loss 7480.17822\n",
      "Trained batch 200 batch loss 16255.3008 epoch total loss 7524.05371\n",
      "Trained batch 201 batch loss 16543.2227 epoch total loss 7568.92529\n",
      "Trained batch 202 batch loss 16897.9219 epoch total loss 7615.1084\n",
      "Trained batch 203 batch loss 16973.0332 epoch total loss 7661.20605\n",
      "Trained batch 204 batch loss 17135.6309 epoch total loss 7707.64941\n",
      "Trained batch 205 batch loss 17114.1758 epoch total loss 7753.53467\n",
      "Trained batch 206 batch loss 17363.127 epoch total loss 7800.18311\n",
      "Trained batch 207 batch loss 17347.9102 epoch total loss 7846.30713\n",
      "Trained batch 208 batch loss 17516.1191 epoch total loss 7892.79688\n",
      "Trained batch 209 batch loss 17500.5938 epoch total loss 7938.76758\n",
      "Trained batch 210 batch loss 17694.627 epoch total loss 7985.22363\n",
      "Trained batch 211 batch loss 17781.8809 epoch total loss 8031.65332\n",
      "Trained batch 212 batch loss 17673.3672 epoch total loss 8077.1333\n",
      "Trained batch 213 batch loss 17653.4023 epoch total loss 8122.09229\n",
      "Trained batch 214 batch loss 17964.1289 epoch total loss 8168.08301\n",
      "Trained batch 215 batch loss 17939.5488 epoch total loss 8213.53125\n",
      "Trained batch 216 batch loss 18133.6562 epoch total loss 8259.45801\n",
      "Trained batch 217 batch loss 17384.8105 epoch total loss 8301.51\n",
      "Trained batch 218 batch loss 17236.6934 epoch total loss 8342.49707\n",
      "Trained batch 219 batch loss 17584.5312 epoch total loss 8384.69824\n",
      "Trained batch 220 batch loss 18245.168 epoch total loss 8429.51855\n",
      "Trained batch 221 batch loss 18293.8457 epoch total loss 8474.15332\n",
      "Trained batch 222 batch loss 18986.7734 epoch total loss 8521.50684\n",
      "Trained batch 223 batch loss 19112.2227 epoch total loss 8568.99902\n",
      "Trained batch 224 batch loss 19075.9277 epoch total loss 8615.90527\n",
      "Trained batch 225 batch loss 19228.9922 epoch total loss 8663.07422\n",
      "Trained batch 226 batch loss 19133.6309 epoch total loss 8709.4043\n",
      "Trained batch 227 batch loss 19355.2578 epoch total loss 8756.30273\n",
      "Trained batch 228 batch loss 19205.1582 epoch total loss 8802.13086\n",
      "Trained batch 229 batch loss 19092.7148 epoch total loss 8847.06738\n",
      "Trained batch 230 batch loss 19701.2109 epoch total loss 8894.26\n",
      "Trained batch 231 batch loss 19492.8164 epoch total loss 8940.1416\n",
      "Trained batch 232 batch loss 19521.5352 epoch total loss 8985.75098\n",
      "Trained batch 233 batch loss 20015.9961 epoch total loss 9033.09\n",
      "Trained batch 234 batch loss 20152.6055 epoch total loss 9080.60938\n",
      "Trained batch 235 batch loss 20225.9707 epoch total loss 9128.03613\n",
      "Trained batch 236 batch loss 20064.2422 epoch total loss 9174.37598\n",
      "Trained batch 237 batch loss 19727.9531 epoch total loss 9218.90625\n",
      "Trained batch 238 batch loss 20074.0625 epoch total loss 9264.51562\n",
      "Trained batch 239 batch loss 20704.1445 epoch total loss 9312.38086\n",
      "Trained batch 240 batch loss 20920.0859 epoch total loss 9360.74609\n",
      "Trained batch 241 batch loss 21363.666 epoch total loss 9410.55078\n",
      "Trained batch 242 batch loss 21218.8906 epoch total loss 9459.3457\n",
      "Trained batch 243 batch loss 21284.4648 epoch total loss 9508.00879\n",
      "Trained batch 244 batch loss 21208.9238 epoch total loss 9555.96387\n",
      "Trained batch 245 batch loss 21760.0039 epoch total loss 9605.77637\n",
      "Trained batch 246 batch loss 22161.8789 epoch total loss 9656.81836\n",
      "Trained batch 247 batch loss 22015.9727 epoch total loss 9706.85547\n",
      "Trained batch 248 batch loss 22388.2988 epoch total loss 9757.99\n",
      "Trained batch 249 batch loss 22410.5332 epoch total loss 9808.80273\n",
      "Trained batch 250 batch loss 22689.5781 epoch total loss 9860.32617\n",
      "Trained batch 251 batch loss 22662.6445 epoch total loss 9911.33203\n",
      "Trained batch 252 batch loss 22728.1035 epoch total loss 9962.19141\n",
      "Trained batch 253 batch loss 22943.2168 epoch total loss 10013.5\n",
      "Trained batch 254 batch loss 22746.2441 epoch total loss 10063.6289\n",
      "Trained batch 255 batch loss 22647.3 epoch total loss 10112.9766\n",
      "Trained batch 256 batch loss 22933.6 epoch total loss 10163.0566\n",
      "Trained batch 257 batch loss 22976.5273 epoch total loss 10212.9141\n",
      "Trained batch 258 batch loss 22839.7832 epoch total loss 10261.8555\n",
      "Trained batch 259 batch loss 22857.5684 epoch total loss 10310.4873\n",
      "Trained batch 260 batch loss 22856.4766 epoch total loss 10358.7412\n",
      "Trained batch 261 batch loss 23612.0391 epoch total loss 10409.5205\n",
      "Trained batch 262 batch loss 23557.2812 epoch total loss 10459.7021\n",
      "Trained batch 263 batch loss 23962.0352 epoch total loss 10511.042\n",
      "Trained batch 264 batch loss 23753.8672 epoch total loss 10561.2031\n",
      "Trained batch 265 batch loss 23979.584 epoch total loss 10611.8389\n",
      "Trained batch 266 batch loss 24451.3574 epoch total loss 10663.8662\n",
      "Trained batch 267 batch loss 24415.9199 epoch total loss 10715.373\n",
      "Trained batch 268 batch loss 24485.4102 epoch total loss 10766.7539\n",
      "Trained batch 269 batch loss 24821.9609 epoch total loss 10819.0039\n",
      "Trained batch 270 batch loss 24437.416 epoch total loss 10869.4424\n",
      "Trained batch 271 batch loss 24633.7441 epoch total loss 10920.2334\n",
      "Trained batch 272 batch loss 24895.9023 epoch total loss 10971.6152\n",
      "Trained batch 273 batch loss 24733.8555 epoch total loss 11022.0254\n",
      "Trained batch 274 batch loss 25521.2266 epoch total loss 11074.9424\n",
      "Trained batch 275 batch loss 25215.8477 epoch total loss 11126.3633\n",
      "Trained batch 276 batch loss 25551.916 epoch total loss 11178.6309\n",
      "Trained batch 277 batch loss 25750.5879 epoch total loss 11231.2363\n",
      "Trained batch 278 batch loss 26001.7461 epoch total loss 11284.3682\n",
      "Trained batch 279 batch loss 25709.5352 epoch total loss 11336.0703\n",
      "Trained batch 280 batch loss 25510.4219 epoch total loss 11386.6934\n",
      "Trained batch 281 batch loss 25751.0957 epoch total loss 11437.8125\n",
      "Trained batch 282 batch loss 26038.6191 epoch total loss 11489.5879\n",
      "Trained batch 283 batch loss 26406.4961 epoch total loss 11542.2979\n",
      "Trained batch 284 batch loss 26679.7148 epoch total loss 11595.5986\n",
      "Trained batch 285 batch loss 26542.3203 epoch total loss 11648.043\n",
      "Trained batch 286 batch loss 26840.2578 epoch total loss 11701.1621\n",
      "Trained batch 287 batch loss 27069.1172 epoch total loss 11754.709\n",
      "Trained batch 288 batch loss 26973.7949 epoch total loss 11807.5527\n",
      "Trained batch 289 batch loss 26547.6836 epoch total loss 11858.5566\n",
      "Trained batch 290 batch loss 26659.5273 epoch total loss 11909.5947\n",
      "Trained batch 291 batch loss 26015.4453 epoch total loss 11958.0684\n",
      "Trained batch 292 batch loss 26353.7227 epoch total loss 12007.3691\n",
      "Trained batch 293 batch loss 27014.5137 epoch total loss 12058.5879\n",
      "Trained batch 294 batch loss 26648.5488 epoch total loss 12108.2139\n",
      "Trained batch 295 batch loss 25658.752 epoch total loss 12154.1475\n",
      "Trained batch 296 batch loss 26165.8457 epoch total loss 12201.4844\n",
      "Trained batch 297 batch loss 26847.8223 epoch total loss 12250.7979\n",
      "Trained batch 298 batch loss 27457.2578 epoch total loss 12301.8262\n",
      "Trained batch 299 batch loss 27748.6621 epoch total loss 12353.4883\n",
      "Trained batch 300 batch loss 28508.4551 epoch total loss 12407.3379\n",
      "Trained batch 301 batch loss 28639.7188 epoch total loss 12461.2666\n",
      "Trained batch 302 batch loss 29009.1 epoch total loss 12516.0605\n",
      "Trained batch 303 batch loss 28948.3691 epoch total loss 12570.292\n",
      "Trained batch 304 batch loss 29143.3945 epoch total loss 12624.8096\n",
      "Trained batch 305 batch loss 29087.9941 epoch total loss 12678.7871\n",
      "Trained batch 306 batch loss 28945.7598 epoch total loss 12731.9473\n",
      "Trained batch 307 batch loss 29036.9629 epoch total loss 12785.0576\n",
      "Trained batch 308 batch loss 29645.584 epoch total loss 12839.8\n",
      "Trained batch 309 batch loss 29295.1445 epoch total loss 12893.0537\n",
      "Trained batch 310 batch loss 28940.5508 epoch total loss 12944.8193\n",
      "Trained batch 311 batch loss 29559.084 epoch total loss 12998.2412\n",
      "Trained batch 312 batch loss 29526.4883 epoch total loss 13051.2168\n",
      "Trained batch 313 batch loss 30249.0957 epoch total loss 13106.1611\n",
      "Trained batch 314 batch loss 29902.0371 epoch total loss 13159.6514\n",
      "Trained batch 315 batch loss 30259.959 epoch total loss 13213.9385\n",
      "Trained batch 316 batch loss 30476.4551 epoch total loss 13268.5664\n",
      "Trained batch 317 batch loss 30549.0137 epoch total loss 13323.0791\n",
      "Trained batch 318 batch loss 29796.2285 epoch total loss 13374.8809\n",
      "Trained batch 319 batch loss 29609.7461 epoch total loss 13425.7725\n",
      "Trained batch 320 batch loss 30073.334 epoch total loss 13477.7969\n",
      "Trained batch 321 batch loss 30375.4727 epoch total loss 13530.4375\n",
      "Trained batch 322 batch loss 30377.7012 epoch total loss 13582.7578\n",
      "Trained batch 323 batch loss 31028.5625 epoch total loss 13636.7695\n",
      "Trained batch 324 batch loss 30602.5527 epoch total loss 13689.1328\n",
      "Trained batch 325 batch loss 30606.1309 epoch total loss 13741.1846\n",
      "Trained batch 326 batch loss 29936.2988 epoch total loss 13790.8633\n",
      "Trained batch 327 batch loss 29892.9609 epoch total loss 13840.1055\n",
      "Trained batch 328 batch loss 30997.5898 epoch total loss 13892.415\n",
      "Trained batch 329 batch loss 31731.8262 epoch total loss 13946.6387\n",
      "Trained batch 330 batch loss 31674.1641 epoch total loss 14000.3574\n",
      "Trained batch 331 batch loss 31299.2266 epoch total loss 14052.6191\n",
      "Trained batch 332 batch loss 32103.75 epoch total loss 14106.9912\n",
      "Trained batch 333 batch loss 32045.8887 epoch total loss 14160.8623\n",
      "Trained batch 334 batch loss 31986.0176 epoch total loss 14214.2305\n",
      "Trained batch 335 batch loss 32293.5957 epoch total loss 14268.1982\n",
      "Trained batch 336 batch loss 32505.7422 epoch total loss 14322.4766\n",
      "Trained batch 337 batch loss 32516.8984 epoch total loss 14376.4658\n",
      "Trained batch 338 batch loss 32776.4609 epoch total loss 14430.9043\n",
      "Trained batch 339 batch loss 32092.0938 epoch total loss 14483.002\n",
      "Trained batch 340 batch loss 31071.0898 epoch total loss 14531.79\n",
      "Trained batch 341 batch loss 31707.5918 epoch total loss 14582.1582\n",
      "Trained batch 342 batch loss 32007.1797 epoch total loss 14633.1084\n",
      "Trained batch 343 batch loss 33059.6172 epoch total loss 14686.8291\n",
      "Trained batch 344 batch loss 31815.8828 epoch total loss 14736.624\n",
      "Trained batch 345 batch loss 33113.6367 epoch total loss 14789.8896\n",
      "Trained batch 346 batch loss 32696.4258 epoch total loss 14841.6426\n",
      "Trained batch 347 batch loss 32627.1777 epoch total loss 14892.8975\n",
      "Trained batch 348 batch loss 33379.3398 epoch total loss 14946.0205\n",
      "Trained batch 349 batch loss 32978.2461 epoch total loss 14997.6875\n",
      "Trained batch 350 batch loss 33194.4805 epoch total loss 15049.6787\n",
      "Trained batch 351 batch loss 33904.9492 epoch total loss 15103.3975\n",
      "Trained batch 352 batch loss 33916.4727 epoch total loss 15156.8438\n",
      "Trained batch 353 batch loss 34377.6719 epoch total loss 15211.293\n",
      "Trained batch 354 batch loss 34252.8906 epoch total loss 15265.083\n",
      "Trained batch 355 batch loss 33570.1 epoch total loss 15316.6465\n",
      "Trained batch 356 batch loss 33342.2422 epoch total loss 15367.2793\n",
      "Trained batch 357 batch loss 32966.7891 epoch total loss 15416.5781\n",
      "Trained batch 358 batch loss 33393.8164 epoch total loss 15466.7949\n",
      "Trained batch 359 batch loss 33848.1562 epoch total loss 15517.9961\n",
      "Trained batch 360 batch loss 32897.1602 epoch total loss 15566.2705\n",
      "Trained batch 361 batch loss 32853.4648 epoch total loss 15614.1582\n",
      "Trained batch 362 batch loss 33015.1562 epoch total loss 15662.2266\n",
      "Trained batch 363 batch loss 32995.7617 epoch total loss 15709.9775\n",
      "Trained batch 364 batch loss 34244.6758 epoch total loss 15760.8975\n",
      "Trained batch 365 batch loss 35549.7148 epoch total loss 15815.1123\n",
      "Trained batch 366 batch loss 35436.1875 epoch total loss 15868.7217\n",
      "Trained batch 367 batch loss 35343.0469 epoch total loss 15921.7852\n",
      "Trained batch 368 batch loss 35280.5352 epoch total loss 15974.3896\n",
      "Trained batch 369 batch loss 35270.3711 epoch total loss 16026.6826\n",
      "Trained batch 370 batch loss 35482.7695 epoch total loss 16079.2676\n",
      "Trained batch 371 batch loss 35617.4727 epoch total loss 16131.9316\n",
      "Trained batch 372 batch loss 35462.9258 epoch total loss 16183.8965\n",
      "Trained batch 373 batch loss 35039.3203 epoch total loss 16234.4473\n",
      "Trained batch 374 batch loss 35177.5664 epoch total loss 16285.0977\n",
      "Trained batch 375 batch loss 36238.8359 epoch total loss 16338.3076\n",
      "Trained batch 376 batch loss 35775.4844 epoch total loss 16390.002\n",
      "Trained batch 377 batch loss 36759.375 epoch total loss 16444.0332\n",
      "Trained batch 378 batch loss 36875.1289 epoch total loss 16498.084\n",
      "Trained batch 379 batch loss 37015.2812 epoch total loss 16552.2188\n",
      "Trained batch 380 batch loss 36161.3711 epoch total loss 16603.8223\n",
      "Trained batch 381 batch loss 36156.4805 epoch total loss 16655.1426\n",
      "Trained batch 382 batch loss 36056.8594 epoch total loss 16705.9316\n",
      "Trained batch 383 batch loss 36818.4609 epoch total loss 16758.4453\n",
      "Trained batch 384 batch loss 37142.2188 epoch total loss 16811.5273\n",
      "Trained batch 385 batch loss 37401.5469 epoch total loss 16865.0078\n",
      "Trained batch 386 batch loss 37196.4102 epoch total loss 16917.6797\n",
      "Trained batch 387 batch loss 37654.4062 epoch total loss 16971.2637\n",
      "Trained batch 388 batch loss 37825.9414 epoch total loss 17025.0137\n",
      "Trained batch 389 batch loss 38027.7695 epoch total loss 17079.0059\n",
      "Trained batch 390 batch loss 37346.4336 epoch total loss 17130.9727\n",
      "Trained batch 391 batch loss 37340.3 epoch total loss 17182.6602\n",
      "Trained batch 392 batch loss 37481.1172 epoch total loss 17234.4414\n",
      "Trained batch 393 batch loss 38357.9258 epoch total loss 17288.1914\n",
      "Trained batch 394 batch loss 38295.9414 epoch total loss 17341.5098\n",
      "Trained batch 395 batch loss 38422.5352 epoch total loss 17394.8789\n",
      "Trained batch 396 batch loss 38440.6 epoch total loss 17448.0254\n",
      "Trained batch 397 batch loss 37303.5195 epoch total loss 17498.0391\n",
      "Trained batch 398 batch loss 36890.625 epoch total loss 17546.7637\n",
      "Trained batch 399 batch loss 37839.8867 epoch total loss 17597.625\n",
      "Trained batch 400 batch loss 38706.3477 epoch total loss 17650.3965\n",
      "Trained batch 401 batch loss 37816.8867 epoch total loss 17700.6875\n",
      "Trained batch 402 batch loss 37642.1602 epoch total loss 17750.293\n",
      "Trained batch 403 batch loss 37859.7852 epoch total loss 17800.1914\n",
      "Trained batch 404 batch loss 37535.793 epoch total loss 17849.043\n",
      "Trained batch 405 batch loss 38760.8867 epoch total loss 17900.6777\n",
      "Trained batch 406 batch loss 38737.9688 epoch total loss 17952.002\n",
      "Trained batch 407 batch loss 38886.1133 epoch total loss 18003.4355\n",
      "Trained batch 408 batch loss 38409.5078 epoch total loss 18053.4512\n",
      "Trained batch 409 batch loss 38737.1328 epoch total loss 18104.0215\n",
      "Trained batch 410 batch loss 39465.2461 epoch total loss 18156.1211\n",
      "Trained batch 411 batch loss 38325.3398 epoch total loss 18205.1953\n",
      "Trained batch 412 batch loss 39050.5547 epoch total loss 18255.791\n",
      "Trained batch 413 batch loss 39672.625 epoch total loss 18307.6484\n",
      "Trained batch 414 batch loss 39697.5938 epoch total loss 18359.3145\n",
      "Trained batch 415 batch loss 39126.875 epoch total loss 18409.3574\n",
      "Trained batch 416 batch loss 39247.6836 epoch total loss 18459.4492\n",
      "Trained batch 417 batch loss 38916.8125 epoch total loss 18508.5078\n",
      "Trained batch 418 batch loss 39591.4102 epoch total loss 18558.9453\n",
      "Trained batch 419 batch loss 38985.1289 epoch total loss 18607.6953\n",
      "Trained batch 420 batch loss 39450.9375 epoch total loss 18657.3223\n",
      "Trained batch 421 batch loss 38760.0664 epoch total loss 18705.0703\n",
      "Trained batch 422 batch loss 39711.4805 epoch total loss 18754.8496\n",
      "Trained batch 423 batch loss 40276.9727 epoch total loss 18805.7285\n",
      "Trained batch 424 batch loss 40759.0781 epoch total loss 18857.5059\n",
      "Trained batch 425 batch loss 40871.2734 epoch total loss 18909.3027\n",
      "Trained batch 426 batch loss 41109.0664 epoch total loss 18961.416\n",
      "Trained batch 427 batch loss 40260.5586 epoch total loss 19011.2969\n",
      "Trained batch 428 batch loss 41019.8438 epoch total loss 19062.7188\n",
      "Trained batch 429 batch loss 41017.5312 epoch total loss 19113.8945\n",
      "Trained batch 430 batch loss 40879.4609 epoch total loss 19164.5137\n",
      "Trained batch 431 batch loss 40912.3047 epoch total loss 19214.9727\n",
      "Trained batch 432 batch loss 40723.4922 epoch total loss 19264.7598\n",
      "Trained batch 433 batch loss 39847.1523 epoch total loss 19312.2949\n",
      "Trained batch 434 batch loss 39700.1719 epoch total loss 19359.2715\n",
      "Trained batch 435 batch loss 38873.4922 epoch total loss 19404.1309\n",
      "Trained batch 436 batch loss 38396.043 epoch total loss 19447.6895\n",
      "Trained batch 437 batch loss 40271.4 epoch total loss 19495.3418\n",
      "Trained batch 438 batch loss 41355.4609 epoch total loss 19545.248\n",
      "Trained batch 439 batch loss 41762.7461 epoch total loss 19595.8594\n",
      "Trained batch 440 batch loss 41019.3047 epoch total loss 19644.5469\n",
      "Trained batch 441 batch loss 40685.043 epoch total loss 19692.2578\n",
      "Trained batch 442 batch loss 40160.1055 epoch total loss 19738.5664\n",
      "Trained batch 443 batch loss 42470.3828 epoch total loss 19789.8789\n",
      "Trained batch 444 batch loss 42410.5195 epoch total loss 19840.8262\n",
      "Trained batch 445 batch loss 42733.5312 epoch total loss 19892.2715\n",
      "Trained batch 446 batch loss 42960.6758 epoch total loss 19943.9961\n",
      "Trained batch 447 batch loss 43066.4297 epoch total loss 19995.7227\n",
      "Trained batch 448 batch loss 43034.7617 epoch total loss 20047.1504\n",
      "Trained batch 449 batch loss 42916.3672 epoch total loss 20098.082\n",
      "Trained batch 450 batch loss 42686.9688 epoch total loss 20148.2793\n",
      "Trained batch 451 batch loss 42326.418 epoch total loss 20197.4551\n",
      "Trained batch 452 batch loss 43131.6719 epoch total loss 20248.1953\n",
      "Trained batch 453 batch loss 42725.8906 epoch total loss 20297.8145\n",
      "Trained batch 454 batch loss 42605.8906 epoch total loss 20346.9512\n",
      "Trained batch 455 batch loss 41833.75 epoch total loss 20394.1758\n",
      "Trained batch 456 batch loss 41925.0703 epoch total loss 20441.3926\n",
      "Trained batch 457 batch loss 41989.3789 epoch total loss 20488.543\n",
      "Trained batch 458 batch loss 42603.8477 epoch total loss 20536.8301\n",
      "Trained batch 459 batch loss 44088.1641 epoch total loss 20588.1387\n",
      "Trained batch 460 batch loss 44389.6 epoch total loss 20639.8828\n",
      "Trained batch 461 batch loss 44364.6172 epoch total loss 20691.3477\n",
      "Trained batch 462 batch loss 44575.3789 epoch total loss 20743.043\n",
      "Trained batch 463 batch loss 43482.8945 epoch total loss 20792.1582\n",
      "Trained batch 464 batch loss 42727.5195 epoch total loss 20839.4336\n",
      "Trained batch 465 batch loss 44581.4023 epoch total loss 20890.4902\n",
      "Trained batch 466 batch loss 44344.418 epoch total loss 20940.8203\n",
      "Trained batch 467 batch loss 43221.6406 epoch total loss 20988.5312\n",
      "Trained batch 468 batch loss 43935.832 epoch total loss 21037.5645\n",
      "Trained batch 469 batch loss 42035.8164 epoch total loss 21082.3359\n",
      "Trained batch 470 batch loss 43355.4688 epoch total loss 21129.7246\n",
      "Trained batch 471 batch loss 44588.8672 epoch total loss 21179.5332\n",
      "Trained batch 472 batch loss 44774.9727 epoch total loss 21229.5234\n",
      "Trained batch 473 batch loss 45244.6367 epoch total loss 21280.2969\n",
      "Trained batch 474 batch loss 45666.4 epoch total loss 21331.7422\n",
      "Trained batch 475 batch loss 44874.3398 epoch total loss 21381.3047\n",
      "Trained batch 476 batch loss 44974.6172 epoch total loss 21430.8711\n",
      "Trained batch 477 batch loss 44222.5195 epoch total loss 21478.6543\n",
      "Trained batch 478 batch loss 43782.5977 epoch total loss 21525.3164\n",
      "Trained batch 479 batch loss 44144.4922 epoch total loss 21572.5371\n",
      "Trained batch 480 batch loss 45290.1914 epoch total loss 21621.9473\n",
      "Trained batch 481 batch loss 45753.4688 epoch total loss 21672.1172\n",
      "Trained batch 482 batch loss 45089.0508 epoch total loss 21720.7\n",
      "Trained batch 483 batch loss 43894.8398 epoch total loss 21766.6094\n",
      "Trained batch 484 batch loss 45992.0586 epoch total loss 21816.6621\n",
      "Trained batch 485 batch loss 46377.9766 epoch total loss 21867.3027\n",
      "Trained batch 486 batch loss 45781.9727 epoch total loss 21916.5098\n",
      "Trained batch 487 batch loss 45949.1758 epoch total loss 21965.8574\n",
      "Trained batch 488 batch loss 46303.0859 epoch total loss 22015.7305\n",
      "Trained batch 489 batch loss 45611.8711 epoch total loss 22063.9844\n",
      "Trained batch 490 batch loss 46169.3047 epoch total loss 22113.1777\n",
      "Trained batch 491 batch loss 47065.8711 epoch total loss 22163.998\n",
      "Trained batch 492 batch loss 47585.5156 epoch total loss 22215.668\n",
      "Trained batch 493 batch loss 47711.2266 epoch total loss 22267.3828\n",
      "Trained batch 494 batch loss 47753.25 epoch total loss 22318.9746\n",
      "Trained batch 495 batch loss 47484.1641 epoch total loss 22369.8125\n",
      "Trained batch 496 batch loss 46734.668 epoch total loss 22418.9355\n",
      "Trained batch 497 batch loss 47173.6602 epoch total loss 22468.7441\n",
      "Trained batch 498 batch loss 46880.2344 epoch total loss 22517.7637\n",
      "Trained batch 499 batch loss 46546.7266 epoch total loss 22565.918\n",
      "Trained batch 500 batch loss 47349.0742 epoch total loss 22615.4844\n",
      "Trained batch 501 batch loss 48661.2 epoch total loss 22667.4707\n",
      "Trained batch 502 batch loss 48401.8281 epoch total loss 22718.7344\n",
      "Trained batch 503 batch loss 48834.332 epoch total loss 22770.6543\n",
      "Trained batch 504 batch loss 49090.9727 epoch total loss 22822.877\n",
      "Trained batch 505 batch loss 49367.9453 epoch total loss 22875.4414\n",
      "Trained batch 506 batch loss 48690.3711 epoch total loss 22926.459\n",
      "Trained batch 507 batch loss 48012.6562 epoch total loss 22975.9395\n",
      "Trained batch 508 batch loss 48341.543 epoch total loss 23025.8711\n",
      "Trained batch 509 batch loss 48495.8867 epoch total loss 23075.9121\n",
      "Trained batch 510 batch loss 49759.4688 epoch total loss 23128.2305\n",
      "Trained batch 511 batch loss 50027.6367 epoch total loss 23180.873\n",
      "Trained batch 512 batch loss 50060.0703 epoch total loss 23233.3711\n",
      "Trained batch 513 batch loss 49995.9766 epoch total loss 23285.5391\n",
      "Trained batch 514 batch loss 50117.5312 epoch total loss 23337.7441\n",
      "Trained batch 515 batch loss 50377.3281 epoch total loss 23390.2461\n",
      "Trained batch 516 batch loss 50360.8398 epoch total loss 23442.5156\n",
      "Trained batch 517 batch loss 50785.9141 epoch total loss 23495.4043\n",
      "Trained batch 518 batch loss 50740.6602 epoch total loss 23548.002\n",
      "Trained batch 519 batch loss 50969.3047 epoch total loss 23600.8359\n",
      "Trained batch 520 batch loss 50305.0117 epoch total loss 23652.1895\n",
      "Trained batch 521 batch loss 50262.9492 epoch total loss 23703.2676\n",
      "Trained batch 522 batch loss 49655.793 epoch total loss 23752.9844\n",
      "Trained batch 523 batch loss 49342.8828 epoch total loss 23801.9141\n",
      "Trained batch 524 batch loss 48210.7852 epoch total loss 23848.4961\n",
      "Trained batch 525 batch loss 48674.5938 epoch total loss 23895.7852\n",
      "Trained batch 526 batch loss 49924.8164 epoch total loss 23945.2695\n",
      "Trained batch 527 batch loss 51232.1328 epoch total loss 23997.0469\n",
      "Trained batch 528 batch loss 51465.0391 epoch total loss 24049.0703\n",
      "Trained batch 529 batch loss 51802.8789 epoch total loss 24101.5352\n",
      "Trained batch 530 batch loss 52220.4531 epoch total loss 24154.5879\n",
      "Trained batch 531 batch loss 52357.4844 epoch total loss 24207.7012\n",
      "Trained batch 532 batch loss 51615.2539 epoch total loss 24259.2188\n",
      "Trained batch 533 batch loss 52525.8281 epoch total loss 24312.252\n",
      "Trained batch 534 batch loss 51730.1719 epoch total loss 24363.5957\n",
      "Trained batch 535 batch loss 52293.9453 epoch total loss 24415.8027\n",
      "Trained batch 536 batch loss 52183.2188 epoch total loss 24467.6055\n",
      "Trained batch 537 batch loss 52182.0312 epoch total loss 24519.2168\n",
      "Trained batch 538 batch loss 52718.1875 epoch total loss 24571.6309\n",
      "Trained batch 539 batch loss 53123.9961 epoch total loss 24624.6035\n",
      "Trained batch 540 batch loss 52374.5 epoch total loss 24675.9922\n",
      "Trained batch 541 batch loss 52475.4062 epoch total loss 24727.377\n",
      "Trained batch 542 batch loss 53014.375 epoch total loss 24779.5664\n",
      "Trained batch 543 batch loss 53002.3828 epoch total loss 24831.541\n",
      "Trained batch 544 batch loss 52769.9727 epoch total loss 24882.8984\n",
      "Trained batch 545 batch loss 52375.875 epoch total loss 24933.3457\n",
      "Trained batch 546 batch loss 52446.5273 epoch total loss 24983.7363\n",
      "Trained batch 547 batch loss 53107.9609 epoch total loss 25035.1523\n",
      "Trained batch 548 batch loss 54160.9961 epoch total loss 25088.3\n",
      "Trained batch 549 batch loss 53715.8672 epoch total loss 25140.4453\n",
      "Trained batch 550 batch loss 53323.1641 epoch total loss 25191.6875\n",
      "Trained batch 551 batch loss 52838.4062 epoch total loss 25241.8613\n",
      "Trained batch 552 batch loss 54058.0586 epoch total loss 25294.0645\n",
      "Trained batch 553 batch loss 54239.2148 epoch total loss 25346.4062\n",
      "Trained batch 554 batch loss 52405.5469 epoch total loss 25395.25\n",
      "Trained batch 555 batch loss 54517.3711 epoch total loss 25447.7227\n",
      "Trained batch 556 batch loss 54061.8711 epoch total loss 25499.1875\n",
      "Trained batch 557 batch loss 54837.207 epoch total loss 25551.8574\n",
      "Trained batch 558 batch loss 54942.5156 epoch total loss 25604.5312\n",
      "Trained batch 559 batch loss 54830.9844 epoch total loss 25656.8145\n",
      "Trained batch 560 batch loss 54603.4844 epoch total loss 25708.5039\n",
      "Trained batch 561 batch loss 54322.9023 epoch total loss 25759.5098\n",
      "Trained batch 562 batch loss 54483.4023 epoch total loss 25810.6191\n",
      "Trained batch 563 batch loss 54799.4805 epoch total loss 25862.1074\n",
      "Trained batch 564 batch loss 55331.6445 epoch total loss 25914.3594\n",
      "Trained batch 565 batch loss 55695.5469 epoch total loss 25967.0703\n",
      "Trained batch 566 batch loss 55907.5352 epoch total loss 26019.9707\n",
      "Trained batch 567 batch loss 55628.125 epoch total loss 26072.1895\n",
      "Trained batch 568 batch loss 55667.0977 epoch total loss 26124.293\n",
      "Trained batch 569 batch loss 56171.1406 epoch total loss 26177.0977\n",
      "Trained batch 570 batch loss 55761.082 epoch total loss 26229\n",
      "Trained batch 571 batch loss 54913.1406 epoch total loss 26279.2344\n",
      "Trained batch 572 batch loss 55078.9 epoch total loss 26329.584\n",
      "Trained batch 573 batch loss 55225.1484 epoch total loss 26380.0117\n",
      "Trained batch 574 batch loss 54054.0312 epoch total loss 26428.2246\n",
      "Trained batch 575 batch loss 55251.6562 epoch total loss 26478.3535\n",
      "Trained batch 576 batch loss 55279.7148 epoch total loss 26528.3555\n",
      "Trained batch 577 batch loss 56004.2305 epoch total loss 26579.4395\n",
      "Trained batch 578 batch loss 55627.3164 epoch total loss 26629.6953\n",
      "Trained batch 579 batch loss 55986.8047 epoch total loss 26680.3984\n",
      "Trained batch 580 batch loss 55404.4102 epoch total loss 26729.9219\n",
      "Trained batch 581 batch loss 54545.7773 epoch total loss 26777.7988\n",
      "Trained batch 582 batch loss 56813.3164 epoch total loss 26829.4062\n",
      "Trained batch 583 batch loss 56092.3164 epoch total loss 26879.5977\n",
      "Trained batch 584 batch loss 57259.2969 epoch total loss 26931.6172\n",
      "Trained batch 585 batch loss 57740.4102 epoch total loss 26984.2812\n",
      "Trained batch 586 batch loss 57203.9883 epoch total loss 27035.8516\n",
      "Trained batch 587 batch loss 57075.293 epoch total loss 27087.0254\n",
      "Trained batch 588 batch loss 57129.6641 epoch total loss 27138.1191\n",
      "Trained batch 589 batch loss 56916.9766 epoch total loss 27188.6777\n",
      "Trained batch 590 batch loss 56097.7539 epoch total loss 27237.6758\n",
      "Trained batch 591 batch loss 56415.1875 epoch total loss 27287.0449\n",
      "Trained batch 592 batch loss 57134.6953 epoch total loss 27337.4648\n",
      "Trained batch 593 batch loss 57082.8203 epoch total loss 27387.625\n",
      "Trained batch 594 batch loss 56717.582 epoch total loss 27437.0039\n",
      "Trained batch 595 batch loss 57284.0898 epoch total loss 27487.166\n",
      "Trained batch 596 batch loss 57092.75 epoch total loss 27536.8398\n",
      "Trained batch 597 batch loss 56936.957 epoch total loss 27586.0879\n",
      "Trained batch 598 batch loss 56361.2852 epoch total loss 27634.2051\n",
      "Trained batch 599 batch loss 56399.3086 epoch total loss 27682.2266\n",
      "Trained batch 600 batch loss 58926.4922 epoch total loss 27734.3\n",
      "Trained batch 601 batch loss 60093.9414 epoch total loss 27788.1426\n",
      "Trained batch 602 batch loss 59214.543 epoch total loss 27840.3477\n",
      "Trained batch 603 batch loss 60672.0195 epoch total loss 27894.7969\n",
      "Trained batch 604 batch loss 60559.3555 epoch total loss 27948.877\n",
      "Trained batch 605 batch loss 60334.6953 epoch total loss 28002.4062\n",
      "Trained batch 606 batch loss 61065.457 epoch total loss 28056.9668\n",
      "Trained batch 607 batch loss 59953.6055 epoch total loss 28109.5156\n",
      "Trained batch 608 batch loss 60643.668 epoch total loss 28163.0254\n",
      "Trained batch 609 batch loss 58834.1133 epoch total loss 28213.3887\n",
      "Trained batch 610 batch loss 58471.7891 epoch total loss 28262.9941\n",
      "Trained batch 611 batch loss 59430.9336 epoch total loss 28314.0039\n",
      "Trained batch 612 batch loss 59426.7031 epoch total loss 28364.8398\n",
      "Trained batch 613 batch loss 59862.1484 epoch total loss 28416.2227\n",
      "Trained batch 614 batch loss 59176.9258 epoch total loss 28466.3184\n",
      "Trained batch 615 batch loss 60859.1875 epoch total loss 28518.9922\n",
      "Trained batch 616 batch loss 61334.1641 epoch total loss 28572.2637\n",
      "Trained batch 617 batch loss 61257.4414 epoch total loss 28625.2383\n",
      "Trained batch 618 batch loss 61181.8672 epoch total loss 28677.9199\n",
      "Trained batch 619 batch loss 61498.168 epoch total loss 28730.9395\n",
      "Trained batch 620 batch loss 61163.8477 epoch total loss 28783.252\n",
      "Trained batch 621 batch loss 61600.8789 epoch total loss 28836.0957\n",
      "Trained batch 622 batch loss 62260.6875 epoch total loss 28889.832\n",
      "Trained batch 623 batch loss 62220.332 epoch total loss 28943.332\n",
      "Trained batch 624 batch loss 62440.5117 epoch total loss 28997.0137\n",
      "Trained batch 625 batch loss 62432.6133 epoch total loss 29050.5098\n",
      "Trained batch 626 batch loss 62909.5469 epoch total loss 29104.5977\n",
      "Trained batch 627 batch loss 60864.3438 epoch total loss 29155.25\n",
      "Trained batch 628 batch loss 60454.4062 epoch total loss 29205.0898\n",
      "Trained batch 629 batch loss 63499.1953 epoch total loss 29259.6113\n",
      "Trained batch 630 batch loss 62257.5703 epoch total loss 29311.9902\n",
      "Trained batch 631 batch loss 62906.5 epoch total loss 29365.2305\n",
      "Trained batch 632 batch loss 62136.8477 epoch total loss 29417.082\n",
      "Trained batch 633 batch loss 61581.7617 epoch total loss 29467.8965\n",
      "Trained batch 634 batch loss 63949.3125 epoch total loss 29522.2832\n",
      "Trained batch 635 batch loss 63385.0938 epoch total loss 29575.6133\n",
      "Trained batch 636 batch loss 63100.543 epoch total loss 29628.3242\n",
      "Trained batch 637 batch loss 63058.6758 epoch total loss 29680.8047\n",
      "Trained batch 638 batch loss 64474.6055 epoch total loss 29735.3379\n",
      "Trained batch 639 batch loss 62015.0703 epoch total loss 29785.8555\n",
      "Trained batch 640 batch loss 63379.5664 epoch total loss 29838.3477\n",
      "Trained batch 641 batch loss 63881.7734 epoch total loss 29891.457\n",
      "Trained batch 642 batch loss 62925.6328 epoch total loss 29942.9121\n",
      "Trained batch 643 batch loss 61389.793 epoch total loss 29991.8203\n",
      "Trained batch 644 batch loss 64123.9062 epoch total loss 30044.8203\n",
      "Trained batch 645 batch loss 63578.2344 epoch total loss 30096.8086\n",
      "Trained batch 646 batch loss 64167.2617 epoch total loss 30149.5508\n",
      "Trained batch 647 batch loss 64868.9453 epoch total loss 30203.2109\n",
      "Trained batch 648 batch loss 63962.8 epoch total loss 30255.3086\n",
      "Trained batch 649 batch loss 62288.418 epoch total loss 30304.666\n",
      "Trained batch 650 batch loss 62141.7461 epoch total loss 30353.6465\n",
      "Trained batch 651 batch loss 64293.0742 epoch total loss 30405.7812\n",
      "Trained batch 652 batch loss 64473.4844 epoch total loss 30458.0332\n",
      "Trained batch 653 batch loss 63871.3633 epoch total loss 30509.2031\n",
      "Trained batch 654 batch loss 65137.9 epoch total loss 30562.1523\n",
      "Trained batch 655 batch loss 66084.3125 epoch total loss 30616.3848\n",
      "Trained batch 656 batch loss 65966.5469 epoch total loss 30670.2715\n",
      "Trained batch 657 batch loss 66055.7812 epoch total loss 30724.1309\n",
      "Trained batch 658 batch loss 65810.4531 epoch total loss 30777.4531\n",
      "Trained batch 659 batch loss 64997.3594 epoch total loss 30829.3809\n",
      "Trained batch 660 batch loss 64784.457 epoch total loss 30880.8281\n",
      "Trained batch 661 batch loss 64866.9805 epoch total loss 30932.2422\n",
      "Trained batch 662 batch loss 66143.8359 epoch total loss 30985.4316\n",
      "Trained batch 663 batch loss 64565.1289 epoch total loss 31036.082\n",
      "Trained batch 664 batch loss 64707.5 epoch total loss 31086.793\n",
      "Trained batch 665 batch loss 66032.6562 epoch total loss 31139.3418\n",
      "Trained batch 666 batch loss 67072.3359 epoch total loss 31193.2949\n",
      "Trained batch 667 batch loss 65623.7 epoch total loss 31244.9141\n",
      "Trained batch 668 batch loss 64206.8633 epoch total loss 31294.2578\n",
      "Trained batch 669 batch loss 65573.0234 epoch total loss 31345.498\n",
      "Trained batch 670 batch loss 67393.3672 epoch total loss 31399.3\n",
      "Trained batch 671 batch loss 67258.5859 epoch total loss 31452.7422\n",
      "Trained batch 672 batch loss 68056.6406 epoch total loss 31507.2109\n",
      "Trained batch 673 batch loss 67946.25 epoch total loss 31561.3555\n",
      "Trained batch 674 batch loss 67290.0391 epoch total loss 31614.3652\n",
      "Trained batch 675 batch loss 67550.9922 epoch total loss 31667.6035\n",
      "Trained batch 676 batch loss 66788.3828 epoch total loss 31719.5566\n",
      "Trained batch 677 batch loss 67610.5625 epoch total loss 31772.5703\n",
      "Trained batch 678 batch loss 67733.1094 epoch total loss 31825.6113\n",
      "Trained batch 679 batch loss 67317.9922 epoch total loss 31877.8828\n",
      "Trained batch 680 batch loss 68472.625 epoch total loss 31931.6973\n",
      "Trained batch 681 batch loss 66911.4453 epoch total loss 31983.0625\n",
      "Trained batch 682 batch loss 67830.2734 epoch total loss 32035.625\n",
      "Trained batch 683 batch loss 66232.3516 epoch total loss 32085.6934\n",
      "Trained batch 684 batch loss 68838.1484 epoch total loss 32139.4238\n",
      "Trained batch 685 batch loss 68714.375 epoch total loss 32192.8184\n",
      "Trained batch 686 batch loss 68794.5859 epoch total loss 32246.1719\n",
      "Trained batch 687 batch loss 68954.2266 epoch total loss 32299.6035\n",
      "Trained batch 688 batch loss 69586.5703 epoch total loss 32353.7988\n",
      "Trained batch 689 batch loss 69000.4062 epoch total loss 32406.9863\n",
      "Trained batch 690 batch loss 67257.2891 epoch total loss 32457.4961\n",
      "Trained batch 691 batch loss 68214.9219 epoch total loss 32509.2422\n",
      "Trained batch 692 batch loss 67682.1641 epoch total loss 32560.0703\n",
      "Trained batch 693 batch loss 67381.8516 epoch total loss 32610.3184\n",
      "Trained batch 694 batch loss 66608.4766 epoch total loss 32659.3047\n",
      "Trained batch 695 batch loss 68603.5469 epoch total loss 32711.0254\n",
      "Trained batch 696 batch loss 68881.0156 epoch total loss 32762.9941\n",
      "Trained batch 697 batch loss 69967.0703 epoch total loss 32816.3711\n",
      "Trained batch 698 batch loss 69217.7734 epoch total loss 32868.5234\n",
      "Trained batch 699 batch loss 69031.1172 epoch total loss 32920.2617\n",
      "Trained batch 700 batch loss 70810.2422 epoch total loss 32974.3867\n",
      "Trained batch 701 batch loss 70572.0859 epoch total loss 33028.0234\n",
      "Trained batch 702 batch loss 70372.9375 epoch total loss 33081.2188\n",
      "Trained batch 703 batch loss 70866.6172 epoch total loss 33134.9688\n",
      "Trained batch 704 batch loss 70860.0781 epoch total loss 33188.5547\n",
      "Trained batch 705 batch loss 67945.4844 epoch total loss 33237.8555\n",
      "Trained batch 706 batch loss 69544.7891 epoch total loss 33289.2812\n",
      "Trained batch 707 batch loss 69428.7188 epoch total loss 33340.3945\n",
      "Trained batch 708 batch loss 71337.1406 epoch total loss 33394.0664\n",
      "Trained batch 709 batch loss 71183.2656 epoch total loss 33447.3672\n",
      "Trained batch 710 batch loss 71166.2344 epoch total loss 33500.4883\n",
      "Trained batch 711 batch loss 69968.0625 epoch total loss 33551.7812\n",
      "Trained batch 712 batch loss 70764.8 epoch total loss 33604.0469\n",
      "Trained batch 713 batch loss 70664.8125 epoch total loss 33656.0234\n",
      "Trained batch 714 batch loss 71783.9 epoch total loss 33709.4219\n",
      "Trained batch 715 batch loss 71671.3516 epoch total loss 33762.5156\n",
      "Trained batch 716 batch loss 70626.7656 epoch total loss 33814.0039\n",
      "Trained batch 717 batch loss 72829.6953 epoch total loss 33868.418\n",
      "Trained batch 718 batch loss 72650.2891 epoch total loss 33922.4336\n",
      "Trained batch 719 batch loss 72638.1719 epoch total loss 33976.2773\n",
      "Trained batch 720 batch loss 71864 epoch total loss 34028.9\n",
      "Trained batch 721 batch loss 73092.5938 epoch total loss 34083.0781\n",
      "Trained batch 722 batch loss 71614.0625 epoch total loss 34135.0625\n",
      "Trained batch 723 batch loss 73343.1328 epoch total loss 34189.293\n",
      "Trained batch 724 batch loss 73510.4375 epoch total loss 34243.6\n",
      "Trained batch 725 batch loss 73454.7344 epoch total loss 34297.6836\n",
      "Trained batch 726 batch loss 72597.6 epoch total loss 34350.4414\n",
      "Trained batch 727 batch loss 73799.4453 epoch total loss 34404.7031\n",
      "Trained batch 728 batch loss 73547.8906 epoch total loss 34458.4727\n",
      "Trained batch 729 batch loss 73676.3672 epoch total loss 34512.2695\n",
      "Trained batch 730 batch loss 72310.9922 epoch total loss 34564.0469\n",
      "Trained batch 731 batch loss 71964.4688 epoch total loss 34615.2109\n",
      "Trained batch 732 batch loss 73573.25 epoch total loss 34668.4336\n",
      "Trained batch 733 batch loss 73219.6719 epoch total loss 34721.0273\n",
      "Trained batch 734 batch loss 74364.7344 epoch total loss 34775.0352\n",
      "Trained batch 735 batch loss 73618.6641 epoch total loss 34827.8828\n",
      "Trained batch 736 batch loss 73307.875 epoch total loss 34880.1641\n",
      "Trained batch 737 batch loss 73213.9219 epoch total loss 34932.1797\n",
      "Trained batch 738 batch loss 74259.5938 epoch total loss 34985.4688\n",
      "Trained batch 739 batch loss 74601.8203 epoch total loss 35039.0781\n",
      "Trained batch 740 batch loss 73344.8828 epoch total loss 35090.8398\n",
      "Trained batch 741 batch loss 72952.5469 epoch total loss 35141.9336\n",
      "Trained batch 742 batch loss 75010.625 epoch total loss 35195.6641\n",
      "Trained batch 743 batch loss 73555.3516 epoch total loss 35247.293\n",
      "Trained batch 744 batch loss 74960.7812 epoch total loss 35300.6719\n",
      "Trained batch 745 batch loss 74665.6172 epoch total loss 35353.5117\n",
      "Trained batch 746 batch loss 73795.9453 epoch total loss 35405.043\n",
      "Trained batch 747 batch loss 72668.2266 epoch total loss 35454.9258\n",
      "Trained batch 748 batch loss 73462.125 epoch total loss 35505.7383\n",
      "Trained batch 749 batch loss 74637.6953 epoch total loss 35557.9844\n",
      "Trained batch 750 batch loss 75638.5078 epoch total loss 35611.4258\n",
      "Trained batch 751 batch loss 74887.375 epoch total loss 35663.7227\n",
      "Trained batch 752 batch loss 75932.2812 epoch total loss 35717.2695\n",
      "Trained batch 753 batch loss 74790.0547 epoch total loss 35769.1602\n",
      "Trained batch 754 batch loss 70641 epoch total loss 35815.4102\n",
      "Trained batch 755 batch loss 68684.2734 epoch total loss 35858.9453\n",
      "Trained batch 756 batch loss 73293.9844 epoch total loss 35908.4648\n",
      "Trained batch 757 batch loss 75284.2656 epoch total loss 35960.4766\n",
      "Trained batch 758 batch loss 76177.0938 epoch total loss 36013.5352\n",
      "Trained batch 759 batch loss 73920.4688 epoch total loss 36063.4766\n",
      "Trained batch 760 batch loss 75632.6562 epoch total loss 36115.543\n",
      "Trained batch 761 batch loss 75175.9375 epoch total loss 36166.8711\n",
      "Trained batch 762 batch loss 76723.0234 epoch total loss 36220.0938\n",
      "Trained batch 763 batch loss 76518.8359 epoch total loss 36272.9102\n",
      "Trained batch 764 batch loss 76947.5391 epoch total loss 36326.1484\n",
      "Trained batch 765 batch loss 77256.7266 epoch total loss 36379.6523\n",
      "Trained batch 766 batch loss 76580.7188 epoch total loss 36432.1328\n",
      "Trained batch 767 batch loss 75661.8906 epoch total loss 36483.2812\n",
      "Trained batch 768 batch loss 75789.4141 epoch total loss 36534.4609\n",
      "Trained batch 769 batch loss 77422.7266 epoch total loss 36587.6289\n",
      "Trained batch 770 batch loss 76429.7266 epoch total loss 36639.375\n",
      "Trained batch 771 batch loss 75583.3594 epoch total loss 36689.8867\n",
      "Trained batch 772 batch loss 76650.2578 epoch total loss 36741.6484\n",
      "Trained batch 773 batch loss 76689.2734 epoch total loss 36793.3281\n",
      "Trained batch 774 batch loss 75509.9922 epoch total loss 36843.3477\n",
      "Trained batch 775 batch loss 75161.8125 epoch total loss 36892.793\n",
      "Trained batch 776 batch loss 77375.3203 epoch total loss 36944.9609\n",
      "Trained batch 777 batch loss 78698.8672 epoch total loss 36998.7\n",
      "Trained batch 778 batch loss 77762.9688 epoch total loss 37051.0938\n",
      "Trained batch 779 batch loss 78440.5312 epoch total loss 37104.2227\n",
      "Trained batch 780 batch loss 79112.3 epoch total loss 37158.0781\n",
      "Trained batch 781 batch loss 79363.4766 epoch total loss 37212.1211\n",
      "Trained batch 782 batch loss 77962.625 epoch total loss 37264.2305\n",
      "Trained batch 783 batch loss 76664.4453 epoch total loss 37314.5508\n",
      "Trained batch 784 batch loss 79183.7734 epoch total loss 37367.9531\n",
      "Trained batch 785 batch loss 77256.1406 epoch total loss 37418.7656\n",
      "Trained batch 786 batch loss 78950.4 epoch total loss 37471.6055\n",
      "Trained batch 920 batch loss 95580.5625 epoch total loss 44510.1\n",
      "Trained batch 921 batch loss 94549.9375 epoch total loss 44564.4297\n",
      "Trained batch 922 batch loss 96511.7188 epoch total loss 44620.7734\n",
      "Trained batch 923 batch loss 94587.8125 epoch total loss 44674.9062\n",
      "Trained batch 924 batch loss 94734.0234 epoch total loss 44729.0859\n",
      "Trained batch 925 batch loss 90290.4844 epoch total loss 44778.3438\n",
      "Trained batch 926 batch loss 94297.5781 epoch total loss 44831.8203\n",
      "Trained batch 927 batch loss 95166.5781 epoch total loss 44886.1172\n",
      "Trained batch 928 batch loss 96024.6172 epoch total loss 44941.2227\n",
      "Trained batch 929 batch loss 95923.5234 epoch total loss 44996.1\n",
      "Trained batch 930 batch loss 95776.2422 epoch total loss 45050.707\n",
      "Trained batch 931 batch loss 96078.1094 epoch total loss 45105.5156\n",
      "Trained batch 932 batch loss 95835.6 epoch total loss 45159.9492\n",
      "Trained batch 933 batch loss 95123.1484 epoch total loss 45213.5\n",
      "Trained batch 934 batch loss 94749.5391 epoch total loss 45266.5352\n",
      "Trained batch 935 batch loss 91900.3438 epoch total loss 45316.4102\n",
      "Trained batch 936 batch loss 95912.125 epoch total loss 45370.4648\n",
      "Trained batch 937 batch loss 96232.8906 epoch total loss 45424.7461\n",
      "Trained batch 938 batch loss 93229.9531 epoch total loss 45475.7109\n",
      "Trained batch 939 batch loss 96459.6172 epoch total loss 45530.0078\n",
      "Trained batch 940 batch loss 96267.5625 epoch total loss 45583.9844\n",
      "Trained batch 941 batch loss 96456.4453 epoch total loss 45638.043\n",
      "Trained batch 942 batch loss 93764.1875 epoch total loss 45689.1328\n",
      "Trained batch 943 batch loss 91426.5547 epoch total loss 45737.6367\n",
      "Trained batch 944 batch loss 92276.8281 epoch total loss 45786.9375\n",
      "Trained batch 945 batch loss 94135.5547 epoch total loss 45838.0977\n",
      "Trained batch 946 batch loss 96247.1641 epoch total loss 45891.3867\n",
      "Trained batch 947 batch loss 95008.6875 epoch total loss 45943.2539\n",
      "Trained batch 948 batch loss 96935.7266 epoch total loss 45997.043\n",
      "Trained batch 949 batch loss 96363.6719 epoch total loss 46050.1172\n",
      "Trained batch 950 batch loss 97979.1797 epoch total loss 46104.7773\n",
      "Trained batch 951 batch loss 94870.4219 epoch total loss 46156.0586\n",
      "Trained batch 952 batch loss 95963.4375 epoch total loss 46208.3789\n",
      "Trained batch 953 batch loss 97392.8281 epoch total loss 46262.0859\n",
      "Trained batch 954 batch loss 98252.5547 epoch total loss 46316.582\n",
      "Trained batch 955 batch loss 97160.0547 epoch total loss 46369.8203\n",
      "Trained batch 956 batch loss 97624.4453 epoch total loss 46423.4336\n",
      "Trained batch 957 batch loss 98477.6 epoch total loss 46477.8281\n",
      "Trained batch 958 batch loss 96947.9844 epoch total loss 46530.5078\n",
      "Trained batch 959 batch loss 96384.9141 epoch total loss 46582.4961\n",
      "Trained batch 960 batch loss 95835.3516 epoch total loss 46633.8\n",
      "Trained batch 961 batch loss 96493.7188 epoch total loss 46685.6797\n",
      "Trained batch 962 batch loss 96525.8594 epoch total loss 46737.4883\n",
      "Trained batch 963 batch loss 96640.1719 epoch total loss 46789.3086\n",
      "Trained batch 964 batch loss 94877.6172 epoch total loss 46839.1914\n",
      "Trained batch 965 batch loss 97457.3 epoch total loss 46891.6445\n",
      "Trained batch 966 batch loss 98443.0547 epoch total loss 46945.0117\n",
      "Trained batch 967 batch loss 98251.7188 epoch total loss 46998.0664\n",
      "Trained batch 968 batch loss 98489.6953 epoch total loss 47051.2617\n",
      "Trained batch 969 batch loss 98822.4375 epoch total loss 47104.6875\n",
      "Trained batch 970 batch loss 97638.3281 epoch total loss 47156.7891\n",
      "Trained batch 971 batch loss 97831.1797 epoch total loss 47208.9766\n",
      "Trained batch 972 batch loss 100409.422 epoch total loss 47263.707\n",
      "Trained batch 973 batch loss 98485.1328 epoch total loss 47316.3477\n",
      "Trained batch 974 batch loss 98923.9688 epoch total loss 47369.3359\n",
      "Trained batch 975 batch loss 96456.3203 epoch total loss 47419.6797\n",
      "Trained batch 976 batch loss 95994.4453 epoch total loss 47469.4492\n",
      "Trained batch 977 batch loss 98977.0078 epoch total loss 47522.168\n",
      "Trained batch 978 batch loss 97523.7188 epoch total loss 47573.2969\n",
      "Trained batch 979 batch loss 100574.25 epoch total loss 47627.4375\n",
      "Trained batch 980 batch loss 99996.6094 epoch total loss 47680.875\n",
      "Trained batch 981 batch loss 95977.0391 epoch total loss 47730.1055\n",
      "Trained batch 982 batch loss 97357.0312 epoch total loss 47780.6406\n",
      "Trained batch 983 batch loss 97485.2734 epoch total loss 47831.2031\n",
      "Trained batch 984 batch loss 101123.477 epoch total loss 47885.3633\n",
      "Trained batch 985 batch loss 102261.695 epoch total loss 47940.5664\n",
      "Trained batch 986 batch loss 101970.758 epoch total loss 47995.3633\n",
      "Trained batch 987 batch loss 102458.938 epoch total loss 48050.5469\n",
      "Trained batch 988 batch loss 102290.133 epoch total loss 48105.4453\n",
      "Trained batch 989 batch loss 102605.133 epoch total loss 48160.5508\n",
      "Trained batch 990 batch loss 100277.383 epoch total loss 48213.1914\n",
      "Trained batch 991 batch loss 101598.695 epoch total loss 48267.0625\n",
      "Trained batch 992 batch loss 99763.625 epoch total loss 48318.9766\n",
      "Trained batch 993 batch loss 100500.719 epoch total loss 48371.5234\n",
      "Trained batch 994 batch loss 100586.188 epoch total loss 48424.0547\n",
      "Trained batch 995 batch loss 102358.68 epoch total loss 48478.2617\n",
      "Trained batch 996 batch loss 98618.5859 epoch total loss 48528.6055\n",
      "Trained batch 997 batch loss 99083.9844 epoch total loss 48579.3125\n",
      "Trained batch 998 batch loss 96801.6172 epoch total loss 48627.6328\n",
      "Trained batch 999 batch loss 101097 epoch total loss 48680.1523\n",
      "Trained batch 1000 batch loss 101071.359 epoch total loss 48732.543\n",
      "Trained batch 1001 batch loss 101058.797 epoch total loss 48784.8203\n",
      "Trained batch 1002 batch loss 102412.039 epoch total loss 48838.3398\n",
      "Trained batch 1003 batch loss 102246.812 epoch total loss 48891.5898\n",
      "Trained batch 1004 batch loss 100800.414 epoch total loss 48943.2891\n",
      "Trained batch 1005 batch loss 100722.531 epoch total loss 48994.8125\n",
      "Trained batch 1006 batch loss 98720.7 epoch total loss 49044.2422\n",
      "Trained batch 1007 batch loss 100976.109 epoch total loss 49095.8125\n",
      "Trained batch 1008 batch loss 102297.047 epoch total loss 49148.5898\n",
      "Trained batch 1009 batch loss 102193.477 epoch total loss 49201.1602\n",
      "Trained batch 1010 batch loss 102073.812 epoch total loss 49253.5078\n",
      "Trained batch 1011 batch loss 101790.789 epoch total loss 49305.4766\n",
      "Trained batch 1012 batch loss 102415.539 epoch total loss 49357.957\n",
      "Trained batch 1013 batch loss 101010.898 epoch total loss 49408.9492\n",
      "Trained batch 1014 batch loss 102460.578 epoch total loss 49461.2656\n",
      "Trained batch 1015 batch loss 102749.766 epoch total loss 49513.7656\n",
      "Trained batch 1016 batch loss 103449.852 epoch total loss 49566.8516\n",
      "Trained batch 1017 batch loss 102535.727 epoch total loss 49618.9336\n",
      "Trained batch 1018 batch loss 100922.945 epoch total loss 49669.332\n",
      "Trained batch 1019 batch loss 101026.109 epoch total loss 49719.7344\n",
      "Trained batch 1020 batch loss 103887.453 epoch total loss 49772.8398\n",
      "Trained batch 1021 batch loss 104545.344 epoch total loss 49826.4844\n",
      "Trained batch 1022 batch loss 103870.453 epoch total loss 49879.3672\n",
      "Trained batch 1023 batch loss 103175.227 epoch total loss 49931.4648\n",
      "Trained batch 1024 batch loss 103664.883 epoch total loss 49983.9375\n",
      "Trained batch 1025 batch loss 103595.023 epoch total loss 50036.2422\n",
      "Trained batch 1026 batch loss 101601.375 epoch total loss 50086.5\n",
      "Trained batch 1027 batch loss 103346.094 epoch total loss 50138.3594\n",
      "Trained batch 1028 batch loss 102802.469 epoch total loss 50189.5898\n",
      "Trained batch 1029 batch loss 104411.039 epoch total loss 50242.2852\n",
      "Trained batch 1030 batch loss 104794.562 epoch total loss 50295.25\n",
      "Trained batch 1031 batch loss 103468.461 epoch total loss 50346.8242\n",
      "Trained batch 1032 batch loss 104377.062 epoch total loss 50399.1797\n",
      "Trained batch 1033 batch loss 104936.641 epoch total loss 50451.9727\n",
      "Trained batch 1034 batch loss 105376.203 epoch total loss 50505.0898\n",
      "Trained batch 1035 batch loss 105194.156 epoch total loss 50557.9336\n",
      "Trained batch 1036 batch loss 105775.711 epoch total loss 50611.2305\n",
      "Trained batch 1037 batch loss 103759.234 epoch total loss 50662.4844\n",
      "Trained batch 1038 batch loss 104512.172 epoch total loss 50714.3633\n",
      "Trained batch 1039 batch loss 103402.125 epoch total loss 50765.0742\n",
      "Trained batch 1040 batch loss 106141.547 epoch total loss 50818.3203\n",
      "Trained batch 1041 batch loss 106110.18 epoch total loss 50871.4336\n",
      "Trained batch 1042 batch loss 105623.766 epoch total loss 50923.9805\n",
      "Trained batch 1043 batch loss 104529.695 epoch total loss 50975.375\n",
      "Trained batch 1044 batch loss 103606.094 epoch total loss 51025.7891\n",
      "Trained batch 1045 batch loss 102754.812 epoch total loss 51075.293\n",
      "Trained batch 1046 batch loss 102859.906 epoch total loss 51124.8\n",
      "Trained batch 1047 batch loss 105222.18 epoch total loss 51176.4688\n",
      "Trained batch 1048 batch loss 102001.359 epoch total loss 51224.9648\n",
      "Trained batch 1049 batch loss 102251.805 epoch total loss 51273.6094\n",
      "Trained batch 1050 batch loss 100119.977 epoch total loss 51320.1289\n",
      "Trained batch 1051 batch loss 101076.914 epoch total loss 51367.4727\n",
      "Trained batch 1052 batch loss 103082.156 epoch total loss 51416.6328\n",
      "Trained batch 1053 batch loss 103459.492 epoch total loss 51466.0547\n",
      "Trained batch 1054 batch loss 104789.422 epoch total loss 51516.6445\n",
      "Trained batch 1055 batch loss 105383.242 epoch total loss 51567.7031\n",
      "Trained batch 1056 batch loss 107407.945 epoch total loss 51620.582\n",
      "Trained batch 1057 batch loss 104915.047 epoch total loss 51671.0039\n",
      "Trained batch 1058 batch loss 105387.984 epoch total loss 51721.7773\n",
      "Trained batch 1059 batch loss 105635.555 epoch total loss 51772.6875\n",
      "Trained batch 1060 batch loss 108287.688 epoch total loss 51826.0039\n",
      "Trained batch 1061 batch loss 108678.82 epoch total loss 51879.5898\n",
      "Trained batch 1062 batch loss 109723.656 epoch total loss 51934.0547\n",
      "Trained batch 1063 batch loss 108499.68 epoch total loss 51987.2695\n",
      "Trained batch 1064 batch loss 110271.508 epoch total loss 52042.0508\n",
      "Trained batch 1065 batch loss 109796.344 epoch total loss 52096.2773\n",
      "Trained batch 1066 batch loss 110335.742 epoch total loss 52150.9102\n",
      "Trained batch 1067 batch loss 109654.656 epoch total loss 52204.8047\n",
      "Trained batch 1068 batch loss 111046.602 epoch total loss 52259.9023\n",
      "Trained batch 1069 batch loss 110916.273 epoch total loss 52314.7734\n",
      "Trained batch 1070 batch loss 110860.984 epoch total loss 52369.4883\n",
      "Trained batch 1071 batch loss 111493.008 epoch total loss 52424.6914\n",
      "Trained batch 1072 batch loss 111329.586 epoch total loss 52479.6367\n",
      "Trained batch 1073 batch loss 110954.672 epoch total loss 52534.1367\n",
      "Trained batch 1074 batch loss 111043.773 epoch total loss 52588.6133\n",
      "Trained batch 1075 batch loss 111283.992 epoch total loss 52643.2148\n",
      "Trained batch 1076 batch loss 109355.508 epoch total loss 52695.9219\n",
      "Trained batch 1077 batch loss 111224.57 epoch total loss 52750.2656\n",
      "Trained batch 1078 batch loss 111433.617 epoch total loss 52804.7031\n",
      "Trained batch 1079 batch loss 109050.633 epoch total loss 52856.832\n",
      "Trained batch 1080 batch loss 111449.695 epoch total loss 52911.082\n",
      "Trained batch 1081 batch loss 110528.219 epoch total loss 52964.3828\n",
      "Trained batch 1082 batch loss 109789.562 epoch total loss 53016.9\n",
      "Trained batch 1083 batch loss 106409.234 epoch total loss 53066.2\n",
      "Trained batch 1084 batch loss 110048.07 epoch total loss 53118.7656\n",
      "Trained batch 1085 batch loss 110079.258 epoch total loss 53171.2617\n",
      "Trained batch 1086 batch loss 110772.695 epoch total loss 53224.3\n",
      "Trained batch 1087 batch loss 109799.695 epoch total loss 53276.3477\n",
      "Trained batch 1088 batch loss 110086.242 epoch total loss 53328.5664\n",
      "Trained batch 1089 batch loss 111532.391 epoch total loss 53382.0117\n",
      "Trained batch 1090 batch loss 113222.625 epoch total loss 53436.9141\n",
      "Trained batch 1091 batch loss 113305.688 epoch total loss 53491.7891\n",
      "Trained batch 1092 batch loss 112988.031 epoch total loss 53546.2695\n",
      "Trained batch 1093 batch loss 112400.422 epoch total loss 53600.1172\n",
      "Trained batch 1094 batch loss 111289.461 epoch total loss 53652.8477\n",
      "Trained batch 1095 batch loss 111168.672 epoch total loss 53705.375\n",
      "Trained batch 1096 batch loss 113256.516 epoch total loss 53759.707\n",
      "Trained batch 1097 batch loss 107441.07 epoch total loss 53808.6406\n",
      "Trained batch 1098 batch loss 112217.422 epoch total loss 53861.8359\n",
      "Trained batch 1099 batch loss 112075.891 epoch total loss 53914.8047\n",
      "Trained batch 1100 batch loss 113858.375 epoch total loss 53969.3\n",
      "Trained batch 1101 batch loss 114073.109 epoch total loss 54023.8906\n",
      "Trained batch 1102 batch loss 113748.258 epoch total loss 54078.0859\n",
      "Trained batch 1103 batch loss 114460.477 epoch total loss 54132.832\n",
      "Trained batch 1104 batch loss 114621.711 epoch total loss 54187.6211\n",
      "Trained batch 1105 batch loss 112884.648 epoch total loss 54240.7383\n",
      "Trained batch 1106 batch loss 111436.586 epoch total loss 54292.4531\n",
      "Trained batch 1107 batch loss 112983.492 epoch total loss 54345.4688\n",
      "Trained batch 1108 batch loss 113888.32 epoch total loss 54399.2109\n",
      "Trained batch 1109 batch loss 113986.727 epoch total loss 54452.9414\n",
      "Trained batch 1110 batch loss 112665.203 epoch total loss 54505.3828\n",
      "Trained batch 1111 batch loss 112986.43 epoch total loss 54558.0234\n",
      "Trained batch 1112 batch loss 114426.555 epoch total loss 54611.8633\n",
      "Trained batch 1113 batch loss 111803.695 epoch total loss 54663.25\n",
      "Trained batch 1114 batch loss 109189.828 epoch total loss 54712.1953\n",
      "Trained batch 1115 batch loss 106217.234 epoch total loss 54758.3867\n",
      "Trained batch 1116 batch loss 113567.617 epoch total loss 54811.082\n",
      "Trained batch 1117 batch loss 113741.609 epoch total loss 54863.8398\n",
      "Trained batch 1118 batch loss 112865.68 epoch total loss 54915.7188\n",
      "Trained batch 1119 batch loss 114777.992 epoch total loss 54969.2109\n",
      "Trained batch 1120 batch loss 114611.203 epoch total loss 55022.4648\n",
      "Trained batch 1121 batch loss 114261.383 epoch total loss 55075.3086\n",
      "Trained batch 1122 batch loss 113304.367 epoch total loss 55127.2031\n",
      "Trained batch 1123 batch loss 114339.445 epoch total loss 55179.9336\n",
      "Trained batch 1124 batch loss 112383.625 epoch total loss 55230.8242\n",
      "Trained batch 1125 batch loss 113193.547 epoch total loss 55282.3477\n",
      "Trained batch 1126 batch loss 115725.516 epoch total loss 55336.0234\n",
      "Trained batch 1127 batch loss 114961.211 epoch total loss 55388.9297\n",
      "Trained batch 1128 batch loss 114754.148 epoch total loss 55441.5586\n",
      "Trained batch 1129 batch loss 109127.758 epoch total loss 55489.1133\n",
      "Trained batch 1130 batch loss 113904.273 epoch total loss 55540.8086\n",
      "Trained batch 1131 batch loss 115204.016 epoch total loss 55593.5586\n",
      "Trained batch 1132 batch loss 116188.234 epoch total loss 55647.0898\n",
      "Trained batch 1133 batch loss 116446.539 epoch total loss 55700.7539\n",
      "Trained batch 1134 batch loss 114062.258 epoch total loss 55752.2188\n",
      "Trained batch 1135 batch loss 112577.438 epoch total loss 55802.2852\n",
      "Trained batch 1136 batch loss 115329.375 epoch total loss 55854.6836\n",
      "Trained batch 1137 batch loss 113297.375 epoch total loss 55905.2031\n",
      "Trained batch 1138 batch loss 115440.398 epoch total loss 55957.5195\n",
      "Trained batch 1139 batch loss 115232.82 epoch total loss 56009.5586\n",
      "Trained batch 1140 batch loss 113180.539 epoch total loss 56059.707\n",
      "Trained batch 1141 batch loss 112823.898 epoch total loss 56109.457\n",
      "Trained batch 1142 batch loss 110560.445 epoch total loss 56157.1367\n",
      "Trained batch 1143 batch loss 113572.633 epoch total loss 56207.3711\n",
      "Trained batch 1144 batch loss 115904.797 epoch total loss 56259.5508\n",
      "Trained batch 1145 batch loss 114804.531 epoch total loss 56310.6836\n",
      "Trained batch 1146 batch loss 116670.156 epoch total loss 56363.3555\n",
      "Trained batch 1147 batch loss 117972.328 epoch total loss 56417.0664\n",
      "Trained batch 1148 batch loss 119338.93 epoch total loss 56471.8789\n",
      "Trained batch 1149 batch loss 118033.664 epoch total loss 56525.457\n",
      "Trained batch 1150 batch loss 116095.578 epoch total loss 56577.2539\n",
      "Trained batch 1151 batch loss 117954.625 epoch total loss 56630.582\n",
      "Trained batch 1152 batch loss 117988.062 epoch total loss 56683.8438\n",
      "Trained batch 1153 batch loss 119336.828 epoch total loss 56738.1836\n",
      "Trained batch 1154 batch loss 118424.125 epoch total loss 56791.6367\n",
      "Trained batch 1155 batch loss 119251.188 epoch total loss 56845.7148\n",
      "Trained batch 1156 batch loss 120203.375 epoch total loss 56900.5234\n",
      "Trained batch 1157 batch loss 115959.828 epoch total loss 56951.5664\n",
      "Trained batch 1158 batch loss 118254.742 epoch total loss 57004.5078\n",
      "Trained batch 1159 batch loss 119866.195 epoch total loss 57058.7461\n",
      "Trained batch 1160 batch loss 120707.57 epoch total loss 57113.6172\n",
      "Trained batch 1161 batch loss 120576.438 epoch total loss 57168.2773\n",
      "Trained batch 1162 batch loss 118767.594 epoch total loss 57221.2891\n",
      "Trained batch 1163 batch loss 120949.055 epoch total loss 57276.0859\n",
      "Trained batch 1164 batch loss 121395.625 epoch total loss 57331.1719\n",
      "Trained batch 1165 batch loss 120977.023 epoch total loss 57385.8\n",
      "Trained batch 1166 batch loss 119294.898 epoch total loss 57438.9\n",
      "Trained batch 1167 batch loss 117592.516 epoch total loss 57490.4453\n",
      "Trained batch 1168 batch loss 115874.906 epoch total loss 57540.4297\n",
      "Trained batch 1169 batch loss 117850.836 epoch total loss 57592.0195\n",
      "Trained batch 1170 batch loss 114097.281 epoch total loss 57640.3164\n",
      "Trained batch 1171 batch loss 115908.086 epoch total loss 57690.0781\n",
      "Trained batch 1172 batch loss 114099.805 epoch total loss 57738.2031\n",
      "Trained batch 1173 batch loss 115357.703 epoch total loss 57787.3281\n",
      "Trained batch 1174 batch loss 117780.719 epoch total loss 57838.4336\n",
      "Trained batch 1175 batch loss 115898.102 epoch total loss 57887.8438\n",
      "Trained batch 1176 batch loss 114087.859 epoch total loss 57935.6328\n",
      "Trained batch 1177 batch loss 116664.531 epoch total loss 57985.5312\n",
      "Trained batch 1178 batch loss 115713.398 epoch total loss 58034.5312\n",
      "Trained batch 1179 batch loss 120148.398 epoch total loss 58087.2188\n",
      "Trained batch 1180 batch loss 120412.945 epoch total loss 58140.0391\n",
      "Trained batch 1181 batch loss 122087.648 epoch total loss 58194.1875\n",
      "Trained batch 1182 batch loss 122681.68 epoch total loss 58248.7461\n",
      "Trained batch 1183 batch loss 123359.32 epoch total loss 58303.7852\n",
      "Trained batch 1184 batch loss 122872.594 epoch total loss 58358.3164\n",
      "Trained batch 1185 batch loss 121213.922 epoch total loss 58411.3633\n",
      "Trained batch 1186 batch loss 116631.859 epoch total loss 58460.4531\n",
      "Trained batch 1187 batch loss 118056.742 epoch total loss 58510.6602\n",
      "Trained batch 1188 batch loss 119908.641 epoch total loss 58562.3438\n",
      "Trained batch 1189 batch loss 123031.188 epoch total loss 58616.5664\n",
      "Trained batch 1190 batch loss 123286.469 epoch total loss 58670.9102\n",
      "Trained batch 1191 batch loss 122962.188 epoch total loss 58724.8906\n",
      "Trained batch 1192 batch loss 123581.625 epoch total loss 58779.3\n",
      "Trained batch 1193 batch loss 123637.148 epoch total loss 58833.668\n",
      "Trained batch 1194 batch loss 120201.992 epoch total loss 58885.0664\n",
      "Trained batch 1195 batch loss 116412.516 epoch total loss 58933.207\n",
      "Trained batch 1196 batch loss 119704.469 epoch total loss 58984.0195\n",
      "Trained batch 1197 batch loss 118634.375 epoch total loss 59033.8516\n",
      "Trained batch 1198 batch loss 116112.922 epoch total loss 59081.4961\n",
      "Trained batch 1199 batch loss 119262.523 epoch total loss 59131.6914\n",
      "Trained batch 1200 batch loss 117129.664 epoch total loss 59180.0195\n",
      "Trained batch 1201 batch loss 118606.188 epoch total loss 59229.5039\n",
      "Trained batch 1202 batch loss 119526.438 epoch total loss 59279.668\n",
      "Trained batch 1203 batch loss 122773.062 epoch total loss 59332.4492\n",
      "Trained batch 1204 batch loss 123602.023 epoch total loss 59385.8281\n",
      "Trained batch 1205 batch loss 120051.852 epoch total loss 59436.168\n",
      "Trained batch 1206 batch loss 121106.797 epoch total loss 59487.3047\n",
      "Trained batch 1207 batch loss 120567.828 epoch total loss 59537.9102\n",
      "Trained batch 1208 batch loss 121693.297 epoch total loss 59589.3633\n",
      "Trained batch 1209 batch loss 124352.125 epoch total loss 59642.9297\n",
      "Trained batch 1210 batch loss 120637.242 epoch total loss 59693.3438\n",
      "Trained batch 1211 batch loss 123601.438 epoch total loss 59746.1133\n",
      "Trained batch 1212 batch loss 124865.422 epoch total loss 59799.8398\n",
      "Trained batch 1213 batch loss 125143.883 epoch total loss 59853.7109\n",
      "Trained batch 1214 batch loss 125012.336 epoch total loss 59907.3867\n",
      "Trained batch 1215 batch loss 125014.242 epoch total loss 59960.9727\n",
      "Trained batch 1216 batch loss 123546.789 epoch total loss 60013.2617\n",
      "Trained batch 1217 batch loss 120736.578 epoch total loss 60063.1602\n",
      "Trained batch 1218 batch loss 123562.094 epoch total loss 60115.2891\n",
      "Trained batch 1219 batch loss 126562.148 epoch total loss 60169.7969\n",
      "Trained batch 1220 batch loss 127259.625 epoch total loss 60224.7852\n",
      "Trained batch 1221 batch loss 126882.109 epoch total loss 60279.3789\n",
      "Trained batch 1222 batch loss 121473.805 epoch total loss 60329.4531\n",
      "Trained batch 1223 batch loss 116864.508 epoch total loss 60375.6797\n",
      "Trained batch 1224 batch loss 122654.656 epoch total loss 60426.5625\n",
      "Trained batch 1225 batch loss 126399.984 epoch total loss 60480.418\n",
      "Trained batch 1226 batch loss 127986.703 epoch total loss 60535.4766\n",
      "Trained batch 1227 batch loss 127661.977 epoch total loss 60590.1875\n",
      "Trained batch 1228 batch loss 126094.547 epoch total loss 60643.5312\n",
      "Trained batch 1229 batch loss 129144.648 epoch total loss 60699.2695\n",
      "Trained batch 1230 batch loss 128754.766 epoch total loss 60754.5938\n",
      "Trained batch 1231 batch loss 129411.469 epoch total loss 60810.3672\n",
      "Trained batch 1232 batch loss 129653.055 epoch total loss 60866.2461\n",
      "Trained batch 1233 batch loss 129827.141 epoch total loss 60922.1719\n",
      "Trained batch 1234 batch loss 129824.477 epoch total loss 60978.0078\n",
      "Trained batch 1235 batch loss 130096.594 epoch total loss 61033.9766\n",
      "Trained batch 1236 batch loss 128448.43 epoch total loss 61088.5195\n",
      "Trained batch 1237 batch loss 130053.297 epoch total loss 61144.2734\n",
      "Trained batch 1238 batch loss 128824.594 epoch total loss 61198.9414\n",
      "Trained batch 1239 batch loss 129306.414 epoch total loss 61253.9062\n",
      "Trained batch 1240 batch loss 120685.836 epoch total loss 61301.8398\n",
      "Trained batch 1241 batch loss 122592.844 epoch total loss 61351.2266\n",
      "Trained batch 1242 batch loss 125178.211 epoch total loss 61402.6133\n",
      "Trained batch 1243 batch loss 123978.086 epoch total loss 61452.957\n",
      "Trained batch 1244 batch loss 127118.625 epoch total loss 61505.7422\n",
      "Trained batch 1245 batch loss 129293.508 epoch total loss 61560.1914\n",
      "Trained batch 1246 batch loss 130922.93 epoch total loss 61615.8594\n",
      "Trained batch 1247 batch loss 129234.438 epoch total loss 61670.082\n",
      "Trained batch 1248 batch loss 131845.141 epoch total loss 61726.3125\n",
      "Trained batch 1249 batch loss 131406.141 epoch total loss 61782.1055\n",
      "Trained batch 1250 batch loss 130589.906 epoch total loss 61837.1523\n",
      "Trained batch 1251 batch loss 129078.211 epoch total loss 61890.9023\n",
      "Trained batch 1252 batch loss 127247.273 epoch total loss 61943.1055\n",
      "Trained batch 1253 batch loss 130660.586 epoch total loss 61997.9492\n",
      "Trained batch 1254 batch loss 129164.07 epoch total loss 62051.5156\n",
      "Trained batch 1255 batch loss 129080.023 epoch total loss 62104.9258\n",
      "Trained batch 1256 batch loss 127681.648 epoch total loss 62157.1328\n",
      "Trained batch 1257 batch loss 127852.773 epoch total loss 62209.4\n",
      "Trained batch 1258 batch loss 128533.602 epoch total loss 62262.125\n",
      "Trained batch 1259 batch loss 129744.688 epoch total loss 62315.7227\n",
      "Trained batch 1260 batch loss 130526.398 epoch total loss 62369.8594\n",
      "Trained batch 1261 batch loss 130991.359 epoch total loss 62424.2773\n",
      "Trained batch 1262 batch loss 133469.25 epoch total loss 62480.5781\n",
      "Trained batch 1263 batch loss 133718.156 epoch total loss 62536.9805\n",
      "Trained batch 1264 batch loss 134020.969 epoch total loss 62593.5391\n",
      "Trained batch 1265 batch loss 133180.141 epoch total loss 62649.3398\n",
      "Trained batch 1266 batch loss 132733.5 epoch total loss 62704.7031\n",
      "Trained batch 1267 batch loss 127393.422 epoch total loss 62755.7578\n",
      "Trained batch 1268 batch loss 130562.438 epoch total loss 62809.2305\n",
      "Trained batch 1269 batch loss 129607.195 epoch total loss 62861.8711\n",
      "Trained batch 1270 batch loss 134609.047 epoch total loss 62918.3633\n",
      "Trained batch 1271 batch loss 130091.25 epoch total loss 62971.2109\n",
      "Trained batch 1272 batch loss 131316.875 epoch total loss 63024.9453\n",
      "Trained batch 1273 batch loss 130628.094 epoch total loss 63078.0508\n",
      "Trained batch 1274 batch loss 134213.75 epoch total loss 63133.8906\n",
      "Trained batch 1275 batch loss 128477.461 epoch total loss 63185.1406\n",
      "Trained batch 1276 batch loss 128959.148 epoch total loss 63236.6914\n",
      "Trained batch 1277 batch loss 133045.688 epoch total loss 63291.3594\n",
      "Trained batch 1278 batch loss 135491.828 epoch total loss 63347.8516\n",
      "Trained batch 1279 batch loss 133548.438 epoch total loss 63402.7383\n",
      "Trained batch 1280 batch loss 131742.203 epoch total loss 63456.1328\n",
      "Trained batch 1281 batch loss 133301.75 epoch total loss 63510.6562\n",
      "Trained batch 1282 batch loss 129268.383 epoch total loss 63561.9531\n",
      "Trained batch 1283 batch loss 127138.828 epoch total loss 63611.5039\n",
      "Trained batch 1284 batch loss 128245.211 epoch total loss 63661.8438\n",
      "Trained batch 1285 batch loss 132465.375 epoch total loss 63715.3867\n",
      "Trained batch 1286 batch loss 132842.578 epoch total loss 63769.1367\n",
      "Trained batch 1287 batch loss 134264.984 epoch total loss 63823.9141\n",
      "Trained batch 1288 batch loss 136225.547 epoch total loss 63880.125\n",
      "Trained batch 1289 batch loss 136487.859 epoch total loss 63936.4531\n",
      "Trained batch 1290 batch loss 138645.297 epoch total loss 63994.3672\n",
      "Trained batch 1291 batch loss 137950.297 epoch total loss 64051.6562\n",
      "Trained batch 1292 batch loss 137549.781 epoch total loss 64108.543\n",
      "Trained batch 1293 batch loss 139903.312 epoch total loss 64167.1641\n",
      "Trained batch 1294 batch loss 138856.344 epoch total loss 64224.8828\n",
      "Trained batch 1295 batch loss 137965.609 epoch total loss 64281.8281\n",
      "Trained batch 1296 batch loss 139908 epoch total loss 64340.1836\n",
      "Trained batch 1297 batch loss 136396.688 epoch total loss 64395.7422\n",
      "Trained batch 1298 batch loss 139157.328 epoch total loss 64453.3438\n",
      "Trained batch 1299 batch loss 135589.734 epoch total loss 64508.1094\n",
      "Trained batch 1300 batch loss 132761.25 epoch total loss 64560.6094\n",
      "Trained batch 1301 batch loss 131906.297 epoch total loss 64612.3711\n",
      "Trained batch 1302 batch loss 136519.359 epoch total loss 64667.6\n",
      "Trained batch 1303 batch loss 139876.359 epoch total loss 64725.3242\n",
      "Trained batch 1304 batch loss 141405.828 epoch total loss 64784.1289\n",
      "Trained batch 1305 batch loss 141786.703 epoch total loss 64843.1328\n",
      "Trained batch 1306 batch loss 140195.188 epoch total loss 64900.8281\n",
      "Trained batch 1307 batch loss 142413.516 epoch total loss 64960.1328\n",
      "Trained batch 1308 batch loss 140538.516 epoch total loss 65017.9141\n",
      "Trained batch 1309 batch loss 140753.016 epoch total loss 65075.7695\n",
      "Trained batch 1310 batch loss 140002.031 epoch total loss 65132.9648\n",
      "Trained batch 1311 batch loss 142990.484 epoch total loss 65192.3555\n",
      "Trained batch 1312 batch loss 142921.062 epoch total loss 65251.5977\n",
      "Trained batch 1313 batch loss 140908.203 epoch total loss 65309.2227\n",
      "Trained batch 1314 batch loss 138458.062 epoch total loss 65364.8906\n",
      "Trained batch 1315 batch loss 140220.562 epoch total loss 65421.8164\n",
      "Trained batch 1316 batch loss 136342.969 epoch total loss 65475.707\n",
      "Trained batch 1317 batch loss 133530.484 epoch total loss 65527.3789\n",
      "Trained batch 1318 batch loss 139635.672 epoch total loss 65583.6\n",
      "Trained batch 1319 batch loss 143429.047 epoch total loss 65642.625\n",
      "Trained batch 1320 batch loss 144534.328 epoch total loss 65702.3906\n",
      "Trained batch 1321 batch loss 140633.812 epoch total loss 65759.1172\n",
      "Trained batch 1322 batch loss 138853.156 epoch total loss 65814.4062\n",
      "Trained batch 1323 batch loss 141785.281 epoch total loss 65871.8281\n",
      "Trained batch 1324 batch loss 141904.109 epoch total loss 65929.2578\n",
      "Trained batch 1325 batch loss 145018.438 epoch total loss 65988.9453\n",
      "Trained batch 1326 batch loss 145095.172 epoch total loss 66048.6\n",
      "Trained batch 1327 batch loss 143630.625 epoch total loss 66107.0703\n",
      "Trained batch 1328 batch loss 142132.125 epoch total loss 66164.3203\n",
      "Trained batch 1329 batch loss 142921.188 epoch total loss 66222.0703\n",
      "Trained batch 1330 batch loss 139925.859 epoch total loss 66277.4922\n",
      "Trained batch 1331 batch loss 143202.641 epoch total loss 66335.2812\n",
      "Trained batch 1332 batch loss 140196.375 epoch total loss 66390.7422\n",
      "Trained batch 1333 batch loss 144315.328 epoch total loss 66449.1953\n",
      "Trained batch 1334 batch loss 142345.656 epoch total loss 66506.0859\n",
      "Trained batch 1335 batch loss 143375.422 epoch total loss 66563.6641\n",
      "Trained batch 1336 batch loss 142609.578 epoch total loss 66620.5859\n",
      "Trained batch 1337 batch loss 141289.297 epoch total loss 66676.4375\n",
      "Trained batch 1338 batch loss 145395.844 epoch total loss 66735.2656\n",
      "Trained batch 1339 batch loss 142584.859 epoch total loss 66791.9141\n",
      "Trained batch 1340 batch loss 144918.891 epoch total loss 66850.2188\n",
      "Trained batch 1341 batch loss 143930.453 epoch total loss 66907.6953\n",
      "Trained batch 1342 batch loss 142588.922 epoch total loss 66964.0859\n",
      "Trained batch 1343 batch loss 141768.234 epoch total loss 67019.7891\n",
      "Trained batch 1344 batch loss 142681.781 epoch total loss 67076.0859\n",
      "Trained batch 1345 batch loss 145797.625 epoch total loss 67134.6172\n",
      "Trained batch 1346 batch loss 147305.266 epoch total loss 67194.1719\n",
      "Trained batch 1347 batch loss 149214.344 epoch total loss 67255.0703\n",
      "Trained batch 1348 batch loss 149645.828 epoch total loss 67316.1875\n",
      "Trained batch 1349 batch loss 149644.031 epoch total loss 67377.2188\n",
      "Trained batch 1350 batch loss 148918.703 epoch total loss 67437.625\n",
      "Trained batch 1351 batch loss 148550.984 epoch total loss 67497.6641\n",
      "Trained batch 1352 batch loss 149462.438 epoch total loss 67558.2891\n",
      "Trained batch 1353 batch loss 146975.375 epoch total loss 67616.9844\n",
      "Trained batch 1354 batch loss 147348.781 epoch total loss 67675.875\n",
      "Trained batch 1355 batch loss 149272.266 epoch total loss 67736.0938\n",
      "Trained batch 1356 batch loss 147932.266 epoch total loss 67795.2422\n",
      "Trained batch 1357 batch loss 146312.391 epoch total loss 67853.1\n",
      "Trained batch 1358 batch loss 149676.766 epoch total loss 67913.3516\n",
      "Trained batch 1359 batch loss 149626.125 epoch total loss 67973.4844\n",
      "Trained batch 1360 batch loss 146996.172 epoch total loss 68031.5859\n",
      "Trained batch 1361 batch loss 147339.75 epoch total loss 68089.8594\n",
      "Trained batch 1362 batch loss 144460.078 epoch total loss 68145.9297\n",
      "Trained batch 1363 batch loss 146870.422 epoch total loss 68203.6953\n",
      "Trained batch 1364 batch loss 147823.406 epoch total loss 68262.0625\n",
      "Trained batch 1365 batch loss 149289.562 epoch total loss 68321.4219\n",
      "Trained batch 1366 batch loss 149769.719 epoch total loss 68381.0469\n",
      "Trained batch 1367 batch loss 140213.625 epoch total loss 68433.6\n",
      "Trained batch 1368 batch loss 142831.688 epoch total loss 68487.9844\n",
      "Trained batch 1369 batch loss 147814.938 epoch total loss 68545.9297\n",
      "Trained batch 1370 batch loss 150068.734 epoch total loss 68605.4375\n",
      "Trained batch 1371 batch loss 148489.484 epoch total loss 68663.7\n",
      "Trained batch 1372 batch loss 148878.688 epoch total loss 68722.1719\n",
      "Trained batch 1373 batch loss 150080.703 epoch total loss 68781.4219\n",
      "Trained batch 1374 batch loss 150694.609 epoch total loss 68841.0391\n",
      "Trained batch 1375 batch loss 152068.469 epoch total loss 68901.5703\n",
      "Trained batch 1376 batch loss 152002.25 epoch total loss 68961.9688\n",
      "Trained batch 1377 batch loss 150734.609 epoch total loss 69021.3516\n",
      "Trained batch 1378 batch loss 152018.484 epoch total loss 69081.5781\n",
      "Trained batch 1379 batch loss 150584.797 epoch total loss 69140.6797\n",
      "Trained batch 1380 batch loss 143985.359 epoch total loss 69194.9141\n",
      "Trained batch 1381 batch loss 150051.062 epoch total loss 69253.4609\n",
      "Trained batch 1382 batch loss 150359.781 epoch total loss 69312.1484\n",
      "Trained batch 1383 batch loss 148546.906 epoch total loss 69369.4375\n",
      "Trained batch 1384 batch loss 149266.516 epoch total loss 69427.1641\n",
      "Trained batch 1385 batch loss 144663.5 epoch total loss 69481.4922\n",
      "Trained batch 1386 batch loss 145830.484 epoch total loss 69536.5781\n",
      "Trained batch 1387 batch loss 145010.422 epoch total loss 69590.9922\n",
      "Trained batch 1388 batch loss 147446.281 epoch total loss 69647.0859\n",
      "Epoch 10 train loss 69647.0859375\n",
      "Validated batch 1 batch loss inf\n",
      "Validated batch 2 batch loss inf\n",
      "Validated batch 3 batch loss inf\n",
      "Validated batch 4 batch loss inf\n",
      "Validated batch 5 batch loss inf\n",
      "Validated batch 6 batch loss inf\n",
      "Validated batch 7 batch loss inf\n",
      "Validated batch 8 batch loss inf\n",
      "Validated batch 9 batch loss inf\n",
      "Validated batch 10 batch loss inf\n",
      "Validated batch 11 batch loss inf\n",
      "Validated batch 12 batch loss inf\n",
      "Validated batch 13 batch loss inf\n",
      "Validated batch 14 batch loss inf\n",
      "Validated batch 15 batch loss inf\n",
      "Validated batch 16 batch loss inf\n",
      "Validated batch 17 batch loss inf\n",
      "Validated batch 18 batch loss inf\n",
      "Validated batch 19 batch loss inf\n",
      "Validated batch 20 batch loss inf\n",
      "Validated batch 21 batch loss inf\n",
      "Validated batch 22 batch loss inf\n",
      "Validated batch 23 batch loss inf\n",
      "Validated batch 24 batch loss inf\n",
      "Validated batch 25 batch loss inf\n",
      "Validated batch 26 batch loss inf\n",
      "Validated batch 27 batch loss inf\n",
      "Validated batch 28 batch loss inf\n",
      "Validated batch 29 batch loss inf\n",
      "Validated batch 30 batch loss inf\n",
      "Validated batch 31 batch loss inf\n",
      "Validated batch 32 batch loss inf\n",
      "Validated batch 33 batch loss inf\n",
      "Validated batch 34 batch loss inf\n",
      "Validated batch 35 batch loss inf\n",
      "Validated batch 36 batch loss inf\n",
      "Validated batch 37 batch loss inf\n",
      "Validated batch 38 batch loss inf\n",
      "Validated batch 39 batch loss inf\n",
      "Validated batch 40 batch loss inf\n",
      "Validated batch 41 batch loss inf\n",
      "Validated batch 42 batch loss inf\n",
      "Validated batch 43 batch loss inf\n",
      "Validated batch 44 batch loss inf\n",
      "Validated batch 45 batch loss inf\n",
      "Validated batch 46 batch loss inf\n",
      "Validated batch 47 batch loss inf\n",
      "Validated batch 48 batch loss inf\n",
      "Validated batch 49 batch loss inf\n",
      "Validated batch 50 batch loss inf\n",
      "Validated batch 51 batch loss inf\n",
      "Validated batch 52 batch loss inf\n",
      "Validated batch 53 batch loss inf\n",
      "Validated batch 54 batch loss inf\n",
      "Validated batch 55 batch loss inf\n",
      "Validated batch 56 batch loss inf\n",
      "Validated batch 57 batch loss inf\n",
      "Validated batch 58 batch loss inf\n",
      "Validated batch 59 batch loss inf\n",
      "Validated batch 60 batch loss inf\n",
      "Validated batch 61 batch loss inf\n",
      "Validated batch 62 batch loss inf\n",
      "Validated batch 63 batch loss inf\n",
      "Validated batch 64 batch loss inf\n",
      "Validated batch 65 batch loss inf\n",
      "Validated batch 66 batch loss inf\n",
      "Validated batch 67 batch loss inf\n",
      "Validated batch 68 batch loss inf\n",
      "Validated batch 69 batch loss inf\n",
      "Validated batch 70 batch loss inf\n",
      "Validated batch 71 batch loss inf\n",
      "Validated batch 72 batch loss inf\n",
      "Validated batch 73 batch loss inf\n",
      "Validated batch 74 batch loss inf\n",
      "Validated batch 75 batch loss inf\n",
      "Validated batch 76 batch loss inf\n",
      "Validated batch 77 batch loss inf\n",
      "Validated batch 78 batch loss inf\n",
      "Validated batch 79 batch loss inf\n",
      "Validated batch 80 batch loss inf\n",
      "Validated batch 81 batch loss inf\n",
      "Validated batch 82 batch loss inf\n",
      "Validated batch 83 batch loss inf\n",
      "Validated batch 84 batch loss inf\n",
      "Validated batch 85 batch loss inf\n",
      "Validated batch 86 batch loss inf\n",
      "Validated batch 87 batch loss inf\n",
      "Validated batch 88 batch loss inf\n",
      "Validated batch 89 batch loss inf\n",
      "Validated batch 90 batch loss inf\n",
      "Validated batch 91 batch loss inf\n",
      "Validated batch 92 batch loss inf\n",
      "Validated batch 93 batch loss inf\n",
      "Validated batch 94 batch loss inf\n",
      "Validated batch 95 batch loss inf\n",
      "Validated batch 96 batch loss inf\n",
      "Validated batch 97 batch loss inf\n",
      "Validated batch 98 batch loss inf\n",
      "Validated batch 99 batch loss inf\n",
      "Validated batch 100 batch loss inf\n",
      "Validated batch 101 batch loss inf\n",
      "Validated batch 102 batch loss inf\n",
      "Validated batch 103 batch loss inf\n",
      "Validated batch 104 batch loss inf\n",
      "Validated batch 105 batch loss inf\n",
      "Validated batch 106 batch loss inf\n",
      "Validated batch 107 batch loss inf\n",
      "Validated batch 108 batch loss inf\n",
      "Validated batch 109 batch loss inf\n",
      "Validated batch 110 batch loss inf\n",
      "Validated batch 111 batch loss inf\n",
      "Validated batch 112 batch loss inf\n",
      "Validated batch 113 batch loss inf\n",
      "Validated batch 114 batch loss inf\n",
      "Validated batch 115 batch loss inf\n",
      "Validated batch 116 batch loss inf\n",
      "Validated batch 117 batch loss inf\n",
      "Validated batch 118 batch loss inf\n",
      "Validated batch 119 batch loss inf\n",
      "Validated batch 120 batch loss inf\n",
      "Validated batch 121 batch loss inf\n",
      "Validated batch 122 batch loss inf\n",
      "Validated batch 123 batch loss inf\n",
      "Validated batch 124 batch loss inf\n",
      "Validated batch 125 batch loss inf\n",
      "Validated batch 126 batch loss inf\n",
      "Validated batch 127 batch loss inf\n",
      "Validated batch 128 batch loss inf\n",
      "Validated batch 129 batch loss inf\n",
      "Validated batch 130 batch loss inf\n",
      "Validated batch 131 batch loss inf\n",
      "Validated batch 132 batch loss inf\n",
      "Validated batch 133 batch loss inf\n",
      "Validated batch 134 batch loss inf\n",
      "Validated batch 135 batch loss inf\n",
      "Validated batch 136 batch loss inf\n",
      "Validated batch 137 batch loss inf\n",
      "Validated batch 138 batch loss inf\n",
      "Validated batch 139 batch loss inf\n",
      "Validated batch 140 batch loss inf\n",
      "Validated batch 141 batch loss inf\n",
      "Validated batch 142 batch loss inf\n",
      "Validated batch 143 batch loss inf\n",
      "Validated batch 144 batch loss inf\n",
      "Validated batch 145 batch loss inf\n",
      "Validated batch 146 batch loss inf\n",
      "Validated batch 147 batch loss inf\n",
      "Validated batch 148 batch loss inf\n",
      "Validated batch 149 batch loss inf\n",
      "Validated batch 150 batch loss inf\n",
      "Validated batch 151 batch loss inf\n",
      "Validated batch 152 batch loss inf\n",
      "Validated batch 153 batch loss inf\n",
      "Validated batch 154 batch loss inf\n",
      "Validated batch 155 batch loss inf\n",
      "Validated batch 156 batch loss inf\n",
      "Validated batch 157 batch loss inf\n",
      "Validated batch 158 batch loss inf\n",
      "Validated batch 159 batch loss inf\n",
      "Validated batch 160 batch loss inf\n",
      "Validated batch 161 batch loss inf\n",
      "Validated batch 162 batch loss inf\n",
      "Validated batch 163 batch loss inf\n",
      "Validated batch 164 batch loss inf\n",
      "Validated batch 165 batch loss inf\n",
      "Validated batch 166 batch loss inf\n",
      "Validated batch 167 batch loss inf\n",
      "Validated batch 168 batch loss inf\n",
      "Validated batch 169 batch loss inf\n",
      "Validated batch 170 batch loss inf\n",
      "Validated batch 171 batch loss inf\n",
      "Validated batch 172 batch loss inf\n",
      "Validated batch 173 batch loss inf\n",
      "Validated batch 174 batch loss inf\n",
      "Validated batch 175 batch loss inf\n",
      "Validated batch 176 batch loss inf\n",
      "Validated batch 177 batch loss inf\n",
      "Validated batch 178 batch loss inf\n",
      "Validated batch 179 batch loss inf\n",
      "Validated batch 180 batch loss inf\n",
      "Validated batch 181 batch loss inf\n",
      "Validated batch 182 batch loss inf\n",
      "Validated batch 183 batch loss inf\n",
      "Validated batch 184 batch loss inf\n",
      "Validated batch 185 batch loss inf\n",
      "Epoch 10 val loss inf\n",
      "Training time: 206.91 minutes\n"
     ]
    }
   ],
   "source": [
    "train_tfrecords = os.path.join(TFRECORD_PATH, 'train*')\n",
    "val_tfrecords = os.path.join(TFRECORD_PATH, 'val*')\n",
    "epochs = 10\n",
    "batch_size = 16\n",
    "num_heatmap = 16\n",
    "learning_rate = 0.5\n",
    "\n",
    "start_time = time.time()\n",
    "best_model_file = train(epochs, learning_rate, num_heatmap, batch_size, train_tfrecords, val_tfrecords)\n",
    "\n",
    "# 종료 시간 기록 및 경과 시간 계산\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "# 학습 소요 시간 출력 (분 단위로 변환)\n",
    "elapsed_minutes = elapsed_time / 60\n",
    "print(f\"Training time: {elapsed_minutes:.2f} minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6234041",
   "metadata": {},
   "source": [
    "epoch 5에서 최적의 loss 값을 찾았고 10 epoch 206.91 분 걸림으로 1epoch 당 20분 정도 소요 된것을 확인 할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32bd553d",
   "metadata": {},
   "source": [
    "## 동작 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a88d310f",
   "metadata": {},
   "outputs": [],
   "source": [
    "WEIGHTS_PATH = './model_hourglass-epoch-5-loss-1.5745.h5'\n",
    "\n",
    "model = StackedHourglassNetwork(IMAGE_SHAPE, 4, 1)\n",
    "model.load_weights(WEIGHTS_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8f080b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "R_ANKLE = 0\n",
    "R_KNEE = 1\n",
    "R_HIP = 2\n",
    "L_HIP = 3\n",
    "L_KNEE = 4\n",
    "L_ANKLE = 5\n",
    "PELVIS = 6\n",
    "THORAX = 7\n",
    "UPPER_NECK = 8\n",
    "HEAD_TOP = 9\n",
    "R_WRIST = 10\n",
    "R_ELBOW = 11\n",
    "R_SHOULDER = 12\n",
    "L_SHOULDER = 13\n",
    "L_ELBOW = 14\n",
    "L_WRIST = 15\n",
    "\n",
    "MPII_BONES = [\n",
    "    [R_ANKLE, R_KNEE],\n",
    "    [R_KNEE, R_HIP],\n",
    "    [R_HIP, PELVIS],\n",
    "    [L_HIP, PELVIS],\n",
    "    [L_HIP, L_KNEE],\n",
    "    [L_KNEE, L_ANKLE],\n",
    "    [PELVIS, THORAX],\n",
    "    [THORAX, UPPER_NECK],\n",
    "    [UPPER_NECK, HEAD_TOP],\n",
    "    [R_WRIST, R_ELBOW],\n",
    "    [R_ELBOW, R_SHOULDER],\n",
    "    [THORAX, R_SHOULDER],\n",
    "    [THORAX, L_SHOULDER],\n",
    "    [L_SHOULDER, L_ELBOW],\n",
    "    [L_ELBOW, L_WRIST]\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "11b24ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_max_coordinates(heatmaps):\n",
    "    flatten_heatmaps = tf.reshape(heatmaps, (-1, 16))\n",
    "    indices = tf.math.argmax(flatten_heatmaps, axis=0)\n",
    "    y = tf.cast(indices / 64, dtype=tf.int64)\n",
    "    x = indices - 64 * y\n",
    "    return tf.stack([x, y], axis=1).numpy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "90f2703b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keypoints_from_heatmap(heatmaps):\n",
    "    max_keypoints = find_max_coordinates(heatmaps)\n",
    "\n",
    "    padded_heatmap = np.pad(heatmaps, [[1,1],[1,1],[0,0]], mode='constant')\n",
    "    adjusted_keypoints = []\n",
    "    for i, keypoint in enumerate(max_keypoints):\n",
    "        max_y = keypoint[1]+1\n",
    "        max_x = keypoint[0]+1\n",
    "        \n",
    "        patch = padded_heatmap[max_y-1:max_y+2, max_x-1:max_x+2, i]\n",
    "        patch[1][1] = 0\n",
    "        \n",
    "        index = np.argmax(patch)\n",
    "        \n",
    "        next_y = index // 3\n",
    "        next_x = index - next_y * 3\n",
    "        delta_y = (next_y - 1) / 4\n",
    "        delta_x = (next_x - 1) / 4\n",
    "        \n",
    "        adjusted_keypoint_x = keypoint[0] + delta_x\n",
    "        adjusted_keypoint_y = keypoint[1] + delta_y\n",
    "        adjusted_keypoints.append((adjusted_keypoint_x, adjusted_keypoint_y))\n",
    "        \n",
    "    adjusted_keypoints = np.clip(adjusted_keypoints, 0, 64)\n",
    "    normalized_keypoints = adjusted_keypoints / 64\n",
    "    return normalized_keypoints\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c3804a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, image_path):\n",
    "    encoded = tf.io.read_file(image_path)\n",
    "    image = tf.io.decode_jpeg(encoded)\n",
    "    inputs = tf.image.resize(image, (256, 256))\n",
    "    inputs = tf.cast(inputs, tf.float32) / 127.5 - 1\n",
    "    inputs = tf.expand_dims(inputs, 0)\n",
    "    outputs = model(inputs, training=False)\n",
    "    if type(outputs) != list:\n",
    "        outputs = [outputs]\n",
    "    heatmap = tf.squeeze(outputs[-1], axis=0).numpy()\n",
    "    kp = extract_keypoints_from_heatmap(heatmap)\n",
    "    return image, kp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "11c52e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_keypoints_on_image(image, keypoints, index=None):\n",
    "    fig,ax = plt.subplots(1)\n",
    "    ax.imshow(image)\n",
    "    joints = []\n",
    "    for i, joint in enumerate(keypoints):\n",
    "        joint_x = joint[0] * image.shape[1]\n",
    "        joint_y = joint[1] * image.shape[0]\n",
    "        if index is not None and index != i:\n",
    "            continue\n",
    "        plt.scatter(joint_x, joint_y, s=10, c='red', marker='o')\n",
    "    plt.show()\n",
    "\n",
    "def draw_skeleton_on_image(image, keypoints, index=None):\n",
    "    fig,ax = plt.subplots(1)\n",
    "    ax.imshow(image)\n",
    "    joints = []\n",
    "    for i, joint in enumerate(keypoints):\n",
    "        joint_x = joint[0] * image.shape[1]\n",
    "        joint_y = joint[1] * image.shape[0]\n",
    "        joints.append((joint_x, joint_y))\n",
    "    \n",
    "    for bone in MPII_BONES:\n",
    "        joint_1 = joints[bone[0]]\n",
    "        joint_2 = joints[bone[1]]\n",
    "        plt.plot([joint_1[0], joint_2[0]], [joint_1[1], joint_2[1]], linewidth=5, alpha=0.7)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d75d4ec",
   "metadata": {},
   "source": [
    "## simplebase model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cc255473",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "94773248/94765736 [==============================] - 1s 0us/step\n",
      "94781440/94765736 [==============================] - 1s 0us/step\n",
      "Model: \"simple_baseline\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 256, 256, 3)]     0         \n",
      "_________________________________________________________________\n",
      "resnet50 (Functional)        (None, None, None, 2048)  23587712  \n",
      "_________________________________________________________________\n",
      "sequential (Sequential)      (None, 64, 64, 256)       10489600  \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 64, 64, 16)        4112      \n",
      "=================================================================\n",
      "Total params: 34,081,424\n",
      "Trainable params: 34,026,768\n",
      "Non-trainable params: 54,656\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(os.path.join(os.getenv('HOME'), 'aiffel/mpii'))\n",
    "\n",
    "\n",
    "from simplebaseline import Simplebaseline\n",
    "\n",
    "simplebase_model = Simplebaseline(input_shape=(256, 256, 3), num_heatmap=16)\n",
    "simplebase_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd03666",
   "metadata": {},
   "source": [
    "   simplebased model 같은 경우 hourglass의 model 에서는 polling을 진행하여 param 개수를 줄였지만 simplebase mdoel에서는 resnet으로 feature map를 뽑을때 downsampling을 진행하여 hourglass model보다는 parm 개수가 더 많은거 같다. \n",
    "   parmater의 변형도 연산속도에 영향을 미친다."
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "e6336498",
   "metadata": {},
   "source": [
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "450c3a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import traceback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f32ecf77",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer(object):\n",
    "    def __init__(self, model, epochs, global_batch_size, strategy, initial_learning_rate, l2_lambda=0.01):\n",
    "        self.model = model\n",
    "        self.epochs = epochs\n",
    "        self.strategy = strategy\n",
    "        self.global_batch_size = global_batch_size\n",
    "        self.loss_object = tf.keras.losses.MeanSquaredError(reduction=tf.keras.losses.Reduction.NONE)\n",
    "        self.current_learning_rate = initial_learning_rate\n",
    "        self.last_val_loss = math.inf\n",
    "        self.lowest_val_loss = math.inf\n",
    "        self.patience_count = 0\n",
    "        self.max_patience = 10\n",
    "        self.best_model = None\n",
    "        self.l2_lambda = l2_lambda  # L2 정규화 강도\n",
    "\n",
    "        # 분산 전략 내에서 옵티마이저 초기화\n",
    "        with self.strategy.scope():\n",
    "            self.optimizer = tf.keras.optimizers.Adam(learning_rate=self.current_learning_rate)\n",
    "\n",
    "    def lr_decay(self):\n",
    "        if self.patience_count >= self.max_patience:\n",
    "            self.current_learning_rate /= 10.0\n",
    "            self.patience_count = 0\n",
    "        elif self.last_val_loss == self.lowest_val_loss:\n",
    "            self.patience_count = 0\n",
    "        self.patience_count += 1\n",
    "        self.optimizer.learning_rate = self.current_learning_rate\n",
    "\n",
    "    def compute_loss(self, labels, outputs):\n",
    "        # 기본 MSE 손실 계산\n",
    "        loss = 0.0\n",
    "        for output in outputs:\n",
    "            weights = tf.cast(labels > 0, dtype=tf.float32) * 81 + 1\n",
    "            loss += tf.math.reduce_mean(tf.math.square(labels - output) * weights) * (1. / self.global_batch_size)\n",
    "\n",
    "        # L2 정규화 손실 계산 및 추가\n",
    "        l2_loss = tf.add_n([tf.nn.l2_loss(var) for var in self.model.trainable_variables])\n",
    "        loss += self.l2_lambda * l2_loss\n",
    "        return loss\n",
    "\n",
    "    def train_step(self, inputs):\n",
    "        images, labels = inputs\n",
    "        with tf.GradientTape() as tape:\n",
    "            outputs = self.model(images, training=True)\n",
    "            loss = self.compute_loss(labels, outputs)\n",
    "\n",
    "        grads = tape.gradient(target=loss, sources=self.model.trainable_variables)\n",
    "        try:\n",
    "            self.optimizer.apply_gradients(zip(grads, self.model.trainable_variables))\n",
    "        except Exception as e:\n",
    "            tf.print(\"Error during optimizer application:\", e)\n",
    "            raise\n",
    "        return loss\n",
    "\n",
    "    def val_step(self, inputs):\n",
    "        images, labels = inputs\n",
    "        outputs = self.model(images, training=False)\n",
    "        loss = self.compute_loss(labels, outputs)\n",
    "        return loss\n",
    "\n",
    "    def run(self, train_dist_dataset, val_dist_dataset):\n",
    "        print(\"Starting training...\")\n",
    "\n",
    "        @tf.function\n",
    "        def distributed_train_epoch(dataset):\n",
    "            total_loss = 0.0\n",
    "            num_train_batches = 0.0\n",
    "            for one_batch in dataset:\n",
    "                try:\n",
    "                    per_replica_loss = self.strategy.run(self.train_step, args=(one_batch,))\n",
    "                    batch_loss = self.strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_loss, axis=None)\n",
    "                    total_loss += batch_loss\n",
    "                    num_train_batches += 1\n",
    "                    tf.print('Trained batch', num_train_batches, 'batch loss', batch_loss, 'epoch total loss', total_loss / num_train_batches)\n",
    "                except Exception as e:\n",
    "                    tf.print(\"Error during training epoch:\", e)\n",
    "                    raise\n",
    "            \n",
    "            return total_loss, num_train_batches\n",
    "\n",
    "        @tf.function\n",
    "        def distributed_val_epoch(dataset):\n",
    "            total_loss = 0.0\n",
    "            num_val_batches = 0.0\n",
    "            for one_batch in dataset:\n",
    "                try:\n",
    "                    per_replica_loss = self.strategy.run(self.val_step, args=(one_batch,))\n",
    "                    num_val_batches += 1\n",
    "                    batch_loss = self.strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_loss, axis=None)\n",
    "                    tf.print('Validated batch', num_val_batches, 'batch loss', batch_loss)\n",
    "                    if not tf.math.is_nan(batch_loss):\n",
    "                        total_loss += batch_loss\n",
    "                    else:\n",
    "                        num_val_batches -= 1\n",
    "                except Exception as e:\n",
    "                    tf.print(\"Error during validation epoch:\", e)\n",
    "                    raise\n",
    "\n",
    "            return total_loss, num_val_batches\n",
    "\n",
    "        for epoch in range(1, self.epochs + 1):\n",
    "            self.lr_decay()\n",
    "            print('Start epoch {} with learning rate {}'.format(epoch, self.current_learning_rate))\n",
    "            start_time = time.time()\n",
    "            \n",
    "            train_total_loss, num_train_batches = distributed_train_epoch(train_dist_dataset)\n",
    "            train_loss = train_total_loss / num_train_batches\n",
    "            print('Epoch {} train loss {}'.format(epoch, train_loss))\n",
    "\n",
    "            val_total_loss, num_val_batches = distributed_val_epoch(val_dist_dataset)\n",
    "            val_loss = val_total_loss / num_val_batches\n",
    "            print('Epoch {} val loss {}'.format(epoch, val_loss))\n",
    "            \n",
    "            end_time = time.time()\n",
    "            epoch_duration = end_time - start_time\n",
    "            print('Epoch {} completed in {:.2f} seconds'.format(epoch, epoch_duration))\n",
    "\n",
    "\n",
    "            if val_loss < self.lowest_val_loss:\n",
    "                self.save_model(epoch, val_loss)\n",
    "                self.lowest_val_loss = val_loss\n",
    "            self.last_val_loss = val_loss\n",
    "\n",
    "        return self.best_model\n",
    "\n",
    "    def save_model(self, epoch, loss):\n",
    "        model_name = './model_simplebase-epoch-{}-loss-{:.4f}.h5'.format(epoch, loss)\n",
    "        self.model.save_weights(model_name)\n",
    "        self.best_model = model_name\n",
    "        print(\"Model {} saved.\".format(model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "100a3400",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs, learning_rate, num_heatmap, batch_size, train_tfrecords, val_tfrecords):\n",
    "    strategy = tf.distribute.MirroredStrategy()\n",
    "    global_batch_size = strategy.num_replicas_in_sync * batch_size\n",
    "    train_dataset = create_dataset(train_tfrecords, global_batch_size, num_heatmap, is_train=True)\n",
    "    val_dataset = create_dataset(val_tfrecords, global_batch_size, num_heatmap, is_train=False)\n",
    "\n",
    "    if not os.path.exists(MODEL_PATH):\n",
    "        os.makedirs(MODEL_PATH)\n",
    "\n",
    "    with strategy.scope():\n",
    "        train_dist_dataset = strategy.experimental_distribute_dataset(train_dataset)\n",
    "        val_dist_dataset = strategy.experimental_distribute_dataset(val_dataset)\n",
    "\n",
    "        model = Simplebaseline(input_shape=(256, 256, 3), num_heatmap=16)\n",
    "\n",
    "        trainer = Trainer(model, epochs, global_batch_size, strategy, initial_learning_rate=learning_rate)\n",
    "\n",
    "        print('Start training...')\n",
    "        return trainer.run(train_dist_dataset, val_dist_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "170e2978",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "019bad5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "Start training...\n",
      "Starting training...\n",
      "Start epoch 1 with learning rate 0.0007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/tensorflow/python/data/ops/dataset_ops.py:374: UserWarning: To make it possible to preserve tf.data options across serialization boundaries, their implementation has moved to be part of the TensorFlow graph. As a consequence, the options value is in general no longer known at graph construction time. Invoking this method in graph mode retains the legacy behavior of the original implementation, but note that the returned value might not reflect the actual value of the options.\n",
      "  warnings.warn(\"To make it possible to preserve tf.data options across \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1 batch loss 392.601166 epoch total loss 392.601166\n",
      "Trained batch 2 batch loss 391.491 epoch total loss 392.046082\n",
      "Trained batch 3 batch loss 388.22 epoch total loss 390.770721\n",
      "Trained batch 4 batch loss 386.26004 epoch total loss 389.643036\n",
      "Trained batch 5 batch loss 384.914337 epoch total loss 388.697296\n",
      "Trained batch 6 batch loss 382.902039 epoch total loss 387.731415\n",
      "Trained batch 7 batch loss 381.449829 epoch total loss 386.834045\n",
      "Trained batch 8 batch loss 380.099243 epoch total loss 385.992188\n",
      "Trained batch 9 batch loss 378.722321 epoch total loss 385.184448\n",
      "Trained batch 10 batch loss 377.399841 epoch total loss 384.405975\n",
      "Trained batch 11 batch loss 376.336914 epoch total loss 383.672394\n",
      "Trained batch 12 batch loss 375.575439 epoch total loss 382.99765\n",
      "Trained batch 13 batch loss 374.577423 epoch total loss 382.349945\n",
      "Trained batch 14 batch loss 373.547546 epoch total loss 381.721191\n",
      "Trained batch 15 batch loss 372.50528 epoch total loss 381.106812\n",
      "Trained batch 16 batch loss 371.321075 epoch total loss 380.495209\n",
      "Trained batch 17 batch loss 370.045 epoch total loss 379.880493\n",
      "Trained batch 18 batch loss 368.827698 epoch total loss 379.266449\n",
      "Trained batch 19 batch loss 367.351196 epoch total loss 378.639313\n",
      "Trained batch 20 batch loss 366.626068 epoch total loss 378.038635\n",
      "Trained batch 21 batch loss 365.640656 epoch total loss 377.448273\n",
      "Trained batch 22 batch loss 365.57547 epoch total loss 376.9086\n",
      "Trained batch 23 batch loss 365.005554 epoch total loss 376.391083\n",
      "Trained batch 24 batch loss 364.127 epoch total loss 375.880096\n",
      "Trained batch 25 batch loss 363.322083 epoch total loss 375.377777\n",
      "Trained batch 26 batch loss 362.376251 epoch total loss 374.877716\n",
      "Trained batch 27 batch loss 361.432922 epoch total loss 374.37973\n",
      "Trained batch 28 batch loss 361.046936 epoch total loss 373.903564\n",
      "Trained batch 29 batch loss 360.318665 epoch total loss 373.43512\n",
      "Trained batch 30 batch loss 359.516602 epoch total loss 372.971161\n",
      "Trained batch 31 batch loss 358.655426 epoch total loss 372.509369\n",
      "Trained batch 32 batch loss 357.892151 epoch total loss 372.052582\n",
      "Trained batch 33 batch loss 357.068 epoch total loss 371.598511\n",
      "Trained batch 34 batch loss 356.661285 epoch total loss 371.15918\n",
      "Trained batch 35 batch loss 355.932556 epoch total loss 370.724121\n",
      "Trained batch 36 batch loss 356.074432 epoch total loss 370.3172\n",
      "Trained batch 37 batch loss 355.533783 epoch total loss 369.917664\n",
      "Trained batch 38 batch loss 354.60495 epoch total loss 369.514679\n",
      "Trained batch 39 batch loss 354.373688 epoch total loss 369.126465\n",
      "Trained batch 40 batch loss 353.67627 epoch total loss 368.740173\n",
      "Trained batch 41 batch loss 353.352539 epoch total loss 368.364868\n",
      "Trained batch 42 batch loss 352.58197 epoch total loss 367.989105\n",
      "Trained batch 43 batch loss 352.080048 epoch total loss 367.61911\n",
      "Trained batch 44 batch loss 351.249451 epoch total loss 367.24707\n",
      "Trained batch 45 batch loss 350.510956 epoch total loss 366.875183\n",
      "Trained batch 46 batch loss 350.508057 epoch total loss 366.519348\n",
      "Trained batch 47 batch loss 349.692841 epoch total loss 366.161346\n",
      "Trained batch 48 batch loss 349.264832 epoch total loss 365.809357\n",
      "Trained batch 49 batch loss 348.80188 epoch total loss 365.46228\n",
      "Trained batch 50 batch loss 348.250214 epoch total loss 365.118042\n",
      "Trained batch 51 batch loss 347.670837 epoch total loss 364.77594\n",
      "Trained batch 52 batch loss 347.264038 epoch total loss 364.439148\n",
      "Trained batch 53 batch loss 346.710876 epoch total loss 364.104645\n",
      "Trained batch 54 batch loss 346.162537 epoch total loss 363.7724\n",
      "Trained batch 55 batch loss 345.677643 epoch total loss 363.44339\n",
      "Trained batch 56 batch loss 344.811951 epoch total loss 363.110687\n",
      "Trained batch 57 batch loss 344.733917 epoch total loss 362.7883\n",
      "Trained batch 58 batch loss 344.684 epoch total loss 362.476166\n",
      "Trained batch 59 batch loss 344.208954 epoch total loss 362.166534\n",
      "Trained batch 60 batch loss 343.864807 epoch total loss 361.861511\n",
      "Trained batch 61 batch loss 343.691528 epoch total loss 361.56366\n",
      "Trained batch 62 batch loss 342.982 epoch total loss 361.263947\n",
      "Trained batch 63 batch loss 342.415161 epoch total loss 360.964783\n",
      "Trained batch 64 batch loss 341.845428 epoch total loss 360.666046\n",
      "Trained batch 65 batch loss 341.150024 epoch total loss 360.365814\n",
      "Trained batch 66 batch loss 340.770416 epoch total loss 360.068878\n",
      "Trained batch 67 batch loss 339.905609 epoch total loss 359.767944\n",
      "Trained batch 68 batch loss 339.451111 epoch total loss 359.469177\n",
      "Trained batch 69 batch loss 339.408051 epoch total loss 359.178436\n",
      "Trained batch 70 batch loss 338.943054 epoch total loss 358.889374\n",
      "Trained batch 71 batch loss 338.603821 epoch total loss 358.603668\n",
      "Trained batch 72 batch loss 338.142334 epoch total loss 358.319458\n",
      "Trained batch 73 batch loss 337.537323 epoch total loss 358.03479\n",
      "Trained batch 74 batch loss 337.125 epoch total loss 357.752228\n",
      "Trained batch 75 batch loss 336.710175 epoch total loss 357.47168\n",
      "Trained batch 76 batch loss 336.08313 epoch total loss 357.190247\n",
      "Trained batch 77 batch loss 335.390839 epoch total loss 356.907135\n",
      "Trained batch 78 batch loss 334.430542 epoch total loss 356.618958\n",
      "Trained batch 79 batch loss 334.184784 epoch total loss 356.335\n",
      "Trained batch 80 batch loss 334.664886 epoch total loss 356.064117\n",
      "Trained batch 81 batch loss 333.908356 epoch total loss 355.790588\n",
      "Trained batch 82 batch loss 333.621979 epoch total loss 355.520233\n",
      "Trained batch 83 batch loss 333.206 epoch total loss 355.251373\n",
      "Trained batch 84 batch loss 332.842041 epoch total loss 354.984589\n",
      "Trained batch 85 batch loss 331.738861 epoch total loss 354.71109\n",
      "Trained batch 86 batch loss 331.996429 epoch total loss 354.44696\n",
      "Trained batch 87 batch loss 331.268433 epoch total loss 354.180542\n",
      "Trained batch 88 batch loss 330.741394 epoch total loss 353.914185\n",
      "Trained batch 89 batch loss 330.2099 epoch total loss 353.647858\n",
      "Trained batch 90 batch loss 330.426117 epoch total loss 353.389832\n",
      "Trained batch 91 batch loss 330.173096 epoch total loss 353.134705\n",
      "Trained batch 92 batch loss 329.654785 epoch total loss 352.879486\n",
      "Trained batch 93 batch loss 329.131073 epoch total loss 352.624115\n",
      "Trained batch 94 batch loss 329.021271 epoch total loss 352.373016\n",
      "Trained batch 95 batch loss 328.713287 epoch total loss 352.123962\n",
      "Trained batch 96 batch loss 328.195068 epoch total loss 351.874725\n",
      "Trained batch 97 batch loss 327.771057 epoch total loss 351.626221\n",
      "Trained batch 98 batch loss 327.484894 epoch total loss 351.379852\n",
      "Trained batch 99 batch loss 326.873627 epoch total loss 351.132324\n",
      "Trained batch 100 batch loss 326.237793 epoch total loss 350.883392\n",
      "Trained batch 101 batch loss 325.841553 epoch total loss 350.635437\n",
      "Trained batch 102 batch loss 325.895172 epoch total loss 350.392883\n",
      "Trained batch 103 batch loss 325.49884 epoch total loss 350.151215\n",
      "Trained batch 104 batch loss 325.05072 epoch total loss 349.909851\n",
      "Trained batch 105 batch loss 324.086578 epoch total loss 349.66391\n",
      "Trained batch 106 batch loss 324.239868 epoch total loss 349.424042\n",
      "Trained batch 107 batch loss 323.277161 epoch total loss 349.179688\n",
      "Trained batch 108 batch loss 323.309509 epoch total loss 348.940155\n",
      "Trained batch 109 batch loss 323.0578 epoch total loss 348.702698\n",
      "Trained batch 110 batch loss 323.019287 epoch total loss 348.469208\n",
      "Trained batch 111 batch loss 322.645325 epoch total loss 348.236542\n",
      "Trained batch 112 batch loss 322.222687 epoch total loss 348.004303\n",
      "Trained batch 113 batch loss 321.773376 epoch total loss 347.772156\n",
      "Trained batch 114 batch loss 321.343933 epoch total loss 347.540344\n",
      "Trained batch 115 batch loss 320.852722 epoch total loss 347.308258\n",
      "Trained batch 116 batch loss 320.511932 epoch total loss 347.07724\n",
      "Trained batch 117 batch loss 320.447235 epoch total loss 346.84964\n",
      "Trained batch 118 batch loss 320.009857 epoch total loss 346.622192\n",
      "Trained batch 119 batch loss 319.687714 epoch total loss 346.395844\n",
      "Trained batch 120 batch loss 319.284424 epoch total loss 346.169922\n",
      "Trained batch 121 batch loss 318.909454 epoch total loss 345.944641\n",
      "Trained batch 122 batch loss 318.457794 epoch total loss 345.71933\n",
      "Trained batch 123 batch loss 317.831848 epoch total loss 345.492615\n",
      "Trained batch 124 batch loss 317.254059 epoch total loss 345.264862\n",
      "Trained batch 125 batch loss 317.273407 epoch total loss 345.040924\n",
      "Trained batch 126 batch loss 317.078552 epoch total loss 344.819\n",
      "Trained batch 127 batch loss 316.71 epoch total loss 344.597687\n",
      "Trained batch 128 batch loss 316.273651 epoch total loss 344.376404\n",
      "Trained batch 129 batch loss 315.842346 epoch total loss 344.155212\n",
      "Trained batch 130 batch loss 315.491364 epoch total loss 343.934723\n",
      "Trained batch 131 batch loss 315.746796 epoch total loss 343.719543\n",
      "Trained batch 132 batch loss 314.821442 epoch total loss 343.50061\n",
      "Trained batch 133 batch loss 314.195587 epoch total loss 343.280273\n",
      "Trained batch 134 batch loss 314.13 epoch total loss 343.062744\n",
      "Trained batch 135 batch loss 313.610077 epoch total loss 342.844574\n",
      "Trained batch 136 batch loss 313.244598 epoch total loss 342.626923\n",
      "Trained batch 137 batch loss 312.866699 epoch total loss 342.409698\n",
      "Trained batch 138 batch loss 312.417664 epoch total loss 342.192383\n",
      "Trained batch 139 batch loss 312.357391 epoch total loss 341.977722\n",
      "Trained batch 140 batch loss 311.822723 epoch total loss 341.762329\n",
      "Trained batch 141 batch loss 311.480377 epoch total loss 341.547577\n",
      "Trained batch 142 batch loss 311.342712 epoch total loss 341.334869\n",
      "Trained batch 143 batch loss 311.003143 epoch total loss 341.122772\n",
      "Trained batch 144 batch loss 310.632263 epoch total loss 340.911\n",
      "Trained batch 145 batch loss 310.434448 epoch total loss 340.700836\n",
      "Trained batch 146 batch loss 309.909149 epoch total loss 340.489929\n",
      "Trained batch 147 batch loss 309.513092 epoch total loss 340.279205\n",
      "Trained batch 148 batch loss 308.966583 epoch total loss 340.067627\n",
      "Trained batch 149 batch loss 308.399078 epoch total loss 339.855072\n",
      "Trained batch 150 batch loss 307.906281 epoch total loss 339.64209\n",
      "Trained batch 151 batch loss 307.902527 epoch total loss 339.431885\n",
      "Trained batch 152 batch loss 307.754486 epoch total loss 339.22348\n",
      "Trained batch 153 batch loss 307.255341 epoch total loss 339.014526\n",
      "Trained batch 154 batch loss 306.91275 epoch total loss 338.806091\n",
      "Trained batch 155 batch loss 306.645 epoch total loss 338.598602\n",
      "Trained batch 156 batch loss 305.637085 epoch total loss 338.387299\n",
      "Trained batch 157 batch loss 306.307892 epoch total loss 338.182983\n",
      "Trained batch 158 batch loss 306.442596 epoch total loss 337.982086\n",
      "Trained batch 159 batch loss 305.806 epoch total loss 337.779694\n",
      "Trained batch 160 batch loss 305.206879 epoch total loss 337.576111\n",
      "Trained batch 161 batch loss 304.905243 epoch total loss 337.373199\n",
      "Trained batch 162 batch loss 304.33371 epoch total loss 337.16925\n",
      "Trained batch 163 batch loss 304.152679 epoch total loss 336.966675\n",
      "Trained batch 164 batch loss 303.673798 epoch total loss 336.763672\n",
      "Trained batch 165 batch loss 303.632751 epoch total loss 336.562866\n",
      "Trained batch 166 batch loss 302.987549 epoch total loss 336.360626\n",
      "Trained batch 167 batch loss 302.660431 epoch total loss 336.158813\n",
      "Trained batch 168 batch loss 302.869446 epoch total loss 335.960693\n",
      "Trained batch 169 batch loss 302.474762 epoch total loss 335.762543\n",
      "Trained batch 170 batch loss 301.605316 epoch total loss 335.561615\n",
      "Trained batch 171 batch loss 301.270508 epoch total loss 335.361084\n",
      "Trained batch 172 batch loss 301.340515 epoch total loss 335.1633\n",
      "Trained batch 173 batch loss 300.240417 epoch total loss 334.961426\n",
      "Trained batch 174 batch loss 299.444397 epoch total loss 334.757324\n",
      "Trained batch 175 batch loss 299.23877 epoch total loss 334.554352\n",
      "Trained batch 176 batch loss 299.430725 epoch total loss 334.354767\n",
      "Trained batch 177 batch loss 299.684 epoch total loss 334.158905\n",
      "Trained batch 178 batch loss 299.289307 epoch total loss 333.963\n",
      "Trained batch 179 batch loss 298.858673 epoch total loss 333.766876\n",
      "Trained batch 180 batch loss 298.958618 epoch total loss 333.573517\n",
      "Trained batch 181 batch loss 298.677734 epoch total loss 333.380707\n",
      "Trained batch 182 batch loss 298.071411 epoch total loss 333.186676\n",
      "Trained batch 183 batch loss 297.990265 epoch total loss 332.994354\n",
      "Trained batch 184 batch loss 297.488708 epoch total loss 332.801392\n",
      "Trained batch 185 batch loss 297.092743 epoch total loss 332.608368\n",
      "Trained batch 186 batch loss 296.760284 epoch total loss 332.415649\n",
      "Trained batch 187 batch loss 296.197693 epoch total loss 332.222\n",
      "Trained batch 188 batch loss 295.548889 epoch total loss 332.026917\n",
      "Trained batch 189 batch loss 294.937439 epoch total loss 331.830688\n",
      "Trained batch 190 batch loss 295.085236 epoch total loss 331.637299\n",
      "Trained batch 191 batch loss 295.252686 epoch total loss 331.446808\n",
      "Trained batch 192 batch loss 294.536743 epoch total loss 331.254547\n",
      "Trained batch 193 batch loss 294.000854 epoch total loss 331.061523\n",
      "Trained batch 194 batch loss 293.89978 epoch total loss 330.869965\n",
      "Trained batch 195 batch loss 293.456024 epoch total loss 330.678101\n",
      "Trained batch 196 batch loss 293.049835 epoch total loss 330.486115\n",
      "Trained batch 197 batch loss 293.192505 epoch total loss 330.296814\n",
      "Trained batch 198 batch loss 292.719116 epoch total loss 330.107025\n",
      "Trained batch 199 batch loss 292.681396 epoch total loss 329.918976\n",
      "Trained batch 200 batch loss 292.298187 epoch total loss 329.730865\n",
      "Trained batch 201 batch loss 291.912231 epoch total loss 329.542725\n",
      "Trained batch 202 batch loss 291.758942 epoch total loss 329.355652\n",
      "Trained batch 203 batch loss 290.989 epoch total loss 329.166687\n",
      "Trained batch 204 batch loss 290.682465 epoch total loss 328.978027\n",
      "Trained batch 205 batch loss 290.297394 epoch total loss 328.789337\n",
      "Trained batch 206 batch loss 290.324036 epoch total loss 328.6026\n",
      "Trained batch 207 batch loss 289.928406 epoch total loss 328.415771\n",
      "Trained batch 208 batch loss 290.123718 epoch total loss 328.231659\n",
      "Trained batch 209 batch loss 289.668427 epoch total loss 328.04718\n",
      "Trained batch 210 batch loss 289.433228 epoch total loss 327.863281\n",
      "Trained batch 211 batch loss 289.225861 epoch total loss 327.680176\n",
      "Trained batch 212 batch loss 288.831116 epoch total loss 327.496918\n",
      "Trained batch 213 batch loss 288.847504 epoch total loss 327.31543\n",
      "Trained batch 214 batch loss 288.281769 epoch total loss 327.133026\n",
      "Trained batch 215 batch loss 287.783875 epoch total loss 326.95\n",
      "Trained batch 216 batch loss 287.285858 epoch total loss 326.766388\n",
      "Trained batch 217 batch loss 286.917297 epoch total loss 326.582733\n",
      "Trained batch 218 batch loss 286.688904 epoch total loss 326.399719\n",
      "Trained batch 219 batch loss 286.128906 epoch total loss 326.21582\n",
      "Trained batch 220 batch loss 285.181946 epoch total loss 326.029297\n",
      "Trained batch 221 batch loss 284.86322 epoch total loss 325.843018\n",
      "Trained batch 222 batch loss 285.133209 epoch total loss 325.659637\n",
      "Trained batch 223 batch loss 284.829498 epoch total loss 325.476532\n",
      "Trained batch 224 batch loss 284.216858 epoch total loss 325.292328\n",
      "Trained batch 225 batch loss 283.920746 epoch total loss 325.108459\n",
      "Trained batch 226 batch loss 283.957703 epoch total loss 324.926392\n",
      "Trained batch 227 batch loss 283.921234 epoch total loss 324.745758\n",
      "Trained batch 228 batch loss 283.702484 epoch total loss 324.565765\n",
      "Trained batch 229 batch loss 283.239655 epoch total loss 324.385315\n",
      "Trained batch 230 batch loss 283.04 epoch total loss 324.205536\n",
      "Trained batch 231 batch loss 282.862213 epoch total loss 324.02655\n",
      "Trained batch 232 batch loss 281.134491 epoch total loss 323.841675\n",
      "Trained batch 233 batch loss 280.841675 epoch total loss 323.657135\n",
      "Trained batch 234 batch loss 281.008087 epoch total loss 323.474854\n",
      "Trained batch 235 batch loss 281.733093 epoch total loss 323.297241\n",
      "Trained batch 236 batch loss 281.346588 epoch total loss 323.119476\n",
      "Trained batch 237 batch loss 281.336609 epoch total loss 322.943176\n",
      "Trained batch 238 batch loss 280.969025 epoch total loss 322.766815\n",
      "Trained batch 239 batch loss 280.549469 epoch total loss 322.590149\n",
      "Trained batch 240 batch loss 280.237732 epoch total loss 322.413666\n",
      "Trained batch 241 batch loss 280.002869 epoch total loss 322.237671\n",
      "Trained batch 242 batch loss 279.474945 epoch total loss 322.060974\n",
      "Trained batch 243 batch loss 279.442291 epoch total loss 321.88562\n",
      "Trained batch 244 batch loss 279.246307 epoch total loss 321.710876\n",
      "Trained batch 245 batch loss 278.390228 epoch total loss 321.534058\n",
      "Trained batch 246 batch loss 278.052338 epoch total loss 321.3573\n",
      "Trained batch 247 batch loss 277.526764 epoch total loss 321.17984\n",
      "Trained batch 248 batch loss 277.141357 epoch total loss 321.002258\n",
      "Trained batch 249 batch loss 276.62384 epoch total loss 320.824036\n",
      "Trained batch 250 batch loss 276.841858 epoch total loss 320.648132\n",
      "Trained batch 251 batch loss 275.64212 epoch total loss 320.468811\n",
      "Trained batch 252 batch loss 275.34906 epoch total loss 320.289764\n",
      "Trained batch 253 batch loss 274.856628 epoch total loss 320.110199\n",
      "Trained batch 254 batch loss 274.887756 epoch total loss 319.93219\n",
      "Trained batch 255 batch loss 275.107635 epoch total loss 319.756409\n",
      "Trained batch 256 batch loss 275.344086 epoch total loss 319.582916\n",
      "Trained batch 257 batch loss 275.404266 epoch total loss 319.411\n",
      "Trained batch 258 batch loss 275.366455 epoch total loss 319.240295\n",
      "Trained batch 259 batch loss 274.916412 epoch total loss 319.069153\n",
      "Trained batch 260 batch loss 274.53772 epoch total loss 318.897888\n",
      "Trained batch 261 batch loss 274.260284 epoch total loss 318.726868\n",
      "Trained batch 262 batch loss 274.121216 epoch total loss 318.556641\n",
      "Trained batch 263 batch loss 273.624146 epoch total loss 318.385773\n",
      "Trained batch 264 batch loss 273.444641 epoch total loss 318.215546\n",
      "Trained batch 265 batch loss 272.81839 epoch total loss 318.04425\n",
      "Trained batch 266 batch loss 272.412384 epoch total loss 317.872711\n",
      "Trained batch 267 batch loss 272.271332 epoch total loss 317.701935\n",
      "Trained batch 268 batch loss 271.924805 epoch total loss 317.531097\n",
      "Trained batch 269 batch loss 271.470856 epoch total loss 317.359863\n",
      "Trained batch 270 batch loss 271.018951 epoch total loss 317.188232\n",
      "Trained batch 271 batch loss 270.992767 epoch total loss 317.017761\n",
      "Trained batch 272 batch loss 270.50119 epoch total loss 316.846741\n",
      "Trained batch 273 batch loss 270.207245 epoch total loss 316.675903\n",
      "Trained batch 274 batch loss 269.904755 epoch total loss 316.505219\n",
      "Trained batch 275 batch loss 270.004181 epoch total loss 316.336151\n",
      "Trained batch 276 batch loss 269.236511 epoch total loss 316.165466\n",
      "Trained batch 277 batch loss 269.080475 epoch total loss 315.995483\n",
      "Trained batch 278 batch loss 268.813171 epoch total loss 315.825775\n",
      "Trained batch 279 batch loss 268.464783 epoch total loss 315.656\n",
      "Trained batch 280 batch loss 268.074249 epoch total loss 315.486084\n",
      "Trained batch 281 batch loss 267.637573 epoch total loss 315.315796\n",
      "Trained batch 282 batch loss 267.753052 epoch total loss 315.147125\n",
      "Trained batch 283 batch loss 267.512 epoch total loss 314.978821\n",
      "Trained batch 284 batch loss 267.273 epoch total loss 314.810852\n",
      "Trained batch 285 batch loss 266.624634 epoch total loss 314.641785\n",
      "Trained batch 286 batch loss 266.238342 epoch total loss 314.472534\n",
      "Trained batch 287 batch loss 265.82309 epoch total loss 314.30304\n",
      "Trained batch 288 batch loss 266.269501 epoch total loss 314.13623\n",
      "Trained batch 289 batch loss 265.03183 epoch total loss 313.966309\n",
      "Trained batch 290 batch loss 264.817841 epoch total loss 313.796844\n",
      "Trained batch 291 batch loss 264.273956 epoch total loss 313.626678\n",
      "Trained batch 292 batch loss 264.115387 epoch total loss 313.457123\n",
      "Trained batch 293 batch loss 264.519806 epoch total loss 313.2901\n",
      "Trained batch 294 batch loss 263.651703 epoch total loss 313.121246\n",
      "Trained batch 295 batch loss 263.939056 epoch total loss 312.954529\n",
      "Trained batch 296 batch loss 263.777496 epoch total loss 312.788391\n",
      "Trained batch 297 batch loss 263.441925 epoch total loss 312.622253\n",
      "Trained batch 298 batch loss 262.802216 epoch total loss 312.455078\n",
      "Trained batch 299 batch loss 262.812378 epoch total loss 312.289062\n",
      "Trained batch 300 batch loss 262.511963 epoch total loss 312.123138\n",
      "Trained batch 301 batch loss 262.744049 epoch total loss 311.959106\n",
      "Trained batch 302 batch loss 262.455017 epoch total loss 311.795166\n",
      "Trained batch 303 batch loss 262.662872 epoch total loss 311.633026\n",
      "Trained batch 304 batch loss 262.032 epoch total loss 311.469849\n",
      "Trained batch 305 batch loss 262.211212 epoch total loss 311.30835\n",
      "Trained batch 306 batch loss 261.406 epoch total loss 311.145264\n",
      "Trained batch 307 batch loss 261.30368 epoch total loss 310.98291\n",
      "Trained batch 308 batch loss 260.93811 epoch total loss 310.820435\n",
      "Trained batch 309 batch loss 260.150482 epoch total loss 310.656464\n",
      "Trained batch 310 batch loss 260.156433 epoch total loss 310.493561\n",
      "Trained batch 311 batch loss 259.922577 epoch total loss 310.330933\n",
      "Trained batch 312 batch loss 259.836609 epoch total loss 310.169098\n",
      "Trained batch 313 batch loss 259.459198 epoch total loss 310.00708\n",
      "Trained batch 314 batch loss 259.164642 epoch total loss 309.845154\n",
      "Trained batch 315 batch loss 258.879974 epoch total loss 309.68338\n",
      "Trained batch 316 batch loss 258.693573 epoch total loss 309.522034\n",
      "Trained batch 317 batch loss 258.232178 epoch total loss 309.360229\n",
      "Trained batch 318 batch loss 258.028229 epoch total loss 309.198822\n",
      "Trained batch 319 batch loss 257.714508 epoch total loss 309.037415\n",
      "Trained batch 320 batch loss 257.488159 epoch total loss 308.876312\n",
      "Trained batch 321 batch loss 257.026398 epoch total loss 308.714783\n",
      "Trained batch 322 batch loss 256.633636 epoch total loss 308.55304\n",
      "Trained batch 323 batch loss 256.28598 epoch total loss 308.391235\n",
      "Trained batch 324 batch loss 255.828873 epoch total loss 308.229\n",
      "Trained batch 325 batch loss 255.517975 epoch total loss 308.066803\n",
      "Trained batch 326 batch loss 255.761688 epoch total loss 307.906342\n",
      "Trained batch 327 batch loss 255.654327 epoch total loss 307.746552\n",
      "Trained batch 328 batch loss 255.310211 epoch total loss 307.5867\n",
      "Trained batch 329 batch loss 254.825775 epoch total loss 307.426331\n",
      "Trained batch 330 batch loss 254.870911 epoch total loss 307.267059\n",
      "Trained batch 331 batch loss 254.493866 epoch total loss 307.107635\n",
      "Trained batch 332 batch loss 254.277863 epoch total loss 306.948517\n",
      "Trained batch 333 batch loss 253.819977 epoch total loss 306.788971\n",
      "Trained batch 334 batch loss 253.948257 epoch total loss 306.630768\n",
      "Trained batch 335 batch loss 252.897827 epoch total loss 306.470367\n",
      "Trained batch 336 batch loss 253.43425 epoch total loss 306.312531\n",
      "Trained batch 337 batch loss 253.08931 epoch total loss 306.154572\n",
      "Trained batch 338 batch loss 252.678787 epoch total loss 305.996368\n",
      "Trained batch 339 batch loss 252.757462 epoch total loss 305.839325\n",
      "Trained batch 340 batch loss 252.615494 epoch total loss 305.6828\n",
      "Trained batch 341 batch loss 252.198624 epoch total loss 305.52594\n",
      "Trained batch 342 batch loss 251.306686 epoch total loss 305.367401\n",
      "Trained batch 343 batch loss 250.873703 epoch total loss 305.208527\n",
      "Trained batch 344 batch loss 250.67514 epoch total loss 305.05\n",
      "Trained batch 345 batch loss 250.563965 epoch total loss 304.892059\n",
      "Trained batch 346 batch loss 250.865265 epoch total loss 304.735901\n",
      "Trained batch 347 batch loss 250.399567 epoch total loss 304.579315\n",
      "Trained batch 348 batch loss 250.482025 epoch total loss 304.423859\n",
      "Trained batch 349 batch loss 250.004745 epoch total loss 304.267944\n",
      "Trained batch 350 batch loss 249.695221 epoch total loss 304.11203\n",
      "Trained batch 351 batch loss 249.397705 epoch total loss 303.956146\n",
      "Trained batch 352 batch loss 249.161 epoch total loss 303.800507\n",
      "Trained batch 353 batch loss 248.972931 epoch total loss 303.645172\n",
      "Trained batch 354 batch loss 248.779602 epoch total loss 303.490204\n",
      "Trained batch 355 batch loss 248.460312 epoch total loss 303.335175\n",
      "Trained batch 356 batch loss 248.14119 epoch total loss 303.180145\n",
      "Trained batch 357 batch loss 248.065216 epoch total loss 303.025757\n",
      "Trained batch 358 batch loss 247.806335 epoch total loss 302.871521\n",
      "Trained batch 359 batch loss 247.588348 epoch total loss 302.717499\n",
      "Trained batch 360 batch loss 247.017548 epoch total loss 302.562775\n",
      "Trained batch 361 batch loss 246.854065 epoch total loss 302.408447\n",
      "Trained batch 362 batch loss 246.653671 epoch total loss 302.254456\n",
      "Trained batch 363 batch loss 246.418091 epoch total loss 302.100647\n",
      "Trained batch 364 batch loss 246.067673 epoch total loss 301.946716\n",
      "Trained batch 365 batch loss 245.646317 epoch total loss 301.79248\n",
      "Trained batch 366 batch loss 245.647385 epoch total loss 301.639069\n",
      "Trained batch 367 batch loss 245.212051 epoch total loss 301.485321\n",
      "Trained batch 368 batch loss 245.346375 epoch total loss 301.332764\n",
      "Trained batch 369 batch loss 245.354401 epoch total loss 301.18103\n",
      "Trained batch 370 batch loss 245.19986 epoch total loss 301.029755\n",
      "Trained batch 371 batch loss 244.399445 epoch total loss 300.877106\n",
      "Trained batch 372 batch loss 244.14917 epoch total loss 300.724609\n",
      "Trained batch 373 batch loss 243.710938 epoch total loss 300.571747\n",
      "Trained batch 374 batch loss 243.592789 epoch total loss 300.419403\n",
      "Trained batch 375 batch loss 242.957886 epoch total loss 300.266174\n",
      "Trained batch 376 batch loss 242.686829 epoch total loss 300.113068\n",
      "Trained batch 377 batch loss 242.469742 epoch total loss 299.960144\n",
      "Trained batch 378 batch loss 242.251877 epoch total loss 299.807465\n",
      "Trained batch 379 batch loss 242.463562 epoch total loss 299.656158\n",
      "Trained batch 380 batch loss 242.230667 epoch total loss 299.505066\n",
      "Trained batch 381 batch loss 242.118042 epoch total loss 299.354431\n",
      "Trained batch 382 batch loss 241.593948 epoch total loss 299.203217\n",
      "Trained batch 383 batch loss 241.241318 epoch total loss 299.05188\n",
      "Trained batch 384 batch loss 241.03067 epoch total loss 298.900787\n",
      "Trained batch 385 batch loss 240.824051 epoch total loss 298.749939\n",
      "Trained batch 386 batch loss 240.633148 epoch total loss 298.599365\n",
      "Trained batch 387 batch loss 240.166443 epoch total loss 298.448395\n",
      "Trained batch 388 batch loss 239.779022 epoch total loss 298.29718\n",
      "Trained batch 389 batch loss 239.483383 epoch total loss 298.146\n",
      "Trained batch 390 batch loss 239.186203 epoch total loss 297.994812\n",
      "Trained batch 391 batch loss 239.329117 epoch total loss 297.844757\n",
      "Trained batch 392 batch loss 238.991089 epoch total loss 297.694641\n",
      "Trained batch 393 batch loss 238.239944 epoch total loss 297.543365\n",
      "Trained batch 394 batch loss 237.383926 epoch total loss 297.390656\n",
      "Trained batch 395 batch loss 237.792587 epoch total loss 297.239777\n",
      "Trained batch 396 batch loss 237.878 epoch total loss 297.089874\n",
      "Trained batch 397 batch loss 237.657578 epoch total loss 296.940155\n",
      "Trained batch 398 batch loss 237.356155 epoch total loss 296.790466\n",
      "Trained batch 399 batch loss 237.083755 epoch total loss 296.640808\n",
      "Trained batch 400 batch loss 237.198898 epoch total loss 296.492218\n",
      "Trained batch 401 batch loss 237.022293 epoch total loss 296.343903\n",
      "Trained batch 402 batch loss 236.410156 epoch total loss 296.194794\n",
      "Trained batch 403 batch loss 235.856628 epoch total loss 296.045105\n",
      "Trained batch 404 batch loss 235.851898 epoch total loss 295.896088\n",
      "Trained batch 405 batch loss 235.762848 epoch total loss 295.74762\n",
      "Trained batch 406 batch loss 235.250412 epoch total loss 295.598633\n",
      "Trained batch 407 batch loss 235.249084 epoch total loss 295.450348\n",
      "Trained batch 408 batch loss 235.089737 epoch total loss 295.302399\n",
      "Trained batch 409 batch loss 234.735916 epoch total loss 295.154297\n",
      "Trained batch 410 batch loss 234.568787 epoch total loss 295.006531\n",
      "Trained batch 411 batch loss 234.378204 epoch total loss 294.859\n",
      "Trained batch 412 batch loss 234.181992 epoch total loss 294.711731\n",
      "Trained batch 413 batch loss 234.115875 epoch total loss 294.565\n",
      "Trained batch 414 batch loss 233.684952 epoch total loss 294.417969\n",
      "Trained batch 415 batch loss 233.238327 epoch total loss 294.270569\n",
      "Trained batch 416 batch loss 233.295425 epoch total loss 294.124\n",
      "Trained batch 417 batch loss 232.843384 epoch total loss 293.97702\n",
      "Trained batch 418 batch loss 232.763626 epoch total loss 293.830597\n",
      "Trained batch 419 batch loss 232.534409 epoch total loss 293.684296\n",
      "Trained batch 420 batch loss 232.09787 epoch total loss 293.537659\n",
      "Trained batch 421 batch loss 231.310669 epoch total loss 293.389862\n",
      "Trained batch 422 batch loss 231.26564 epoch total loss 293.242645\n",
      "Trained batch 423 batch loss 230.424606 epoch total loss 293.094147\n",
      "Trained batch 424 batch loss 230.299118 epoch total loss 292.946045\n",
      "Trained batch 425 batch loss 230.355103 epoch total loss 292.798737\n",
      "Trained batch 426 batch loss 229.576416 epoch total loss 292.65033\n",
      "Trained batch 427 batch loss 229.076675 epoch total loss 292.501465\n",
      "Trained batch 428 batch loss 229.110748 epoch total loss 292.353363\n",
      "Trained batch 429 batch loss 230.225861 epoch total loss 292.208527\n",
      "Trained batch 430 batch loss 230.149429 epoch total loss 292.064209\n",
      "Trained batch 431 batch loss 229.18544 epoch total loss 291.918335\n",
      "Trained batch 432 batch loss 229.473282 epoch total loss 291.773773\n",
      "Trained batch 433 batch loss 229.194656 epoch total loss 291.629272\n",
      "Trained batch 434 batch loss 229.080353 epoch total loss 291.485138\n",
      "Trained batch 435 batch loss 228.676346 epoch total loss 291.340759\n",
      "Trained batch 436 batch loss 228.137054 epoch total loss 291.195801\n",
      "Trained batch 437 batch loss 228.001495 epoch total loss 291.051178\n",
      "Trained batch 438 batch loss 228.019333 epoch total loss 290.907257\n",
      "Trained batch 439 batch loss 227.393921 epoch total loss 290.762573\n",
      "Trained batch 440 batch loss 227.455246 epoch total loss 290.618683\n",
      "Trained batch 441 batch loss 227.126114 epoch total loss 290.474731\n",
      "Trained batch 442 batch loss 226.861191 epoch total loss 290.33078\n",
      "Trained batch 443 batch loss 227.173492 epoch total loss 290.188232\n",
      "Trained batch 444 batch loss 226.937073 epoch total loss 290.045776\n",
      "Trained batch 445 batch loss 226.447311 epoch total loss 289.902832\n",
      "Trained batch 446 batch loss 225.919128 epoch total loss 289.759399\n",
      "Trained batch 447 batch loss 225.922668 epoch total loss 289.616577\n",
      "Trained batch 448 batch loss 225.561905 epoch total loss 289.473602\n",
      "Trained batch 449 batch loss 224.728088 epoch total loss 289.329407\n",
      "Trained batch 450 batch loss 224.738113 epoch total loss 289.185852\n",
      "Trained batch 451 batch loss 224.725204 epoch total loss 289.042938\n",
      "Trained batch 452 batch loss 224.364746 epoch total loss 288.899841\n",
      "Trained batch 453 batch loss 224.528992 epoch total loss 288.757751\n",
      "Trained batch 454 batch loss 224.155823 epoch total loss 288.615448\n",
      "Trained batch 455 batch loss 223.714828 epoch total loss 288.472809\n",
      "Trained batch 456 batch loss 223.048431 epoch total loss 288.329315\n",
      "Trained batch 457 batch loss 222.446182 epoch total loss 288.185181\n",
      "Trained batch 458 batch loss 222.242157 epoch total loss 288.041168\n",
      "Trained batch 459 batch loss 222.810059 epoch total loss 287.899078\n",
      "Trained batch 460 batch loss 223.048721 epoch total loss 287.758087\n",
      "Trained batch 461 batch loss 223.406158 epoch total loss 287.6185\n",
      "Trained batch 462 batch loss 222.136154 epoch total loss 287.476776\n",
      "Trained batch 463 batch loss 221.17308 epoch total loss 287.333557\n",
      "Trained batch 464 batch loss 222.076523 epoch total loss 287.192932\n",
      "Trained batch 465 batch loss 221.97168 epoch total loss 287.052643\n",
      "Trained batch 466 batch loss 221.721512 epoch total loss 286.912445\n",
      "Trained batch 467 batch loss 221.307907 epoch total loss 286.771973\n",
      "Trained batch 468 batch loss 221.160629 epoch total loss 286.631775\n",
      "Trained batch 469 batch loss 221.042709 epoch total loss 286.491943\n",
      "Trained batch 470 batch loss 220.615921 epoch total loss 286.351776\n",
      "Trained batch 471 batch loss 220.391388 epoch total loss 286.211731\n",
      "Trained batch 472 batch loss 220.230957 epoch total loss 286.07193\n",
      "Trained batch 473 batch loss 219.681061 epoch total loss 285.93158\n",
      "Trained batch 474 batch loss 219.809525 epoch total loss 285.792084\n",
      "Trained batch 475 batch loss 219.455902 epoch total loss 285.652435\n",
      "Trained batch 476 batch loss 219.143906 epoch total loss 285.512695\n",
      "Trained batch 477 batch loss 218.628113 epoch total loss 285.372467\n",
      "Trained batch 478 batch loss 218.065903 epoch total loss 285.231659\n",
      "Trained batch 479 batch loss 217.979156 epoch total loss 285.091278\n",
      "Trained batch 480 batch loss 217.264053 epoch total loss 284.949982\n",
      "Trained batch 481 batch loss 216.939102 epoch total loss 284.808563\n",
      "Trained batch 482 batch loss 216.949875 epoch total loss 284.667786\n",
      "Trained batch 483 batch loss 217.258652 epoch total loss 284.528229\n",
      "Trained batch 484 batch loss 217.158463 epoch total loss 284.389038\n",
      "Trained batch 485 batch loss 217.435791 epoch total loss 284.251\n",
      "Trained batch 486 batch loss 217.405792 epoch total loss 284.113464\n",
      "Trained batch 487 batch loss 217.073318 epoch total loss 283.9758\n",
      "Trained batch 488 batch loss 217.044861 epoch total loss 283.838654\n",
      "Trained batch 489 batch loss 216.777145 epoch total loss 283.701538\n",
      "Trained batch 490 batch loss 216.330215 epoch total loss 283.564026\n",
      "Trained batch 491 batch loss 216.125748 epoch total loss 283.426666\n",
      "Trained batch 492 batch loss 215.885742 epoch total loss 283.289398\n",
      "Trained batch 493 batch loss 215.78598 epoch total loss 283.152466\n",
      "Trained batch 494 batch loss 215.446808 epoch total loss 283.015442\n",
      "Trained batch 495 batch loss 215.107071 epoch total loss 282.878265\n",
      "Trained batch 496 batch loss 214.68335 epoch total loss 282.740784\n",
      "Trained batch 497 batch loss 214.218063 epoch total loss 282.602905\n",
      "Trained batch 498 batch loss 213.912704 epoch total loss 282.464966\n",
      "Trained batch 499 batch loss 213.908264 epoch total loss 282.327576\n",
      "Trained batch 500 batch loss 213.666809 epoch total loss 282.190247\n",
      "Trained batch 501 batch loss 213.742615 epoch total loss 282.05365\n",
      "Trained batch 502 batch loss 213.940857 epoch total loss 281.917938\n",
      "Trained batch 503 batch loss 213.335037 epoch total loss 281.781586\n",
      "Trained batch 504 batch loss 213.147217 epoch total loss 281.645386\n",
      "Trained batch 505 batch loss 212.939926 epoch total loss 281.509338\n",
      "Trained batch 506 batch loss 212.627869 epoch total loss 281.373199\n",
      "Trained batch 507 batch loss 212.42923 epoch total loss 281.237213\n",
      "Trained batch 508 batch loss 212.364838 epoch total loss 281.101624\n",
      "Trained batch 509 batch loss 212.197937 epoch total loss 280.966278\n",
      "Trained batch 510 batch loss 211.92569 epoch total loss 280.830872\n",
      "Trained batch 511 batch loss 211.71315 epoch total loss 280.695648\n",
      "Trained batch 512 batch loss 211.259399 epoch total loss 280.560028\n",
      "Trained batch 513 batch loss 211.326477 epoch total loss 280.425079\n",
      "Trained batch 514 batch loss 210.657349 epoch total loss 280.289337\n",
      "Trained batch 515 batch loss 210.834625 epoch total loss 280.154449\n",
      "Trained batch 516 batch loss 210.294266 epoch total loss 280.019073\n",
      "Trained batch 517 batch loss 210.234497 epoch total loss 279.884094\n",
      "Trained batch 518 batch loss 209.96991 epoch total loss 279.749115\n",
      "Trained batch 519 batch loss 209.187073 epoch total loss 279.613159\n",
      "Trained batch 520 batch loss 208.657074 epoch total loss 279.476715\n",
      "Trained batch 521 batch loss 208.888321 epoch total loss 279.341217\n",
      "Trained batch 522 batch loss 208.998215 epoch total loss 279.206482\n",
      "Trained batch 523 batch loss 209.000412 epoch total loss 279.072235\n",
      "Trained batch 524 batch loss 208.829544 epoch total loss 278.938171\n",
      "Trained batch 525 batch loss 208.392838 epoch total loss 278.803802\n",
      "Trained batch 526 batch loss 208.564423 epoch total loss 278.670258\n",
      "Trained batch 527 batch loss 207.753647 epoch total loss 278.535706\n",
      "Trained batch 528 batch loss 207.453049 epoch total loss 278.401062\n",
      "Trained batch 529 batch loss 207.788574 epoch total loss 278.267578\n",
      "Trained batch 530 batch loss 207.68428 epoch total loss 278.134399\n",
      "Trained batch 531 batch loss 207.028778 epoch total loss 278.000488\n",
      "Trained batch 532 batch loss 206.68396 epoch total loss 277.866455\n",
      "Trained batch 533 batch loss 206.518494 epoch total loss 277.732574\n",
      "Trained batch 534 batch loss 206.507294 epoch total loss 277.599182\n",
      "Trained batch 535 batch loss 206.119644 epoch total loss 277.465607\n",
      "Trained batch 536 batch loss 206.177155 epoch total loss 277.332581\n",
      "Trained batch 537 batch loss 205.961731 epoch total loss 277.199677\n",
      "Trained batch 538 batch loss 205.181305 epoch total loss 277.065826\n",
      "Trained batch 539 batch loss 205.285767 epoch total loss 276.932648\n",
      "Trained batch 540 batch loss 205.455231 epoch total loss 276.800293\n",
      "Trained batch 541 batch loss 204.853745 epoch total loss 276.667297\n",
      "Trained batch 542 batch loss 204.435791 epoch total loss 276.534058\n",
      "Trained batch 543 batch loss 204.489441 epoch total loss 276.401367\n",
      "Trained batch 544 batch loss 204.092789 epoch total loss 276.268433\n",
      "Trained batch 545 batch loss 204.122208 epoch total loss 276.136078\n",
      "Trained batch 546 batch loss 203.822723 epoch total loss 276.003632\n",
      "Trained batch 547 batch loss 203.426056 epoch total loss 275.870941\n",
      "Trained batch 548 batch loss 203.402512 epoch total loss 275.738708\n",
      "Trained batch 549 batch loss 202.956177 epoch total loss 275.60614\n",
      "Trained batch 550 batch loss 202.307251 epoch total loss 275.47287\n",
      "Trained batch 551 batch loss 202.820145 epoch total loss 275.341\n",
      "Trained batch 552 batch loss 202.25885 epoch total loss 275.208618\n",
      "Trained batch 553 batch loss 202.1 epoch total loss 275.076416\n",
      "Trained batch 554 batch loss 202.074875 epoch total loss 274.944641\n",
      "Trained batch 555 batch loss 202.321594 epoch total loss 274.813782\n",
      "Trained batch 556 batch loss 201.922668 epoch total loss 274.682709\n",
      "Trained batch 557 batch loss 201.579071 epoch total loss 274.551453\n",
      "Trained batch 558 batch loss 201.047714 epoch total loss 274.419708\n",
      "Trained batch 559 batch loss 201.282715 epoch total loss 274.288879\n",
      "Trained batch 560 batch loss 201.022644 epoch total loss 274.158051\n",
      "Trained batch 561 batch loss 200.558914 epoch total loss 274.026855\n",
      "Trained batch 562 batch loss 200.636185 epoch total loss 273.896271\n",
      "Trained batch 563 batch loss 200.186844 epoch total loss 273.76535\n",
      "Trained batch 564 batch loss 200.178619 epoch total loss 273.634857\n",
      "Trained batch 565 batch loss 200.015381 epoch total loss 273.504578\n",
      "Trained batch 566 batch loss 199.803299 epoch total loss 273.374329\n",
      "Trained batch 567 batch loss 199.717804 epoch total loss 273.244446\n",
      "Trained batch 568 batch loss 199.54213 epoch total loss 273.114685\n",
      "Trained batch 569 batch loss 198.862411 epoch total loss 272.984192\n",
      "Trained batch 570 batch loss 199.157303 epoch total loss 272.854675\n",
      "Trained batch 571 batch loss 198.937744 epoch total loss 272.72522\n",
      "Trained batch 572 batch loss 198.610123 epoch total loss 272.595642\n",
      "Trained batch 573 batch loss 198.516373 epoch total loss 272.466339\n",
      "Trained batch 574 batch loss 198.407761 epoch total loss 272.337311\n",
      "Trained batch 575 batch loss 198.304047 epoch total loss 272.208557\n",
      "Trained batch 576 batch loss 198.249283 epoch total loss 272.08017\n",
      "Trained batch 577 batch loss 198.059 epoch total loss 271.951874\n",
      "Trained batch 578 batch loss 197.59198 epoch total loss 271.823242\n",
      "Trained batch 579 batch loss 197.379974 epoch total loss 271.694641\n",
      "Trained batch 580 batch loss 197.309799 epoch total loss 271.566406\n",
      "Trained batch 581 batch loss 196.912155 epoch total loss 271.437897\n",
      "Trained batch 582 batch loss 196.629807 epoch total loss 271.309357\n",
      "Trained batch 583 batch loss 196.611282 epoch total loss 271.181213\n",
      "Trained batch 584 batch loss 196.391556 epoch total loss 271.053162\n",
      "Trained batch 585 batch loss 195.81102 epoch total loss 270.924561\n",
      "Trained batch 586 batch loss 195.368347 epoch total loss 270.795624\n",
      "Trained batch 587 batch loss 194.64946 epoch total loss 270.665924\n",
      "Trained batch 588 batch loss 195.383011 epoch total loss 270.537903\n",
      "Trained batch 589 batch loss 195.164398 epoch total loss 270.409943\n",
      "Trained batch 590 batch loss 195.052338 epoch total loss 270.282196\n",
      "Trained batch 591 batch loss 194.722488 epoch total loss 270.154358\n",
      "Trained batch 592 batch loss 194.780548 epoch total loss 270.027039\n",
      "Trained batch 593 batch loss 194.488052 epoch total loss 269.899628\n",
      "Trained batch 594 batch loss 194.209366 epoch total loss 269.772186\n",
      "Trained batch 595 batch loss 194.12851 epoch total loss 269.64505\n",
      "Trained batch 596 batch loss 193.776077 epoch total loss 269.517761\n",
      "Trained batch 597 batch loss 193.654892 epoch total loss 269.390717\n",
      "Trained batch 598 batch loss 193.423187 epoch total loss 269.263672\n",
      "Trained batch 599 batch loss 193.46637 epoch total loss 269.137115\n",
      "Trained batch 600 batch loss 193.409531 epoch total loss 269.010925\n",
      "Trained batch 601 batch loss 192.851852 epoch total loss 268.884216\n",
      "Trained batch 602 batch loss 192.630402 epoch total loss 268.757538\n",
      "Trained batch 603 batch loss 192.444824 epoch total loss 268.630951\n",
      "Trained batch 604 batch loss 192.263123 epoch total loss 268.504517\n",
      "Trained batch 605 batch loss 192.14563 epoch total loss 268.378296\n",
      "Trained batch 606 batch loss 192.07283 epoch total loss 268.252411\n",
      "Trained batch 607 batch loss 191.586411 epoch total loss 268.126099\n",
      "Trained batch 608 batch loss 191.565674 epoch total loss 268.000183\n",
      "Trained batch 609 batch loss 191.338699 epoch total loss 267.874298\n",
      "Trained batch 610 batch loss 190.803162 epoch total loss 267.747955\n",
      "Trained batch 611 batch loss 191.017044 epoch total loss 267.622375\n",
      "Trained batch 612 batch loss 190.691818 epoch total loss 267.496643\n",
      "Trained batch 613 batch loss 190.560257 epoch total loss 267.371155\n",
      "Trained batch 614 batch loss 190.216568 epoch total loss 267.245483\n",
      "Trained batch 615 batch loss 190.113953 epoch total loss 267.120056\n",
      "Trained batch 616 batch loss 189.715057 epoch total loss 266.994415\n",
      "Trained batch 617 batch loss 189.247238 epoch total loss 266.868408\n",
      "Trained batch 618 batch loss 189.190903 epoch total loss 266.742706\n",
      "Trained batch 619 batch loss 189.481354 epoch total loss 266.61792\n",
      "Trained batch 620 batch loss 188.679901 epoch total loss 266.492218\n",
      "Trained batch 621 batch loss 188.936844 epoch total loss 266.36734\n",
      "Trained batch 622 batch loss 188.972641 epoch total loss 266.242889\n",
      "Trained batch 623 batch loss 188.565765 epoch total loss 266.118195\n",
      "Trained batch 624 batch loss 188.13446 epoch total loss 265.993225\n",
      "Trained batch 625 batch loss 187.784973 epoch total loss 265.868103\n",
      "Trained batch 626 batch loss 187.864243 epoch total loss 265.7435\n",
      "Trained batch 627 batch loss 187.847488 epoch total loss 265.619232\n",
      "Trained batch 628 batch loss 187.670883 epoch total loss 265.495117\n",
      "Trained batch 629 batch loss 187.366806 epoch total loss 265.370911\n",
      "Trained batch 630 batch loss 187.103485 epoch total loss 265.246674\n",
      "Trained batch 631 batch loss 187.063782 epoch total loss 265.122772\n",
      "Trained batch 632 batch loss 186.784744 epoch total loss 264.99881\n",
      "Trained batch 633 batch loss 186.57489 epoch total loss 264.874939\n",
      "Trained batch 634 batch loss 186.281464 epoch total loss 264.750946\n",
      "Trained batch 635 batch loss 186.162247 epoch total loss 264.627197\n",
      "Trained batch 636 batch loss 186.099228 epoch total loss 264.503723\n",
      "Trained batch 637 batch loss 185.507065 epoch total loss 264.3797\n",
      "Trained batch 638 batch loss 185.22757 epoch total loss 264.255646\n",
      "Trained batch 639 batch loss 185.262039 epoch total loss 264.132019\n",
      "Trained batch 640 batch loss 185.429382 epoch total loss 264.009033\n",
      "Trained batch 641 batch loss 185.222473 epoch total loss 263.886108\n",
      "Trained batch 642 batch loss 185.180389 epoch total loss 263.763519\n",
      "Trained batch 643 batch loss 184.860641 epoch total loss 263.640808\n",
      "Trained batch 644 batch loss 184.465027 epoch total loss 263.517883\n",
      "Trained batch 645 batch loss 184.022491 epoch total loss 263.394623\n",
      "Trained batch 646 batch loss 184.174927 epoch total loss 263.271973\n",
      "Trained batch 647 batch loss 183.82402 epoch total loss 263.1492\n",
      "Trained batch 648 batch loss 183.684845 epoch total loss 263.026581\n",
      "Trained batch 649 batch loss 183.305115 epoch total loss 262.903748\n",
      "Trained batch 650 batch loss 182.839157 epoch total loss 262.780579\n",
      "Trained batch 651 batch loss 183.209885 epoch total loss 262.658325\n",
      "Trained batch 652 batch loss 182.739487 epoch total loss 262.535767\n",
      "Trained batch 653 batch loss 182.366028 epoch total loss 262.412964\n",
      "Trained batch 654 batch loss 182.525146 epoch total loss 262.290833\n",
      "Trained batch 655 batch loss 182.244507 epoch total loss 262.16864\n",
      "Trained batch 656 batch loss 181.806763 epoch total loss 262.046143\n",
      "Trained batch 657 batch loss 181.701324 epoch total loss 261.923859\n",
      "Trained batch 658 batch loss 181.641525 epoch total loss 261.801849\n",
      "Trained batch 659 batch loss 181.494827 epoch total loss 261.68\n",
      "Trained batch 660 batch loss 181.305908 epoch total loss 261.558228\n",
      "Trained batch 661 batch loss 181.781219 epoch total loss 261.437531\n",
      "Trained batch 662 batch loss 181.180237 epoch total loss 261.316315\n",
      "Trained batch 663 batch loss 180.43634 epoch total loss 261.194305\n",
      "Trained batch 664 batch loss 180.591461 epoch total loss 261.072937\n",
      "Trained batch 665 batch loss 180.944687 epoch total loss 260.952423\n",
      "Trained batch 666 batch loss 180.4254 epoch total loss 260.831512\n",
      "Trained batch 667 batch loss 180.312973 epoch total loss 260.710785\n",
      "Trained batch 668 batch loss 180.171234 epoch total loss 260.59021\n",
      "Trained batch 669 batch loss 179.684479 epoch total loss 260.469299\n",
      "Trained batch 670 batch loss 179.429718 epoch total loss 260.348358\n",
      "Trained batch 671 batch loss 179.361176 epoch total loss 260.227631\n",
      "Trained batch 672 batch loss 178.645599 epoch total loss 260.106232\n",
      "Trained batch 673 batch loss 178.72374 epoch total loss 259.985291\n",
      "Trained batch 674 batch loss 178.900726 epoch total loss 259.865021\n",
      "Trained batch 675 batch loss 178.640549 epoch total loss 259.74469\n",
      "Trained batch 676 batch loss 178.451233 epoch total loss 259.62442\n",
      "Trained batch 677 batch loss 178.856339 epoch total loss 259.505127\n",
      "Trained batch 678 batch loss 178.559738 epoch total loss 259.385742\n",
      "Trained batch 679 batch loss 178.367767 epoch total loss 259.266418\n",
      "Trained batch 680 batch loss 177.981247 epoch total loss 259.146912\n",
      "Trained batch 681 batch loss 177.031906 epoch total loss 259.026306\n",
      "Trained batch 682 batch loss 177.200287 epoch total loss 258.906342\n",
      "Trained batch 683 batch loss 177.254425 epoch total loss 258.786774\n",
      "Trained batch 684 batch loss 177.012085 epoch total loss 258.667236\n",
      "Trained batch 685 batch loss 177.108124 epoch total loss 258.548187\n",
      "Trained batch 686 batch loss 176.902344 epoch total loss 258.429169\n",
      "Trained batch 687 batch loss 176.748169 epoch total loss 258.310272\n",
      "Trained batch 688 batch loss 176.453491 epoch total loss 258.191284\n",
      "Trained batch 689 batch loss 176.266068 epoch total loss 258.072388\n",
      "Trained batch 690 batch loss 175.975052 epoch total loss 257.9534\n",
      "Trained batch 691 batch loss 175.997086 epoch total loss 257.834808\n",
      "Trained batch 692 batch loss 175.755432 epoch total loss 257.716187\n",
      "Trained batch 693 batch loss 175.469757 epoch total loss 257.597504\n",
      "Trained batch 694 batch loss 175.171494 epoch total loss 257.478729\n",
      "Trained batch 695 batch loss 175.213654 epoch total loss 257.360352\n",
      "Trained batch 696 batch loss 174.916504 epoch total loss 257.241913\n",
      "Trained batch 697 batch loss 174.815323 epoch total loss 257.123657\n",
      "Trained batch 698 batch loss 174.647964 epoch total loss 257.005493\n",
      "Trained batch 699 batch loss 174.449814 epoch total loss 256.88739\n",
      "Trained batch 700 batch loss 174.389481 epoch total loss 256.769531\n",
      "Trained batch 701 batch loss 173.874741 epoch total loss 256.651276\n",
      "Trained batch 702 batch loss 173.475189 epoch total loss 256.532776\n",
      "Trained batch 703 batch loss 173.032852 epoch total loss 256.414\n",
      "Trained batch 704 batch loss 173.367691 epoch total loss 256.296051\n",
      "Trained batch 705 batch loss 173.309891 epoch total loss 256.178345\n",
      "Trained batch 706 batch loss 173.197479 epoch total loss 256.060822\n",
      "Trained batch 707 batch loss 173.313416 epoch total loss 255.943771\n",
      "Trained batch 708 batch loss 172.714188 epoch total loss 255.826233\n",
      "Trained batch 709 batch loss 172.616287 epoch total loss 255.708862\n",
      "Trained batch 710 batch loss 172.402893 epoch total loss 255.591522\n",
      "Trained batch 711 batch loss 172.462357 epoch total loss 255.474625\n",
      "Trained batch 712 batch loss 172.061127 epoch total loss 255.357468\n",
      "Trained batch 713 batch loss 171.553284 epoch total loss 255.239914\n",
      "Trained batch 714 batch loss 171.523697 epoch total loss 255.122681\n",
      "Trained batch 715 batch loss 171.707504 epoch total loss 255.006012\n",
      "Trained batch 716 batch loss 171.661133 epoch total loss 254.889603\n",
      "Trained batch 717 batch loss 171.606033 epoch total loss 254.773453\n",
      "Trained batch 718 batch loss 171.164688 epoch total loss 254.657013\n",
      "Trained batch 719 batch loss 171.061249 epoch total loss 254.540741\n",
      "Trained batch 720 batch loss 170.808212 epoch total loss 254.424454\n",
      "Trained batch 721 batch loss 170.675888 epoch total loss 254.308289\n",
      "Trained batch 722 batch loss 170.548386 epoch total loss 254.192276\n",
      "Trained batch 723 batch loss 170.431396 epoch total loss 254.076447\n",
      "Trained batch 724 batch loss 170.406815 epoch total loss 253.960876\n",
      "Trained batch 725 batch loss 169.980209 epoch total loss 253.845047\n",
      "Trained batch 726 batch loss 169.656204 epoch total loss 253.72908\n",
      "Trained batch 727 batch loss 169.518158 epoch total loss 253.613251\n",
      "Trained batch 728 batch loss 169.307556 epoch total loss 253.497452\n",
      "Trained batch 729 batch loss 169.118866 epoch total loss 253.381714\n",
      "Trained batch 730 batch loss 169.072662 epoch total loss 253.26622\n",
      "Trained batch 731 batch loss 168.755798 epoch total loss 253.150604\n",
      "Trained batch 732 batch loss 168.393295 epoch total loss 253.034821\n",
      "Trained batch 733 batch loss 168.470581 epoch total loss 252.919449\n",
      "Trained batch 734 batch loss 168.375565 epoch total loss 252.80426\n",
      "Trained batch 735 batch loss 167.909164 epoch total loss 252.688751\n",
      "Trained batch 736 batch loss 167.943298 epoch total loss 252.573608\n",
      "Trained batch 737 batch loss 167.641617 epoch total loss 252.458359\n",
      "Trained batch 738 batch loss 167.21817 epoch total loss 252.342865\n",
      "Trained batch 739 batch loss 167.436325 epoch total loss 252.227966\n",
      "Trained batch 740 batch loss 167.699646 epoch total loss 252.113739\n",
      "Trained batch 741 batch loss 166.771561 epoch total loss 251.998566\n",
      "Trained batch 742 batch loss 166.773209 epoch total loss 251.883698\n",
      "Trained batch 743 batch loss 166.877411 epoch total loss 251.769287\n",
      "Trained batch 744 batch loss 167.157715 epoch total loss 251.655563\n",
      "Trained batch 745 batch loss 166.685715 epoch total loss 251.541504\n",
      "Trained batch 746 batch loss 166.534119 epoch total loss 251.427551\n",
      "Trained batch 747 batch loss 166.347198 epoch total loss 251.313644\n",
      "Trained batch 748 batch loss 165.474686 epoch total loss 251.198883\n",
      "Trained batch 749 batch loss 165.320648 epoch total loss 251.084244\n",
      "Trained batch 750 batch loss 165.62294 epoch total loss 250.970291\n",
      "Trained batch 751 batch loss 165.303406 epoch total loss 250.856216\n",
      "Trained batch 752 batch loss 165.40657 epoch total loss 250.742584\n",
      "Trained batch 753 batch loss 165.253754 epoch total loss 250.629044\n",
      "Trained batch 754 batch loss 164.962494 epoch total loss 250.515442\n",
      "Trained batch 755 batch loss 164.555206 epoch total loss 250.401596\n",
      "Trained batch 756 batch loss 164.188309 epoch total loss 250.287552\n",
      "Trained batch 757 batch loss 164.727188 epoch total loss 250.17453\n",
      "Trained batch 758 batch loss 164.455872 epoch total loss 250.061447\n",
      "Trained batch 759 batch loss 163.735306 epoch total loss 249.947708\n",
      "Trained batch 760 batch loss 164.109024 epoch total loss 249.834763\n",
      "Trained batch 761 batch loss 163.67482 epoch total loss 249.721542\n",
      "Trained batch 762 batch loss 163.661957 epoch total loss 249.608597\n",
      "Trained batch 763 batch loss 163.160873 epoch total loss 249.495285\n",
      "Trained batch 764 batch loss 162.727264 epoch total loss 249.381729\n",
      "Trained batch 765 batch loss 163.101028 epoch total loss 249.268936\n",
      "Trained batch 766 batch loss 162.679596 epoch total loss 249.155884\n",
      "Trained batch 767 batch loss 162.909012 epoch total loss 249.043427\n",
      "Trained batch 768 batch loss 162.761948 epoch total loss 248.931091\n",
      "Trained batch 769 batch loss 162.391113 epoch total loss 248.818558\n",
      "Trained batch 770 batch loss 162.094421 epoch total loss 248.705933\n",
      "Trained batch 771 batch loss 162.058548 epoch total loss 248.593552\n",
      "Trained batch 772 batch loss 161.577606 epoch total loss 248.480835\n",
      "Trained batch 773 batch loss 161.575592 epoch total loss 248.368408\n",
      "Trained batch 774 batch loss 161.389709 epoch total loss 248.256042\n",
      "Trained batch 775 batch loss 161.43248 epoch total loss 248.144012\n",
      "Trained batch 776 batch loss 161.820023 epoch total loss 248.032761\n",
      "Trained batch 777 batch loss 161.66832 epoch total loss 247.921616\n",
      "Trained batch 778 batch loss 161.56398 epoch total loss 247.810608\n",
      "Trained batch 779 batch loss 160.839508 epoch total loss 247.698975\n",
      "Trained batch 780 batch loss 160.609955 epoch total loss 247.587326\n",
      "Trained batch 781 batch loss 160.185043 epoch total loss 247.475418\n",
      "Trained batch 782 batch loss 159.834457 epoch total loss 247.363327\n",
      "Trained batch 783 batch loss 159.800537 epoch total loss 247.251495\n",
      "Trained batch 784 batch loss 159.98233 epoch total loss 247.140182\n",
      "Trained batch 785 batch loss 159.05751 epoch total loss 247.027985\n",
      "Trained batch 786 batch loss 159.406403 epoch total loss 246.916504\n",
      "Trained batch 787 batch loss 159.333252 epoch total loss 246.805206\n",
      "Trained batch 788 batch loss 158.989777 epoch total loss 246.693771\n",
      "Trained batch 789 batch loss 159.455536 epoch total loss 246.583191\n",
      "Trained batch 790 batch loss 158.472687 epoch total loss 246.471664\n",
      "Trained batch 791 batch loss 158.058472 epoch total loss 246.359894\n",
      "Trained batch 792 batch loss 157.641159 epoch total loss 246.247864\n",
      "Trained batch 793 batch loss 158.095963 epoch total loss 246.136703\n",
      "Trained batch 794 batch loss 158.04213 epoch total loss 246.025757\n",
      "Trained batch 795 batch loss 157.728 epoch total loss 245.914703\n",
      "Trained batch 796 batch loss 157.67897 epoch total loss 245.803848\n",
      "Trained batch 797 batch loss 157.57489 epoch total loss 245.693146\n",
      "Trained batch 798 batch loss 157.62326 epoch total loss 245.582779\n",
      "Trained batch 799 batch loss 157.195084 epoch total loss 245.472153\n",
      "Trained batch 800 batch loss 157.290466 epoch total loss 245.361938\n",
      "Trained batch 801 batch loss 158.110306 epoch total loss 245.253\n",
      "Trained batch 802 batch loss 157.527786 epoch total loss 245.143631\n",
      "Trained batch 803 batch loss 156.631088 epoch total loss 245.033386\n",
      "Trained batch 804 batch loss 156.215439 epoch total loss 244.922928\n",
      "Trained batch 805 batch loss 156.72998 epoch total loss 244.81337\n",
      "Trained batch 806 batch loss 156.3508 epoch total loss 244.703613\n",
      "Trained batch 807 batch loss 156.307602 epoch total loss 244.594086\n",
      "Trained batch 808 batch loss 155.99118 epoch total loss 244.484421\n",
      "Trained batch 809 batch loss 155.546753 epoch total loss 244.374481\n",
      "Trained batch 810 batch loss 155.222107 epoch total loss 244.264404\n",
      "Trained batch 811 batch loss 155.197754 epoch total loss 244.154587\n",
      "Trained batch 812 batch loss 154.996796 epoch total loss 244.0448\n",
      "Trained batch 813 batch loss 155.099 epoch total loss 243.935379\n",
      "Trained batch 814 batch loss 154.606918 epoch total loss 243.825653\n",
      "Trained batch 815 batch loss 154.310822 epoch total loss 243.71582\n",
      "Trained batch 816 batch loss 154.403549 epoch total loss 243.606369\n",
      "Trained batch 817 batch loss 154.034531 epoch total loss 243.496735\n",
      "Trained batch 818 batch loss 154.315369 epoch total loss 243.387695\n",
      "Trained batch 819 batch loss 153.927856 epoch total loss 243.278458\n",
      "Trained batch 820 batch loss 154.08017 epoch total loss 243.169678\n",
      "Trained batch 821 batch loss 153.951508 epoch total loss 243.06102\n",
      "Trained batch 822 batch loss 154.039719 epoch total loss 242.952728\n",
      "Trained batch 823 batch loss 153.679596 epoch total loss 242.844238\n",
      "Trained batch 824 batch loss 153.374466 epoch total loss 242.735672\n",
      "Trained batch 825 batch loss 153.259506 epoch total loss 242.627213\n",
      "Trained batch 826 batch loss 153.323807 epoch total loss 242.519104\n",
      "Trained batch 827 batch loss 153.025665 epoch total loss 242.410904\n",
      "Trained batch 828 batch loss 152.574738 epoch total loss 242.302399\n",
      "Trained batch 829 batch loss 152.058884 epoch total loss 242.193558\n",
      "Trained batch 830 batch loss 151.881 epoch total loss 242.084732\n",
      "Trained batch 831 batch loss 152.0858 epoch total loss 241.976425\n",
      "Trained batch 832 batch loss 152.448624 epoch total loss 241.86882\n",
      "Trained batch 833 batch loss 152.465668 epoch total loss 241.761505\n",
      "Trained batch 834 batch loss 152.184906 epoch total loss 241.654099\n",
      "Trained batch 835 batch loss 152.087891 epoch total loss 241.546844\n",
      "Trained batch 836 batch loss 151.989014 epoch total loss 241.439713\n",
      "Trained batch 837 batch loss 151.866943 epoch total loss 241.332687\n",
      "Trained batch 838 batch loss 151.586914 epoch total loss 241.225586\n",
      "Trained batch 839 batch loss 151.305801 epoch total loss 241.118423\n",
      "Trained batch 840 batch loss 151.155365 epoch total loss 241.011322\n",
      "Trained batch 841 batch loss 151.054565 epoch total loss 240.904358\n",
      "Trained batch 842 batch loss 150.578506 epoch total loss 240.797073\n",
      "Trained batch 843 batch loss 150.83075 epoch total loss 240.690353\n",
      "Trained batch 844 batch loss 150.329788 epoch total loss 240.583298\n",
      "Trained batch 845 batch loss 150.002899 epoch total loss 240.476089\n",
      "Trained batch 846 batch loss 150.142426 epoch total loss 240.369308\n",
      "Trained batch 847 batch loss 149.78511 epoch total loss 240.26236\n",
      "Trained batch 848 batch loss 149.761124 epoch total loss 240.15564\n",
      "Trained batch 849 batch loss 149.438141 epoch total loss 240.048782\n",
      "Trained batch 850 batch loss 149.333679 epoch total loss 239.942062\n",
      "Trained batch 851 batch loss 149.071304 epoch total loss 239.835281\n",
      "Trained batch 852 batch loss 149.423615 epoch total loss 239.729172\n",
      "Trained batch 853 batch loss 148.822083 epoch total loss 239.622604\n",
      "Trained batch 854 batch loss 148.846802 epoch total loss 239.516296\n",
      "Trained batch 855 batch loss 148.467896 epoch total loss 239.409821\n",
      "Trained batch 856 batch loss 148.364349 epoch total loss 239.303452\n",
      "Trained batch 857 batch loss 148.40831 epoch total loss 239.197388\n",
      "Trained batch 858 batch loss 148.011703 epoch total loss 239.09111\n",
      "Trained batch 859 batch loss 147.925949 epoch total loss 238.98497\n",
      "Trained batch 860 batch loss 147.644058 epoch total loss 238.878754\n",
      "Trained batch 861 batch loss 147.485291 epoch total loss 238.772614\n",
      "Trained batch 862 batch loss 147.651932 epoch total loss 238.666916\n",
      "Trained batch 863 batch loss 147.323181 epoch total loss 238.561066\n",
      "Trained batch 864 batch loss 147.284256 epoch total loss 238.455429\n",
      "Trained batch 865 batch loss 147.184494 epoch total loss 238.349915\n",
      "Trained batch 866 batch loss 147.181915 epoch total loss 238.244644\n",
      "Trained batch 867 batch loss 146.634735 epoch total loss 238.138992\n",
      "Trained batch 868 batch loss 146.697113 epoch total loss 238.033646\n",
      "Trained batch 869 batch loss 146.212296 epoch total loss 237.928\n",
      "Trained batch 870 batch loss 146.800949 epoch total loss 237.823242\n",
      "Trained batch 871 batch loss 145.977722 epoch total loss 237.717804\n",
      "Trained batch 872 batch loss 146.079071 epoch total loss 237.612701\n",
      "Trained batch 873 batch loss 145.887466 epoch total loss 237.507645\n",
      "Trained batch 874 batch loss 145.647415 epoch total loss 237.402527\n",
      "Trained batch 875 batch loss 145.484238 epoch total loss 237.297485\n",
      "Trained batch 876 batch loss 145.387543 epoch total loss 237.192566\n",
      "Trained batch 877 batch loss 145.181671 epoch total loss 237.087662\n",
      "Trained batch 878 batch loss 144.973785 epoch total loss 236.982742\n",
      "Trained batch 879 batch loss 145.244354 epoch total loss 236.878372\n",
      "Trained batch 880 batch loss 145.349747 epoch total loss 236.774368\n",
      "Trained batch 881 batch loss 144.6483 epoch total loss 236.669785\n",
      "Trained batch 882 batch loss 144.701843 epoch total loss 236.565506\n",
      "Trained batch 883 batch loss 144.816345 epoch total loss 236.461594\n",
      "Trained batch 884 batch loss 144.536591 epoch total loss 236.357605\n",
      "Trained batch 885 batch loss 144.283508 epoch total loss 236.253571\n",
      "Trained batch 886 batch loss 143.93306 epoch total loss 236.149368\n",
      "Trained batch 887 batch loss 143.857086 epoch total loss 236.045319\n",
      "Trained batch 888 batch loss 143.439224 epoch total loss 235.94104\n",
      "Trained batch 889 batch loss 143.06575 epoch total loss 235.836563\n",
      "Trained batch 890 batch loss 142.53273 epoch total loss 235.73172\n",
      "Trained batch 891 batch loss 141.935394 epoch total loss 235.62645\n",
      "Trained batch 892 batch loss 142.916 epoch total loss 235.522522\n",
      "Trained batch 893 batch loss 142.6091 epoch total loss 235.418488\n",
      "Trained batch 894 batch loss 142.903168 epoch total loss 235.315\n",
      "Trained batch 895 batch loss 142.895386 epoch total loss 235.211731\n",
      "Trained batch 896 batch loss 142.781097 epoch total loss 235.108566\n",
      "Trained batch 897 batch loss 142.263397 epoch total loss 235.005066\n",
      "Trained batch 898 batch loss 142.229828 epoch total loss 234.901764\n",
      "Trained batch 899 batch loss 142.100677 epoch total loss 234.798523\n",
      "Trained batch 900 batch loss 142.291214 epoch total loss 234.69574\n",
      "Trained batch 901 batch loss 141.51712 epoch total loss 234.592331\n",
      "Trained batch 902 batch loss 141.733566 epoch total loss 234.48938\n",
      "Trained batch 903 batch loss 141.600647 epoch total loss 234.386505\n",
      "Trained batch 904 batch loss 141.317886 epoch total loss 234.283554\n",
      "Trained batch 905 batch loss 141.146332 epoch total loss 234.180634\n",
      "Trained batch 906 batch loss 141.626266 epoch total loss 234.078476\n",
      "Trained batch 907 batch loss 141.268219 epoch total loss 233.976135\n",
      "Trained batch 908 batch loss 141.041901 epoch total loss 233.873795\n",
      "Trained batch 909 batch loss 140.451096 epoch total loss 233.771027\n",
      "Trained batch 910 batch loss 140.153793 epoch total loss 233.668152\n",
      "Trained batch 911 batch loss 140.170761 epoch total loss 233.565521\n",
      "Trained batch 912 batch loss 140.167847 epoch total loss 233.46312\n",
      "Trained batch 913 batch loss 140.016159 epoch total loss 233.360764\n",
      "Trained batch 914 batch loss 139.519775 epoch total loss 233.258087\n",
      "Trained batch 915 batch loss 139.883362 epoch total loss 233.156052\n",
      "Trained batch 916 batch loss 139.408401 epoch total loss 233.053696\n",
      "Trained batch 917 batch loss 139.525726 epoch total loss 232.951706\n",
      "Trained batch 918 batch loss 139.369766 epoch total loss 232.849777\n",
      "Trained batch 919 batch loss 139.139542 epoch total loss 232.747803\n",
      "Trained batch 920 batch loss 139.452499 epoch total loss 232.646393\n",
      "Trained batch 921 batch loss 138.941437 epoch total loss 232.544647\n",
      "Trained batch 922 batch loss 139.316727 epoch total loss 232.443527\n",
      "Trained batch 923 batch loss 138.778091 epoch total loss 232.342056\n",
      "Trained batch 924 batch loss 138.291962 epoch total loss 232.24028\n",
      "Trained batch 925 batch loss 138.573685 epoch total loss 232.139023\n",
      "Trained batch 926 batch loss 138.851349 epoch total loss 232.038269\n",
      "Trained batch 927 batch loss 138.132126 epoch total loss 231.936966\n",
      "Trained batch 928 batch loss 138.342621 epoch total loss 231.836105\n",
      "Trained batch 929 batch loss 137.856659 epoch total loss 231.73494\n",
      "Trained batch 930 batch loss 137.63031 epoch total loss 231.633759\n",
      "Trained batch 931 batch loss 136.974472 epoch total loss 231.532074\n",
      "Trained batch 932 batch loss 137.542847 epoch total loss 231.431229\n",
      "Trained batch 933 batch loss 137.571365 epoch total loss 231.330643\n",
      "Trained batch 934 batch loss 137.385849 epoch total loss 231.230057\n",
      "Trained batch 935 batch loss 137.037186 epoch total loss 231.129318\n",
      "Trained batch 936 batch loss 137.002838 epoch total loss 231.028748\n",
      "Trained batch 937 batch loss 136.627853 epoch total loss 230.928\n",
      "Trained batch 938 batch loss 136.606979 epoch total loss 230.827438\n",
      "Trained batch 939 batch loss 136.540512 epoch total loss 230.727036\n",
      "Trained batch 940 batch loss 136.074829 epoch total loss 230.626343\n",
      "Trained batch 941 batch loss 136.258698 epoch total loss 230.526062\n",
      "Trained batch 942 batch loss 136.434021 epoch total loss 230.426193\n",
      "Trained batch 943 batch loss 136.152054 epoch total loss 230.326218\n",
      "Trained batch 944 batch loss 136.233887 epoch total loss 230.226547\n",
      "Trained batch 945 batch loss 135.948593 epoch total loss 230.126785\n",
      "Trained batch 946 batch loss 135.968918 epoch total loss 230.027252\n",
      "Trained batch 947 batch loss 135.858902 epoch total loss 229.927811\n",
      "Trained batch 948 batch loss 135.601242 epoch total loss 229.828308\n",
      "Trained batch 949 batch loss 135.249542 epoch total loss 229.728638\n",
      "Trained batch 950 batch loss 135.177612 epoch total loss 229.629105\n",
      "Trained batch 951 batch loss 135.280502 epoch total loss 229.529907\n",
      "Trained batch 952 batch loss 135.107101 epoch total loss 229.430725\n",
      "Trained batch 953 batch loss 134.890488 epoch total loss 229.331512\n",
      "Trained batch 954 batch loss 134.571457 epoch total loss 229.232193\n",
      "Trained batch 955 batch loss 134.227707 epoch total loss 229.132721\n",
      "Trained batch 956 batch loss 134.191681 epoch total loss 229.033401\n",
      "Trained batch 957 batch loss 134.183578 epoch total loss 228.934296\n",
      "Trained batch 958 batch loss 134.020889 epoch total loss 228.83522\n",
      "Trained batch 959 batch loss 133.737228 epoch total loss 228.736053\n",
      "Trained batch 960 batch loss 133.68837 epoch total loss 228.637039\n",
      "Trained batch 961 batch loss 133.328842 epoch total loss 228.537872\n",
      "Trained batch 962 batch loss 132.916718 epoch total loss 228.438477\n",
      "Trained batch 963 batch loss 132.629745 epoch total loss 228.338974\n",
      "Trained batch 964 batch loss 131.894547 epoch total loss 228.238922\n",
      "Trained batch 965 batch loss 131.964523 epoch total loss 228.13916\n",
      "Trained batch 966 batch loss 133.016922 epoch total loss 228.040695\n",
      "Trained batch 967 batch loss 133.052887 epoch total loss 227.942459\n",
      "Trained batch 968 batch loss 132.716293 epoch total loss 227.844086\n",
      "Trained batch 969 batch loss 132.264252 epoch total loss 227.745453\n",
      "Trained batch 970 batch loss 132.237518 epoch total loss 227.646988\n",
      "Trained batch 971 batch loss 131.907547 epoch total loss 227.548386\n",
      "Trained batch 972 batch loss 131.716507 epoch total loss 227.449799\n",
      "Trained batch 973 batch loss 131.340866 epoch total loss 227.351028\n",
      "Trained batch 974 batch loss 131.384094 epoch total loss 227.252502\n",
      "Trained batch 975 batch loss 131.233902 epoch total loss 227.154022\n",
      "Trained batch 976 batch loss 131.134735 epoch total loss 227.055649\n",
      "Trained batch 977 batch loss 131.629593 epoch total loss 226.957977\n",
      "Trained batch 978 batch loss 131.273956 epoch total loss 226.860138\n",
      "Trained batch 979 batch loss 131.20842 epoch total loss 226.762436\n",
      "Trained batch 980 batch loss 131.237778 epoch total loss 226.664963\n",
      "Trained batch 981 batch loss 131.040543 epoch total loss 226.56749\n",
      "Trained batch 982 batch loss 130.702072 epoch total loss 226.469864\n",
      "Trained batch 983 batch loss 130.777618 epoch total loss 226.372513\n",
      "Trained batch 984 batch loss 130.560303 epoch total loss 226.275146\n",
      "Trained batch 985 batch loss 130.368225 epoch total loss 226.177795\n",
      "Trained batch 986 batch loss 130.678848 epoch total loss 226.080933\n",
      "Trained batch 987 batch loss 130.031433 epoch total loss 225.983612\n",
      "Trained batch 988 batch loss 129.599808 epoch total loss 225.886047\n",
      "Trained batch 989 batch loss 129.391876 epoch total loss 225.788483\n",
      "Trained batch 990 batch loss 129.876755 epoch total loss 225.691605\n",
      "Trained batch 991 batch loss 129.798386 epoch total loss 225.594833\n",
      "Trained batch 992 batch loss 129.280975 epoch total loss 225.497742\n",
      "Trained batch 993 batch loss 129.239563 epoch total loss 225.400803\n",
      "Trained batch 994 batch loss 129.308228 epoch total loss 225.304138\n",
      "Trained batch 995 batch loss 129.365845 epoch total loss 225.207718\n",
      "Trained batch 996 batch loss 129.024445 epoch total loss 225.111145\n",
      "Trained batch 997 batch loss 128.709732 epoch total loss 225.01445\n",
      "Trained batch 998 batch loss 128.338745 epoch total loss 224.917587\n",
      "Trained batch 999 batch loss 127.90329 epoch total loss 224.82048\n",
      "Trained batch 1000 batch loss 128.108826 epoch total loss 224.72377\n",
      "Trained batch 1001 batch loss 128.282303 epoch total loss 224.627426\n",
      "Trained batch 1002 batch loss 128.366547 epoch total loss 224.531342\n",
      "Trained batch 1003 batch loss 128.007858 epoch total loss 224.43512\n",
      "Trained batch 1004 batch loss 127.736313 epoch total loss 224.338806\n",
      "Trained batch 1005 batch loss 127.028908 epoch total loss 224.241974\n",
      "Trained batch 1006 batch loss 127.411743 epoch total loss 224.145721\n",
      "Trained batch 1007 batch loss 127.482559 epoch total loss 224.049728\n",
      "Trained batch 1008 batch loss 127.674744 epoch total loss 223.954117\n",
      "Trained batch 1009 batch loss 127.134262 epoch total loss 223.85817\n",
      "Trained batch 1010 batch loss 126.763115 epoch total loss 223.762039\n",
      "Trained batch 1011 batch loss 127.019814 epoch total loss 223.666336\n",
      "Trained batch 1012 batch loss 126.419174 epoch total loss 223.570251\n",
      "Trained batch 1013 batch loss 127.197571 epoch total loss 223.475113\n",
      "Trained batch 1014 batch loss 126.656822 epoch total loss 223.379639\n",
      "Trained batch 1015 batch loss 126.725708 epoch total loss 223.284409\n",
      "Trained batch 1016 batch loss 126.494308 epoch total loss 223.189148\n",
      "Trained batch 1017 batch loss 126.107376 epoch total loss 223.093689\n",
      "Trained batch 1018 batch loss 126.003403 epoch total loss 222.998306\n",
      "Trained batch 1019 batch loss 125.785599 epoch total loss 222.902908\n",
      "Trained batch 1020 batch loss 126.169281 epoch total loss 222.808075\n",
      "Trained batch 1021 batch loss 126.195518 epoch total loss 222.713455\n",
      "Trained batch 1022 batch loss 125.751808 epoch total loss 222.618576\n",
      "Trained batch 1023 batch loss 125.282906 epoch total loss 222.523422\n",
      "Trained batch 1024 batch loss 125.228516 epoch total loss 222.428421\n",
      "Trained batch 1025 batch loss 124.982597 epoch total loss 222.333359\n",
      "Trained batch 1026 batch loss 125.183846 epoch total loss 222.238663\n",
      "Trained batch 1027 batch loss 124.993149 epoch total loss 222.143982\n",
      "Trained batch 1028 batch loss 124.699287 epoch total loss 222.049194\n",
      "Trained batch 1029 batch loss 124.838936 epoch total loss 221.954727\n",
      "Trained batch 1030 batch loss 124.299942 epoch total loss 221.859924\n",
      "Trained batch 1031 batch loss 124.295097 epoch total loss 221.765289\n",
      "Trained batch 1032 batch loss 124.341187 epoch total loss 221.670898\n",
      "Trained batch 1033 batch loss 124.392418 epoch total loss 221.576721\n",
      "Trained batch 1034 batch loss 124.1922 epoch total loss 221.482529\n",
      "Trained batch 1035 batch loss 124.140175 epoch total loss 221.388489\n",
      "Trained batch 1036 batch loss 123.832237 epoch total loss 221.294312\n",
      "Trained batch 1037 batch loss 123.663841 epoch total loss 221.20015\n",
      "Trained batch 1038 batch loss 123.570847 epoch total loss 221.10611\n",
      "Trained batch 1039 batch loss 123.211067 epoch total loss 221.011902\n",
      "Trained batch 1040 batch loss 123.415222 epoch total loss 220.91806\n",
      "Trained batch 1041 batch loss 123.179939 epoch total loss 220.824173\n",
      "Trained batch 1042 batch loss 123.033592 epoch total loss 220.730331\n",
      "Trained batch 1043 batch loss 122.625908 epoch total loss 220.636261\n",
      "Trained batch 1044 batch loss 122.811447 epoch total loss 220.542572\n",
      "Trained batch 1045 batch loss 122.671272 epoch total loss 220.448914\n",
      "Trained batch 1046 batch loss 122.477783 epoch total loss 220.355255\n",
      "Trained batch 1047 batch loss 122.318695 epoch total loss 220.261612\n",
      "Trained batch 1048 batch loss 122.064018 epoch total loss 220.167908\n",
      "Trained batch 1049 batch loss 122.302811 epoch total loss 220.074615\n",
      "Trained batch 1050 batch loss 121.994568 epoch total loss 219.981201\n",
      "Trained batch 1051 batch loss 122.110451 epoch total loss 219.888077\n",
      "Trained batch 1052 batch loss 121.84288 epoch total loss 219.794891\n",
      "Trained batch 1053 batch loss 120.966682 epoch total loss 219.701035\n",
      "Trained batch 1054 batch loss 120.722794 epoch total loss 219.607117\n",
      "Trained batch 1055 batch loss 120.661293 epoch total loss 219.513336\n",
      "Trained batch 1056 batch loss 120.780144 epoch total loss 219.41983\n",
      "Trained batch 1057 batch loss 120.976944 epoch total loss 219.326706\n",
      "Trained batch 1058 batch loss 120.642036 epoch total loss 219.233429\n",
      "Trained batch 1059 batch loss 120.689079 epoch total loss 219.140381\n",
      "Trained batch 1060 batch loss 120.250534 epoch total loss 219.047089\n",
      "Trained batch 1061 batch loss 120.144447 epoch total loss 218.953857\n",
      "Trained batch 1062 batch loss 119.965981 epoch total loss 218.860657\n",
      "Trained batch 1063 batch loss 119.702446 epoch total loss 218.76738\n",
      "Trained batch 1064 batch loss 119.813454 epoch total loss 218.674377\n",
      "Trained batch 1065 batch loss 120.270035 epoch total loss 218.58197\n",
      "Trained batch 1066 batch loss 120.093872 epoch total loss 218.489578\n",
      "Trained batch 1067 batch loss 119.994423 epoch total loss 218.397278\n",
      "Trained batch 1068 batch loss 119.862717 epoch total loss 218.305008\n",
      "Trained batch 1069 batch loss 119.813339 epoch total loss 218.212875\n",
      "Trained batch 1070 batch loss 119.680527 epoch total loss 218.120789\n",
      "Trained batch 1071 batch loss 119.339386 epoch total loss 218.028564\n",
      "Trained batch 1072 batch loss 119.37851 epoch total loss 217.936539\n",
      "Trained batch 1073 batch loss 119.343887 epoch total loss 217.84465\n",
      "Trained batch 1074 batch loss 119.416183 epoch total loss 217.753\n",
      "Trained batch 1075 batch loss 118.929192 epoch total loss 217.661072\n",
      "Trained batch 1076 batch loss 118.641701 epoch total loss 217.569046\n",
      "Trained batch 1077 batch loss 118.029808 epoch total loss 217.476624\n",
      "Trained batch 1078 batch loss 118.162666 epoch total loss 217.384491\n",
      "Trained batch 1079 batch loss 117.526939 epoch total loss 217.291946\n",
      "Trained batch 1080 batch loss 117.668922 epoch total loss 217.199707\n",
      "Trained batch 1081 batch loss 118.556015 epoch total loss 217.108459\n",
      "Trained batch 1082 batch loss 117.783875 epoch total loss 217.016663\n",
      "Trained batch 1083 batch loss 118.210938 epoch total loss 216.925446\n",
      "Trained batch 1084 batch loss 118.423271 epoch total loss 216.834564\n",
      "Trained batch 1085 batch loss 117.976601 epoch total loss 216.743469\n",
      "Trained batch 1086 batch loss 117.593 epoch total loss 216.652161\n",
      "Trained batch 1087 batch loss 116.997345 epoch total loss 216.560486\n",
      "Trained batch 1088 batch loss 116.548103 epoch total loss 216.468567\n",
      "Trained batch 1089 batch loss 116.736305 epoch total loss 216.376984\n",
      "Trained batch 1090 batch loss 116.903572 epoch total loss 216.285721\n",
      "Trained batch 1091 batch loss 117.745529 epoch total loss 216.195404\n",
      "Trained batch 1092 batch loss 117.490509 epoch total loss 216.105011\n",
      "Trained batch 1093 batch loss 117.084274 epoch total loss 216.014404\n",
      "Trained batch 1094 batch loss 117.059151 epoch total loss 215.923965\n",
      "Trained batch 1095 batch loss 116.977905 epoch total loss 215.833603\n",
      "Trained batch 1096 batch loss 116.388199 epoch total loss 215.742874\n",
      "Trained batch 1097 batch loss 116.339218 epoch total loss 215.652267\n",
      "Trained batch 1098 batch loss 116.453644 epoch total loss 215.56192\n",
      "Trained batch 1099 batch loss 116.188988 epoch total loss 215.471497\n",
      "Trained batch 1100 batch loss 116.127815 epoch total loss 215.38118\n",
      "Trained batch 1101 batch loss 115.845589 epoch total loss 215.290771\n",
      "Trained batch 1102 batch loss 115.988861 epoch total loss 215.200653\n",
      "Trained batch 1103 batch loss 115.726112 epoch total loss 215.110458\n",
      "Trained batch 1104 batch loss 115.465897 epoch total loss 215.020218\n",
      "Trained batch 1105 batch loss 115.442345 epoch total loss 214.930084\n",
      "Trained batch 1106 batch loss 115.376022 epoch total loss 214.840073\n",
      "Trained batch 1107 batch loss 115.040024 epoch total loss 214.749924\n",
      "Trained batch 1108 batch loss 114.770966 epoch total loss 214.659698\n",
      "Trained batch 1109 batch loss 114.46376 epoch total loss 214.569351\n",
      "Trained batch 1110 batch loss 114.917206 epoch total loss 214.479568\n",
      "Trained batch 1111 batch loss 114.097359 epoch total loss 214.389221\n",
      "Trained batch 1112 batch loss 114.058037 epoch total loss 214.299\n",
      "Trained batch 1113 batch loss 113.785645 epoch total loss 214.208679\n",
      "Trained batch 1114 batch loss 113.479195 epoch total loss 214.118271\n",
      "Trained batch 1115 batch loss 114.00592 epoch total loss 214.028473\n",
      "Trained batch 1116 batch loss 114.324112 epoch total loss 213.939133\n",
      "Trained batch 1117 batch loss 114.175713 epoch total loss 213.849823\n",
      "Trained batch 1118 batch loss 113.989861 epoch total loss 213.760498\n",
      "Trained batch 1119 batch loss 113.625725 epoch total loss 213.671\n",
      "Trained batch 1120 batch loss 113.426865 epoch total loss 213.581497\n",
      "Trained batch 1121 batch loss 113.455238 epoch total loss 213.492188\n",
      "Trained batch 1122 batch loss 113.330948 epoch total loss 213.402908\n",
      "Trained batch 1123 batch loss 113.227036 epoch total loss 213.313705\n",
      "Trained batch 1124 batch loss 112.880722 epoch total loss 213.22435\n",
      "Trained batch 1125 batch loss 113.033066 epoch total loss 213.135284\n",
      "Trained batch 1126 batch loss 113.073524 epoch total loss 213.046432\n",
      "Trained batch 1127 batch loss 112.741219 epoch total loss 212.957428\n",
      "Trained batch 1128 batch loss 112.451408 epoch total loss 212.868317\n",
      "Trained batch 1129 batch loss 112.752182 epoch total loss 212.779648\n",
      "Trained batch 1130 batch loss 112.20562 epoch total loss 212.690643\n",
      "Trained batch 1131 batch loss 112.47451 epoch total loss 212.60202\n",
      "Trained batch 1132 batch loss 112.530769 epoch total loss 212.513626\n",
      "Trained batch 1133 batch loss 112.2276 epoch total loss 212.42511\n",
      "Trained batch 1134 batch loss 112.106316 epoch total loss 212.336655\n",
      "Trained batch 1135 batch loss 112.061256 epoch total loss 212.248306\n",
      "Trained batch 1136 batch loss 111.746521 epoch total loss 212.159836\n",
      "Trained batch 1137 batch loss 111.72998 epoch total loss 212.071518\n",
      "Trained batch 1138 batch loss 111.716881 epoch total loss 211.983337\n",
      "Trained batch 1139 batch loss 111.750893 epoch total loss 211.895325\n",
      "Trained batch 1140 batch loss 111.337532 epoch total loss 211.807129\n",
      "Trained batch 1141 batch loss 111.206352 epoch total loss 211.718948\n",
      "Trained batch 1142 batch loss 111.065361 epoch total loss 211.630814\n",
      "Trained batch 1143 batch loss 110.446297 epoch total loss 211.542297\n",
      "Trained batch 1144 batch loss 110.624352 epoch total loss 211.454086\n",
      "Trained batch 1145 batch loss 110.540894 epoch total loss 211.365952\n",
      "Trained batch 1146 batch loss 111.183891 epoch total loss 211.278534\n",
      "Trained batch 1147 batch loss 110.761284 epoch total loss 211.190903\n",
      "Trained batch 1148 batch loss 110.072113 epoch total loss 211.102829\n",
      "Trained batch 1149 batch loss 110.178238 epoch total loss 211.014984\n",
      "Trained batch 1150 batch loss 110.301239 epoch total loss 210.927399\n",
      "Trained batch 1151 batch loss 110.124298 epoch total loss 210.839828\n",
      "Trained batch 1152 batch loss 110.036568 epoch total loss 210.752319\n",
      "Trained batch 1153 batch loss 109.576019 epoch total loss 210.664566\n",
      "Trained batch 1154 batch loss 108.778091 epoch total loss 210.576279\n",
      "Trained batch 1155 batch loss 109.285835 epoch total loss 210.488586\n",
      "Trained batch 1156 batch loss 108.976097 epoch total loss 210.400757\n",
      "Trained batch 1157 batch loss 109.240486 epoch total loss 210.313324\n",
      "Trained batch 1158 batch loss 109.892845 epoch total loss 210.226608\n",
      "Trained batch 1159 batch loss 109.670509 epoch total loss 210.139847\n",
      "Trained batch 1160 batch loss 109.237259 epoch total loss 210.052856\n",
      "Trained batch 1161 batch loss 109.399086 epoch total loss 209.966171\n",
      "Trained batch 1162 batch loss 109.06958 epoch total loss 209.879333\n",
      "Trained batch 1163 batch loss 109.126411 epoch total loss 209.792694\n",
      "Trained batch 1164 batch loss 108.648636 epoch total loss 209.705811\n",
      "Trained batch 1165 batch loss 108.643906 epoch total loss 209.619064\n",
      "Trained batch 1166 batch loss 108.381355 epoch total loss 209.532227\n",
      "Trained batch 1167 batch loss 107.908096 epoch total loss 209.445145\n",
      "Trained batch 1168 batch loss 108.094841 epoch total loss 209.358368\n",
      "Trained batch 1169 batch loss 108.214127 epoch total loss 209.271851\n",
      "Trained batch 1170 batch loss 107.596558 epoch total loss 209.184952\n",
      "Trained batch 1171 batch loss 107.45076 epoch total loss 209.098068\n",
      "Trained batch 1172 batch loss 107.833748 epoch total loss 209.011673\n",
      "Trained batch 1173 batch loss 107.738213 epoch total loss 208.925323\n",
      "Trained batch 1174 batch loss 107.72773 epoch total loss 208.839127\n",
      "Trained batch 1175 batch loss 107.724983 epoch total loss 208.753067\n",
      "Trained batch 1176 batch loss 107.345215 epoch total loss 208.66684\n",
      "Trained batch 1177 batch loss 107.056313 epoch total loss 208.580521\n",
      "Trained batch 1178 batch loss 107.224075 epoch total loss 208.494476\n",
      "Trained batch 1179 batch loss 107.10865 epoch total loss 208.408478\n",
      "Trained batch 1180 batch loss 106.970985 epoch total loss 208.32251\n",
      "Trained batch 1181 batch loss 107.013344 epoch total loss 208.236725\n",
      "Trained batch 1182 batch loss 106.846359 epoch total loss 208.150955\n",
      "Trained batch 1183 batch loss 106.373512 epoch total loss 208.064911\n",
      "Trained batch 1184 batch loss 106.457535 epoch total loss 207.979095\n",
      "Trained batch 1185 batch loss 106.576851 epoch total loss 207.893524\n",
      "Trained batch 1186 batch loss 106.634949 epoch total loss 207.808151\n",
      "Trained batch 1187 batch loss 106.538383 epoch total loss 207.722824\n",
      "Trained batch 1188 batch loss 106.320305 epoch total loss 207.637466\n",
      "Trained batch 1189 batch loss 106.235779 epoch total loss 207.552185\n",
      "Trained batch 1190 batch loss 106.11171 epoch total loss 207.466934\n",
      "Trained batch 1191 batch loss 105.966805 epoch total loss 207.381714\n",
      "Trained batch 1192 batch loss 105.561607 epoch total loss 207.296295\n",
      "Trained batch 1193 batch loss 105.875694 epoch total loss 207.211288\n",
      "Trained batch 1194 batch loss 104.799782 epoch total loss 207.125504\n",
      "Trained batch 1195 batch loss 105.275055 epoch total loss 207.040283\n",
      "Trained batch 1196 batch loss 105.243553 epoch total loss 206.95517\n",
      "Trained batch 1197 batch loss 105.208023 epoch total loss 206.870163\n",
      "Trained batch 1198 batch loss 105.206268 epoch total loss 206.785309\n",
      "Trained batch 1199 batch loss 105.220535 epoch total loss 206.700592\n",
      "Trained batch 1200 batch loss 104.668671 epoch total loss 206.61557\n",
      "Trained batch 1201 batch loss 104.640305 epoch total loss 206.53067\n",
      "Trained batch 1202 batch loss 104.401787 epoch total loss 206.445709\n",
      "Trained batch 1203 batch loss 104.17852 epoch total loss 206.360687\n",
      "Trained batch 1204 batch loss 104.517166 epoch total loss 206.276093\n",
      "Trained batch 1205 batch loss 104.539642 epoch total loss 206.191681\n",
      "Trained batch 1206 batch loss 104.289375 epoch total loss 206.107193\n",
      "Trained batch 1207 batch loss 104.352623 epoch total loss 206.022888\n",
      "Trained batch 1208 batch loss 104.24588 epoch total loss 205.938644\n",
      "Trained batch 1209 batch loss 104.172737 epoch total loss 205.854462\n",
      "Trained batch 1210 batch loss 104.063019 epoch total loss 205.77034\n",
      "Trained batch 1211 batch loss 104.073174 epoch total loss 205.686371\n",
      "Trained batch 1212 batch loss 103.566338 epoch total loss 205.602097\n",
      "Trained batch 1213 batch loss 103.556213 epoch total loss 205.517975\n",
      "Trained batch 1214 batch loss 103.492966 epoch total loss 205.433945\n",
      "Trained batch 1215 batch loss 103.217758 epoch total loss 205.349823\n",
      "Trained batch 1216 batch loss 103.363602 epoch total loss 205.265945\n",
      "Trained batch 1217 batch loss 102.81871 epoch total loss 205.181763\n",
      "Trained batch 1218 batch loss 103.088219 epoch total loss 205.097946\n",
      "Trained batch 1219 batch loss 103.104134 epoch total loss 205.014282\n",
      "Trained batch 1220 batch loss 102.876907 epoch total loss 204.930557\n",
      "Trained batch 1221 batch loss 102.743835 epoch total loss 204.846878\n",
      "Trained batch 1222 batch loss 102.542747 epoch total loss 204.763153\n",
      "Trained batch 1223 batch loss 102.108414 epoch total loss 204.679214\n",
      "Trained batch 1224 batch loss 102.221985 epoch total loss 204.595505\n",
      "Trained batch 1225 batch loss 102.034248 epoch total loss 204.51178\n",
      "Trained batch 1226 batch loss 102.02784 epoch total loss 204.428192\n",
      "Trained batch 1227 batch loss 102.026947 epoch total loss 204.344742\n",
      "Trained batch 1228 batch loss 101.287659 epoch total loss 204.260818\n",
      "Trained batch 1229 batch loss 101.162781 epoch total loss 204.176926\n",
      "Trained batch 1230 batch loss 101.680199 epoch total loss 204.093597\n",
      "Trained batch 1231 batch loss 101.583405 epoch total loss 204.010315\n",
      "Trained batch 1232 batch loss 101.524 epoch total loss 203.927139\n",
      "Trained batch 1233 batch loss 101.731659 epoch total loss 203.844254\n",
      "Trained batch 1234 batch loss 101.52092 epoch total loss 203.761337\n",
      "Trained batch 1235 batch loss 101.201813 epoch total loss 203.678284\n",
      "Trained batch 1236 batch loss 100.520645 epoch total loss 203.594818\n",
      "Trained batch 1237 batch loss 100.228577 epoch total loss 203.511261\n",
      "Trained batch 1238 batch loss 101.015182 epoch total loss 203.428482\n",
      "Trained batch 1239 batch loss 101.451248 epoch total loss 203.346176\n",
      "Trained batch 1240 batch loss 101.218132 epoch total loss 203.263809\n",
      "Trained batch 1241 batch loss 101.096634 epoch total loss 203.181488\n",
      "Trained batch 1242 batch loss 101.133873 epoch total loss 203.099319\n",
      "Trained batch 1243 batch loss 100.92675 epoch total loss 203.01712\n",
      "Trained batch 1244 batch loss 100.415756 epoch total loss 202.934647\n",
      "Trained batch 1245 batch loss 99.8793182 epoch total loss 202.851868\n",
      "Trained batch 1246 batch loss 100.556854 epoch total loss 202.769775\n",
      "Trained batch 1247 batch loss 99.8732376 epoch total loss 202.687256\n",
      "Trained batch 1248 batch loss 99.7975082 epoch total loss 202.604813\n",
      "Trained batch 1249 batch loss 99.7576 epoch total loss 202.522461\n",
      "Trained batch 1250 batch loss 99.8482056 epoch total loss 202.440323\n",
      "Trained batch 1251 batch loss 99.2357254 epoch total loss 202.357834\n",
      "Trained batch 1252 batch loss 99.2060471 epoch total loss 202.275436\n",
      "Trained batch 1253 batch loss 99.6340485 epoch total loss 202.193527\n",
      "Trained batch 1254 batch loss 99.7567 epoch total loss 202.111832\n",
      "Trained batch 1255 batch loss 99.211 epoch total loss 202.029846\n",
      "Trained batch 1256 batch loss 99.3452301 epoch total loss 201.94809\n",
      "Trained batch 1257 batch loss 99.3901596 epoch total loss 201.866501\n",
      "Trained batch 1258 batch loss 99.1898651 epoch total loss 201.784882\n",
      "Trained batch 1259 batch loss 98.9214554 epoch total loss 201.703171\n",
      "Trained batch 1260 batch loss 98.6426849 epoch total loss 201.621384\n",
      "Trained batch 1261 batch loss 98.8462067 epoch total loss 201.539871\n",
      "Trained batch 1262 batch loss 99.1489792 epoch total loss 201.45874\n",
      "Trained batch 1263 batch loss 99.0163269 epoch total loss 201.37764\n",
      "Trained batch 1264 batch loss 98.8887939 epoch total loss 201.296555\n",
      "Trained batch 1265 batch loss 98.6614838 epoch total loss 201.215408\n",
      "Trained batch 1266 batch loss 98.5484238 epoch total loss 201.134323\n",
      "Trained batch 1267 batch loss 97.5822754 epoch total loss 201.052582\n",
      "Trained batch 1268 batch loss 97.3939438 epoch total loss 200.970825\n",
      "Trained batch 1269 batch loss 97.8264313 epoch total loss 200.889557\n",
      "Trained batch 1270 batch loss 98.0798874 epoch total loss 200.808594\n",
      "Trained batch 1271 batch loss 97.2666245 epoch total loss 200.727127\n",
      "Trained batch 1272 batch loss 97.7202301 epoch total loss 200.646149\n",
      "Trained batch 1273 batch loss 97.1498947 epoch total loss 200.56485\n",
      "Trained batch 1274 batch loss 97.2161789 epoch total loss 200.483734\n",
      "Trained batch 1275 batch loss 96.8656921 epoch total loss 200.402466\n",
      "Trained batch 1276 batch loss 96.9919 epoch total loss 200.321411\n",
      "Trained batch 1277 batch loss 96.7794876 epoch total loss 200.240341\n",
      "Trained batch 1278 batch loss 96.8473053 epoch total loss 200.159424\n",
      "Trained batch 1279 batch loss 96.8150787 epoch total loss 200.078629\n",
      "Trained batch 1280 batch loss 96.7464905 epoch total loss 199.997894\n",
      "Trained batch 1281 batch loss 96.7260818 epoch total loss 199.917282\n",
      "Trained batch 1282 batch loss 96.0624466 epoch total loss 199.836273\n",
      "Trained batch 1283 batch loss 96.5307541 epoch total loss 199.755753\n",
      "Trained batch 1284 batch loss 96.1515427 epoch total loss 199.675064\n",
      "Trained batch 1285 batch loss 95.4443741 epoch total loss 199.593948\n",
      "Trained batch 1286 batch loss 96.1900711 epoch total loss 199.513535\n",
      "Trained batch 1287 batch loss 95.6928482 epoch total loss 199.432861\n",
      "Trained batch 1288 batch loss 96.1716 epoch total loss 199.352692\n",
      "Trained batch 1289 batch loss 95.9700775 epoch total loss 199.272491\n",
      "Trained batch 1290 batch loss 95.878212 epoch total loss 199.192337\n",
      "Trained batch 1291 batch loss 95.7270203 epoch total loss 199.112198\n",
      "Trained batch 1292 batch loss 95.5817413 epoch total loss 199.032059\n",
      "Trained batch 1293 batch loss 95.4702 epoch total loss 198.951965\n",
      "Trained batch 1294 batch loss 95.1490479 epoch total loss 198.87175\n",
      "Trained batch 1295 batch loss 95.1708374 epoch total loss 198.791672\n",
      "Trained batch 1296 batch loss 94.9546509 epoch total loss 198.711548\n",
      "Trained batch 1297 batch loss 94.7880707 epoch total loss 198.631424\n",
      "Trained batch 1298 batch loss 94.9779587 epoch total loss 198.551575\n",
      "Trained batch 1299 batch loss 94.9557953 epoch total loss 198.471817\n",
      "Trained batch 1300 batch loss 94.397 epoch total loss 198.391754\n",
      "Trained batch 1301 batch loss 94.4456787 epoch total loss 198.311859\n",
      "Trained batch 1302 batch loss 94.8774414 epoch total loss 198.232422\n",
      "Trained batch 1303 batch loss 94.7550201 epoch total loss 198.153\n",
      "Trained batch 1304 batch loss 94.2054901 epoch total loss 198.073288\n",
      "Trained batch 1305 batch loss 94.1515427 epoch total loss 197.993652\n",
      "Trained batch 1306 batch loss 94.3673325 epoch total loss 197.914307\n",
      "Trained batch 1307 batch loss 93.562233 epoch total loss 197.834473\n",
      "Trained batch 1308 batch loss 93.6464081 epoch total loss 197.754807\n",
      "Trained batch 1309 batch loss 93.8262405 epoch total loss 197.675415\n",
      "Trained batch 1310 batch loss 93.6335068 epoch total loss 197.596008\n",
      "Trained batch 1311 batch loss 93.6625748 epoch total loss 197.516724\n",
      "Trained batch 1312 batch loss 93.9027328 epoch total loss 197.437744\n",
      "Trained batch 1313 batch loss 93.5848236 epoch total loss 197.358643\n",
      "Trained batch 1314 batch loss 93.8146515 epoch total loss 197.279846\n",
      "Trained batch 1315 batch loss 93.6090317 epoch total loss 197.201\n",
      "Trained batch 1316 batch loss 93.0711594 epoch total loss 197.121887\n",
      "Trained batch 1317 batch loss 93.0009308 epoch total loss 197.042831\n",
      "Trained batch 1318 batch loss 93.003952 epoch total loss 196.963882\n",
      "Trained batch 1319 batch loss 92.9056473 epoch total loss 196.885\n",
      "Trained batch 1320 batch loss 92.884079 epoch total loss 196.806213\n",
      "Trained batch 1321 batch loss 93.2906418 epoch total loss 196.727859\n",
      "Trained batch 1322 batch loss 93.0877304 epoch total loss 196.64946\n",
      "Trained batch 1323 batch loss 92.7672729 epoch total loss 196.570938\n",
      "Trained batch 1324 batch loss 92.6671753 epoch total loss 196.492477\n",
      "Trained batch 1325 batch loss 92.7514496 epoch total loss 196.414169\n",
      "Trained batch 1326 batch loss 92.5193863 epoch total loss 196.335815\n",
      "Trained batch 1327 batch loss 92.3835526 epoch total loss 196.257492\n",
      "Trained batch 1328 batch loss 92.20401 epoch total loss 196.179138\n",
      "Trained batch 1329 batch loss 92.1955185 epoch total loss 196.100891\n",
      "Trained batch 1330 batch loss 91.9750366 epoch total loss 196.022598\n",
      "Trained batch 1331 batch loss 91.7754745 epoch total loss 195.94429\n",
      "Trained batch 1332 batch loss 91.5661392 epoch total loss 195.865921\n",
      "Trained batch 1333 batch loss 91.362587 epoch total loss 195.787521\n",
      "Trained batch 1334 batch loss 91.3305054 epoch total loss 195.709213\n",
      "Trained batch 1335 batch loss 91.3175812 epoch total loss 195.631012\n",
      "Trained batch 1336 batch loss 91.1541214 epoch total loss 195.552811\n",
      "Trained batch 1337 batch loss 91.5711212 epoch total loss 195.475052\n",
      "Trained batch 1338 batch loss 90.8461533 epoch total loss 195.396851\n",
      "Trained batch 1339 batch loss 90.7687759 epoch total loss 195.31871\n",
      "Trained batch 1340 batch loss 90.6377563 epoch total loss 195.240585\n",
      "Trained batch 1341 batch loss 90.7887726 epoch total loss 195.162689\n",
      "Trained batch 1342 batch loss 90.5578537 epoch total loss 195.084747\n",
      "Trained batch 1343 batch loss 90.5963211 epoch total loss 195.006943\n",
      "Trained batch 1344 batch loss 90.836853 epoch total loss 194.929443\n",
      "Trained batch 1345 batch loss 90.7531281 epoch total loss 194.85199\n",
      "Trained batch 1346 batch loss 90.7822418 epoch total loss 194.774673\n",
      "Trained batch 1347 batch loss 90.9616852 epoch total loss 194.697617\n",
      "Trained batch 1348 batch loss 90.8509674 epoch total loss 194.620575\n",
      "Trained batch 1349 batch loss 90.6096344 epoch total loss 194.543488\n",
      "Trained batch 1350 batch loss 90.3987885 epoch total loss 194.466339\n",
      "Trained batch 1351 batch loss 90.2784958 epoch total loss 194.389221\n",
      "Trained batch 1352 batch loss 89.7410049 epoch total loss 194.311829\n",
      "Trained batch 1353 batch loss 89.9020844 epoch total loss 194.234665\n",
      "Trained batch 1354 batch loss 90.1490326 epoch total loss 194.157791\n",
      "Trained batch 1355 batch loss 90.1288223 epoch total loss 194.081024\n",
      "Trained batch 1356 batch loss 89.9560165 epoch total loss 194.004242\n",
      "Trained batch 1357 batch loss 89.7200775 epoch total loss 193.927383\n",
      "Trained batch 1358 batch loss 89.6776581 epoch total loss 193.850632\n",
      "Trained batch 1359 batch loss 89.3692474 epoch total loss 193.773758\n",
      "Trained batch 1360 batch loss 89.2729797 epoch total loss 193.696915\n",
      "Trained batch 1361 batch loss 89.6913452 epoch total loss 193.620499\n",
      "Trained batch 1362 batch loss 89.5817566 epoch total loss 193.544128\n",
      "Trained batch 1363 batch loss 89.2385483 epoch total loss 193.467606\n",
      "Trained batch 1364 batch loss 88.913208 epoch total loss 193.390945\n",
      "Trained batch 1365 batch loss 89.2033 epoch total loss 193.314636\n",
      "Trained batch 1366 batch loss 89.1630936 epoch total loss 193.238373\n",
      "Trained batch 1367 batch loss 88.5721817 epoch total loss 193.161804\n",
      "Trained batch 1368 batch loss 88.3931808 epoch total loss 193.085236\n",
      "Trained batch 1369 batch loss 88.2135391 epoch total loss 193.008621\n",
      "Trained batch 1370 batch loss 87.8657837 epoch total loss 192.931885\n",
      "Trained batch 1371 batch loss 87.496521 epoch total loss 192.85498\n",
      "Trained batch 1372 batch loss 87.8088379 epoch total loss 192.778427\n",
      "Trained batch 1373 batch loss 87.5629883 epoch total loss 192.701797\n",
      "Trained batch 1374 batch loss 87.6108627 epoch total loss 192.62532\n",
      "Trained batch 1375 batch loss 88.1333847 epoch total loss 192.549316\n",
      "Trained batch 1376 batch loss 88.0636826 epoch total loss 192.473389\n",
      "Trained batch 1377 batch loss 87.9178619 epoch total loss 192.397446\n",
      "Trained batch 1378 batch loss 87.740715 epoch total loss 192.321503\n",
      "Trained batch 1379 batch loss 87.5650253 epoch total loss 192.245529\n",
      "Trained batch 1380 batch loss 87.3466263 epoch total loss 192.169525\n",
      "Trained batch 1381 batch loss 87.0444107 epoch total loss 192.093384\n",
      "Trained batch 1382 batch loss 87.0267 epoch total loss 192.017365\n",
      "Trained batch 1383 batch loss 87.0187759 epoch total loss 191.941452\n",
      "Trained batch 1384 batch loss 87.2369919 epoch total loss 191.865814\n",
      "Trained batch 1385 batch loss 87.1949463 epoch total loss 191.790237\n",
      "Trained batch 1386 batch loss 87.2214432 epoch total loss 191.714783\n",
      "Trained batch 1387 batch loss 87.0384598 epoch total loss 191.639313\n",
      "Trained batch 1388 batch loss 86.890358 epoch total loss 191.563828\n",
      "Epoch 1 train loss 191.56382751464844\n",
      "Validated batch 1 batch loss 88.0778809\n",
      "Validated batch 2 batch loss 87.7100143\n",
      "Validated batch 3 batch loss 88.529953\n",
      "Validated batch 4 batch loss 88.528244\n",
      "Validated batch 5 batch loss 88.1797867\n",
      "Validated batch 6 batch loss 88.3981094\n",
      "Validated batch 7 batch loss 88.4452896\n",
      "Validated batch 8 batch loss 88.5380859\n",
      "Validated batch 9 batch loss 88.2088394\n",
      "Validated batch 10 batch loss 88.4381485\n",
      "Validated batch 11 batch loss 88.0339127\n",
      "Validated batch 12 batch loss 88.2345123\n",
      "Validated batch 13 batch loss 88.1974487\n",
      "Validated batch 14 batch loss 88.717308\n",
      "Validated batch 15 batch loss 88.4533234\n",
      "Validated batch 16 batch loss 88.2394\n",
      "Validated batch 17 batch loss 88.6509705\n",
      "Validated batch 18 batch loss 87.6952057\n",
      "Validated batch 19 batch loss 88.7901\n",
      "Validated batch 20 batch loss 88.9201508\n",
      "Validated batch 21 batch loss 88.5445786\n",
      "Validated batch 22 batch loss 88.7217636\n",
      "Validated batch 23 batch loss 88.1247711\n",
      "Validated batch 24 batch loss 88.8709106\n",
      "Validated batch 25 batch loss 88.4549103\n",
      "Validated batch 26 batch loss 88.5510788\n",
      "Validated batch 27 batch loss 88.4502869\n",
      "Validated batch 28 batch loss 88.7172\n",
      "Validated batch 29 batch loss 88.6416473\n",
      "Validated batch 30 batch loss 88.5669632\n",
      "Validated batch 31 batch loss 87.9538727\n",
      "Validated batch 32 batch loss 88.3171616\n",
      "Validated batch 33 batch loss 88.1298\n",
      "Validated batch 34 batch loss 88.5521851\n",
      "Validated batch 35 batch loss 87.9616547\n",
      "Validated batch 36 batch loss 88.1228943\n",
      "Validated batch 37 batch loss 87.8473358\n",
      "Validated batch 38 batch loss 88.3456192\n",
      "Validated batch 39 batch loss 88.7907944\n",
      "Validated batch 40 batch loss 88.7901459\n",
      "Validated batch 41 batch loss 88.4683228\n",
      "Validated batch 42 batch loss 88.9200821\n",
      "Validated batch 43 batch loss 88.7242584\n",
      "Validated batch 44 batch loss 88.8564453\n",
      "Validated batch 45 batch loss 88.0407333\n",
      "Validated batch 46 batch loss 88.029808\n",
      "Validated batch 47 batch loss 88.1793213\n",
      "Validated batch 48 batch loss 88.2660828\n",
      "Validated batch 49 batch loss 87.7604294\n",
      "Validated batch 50 batch loss 87.6572723\n",
      "Validated batch 51 batch loss 87.9451752\n",
      "Validated batch 52 batch loss 88.3997574\n",
      "Validated batch 53 batch loss 88.1316299\n",
      "Validated batch 54 batch loss 88.4529877\n",
      "Validated batch 55 batch loss 88.8233\n",
      "Validated batch 56 batch loss 88.5876389\n",
      "Validated batch 57 batch loss 88.5914307\n",
      "Validated batch 58 batch loss 87.7490158\n",
      "Validated batch 59 batch loss 88.4261246\n",
      "Validated batch 60 batch loss 88.3951\n",
      "Validated batch 61 batch loss 88.5426636\n",
      "Validated batch 62 batch loss 88.7387466\n",
      "Validated batch 63 batch loss 88.1942749\n",
      "Validated batch 64 batch loss 88.5960312\n",
      "Validated batch 65 batch loss 88.6418228\n",
      "Validated batch 66 batch loss 88.5804291\n",
      "Validated batch 67 batch loss 88.4512405\n",
      "Validated batch 68 batch loss 88.7225418\n",
      "Validated batch 69 batch loss 88.6854706\n",
      "Validated batch 70 batch loss 88.6391449\n",
      "Validated batch 71 batch loss 88.0658417\n",
      "Validated batch 72 batch loss 87.908226\n",
      "Validated batch 73 batch loss 88.0272598\n",
      "Validated batch 74 batch loss 88.314888\n",
      "Validated batch 75 batch loss 88.5282822\n",
      "Validated batch 76 batch loss 88.1457596\n",
      "Validated batch 77 batch loss 88.5498\n",
      "Validated batch 78 batch loss 88.196907\n",
      "Validated batch 79 batch loss 88.778595\n",
      "Validated batch 80 batch loss 88.8233185\n",
      "Validated batch 81 batch loss 88.346817\n",
      "Validated batch 82 batch loss 88.4150085\n",
      "Validated batch 83 batch loss 88.2310181\n",
      "Validated batch 84 batch loss 88.5853577\n",
      "Validated batch 85 batch loss 88.2404938\n",
      "Validated batch 86 batch loss 88.5265198\n",
      "Validated batch 87 batch loss 88.0784073\n",
      "Validated batch 88 batch loss 88.352829\n",
      "Validated batch 89 batch loss 88.3049\n",
      "Validated batch 90 batch loss 88.1304474\n",
      "Validated batch 91 batch loss 88.3128\n",
      "Validated batch 92 batch loss 88.4464569\n",
      "Validated batch 93 batch loss 88.2646484\n",
      "Validated batch 94 batch loss 88.3161316\n",
      "Validated batch 95 batch loss 88.1626053\n",
      "Validated batch 96 batch loss 88.5968094\n",
      "Validated batch 97 batch loss 88.9201584\n",
      "Validated batch 98 batch loss 88.8083649\n",
      "Validated batch 99 batch loss 88.5450211\n",
      "Validated batch 100 batch loss 88.3841782\n",
      "Validated batch 101 batch loss 88.718483\n",
      "Validated batch 102 batch loss 88.6777649\n",
      "Validated batch 103 batch loss 88.9197769\n",
      "Validated batch 104 batch loss 88.7308197\n",
      "Validated batch 105 batch loss 87.658493\n",
      "Validated batch 106 batch loss 88.6186905\n",
      "Validated batch 107 batch loss 88.308136\n",
      "Validated batch 108 batch loss 88.2894745\n",
      "Validated batch 109 batch loss 88.3828506\n",
      "Validated batch 110 batch loss 87.4524689\n",
      "Validated batch 111 batch loss 87.8843918\n",
      "Validated batch 112 batch loss 88.9201202\n",
      "Validated batch 113 batch loss 88.2417679\n",
      "Validated batch 114 batch loss 88.8337402\n",
      "Validated batch 115 batch loss 88.4237289\n",
      "Validated batch 116 batch loss 88.7212\n",
      "Validated batch 117 batch loss 88.3921738\n",
      "Validated batch 118 batch loss 88.2672\n",
      "Validated batch 119 batch loss 88.1283264\n",
      "Validated batch 120 batch loss 87.9623871\n",
      "Validated batch 121 batch loss 88.2449265\n",
      "Validated batch 122 batch loss 87.9840851\n",
      "Validated batch 123 batch loss 88.5369949\n",
      "Validated batch 124 batch loss 88.7245178\n",
      "Validated batch 125 batch loss 88.3779297\n",
      "Validated batch 126 batch loss 88.2973862\n",
      "Validated batch 127 batch loss 88.0620117\n",
      "Validated batch 128 batch loss 88.0213242\n",
      "Validated batch 129 batch loss 88.4195938\n",
      "Validated batch 130 batch loss 88.5010605\n",
      "Validated batch 131 batch loss 88.5426254\n",
      "Validated batch 132 batch loss 88.1063309\n",
      "Validated batch 133 batch loss 87.762764\n",
      "Validated batch 134 batch loss 87.8105316\n",
      "Validated batch 135 batch loss 88.4874191\n",
      "Validated batch 136 batch loss 88.1004944\n",
      "Validated batch 137 batch loss 87.9437637\n",
      "Validated batch 138 batch loss 88.0759354\n",
      "Validated batch 139 batch loss 88.7540741\n",
      "Validated batch 140 batch loss 88.8484955\n",
      "Validated batch 141 batch loss 88.4469528\n",
      "Validated batch 142 batch loss 87.5148392\n",
      "Validated batch 143 batch loss 88.2300644\n",
      "Validated batch 144 batch loss 88.0237579\n",
      "Validated batch 145 batch loss 87.8018188\n",
      "Validated batch 146 batch loss 88.0932236\n",
      "Validated batch 147 batch loss 87.6632538\n",
      "Validated batch 148 batch loss 88.6177368\n",
      "Validated batch 149 batch loss 88.1557312\n",
      "Validated batch 150 batch loss 87.9777145\n",
      "Validated batch 151 batch loss 87.5513\n",
      "Validated batch 152 batch loss 88.5561218\n",
      "Validated batch 153 batch loss 87.8428116\n",
      "Validated batch 154 batch loss 88.3414917\n",
      "Validated batch 155 batch loss 88.1212692\n",
      "Validated batch 156 batch loss 88.5594177\n",
      "Validated batch 157 batch loss 88.263649\n",
      "Validated batch 158 batch loss 88.1533737\n",
      "Validated batch 159 batch loss 88.2985306\n",
      "Validated batch 160 batch loss 87.3926773\n",
      "Validated batch 161 batch loss 88.4937439\n",
      "Validated batch 162 batch loss 88.1241608\n",
      "Validated batch 163 batch loss 88.3083\n",
      "Validated batch 164 batch loss 88.418\n",
      "Validated batch 165 batch loss 88.791214\n",
      "Validated batch 166 batch loss 88.2975845\n",
      "Validated batch 167 batch loss 88.5146866\n",
      "Validated batch 168 batch loss 88.2820892\n",
      "Validated batch 169 batch loss 88.7432785\n",
      "Validated batch 170 batch loss 88.3748169\n",
      "Validated batch 171 batch loss 87.9892044\n",
      "Validated batch 172 batch loss 88.2113953\n",
      "Validated batch 173 batch loss 88.2292175\n",
      "Validated batch 174 batch loss 87.0445251\n",
      "Validated batch 175 batch loss 88.731987\n",
      "Validated batch 176 batch loss 88.7805176\n",
      "Validated batch 177 batch loss 88.3724365\n",
      "Validated batch 178 batch loss 88.3010101\n",
      "Validated batch 179 batch loss 88.5569763\n",
      "Validated batch 180 batch loss 88.756\n",
      "Validated batch 181 batch loss 88.7238312\n",
      "Validated batch 182 batch loss 88.8304749\n",
      "Validated batch 183 batch loss 88.7925339\n",
      "Validated batch 184 batch loss 87.4560699\n",
      "Validated batch 185 batch loss 84.2890167\n",
      "Epoch 1 val loss 88.3194808959961\n",
      "Epoch 1 completed in 775.08 seconds\n",
      "Model ./model_simplebase-epoch-1-loss-88.3195.h5 saved.\n",
      "Start epoch 2 with learning rate 0.0007\n",
      "Trained batch 1 batch loss 87.1260376 epoch total loss 87.1260376\n",
      "Trained batch 2 batch loss 87.0122833 epoch total loss 87.0691605\n",
      "Trained batch 3 batch loss 86.8256683 epoch total loss 86.988\n",
      "Trained batch 4 batch loss 86.6730957 epoch total loss 86.9092712\n",
      "Trained batch 5 batch loss 86.3196945 epoch total loss 86.7913589\n",
      "Trained batch 6 batch loss 86.376091 epoch total loss 86.7221451\n",
      "Trained batch 7 batch loss 86.2166824 epoch total loss 86.6499405\n",
      "Trained batch 8 batch loss 86.4119339 epoch total loss 86.6201859\n",
      "Trained batch 9 batch loss 86.4430237 epoch total loss 86.6005\n",
      "Trained batch 10 batch loss 86.4634552 epoch total loss 86.5868\n",
      "Trained batch 11 batch loss 86.2257233 epoch total loss 86.5539703\n",
      "Trained batch 12 batch loss 85.9984055 epoch total loss 86.5076675\n",
      "Trained batch 13 batch loss 85.9208374 epoch total loss 86.462532\n",
      "Trained batch 14 batch loss 85.5218811 epoch total loss 86.39534\n",
      "Trained batch 15 batch loss 85.4141617 epoch total loss 86.3299332\n",
      "Trained batch 16 batch loss 85.503891 epoch total loss 86.2783051\n",
      "Trained batch 17 batch loss 85.5183105 epoch total loss 86.2336\n",
      "Trained batch 18 batch loss 85.4347534 epoch total loss 86.1892242\n",
      "Trained batch 19 batch loss 85.2924805 epoch total loss 86.1420288\n",
      "Trained batch 20 batch loss 85.0957642 epoch total loss 86.0897064\n",
      "Trained batch 21 batch loss 85.0562439 epoch total loss 86.0405\n",
      "Trained batch 22 batch loss 84.9895401 epoch total loss 85.9927292\n",
      "Trained batch 23 batch loss 84.8769302 epoch total loss 85.9442139\n",
      "Trained batch 24 batch loss 84.5367126 epoch total loss 85.8855667\n",
      "Trained batch 25 batch loss 84.8903351 epoch total loss 85.8457642\n",
      "Trained batch 26 batch loss 84.4691315 epoch total loss 85.7928162\n",
      "Trained batch 27 batch loss 84.420639 epoch total loss 85.742\n",
      "Trained batch 28 batch loss 84.4094086 epoch total loss 85.6944046\n",
      "Trained batch 29 batch loss 84.1292648 epoch total loss 85.6404343\n",
      "Trained batch 30 batch loss 83.5817 epoch total loss 85.5718079\n",
      "Trained batch 31 batch loss 83.8888321 epoch total loss 85.5175247\n",
      "Trained batch 32 batch loss 84.0025711 epoch total loss 85.4701843\n",
      "Trained batch 33 batch loss 84.3609772 epoch total loss 85.4365768\n",
      "Trained batch 34 batch loss 83.8198929 epoch total loss 85.3890228\n",
      "Trained batch 35 batch loss 84.0966339 epoch total loss 85.3521\n",
      "Trained batch 36 batch loss 83.8274384 epoch total loss 85.3097458\n",
      "Trained batch 37 batch loss 83.6477203 epoch total loss 85.2648239\n",
      "Trained batch 38 batch loss 83.5598145 epoch total loss 85.2199554\n",
      "Trained batch 39 batch loss 83.3486633 epoch total loss 85.1719742\n",
      "Trained batch 40 batch loss 83.2260666 epoch total loss 85.1233292\n",
      "Trained batch 41 batch loss 82.8785553 epoch total loss 85.0685806\n",
      "Trained batch 42 batch loss 83.1435394 epoch total loss 85.0227432\n",
      "Trained batch 43 batch loss 83.1700363 epoch total loss 84.97966\n",
      "Trained batch 44 batch loss 83.053215 epoch total loss 84.9358749\n",
      "Trained batch 45 batch loss 82.9862289 epoch total loss 84.8925476\n",
      "Trained batch 46 batch loss 82.9891052 epoch total loss 84.8511734\n",
      "Trained batch 47 batch loss 83.0870132 epoch total loss 84.8136292\n",
      "Trained batch 48 batch loss 82.8374939 epoch total loss 84.7724609\n",
      "Trained batch 49 batch loss 82.7114563 epoch total loss 84.7304\n",
      "Trained batch 50 batch loss 82.7508698 epoch total loss 84.6908112\n",
      "Trained batch 51 batch loss 82.6288681 epoch total loss 84.650383\n",
      "Trained batch 52 batch loss 82.551033 epoch total loss 84.6100159\n",
      "Trained batch 53 batch loss 82.4041214 epoch total loss 84.5684\n",
      "Trained batch 54 batch loss 82.3214874 epoch total loss 84.5267868\n",
      "Trained batch 55 batch loss 82.2246246 epoch total loss 84.4849243\n",
      "Trained batch 56 batch loss 81.9437714 epoch total loss 84.4395523\n",
      "Trained batch 57 batch loss 81.9434509 epoch total loss 84.3957596\n",
      "Trained batch 58 batch loss 81.9821854 epoch total loss 84.3541489\n",
      "Trained batch 59 batch loss 81.7488403 epoch total loss 84.3099899\n",
      "Trained batch 60 batch loss 81.4849319 epoch total loss 84.2629089\n",
      "Trained batch 61 batch loss 81.6460342 epoch total loss 84.2200089\n",
      "Trained batch 62 batch loss 81.3977 epoch total loss 84.1744843\n",
      "Trained batch 63 batch loss 81.4955673 epoch total loss 84.131958\n",
      "Trained batch 64 batch loss 81.4125595 epoch total loss 84.0894699\n",
      "Trained batch 65 batch loss 81.2678375 epoch total loss 84.0460663\n",
      "Trained batch 66 batch loss 81.1656189 epoch total loss 84.0024185\n",
      "Trained batch 67 batch loss 80.8737411 epoch total loss 83.955719\n",
      "Trained batch 68 batch loss 80.9876251 epoch total loss 83.9120712\n",
      "Trained batch 69 batch loss 81.330658 epoch total loss 83.8746567\n",
      "Trained batch 70 batch loss 80.9510193 epoch total loss 83.8328934\n",
      "Trained batch 71 batch loss 80.9323807 epoch total loss 83.7920456\n",
      "Trained batch 72 batch loss 80.8359451 epoch total loss 83.7509918\n",
      "Trained batch 73 batch loss 80.6128 epoch total loss 83.708\n",
      "Trained batch 74 batch loss 80.6769257 epoch total loss 83.667038\n",
      "Trained batch 75 batch loss 80.4836273 epoch total loss 83.624588\n",
      "Trained batch 76 batch loss 80.3694305 epoch total loss 83.5817642\n",
      "Trained batch 77 batch loss 80.1187897 epoch total loss 83.5367889\n",
      "Trained batch 78 batch loss 79.5416641 epoch total loss 83.4855652\n",
      "Trained batch 79 batch loss 79.7204056 epoch total loss 83.4379044\n",
      "Trained batch 80 batch loss 80.09758 epoch total loss 83.3961487\n",
      "Trained batch 81 batch loss 79.5343475 epoch total loss 83.3484726\n",
      "Trained batch 82 batch loss 80.4565887 epoch total loss 83.3132\n",
      "Trained batch 83 batch loss 79.9551239 epoch total loss 83.2727432\n",
      "Trained batch 84 batch loss 79.45784 epoch total loss 83.2273331\n",
      "Trained batch 85 batch loss 78.9404831 epoch total loss 83.1768951\n",
      "Trained batch 86 batch loss 79.8224258 epoch total loss 83.137886\n",
      "Trained batch 87 batch loss 79.5877914 epoch total loss 83.097084\n",
      "Trained batch 88 batch loss 79.324707 epoch total loss 83.0542145\n",
      "Trained batch 89 batch loss 79.303215 epoch total loss 83.0120697\n",
      "Trained batch 90 batch loss 79.0997772 epoch total loss 82.9686\n",
      "Trained batch 91 batch loss 79.2846909 epoch total loss 82.9281158\n",
      "Trained batch 92 batch loss 79.3909378 epoch total loss 82.8896713\n",
      "Trained batch 93 batch loss 79.20224 epoch total loss 82.8500214\n",
      "Trained batch 94 batch loss 79.0248337 epoch total loss 82.8093262\n",
      "Trained batch 95 batch loss 78.7247391 epoch total loss 82.7663269\n",
      "Trained batch 96 batch loss 78.5063858 epoch total loss 82.7219543\n",
      "Trained batch 97 batch loss 78.6319046 epoch total loss 82.6797867\n",
      "Trained batch 98 batch loss 78.6622391 epoch total loss 82.6387939\n",
      "Trained batch 99 batch loss 78.1293106 epoch total loss 82.5932388\n",
      "Trained batch 100 batch loss 78.6394196 epoch total loss 82.5537\n",
      "Trained batch 101 batch loss 78.6632309 epoch total loss 82.5151825\n",
      "Trained batch 102 batch loss 78.2552872 epoch total loss 82.4734116\n",
      "Trained batch 103 batch loss 78.1673737 epoch total loss 82.4316\n",
      "Trained batch 104 batch loss 77.8814545 epoch total loss 82.3878555\n",
      "Trained batch 105 batch loss 77.7817383 epoch total loss 82.3439941\n",
      "Trained batch 106 batch loss 77.9672241 epoch total loss 82.3027\n",
      "Trained batch 107 batch loss 78.13974 epoch total loss 82.2637939\n",
      "Trained batch 108 batch loss 77.9578629 epoch total loss 82.2239227\n",
      "Trained batch 109 batch loss 77.9231567 epoch total loss 82.1844635\n",
      "Trained batch 110 batch loss 77.8171234 epoch total loss 82.1447601\n",
      "Trained batch 111 batch loss 77.6878128 epoch total loss 82.1046066\n",
      "Trained batch 112 batch loss 77.5119095 epoch total loss 82.0636\n",
      "Trained batch 113 batch loss 77.4765167 epoch total loss 82.023\n",
      "Trained batch 114 batch loss 77.1816 epoch total loss 81.9805374\n",
      "Trained batch 115 batch loss 77.3798065 epoch total loss 81.9405289\n",
      "Trained batch 116 batch loss 77.0003204 epoch total loss 81.8979416\n",
      "Trained batch 117 batch loss 76.8326721 epoch total loss 81.8546524\n",
      "Trained batch 118 batch loss 77.4611053 epoch total loss 81.8174133\n",
      "Trained batch 119 batch loss 77.5766 epoch total loss 81.7817764\n",
      "Trained batch 120 batch loss 77.3140945 epoch total loss 81.744545\n",
      "Trained batch 121 batch loss 77.0658798 epoch total loss 81.7058792\n",
      "Trained batch 122 batch loss 77.0200424 epoch total loss 81.6674728\n",
      "Trained batch 123 batch loss 77.0150681 epoch total loss 81.6296463\n",
      "Trained batch 124 batch loss 76.8978577 epoch total loss 81.5914841\n",
      "Trained batch 125 batch loss 76.7479477 epoch total loss 81.5527344\n",
      "Trained batch 126 batch loss 76.8908768 epoch total loss 81.5157318\n",
      "Trained batch 127 batch loss 76.7509918 epoch total loss 81.4782181\n",
      "Trained batch 128 batch loss 76.6133652 epoch total loss 81.4402084\n",
      "Trained batch 129 batch loss 76.3992844 epoch total loss 81.4011307\n",
      "Trained batch 130 batch loss 76.2804871 epoch total loss 81.3617401\n",
      "Trained batch 131 batch loss 76.2589722 epoch total loss 81.3227844\n",
      "Trained batch 132 batch loss 76.2341309 epoch total loss 81.2842407\n",
      "Trained batch 133 batch loss 76.1698685 epoch total loss 81.2457886\n",
      "Trained batch 134 batch loss 76.0051727 epoch total loss 81.2066727\n",
      "Trained batch 135 batch loss 75.83992 epoch total loss 81.1669235\n",
      "Trained batch 136 batch loss 75.7463226 epoch total loss 81.1270599\n",
      "Trained batch 137 batch loss 75.5729675 epoch total loss 81.086525\n",
      "Trained batch 138 batch loss 75.5232925 epoch total loss 81.0462112\n",
      "Trained batch 139 batch loss 75.3669434 epoch total loss 81.0053558\n",
      "Trained batch 140 batch loss 74.8997955 epoch total loss 80.9617386\n",
      "Trained batch 141 batch loss 75.0696793 epoch total loss 80.9199524\n",
      "Trained batch 142 batch loss 75.3859329 epoch total loss 80.8809738\n",
      "Trained batch 143 batch loss 75.1033401 epoch total loss 80.8405762\n",
      "Trained batch 144 batch loss 75.3759308 epoch total loss 80.8026276\n",
      "Trained batch 145 batch loss 74.7198257 epoch total loss 80.7606735\n",
      "Trained batch 146 batch loss 75.0641098 epoch total loss 80.7216568\n",
      "Trained batch 147 batch loss 74.5351257 epoch total loss 80.6795731\n",
      "Trained batch 148 batch loss 74.375267 epoch total loss 80.6369781\n",
      "Trained batch 149 batch loss 74.6534119 epoch total loss 80.596817\n",
      "Trained batch 150 batch loss 74.3606339 epoch total loss 80.5552444\n",
      "Trained batch 151 batch loss 74.6134949 epoch total loss 80.515892\n",
      "Trained batch 152 batch loss 74.8395767 epoch total loss 80.4785461\n",
      "Trained batch 153 batch loss 75.175705 epoch total loss 80.4438858\n",
      "Trained batch 154 batch loss 74.6806335 epoch total loss 80.4064636\n",
      "Trained batch 155 batch loss 74.7202072 epoch total loss 80.3697739\n",
      "Trained batch 156 batch loss 74.3362045 epoch total loss 80.3311\n",
      "Trained batch 157 batch loss 73.9141541 epoch total loss 80.2902222\n",
      "Trained batch 158 batch loss 74.1326599 epoch total loss 80.2512512\n",
      "Trained batch 159 batch loss 74.0579 epoch total loss 80.2123\n",
      "Trained batch 160 batch loss 74.0912933 epoch total loss 80.1740417\n",
      "Trained batch 161 batch loss 74.1762466 epoch total loss 80.1367874\n",
      "Trained batch 162 batch loss 73.8168106 epoch total loss 80.0977707\n",
      "Trained batch 163 batch loss 73.5861206 epoch total loss 80.0578232\n",
      "Trained batch 164 batch loss 73.7613449 epoch total loss 80.0194321\n",
      "Trained batch 165 batch loss 73.6114883 epoch total loss 79.9805908\n",
      "Trained batch 166 batch loss 73.5253601 epoch total loss 79.9417\n",
      "Trained batch 167 batch loss 73.5534592 epoch total loss 79.9034576\n",
      "Trained batch 168 batch loss 73.3468628 epoch total loss 79.8644257\n",
      "Trained batch 169 batch loss 73.4185181 epoch total loss 79.8262863\n",
      "Trained batch 170 batch loss 72.9417725 epoch total loss 79.7857895\n",
      "Trained batch 171 batch loss 72.9415817 epoch total loss 79.7457657\n",
      "Trained batch 172 batch loss 72.2837219 epoch total loss 79.7023849\n",
      "Trained batch 173 batch loss 72.6730118 epoch total loss 79.6617508\n",
      "Trained batch 174 batch loss 72.5956192 epoch total loss 79.6211395\n",
      "Trained batch 175 batch loss 72.4686356 epoch total loss 79.5802689\n",
      "Trained batch 176 batch loss 72.958168 epoch total loss 79.5426407\n",
      "Trained batch 177 batch loss 72.8477478 epoch total loss 79.5048141\n",
      "Trained batch 178 batch loss 72.8016739 epoch total loss 79.4671555\n",
      "Trained batch 179 batch loss 72.7381897 epoch total loss 79.4295654\n",
      "Trained batch 180 batch loss 72.5825348 epoch total loss 79.3915329\n",
      "Trained batch 181 batch loss 72.1390686 epoch total loss 79.3514633\n",
      "Trained batch 182 batch loss 72.4264374 epoch total loss 79.3134155\n",
      "Trained batch 183 batch loss 72.3931961 epoch total loss 79.2756\n",
      "Trained batch 184 batch loss 72.2271347 epoch total loss 79.2373\n",
      "Trained batch 185 batch loss 72.2102585 epoch total loss 79.1993103\n",
      "Trained batch 186 batch loss 71.6932678 epoch total loss 79.1589508\n",
      "Trained batch 187 batch loss 71.9623642 epoch total loss 79.1204681\n",
      "Trained batch 188 batch loss 72.2804947 epoch total loss 79.0840836\n",
      "Trained batch 189 batch loss 71.6815643 epoch total loss 79.0449142\n",
      "Trained batch 190 batch loss 71.5753784 epoch total loss 79.0056\n",
      "Trained batch 191 batch loss 71.6000137 epoch total loss 78.9668274\n",
      "Trained batch 192 batch loss 71.5548248 epoch total loss 78.9282227\n",
      "Trained batch 193 batch loss 71.5624313 epoch total loss 78.8900604\n",
      "Trained batch 194 batch loss 71.3971176 epoch total loss 78.8514404\n",
      "Trained batch 195 batch loss 71.3978653 epoch total loss 78.8132095\n",
      "Trained batch 196 batch loss 71.3189 epoch total loss 78.7749786\n",
      "Trained batch 197 batch loss 71.3281403 epoch total loss 78.737175\n",
      "Trained batch 198 batch loss 70.8085327 epoch total loss 78.6971359\n",
      "Trained batch 199 batch loss 71.1183395 epoch total loss 78.65905\n",
      "Trained batch 200 batch loss 70.9795532 epoch total loss 78.6206512\n",
      "Trained batch 201 batch loss 70.8799896 epoch total loss 78.5821381\n",
      "Trained batch 202 batch loss 70.9353256 epoch total loss 78.544281\n",
      "Trained batch 203 batch loss 70.7858353 epoch total loss 78.5060654\n",
      "Trained batch 204 batch loss 71.0904694 epoch total loss 78.4697189\n",
      "Trained batch 205 batch loss 71.0402908 epoch total loss 78.4334717\n",
      "Trained batch 206 batch loss 70.9119492 epoch total loss 78.396965\n",
      "Trained batch 207 batch loss 70.7263412 epoch total loss 78.3599091\n",
      "Trained batch 208 batch loss 70.3539505 epoch total loss 78.3214188\n",
      "Trained batch 209 batch loss 70.2899933 epoch total loss 78.2829895\n",
      "Trained batch 210 batch loss 70.1017609 epoch total loss 78.2440262\n",
      "Trained batch 211 batch loss 70.0353546 epoch total loss 78.2051239\n",
      "Trained batch 212 batch loss 70.0850525 epoch total loss 78.1668243\n",
      "Trained batch 213 batch loss 69.7150879 epoch total loss 78.1271439\n",
      "Trained batch 214 batch loss 70.1450577 epoch total loss 78.0898438\n",
      "Trained batch 215 batch loss 70.1521225 epoch total loss 78.0529251\n",
      "Trained batch 216 batch loss 69.7626953 epoch total loss 78.0145416\n",
      "Trained batch 217 batch loss 69.9516602 epoch total loss 77.9773788\n",
      "Trained batch 218 batch loss 70.0070953 epoch total loss 77.9408264\n",
      "Trained batch 219 batch loss 70.393364 epoch total loss 77.9063568\n",
      "Trained batch 220 batch loss 70.3034592 epoch total loss 77.8717957\n",
      "Trained batch 221 batch loss 70.0209427 epoch total loss 77.8362732\n",
      "Trained batch 222 batch loss 70.0033 epoch total loss 77.8009949\n",
      "Trained batch 223 batch loss 69.6760788 epoch total loss 77.7645569\n",
      "Trained batch 224 batch loss 69.4287567 epoch total loss 77.7273483\n",
      "Trained batch 225 batch loss 69.2285614 epoch total loss 77.6895752\n",
      "Trained batch 226 batch loss 69.6212616 epoch total loss 77.6538696\n",
      "Trained batch 227 batch loss 69.5378952 epoch total loss 77.6181183\n",
      "Trained batch 228 batch loss 69.7547302 epoch total loss 77.5836258\n",
      "Trained batch 229 batch loss 69.1837234 epoch total loss 77.5469437\n",
      "Trained batch 230 batch loss 69.1977768 epoch total loss 77.510643\n",
      "Trained batch 231 batch loss 69.0094528 epoch total loss 77.4738388\n",
      "Trained batch 232 batch loss 68.9781876 epoch total loss 77.4372253\n",
      "Trained batch 233 batch loss 69.3179932 epoch total loss 77.4023743\n",
      "Trained batch 234 batch loss 69.1853638 epoch total loss 77.3672638\n",
      "Trained batch 235 batch loss 69.3203049 epoch total loss 77.3330231\n",
      "Trained batch 236 batch loss 68.5843887 epoch total loss 77.2959518\n",
      "Trained batch 237 batch loss 68.9479141 epoch total loss 77.2607193\n",
      "Trained batch 238 batch loss 68.8129425 epoch total loss 77.2252274\n",
      "Trained batch 239 batch loss 68.6804657 epoch total loss 77.1894684\n",
      "Trained batch 240 batch loss 68.2787552 epoch total loss 77.1523438\n",
      "Trained batch 241 batch loss 68.3109589 epoch total loss 77.115654\n",
      "Trained batch 242 batch loss 68.1702652 epoch total loss 77.0786896\n",
      "Trained batch 243 batch loss 67.8030396 epoch total loss 77.0405197\n",
      "Trained batch 244 batch loss 67.6130066 epoch total loss 77.0018845\n",
      "Trained batch 245 batch loss 67.9885254 epoch total loss 76.9650879\n",
      "Trained batch 246 batch loss 67.4749832 epoch total loss 76.9265137\n",
      "Trained batch 247 batch loss 67.5675888 epoch total loss 76.8886261\n",
      "Trained batch 248 batch loss 67.334053 epoch total loss 76.8501\n",
      "Trained batch 249 batch loss 67.0700378 epoch total loss 76.8108215\n",
      "Trained batch 250 batch loss 67.1733093 epoch total loss 76.7722702\n",
      "Trained batch 251 batch loss 67.1436768 epoch total loss 76.7339172\n",
      "Trained batch 252 batch loss 67.0771561 epoch total loss 76.6956\n",
      "Trained batch 253 batch loss 67.2883682 epoch total loss 76.6584167\n",
      "Trained batch 254 batch loss 67.2574539 epoch total loss 76.6214066\n",
      "Trained batch 255 batch loss 67.1769 epoch total loss 76.5843735\n",
      "Trained batch 256 batch loss 67.297493 epoch total loss 76.5480957\n",
      "Trained batch 257 batch loss 67.3852386 epoch total loss 76.5124435\n",
      "Trained batch 258 batch loss 67.3916245 epoch total loss 76.4770889\n",
      "Trained batch 259 batch loss 67.3707657 epoch total loss 76.4419327\n",
      "Trained batch 260 batch loss 67.0215607 epoch total loss 76.4057\n",
      "Trained batch 261 batch loss 66.8304367 epoch total loss 76.3690109\n",
      "Trained batch 262 batch loss 66.7644196 epoch total loss 76.3323517\n",
      "Trained batch 263 batch loss 66.4905472 epoch total loss 76.2949295\n",
      "Trained batch 264 batch loss 66.6826859 epoch total loss 76.258522\n",
      "Trained batch 265 batch loss 67.0238342 epoch total loss 76.223671\n",
      "Trained batch 266 batch loss 66.5497055 epoch total loss 76.1873\n",
      "Trained batch 267 batch loss 66.6583786 epoch total loss 76.1516113\n",
      "Trained batch 268 batch loss 66.2590714 epoch total loss 76.1147\n",
      "Trained batch 269 batch loss 65.9453201 epoch total loss 76.0769\n",
      "Trained batch 270 batch loss 66.0062485 epoch total loss 76.0396\n",
      "Trained batch 271 batch loss 66.4153671 epoch total loss 76.0040894\n",
      "Trained batch 272 batch loss 66.4486084 epoch total loss 75.968956\n",
      "Trained batch 273 batch loss 66.4409332 epoch total loss 75.9340591\n",
      "Trained batch 274 batch loss 66.4402 epoch total loss 75.8994064\n",
      "Trained batch 275 batch loss 66.5254 epoch total loss 75.8653183\n",
      "Trained batch 276 batch loss 66.4681473 epoch total loss 75.8312759\n",
      "Trained batch 277 batch loss 66.2250519 epoch total loss 75.7965927\n",
      "Trained batch 278 batch loss 65.4138336 epoch total loss 75.7592468\n",
      "Trained batch 279 batch loss 64.9277573 epoch total loss 75.7204208\n",
      "Trained batch 280 batch loss 64.7057266 epoch total loss 75.6810837\n",
      "Trained batch 281 batch loss 64.6984177 epoch total loss 75.642\n",
      "Trained batch 282 batch loss 64.2602158 epoch total loss 75.6016388\n",
      "Trained batch 283 batch loss 64.2821808 epoch total loss 75.5616379\n",
      "Trained batch 284 batch loss 64.6528702 epoch total loss 75.5232239\n",
      "Trained batch 285 batch loss 65.1501465 epoch total loss 75.4868317\n",
      "Trained batch 286 batch loss 64.9997406 epoch total loss 75.4501648\n",
      "Trained batch 287 batch loss 65.1569061 epoch total loss 75.4142914\n",
      "Trained batch 288 batch loss 65.4101791 epoch total loss 75.3795547\n",
      "Trained batch 289 batch loss 65.2566605 epoch total loss 75.3445282\n",
      "Trained batch 290 batch loss 65.070694 epoch total loss 75.3091\n",
      "Trained batch 291 batch loss 64.919281 epoch total loss 75.2734\n",
      "Trained batch 292 batch loss 64.913147 epoch total loss 75.2379227\n",
      "Trained batch 293 batch loss 64.9403839 epoch total loss 75.202774\n",
      "Trained batch 294 batch loss 64.4193954 epoch total loss 75.1660919\n",
      "Trained batch 295 batch loss 64.7907791 epoch total loss 75.130928\n",
      "Trained batch 296 batch loss 64.5139542 epoch total loss 75.0950546\n",
      "Trained batch 297 batch loss 64.6102371 epoch total loss 75.0597534\n",
      "Trained batch 298 batch loss 64.6604538 epoch total loss 75.0248566\n",
      "Trained batch 299 batch loss 64.9734879 epoch total loss 74.9912338\n",
      "Trained batch 300 batch loss 64.3874435 epoch total loss 74.9558868\n",
      "Trained batch 301 batch loss 64.2493515 epoch total loss 74.9203186\n",
      "Trained batch 302 batch loss 64.0904083 epoch total loss 74.8844528\n",
      "Trained batch 303 batch loss 63.7449417 epoch total loss 74.8476868\n",
      "Trained batch 304 batch loss 63.9863358 epoch total loss 74.8119583\n",
      "Trained batch 305 batch loss 64.371048 epoch total loss 74.7777252\n",
      "Trained batch 306 batch loss 64.465271 epoch total loss 74.7440262\n",
      "Trained batch 307 batch loss 64.1123047 epoch total loss 74.7094\n",
      "Trained batch 308 batch loss 64.3478928 epoch total loss 74.6757584\n",
      "Trained batch 309 batch loss 64.2514 epoch total loss 74.6420212\n",
      "Trained batch 310 batch loss 63.7988319 epoch total loss 74.6070404\n",
      "Trained batch 311 batch loss 63.7893257 epoch total loss 74.572258\n",
      "Trained batch 312 batch loss 63.1633301 epoch total loss 74.5357\n",
      "Trained batch 313 batch loss 63.015007 epoch total loss 74.4988861\n",
      "Trained batch 314 batch loss 62.8724442 epoch total loss 74.4618607\n",
      "Trained batch 315 batch loss 63.1582756 epoch total loss 74.4259796\n",
      "Trained batch 316 batch loss 62.5902328 epoch total loss 74.3885269\n",
      "Trained batch 317 batch loss 61.9921761 epoch total loss 74.3494186\n",
      "Trained batch 318 batch loss 62.1534348 epoch total loss 74.3110657\n",
      "Trained batch 319 batch loss 62.4235382 epoch total loss 74.2738\n",
      "Trained batch 320 batch loss 62.7939758 epoch total loss 74.2379303\n",
      "Trained batch 321 batch loss 62.8429604 epoch total loss 74.2024384\n",
      "Trained batch 322 batch loss 63.2193604 epoch total loss 74.1683273\n",
      "Trained batch 323 batch loss 63.2778664 epoch total loss 74.1346054\n",
      "Trained batch 324 batch loss 63.2933235 epoch total loss 74.1011429\n",
      "Trained batch 325 batch loss 63.129055 epoch total loss 74.0673828\n",
      "Trained batch 326 batch loss 62.8444099 epoch total loss 74.032959\n",
      "Trained batch 327 batch loss 62.959137 epoch total loss 73.9990921\n",
      "Trained batch 328 batch loss 62.6535225 epoch total loss 73.9645081\n",
      "Trained batch 329 batch loss 62.8183975 epoch total loss 73.9306259\n",
      "Trained batch 330 batch loss 62.7026443 epoch total loss 73.8966064\n",
      "Trained batch 331 batch loss 62.4214554 epoch total loss 73.8619385\n",
      "Trained batch 332 batch loss 62.449234 epoch total loss 73.8275604\n",
      "Trained batch 333 batch loss 62.6351891 epoch total loss 73.7939453\n",
      "Trained batch 334 batch loss 62.5089302 epoch total loss 73.7601624\n",
      "Trained batch 335 batch loss 62.3467407 epoch total loss 73.7261\n",
      "Trained batch 336 batch loss 62.3334579 epoch total loss 73.6921921\n",
      "Trained batch 337 batch loss 61.4962311 epoch total loss 73.656\n",
      "Trained batch 338 batch loss 61.3873329 epoch total loss 73.6197\n",
      "Trained batch 339 batch loss 61.3370514 epoch total loss 73.5834732\n",
      "Trained batch 340 batch loss 61.5664101 epoch total loss 73.5481262\n",
      "Trained batch 341 batch loss 61.6274643 epoch total loss 73.5131683\n",
      "Trained batch 342 batch loss 61.1541061 epoch total loss 73.4770279\n",
      "Trained batch 343 batch loss 61.4308128 epoch total loss 73.4419098\n",
      "Trained batch 344 batch loss 61.0667572 epoch total loss 73.4059372\n",
      "Trained batch 345 batch loss 61.3077126 epoch total loss 73.3708725\n",
      "Trained batch 346 batch loss 61.3325729 epoch total loss 73.3360748\n",
      "Trained batch 347 batch loss 61.1446762 epoch total loss 73.3009415\n",
      "Trained batch 348 batch loss 61.8479843 epoch total loss 73.2680359\n",
      "Trained batch 349 batch loss 61.8918571 epoch total loss 73.2354355\n",
      "Trained batch 350 batch loss 61.8066063 epoch total loss 73.2027817\n",
      "Trained batch 351 batch loss 61.1131477 epoch total loss 73.1683426\n",
      "Trained batch 352 batch loss 61.0634575 epoch total loss 73.1339493\n",
      "Trained batch 353 batch loss 61.3530846 epoch total loss 73.1005783\n",
      "Trained batch 354 batch loss 61.2739563 epoch total loss 73.0671692\n",
      "Trained batch 355 batch loss 61.2518616 epoch total loss 73.0338821\n",
      "Trained batch 356 batch loss 60.9392853 epoch total loss 72.9999084\n",
      "Trained batch 357 batch loss 61.0564804 epoch total loss 72.9664612\n",
      "Trained batch 358 batch loss 60.7820091 epoch total loss 72.9324188\n",
      "Trained batch 359 batch loss 60.6941071 epoch total loss 72.8983307\n",
      "Trained batch 360 batch loss 60.9568596 epoch total loss 72.8651581\n",
      "Trained batch 361 batch loss 60.9163322 epoch total loss 72.8320618\n",
      "Trained batch 362 batch loss 61.0196304 epoch total loss 72.7994232\n",
      "Trained batch 363 batch loss 60.7810669 epoch total loss 72.7663193\n",
      "Trained batch 364 batch loss 60.6108627 epoch total loss 72.7329254\n",
      "Trained batch 365 batch loss 60.3539047 epoch total loss 72.6990128\n",
      "Trained batch 366 batch loss 60.5108795 epoch total loss 72.6657104\n",
      "Trained batch 367 batch loss 60.4152565 epoch total loss 72.6323318\n",
      "Trained batch 368 batch loss 60.4797783 epoch total loss 72.5993118\n",
      "Trained batch 369 batch loss 60.4809799 epoch total loss 72.5664673\n",
      "Trained batch 370 batch loss 59.7083359 epoch total loss 72.531723\n",
      "Trained batch 371 batch loss 59.9640465 epoch total loss 72.4978485\n",
      "Trained batch 372 batch loss 59.8094063 epoch total loss 72.4637375\n",
      "Trained batch 373 batch loss 59.9415855 epoch total loss 72.4301605\n",
      "Trained batch 374 batch loss 60.2669106 epoch total loss 72.397644\n",
      "Trained batch 375 batch loss 60.2040634 epoch total loss 72.3651276\n",
      "Trained batch 376 batch loss 59.443222 epoch total loss 72.3307571\n",
      "Trained batch 377 batch loss 59.6763954 epoch total loss 72.2971878\n",
      "Trained batch 378 batch loss 59.4456062 epoch total loss 72.2631912\n",
      "Trained batch 379 batch loss 59.5665207 epoch total loss 72.2296906\n",
      "Trained batch 380 batch loss 59.8739777 epoch total loss 72.1971741\n",
      "Trained batch 381 batch loss 59.6165848 epoch total loss 72.1641541\n",
      "Trained batch 382 batch loss 59.6353035 epoch total loss 72.1313553\n",
      "Trained batch 383 batch loss 59.5197525 epoch total loss 72.0984268\n",
      "Trained batch 384 batch loss 59.5636673 epoch total loss 72.0657883\n",
      "Trained batch 385 batch loss 59.7069359 epoch total loss 72.0336838\n",
      "Trained batch 386 batch loss 59.4088745 epoch total loss 72.0009766\n",
      "Trained batch 387 batch loss 59.2205658 epoch total loss 71.9679489\n",
      "Trained batch 388 batch loss 59.0649719 epoch total loss 71.9346924\n",
      "Trained batch 389 batch loss 59.3338 epoch total loss 71.9023056\n",
      "Trained batch 390 batch loss 58.9723473 epoch total loss 71.8691483\n",
      "Trained batch 391 batch loss 59.1183319 epoch total loss 71.8365402\n",
      "Trained batch 392 batch loss 58.9691544 epoch total loss 71.8037186\n",
      "Trained batch 393 batch loss 58.7784653 epoch total loss 71.7705765\n",
      "Trained batch 394 batch loss 58.5324097 epoch total loss 71.7369766\n",
      "Trained batch 395 batch loss 58.3748 epoch total loss 71.7031479\n",
      "Trained batch 396 batch loss 57.6837502 epoch total loss 71.6677475\n",
      "Trained batch 397 batch loss 58.3805237 epoch total loss 71.6342773\n",
      "Trained batch 398 batch loss 58.699646 epoch total loss 71.6017761\n",
      "Trained batch 399 batch loss 58.8815193 epoch total loss 71.5698929\n",
      "Trained batch 400 batch loss 58.804306 epoch total loss 71.5379868\n",
      "Trained batch 401 batch loss 58.6402817 epoch total loss 71.5058212\n",
      "Trained batch 402 batch loss 58.5541344 epoch total loss 71.4736\n",
      "Trained batch 403 batch loss 58.7165375 epoch total loss 71.4419479\n",
      "Trained batch 404 batch loss 58.4723396 epoch total loss 71.4098434\n",
      "Trained batch 405 batch loss 58.2740784 epoch total loss 71.3774109\n",
      "Trained batch 406 batch loss 58.0794525 epoch total loss 71.3446579\n",
      "Trained batch 407 batch loss 58.2146378 epoch total loss 71.3124\n",
      "Trained batch 408 batch loss 57.954567 epoch total loss 71.2796631\n",
      "Trained batch 409 batch loss 57.9685936 epoch total loss 71.2471161\n",
      "Trained batch 410 batch loss 58.2055664 epoch total loss 71.2153091\n",
      "Trained batch 411 batch loss 57.7097 epoch total loss 71.1824417\n",
      "Trained batch 412 batch loss 57.7097206 epoch total loss 71.1497421\n",
      "Trained batch 413 batch loss 57.5797462 epoch total loss 71.1168823\n",
      "Trained batch 414 batch loss 57.7113457 epoch total loss 71.0845\n",
      "Trained batch 415 batch loss 57.5549927 epoch total loss 71.0519\n",
      "Trained batch 416 batch loss 57.5381126 epoch total loss 71.0194168\n",
      "Trained batch 417 batch loss 57.4754868 epoch total loss 70.9869385\n",
      "Trained batch 418 batch loss 57.4332771 epoch total loss 70.9545135\n",
      "Trained batch 419 batch loss 57.6199837 epoch total loss 70.9226837\n",
      "Trained batch 420 batch loss 57.3401 epoch total loss 70.8903427\n",
      "Trained batch 421 batch loss 57.400032 epoch total loss 70.8583069\n",
      "Trained batch 422 batch loss 57.3251572 epoch total loss 70.8262329\n",
      "Trained batch 423 batch loss 57.40345 epoch total loss 70.7945\n",
      "Trained batch 424 batch loss 56.9337 epoch total loss 70.7618103\n",
      "Trained batch 425 batch loss 57.3625336 epoch total loss 70.7302856\n",
      "Trained batch 426 batch loss 56.8006783 epoch total loss 70.6975861\n",
      "Trained batch 427 batch loss 57.0188599 epoch total loss 70.6655502\n",
      "Trained batch 428 batch loss 57.0205307 epoch total loss 70.6336746\n",
      "Trained batch 429 batch loss 56.790741 epoch total loss 70.6014099\n",
      "Trained batch 430 batch loss 56.6052856 epoch total loss 70.5688553\n",
      "Trained batch 431 batch loss 56.9904518 epoch total loss 70.5373535\n",
      "Trained batch 432 batch loss 56.7534904 epoch total loss 70.5054474\n",
      "Trained batch 433 batch loss 56.8892059 epoch total loss 70.474\n",
      "Trained batch 434 batch loss 56.124176 epoch total loss 70.4409409\n",
      "Trained batch 435 batch loss 56.1239929 epoch total loss 70.40802\n",
      "Trained batch 436 batch loss 55.9633026 epoch total loss 70.3748932\n",
      "Trained batch 437 batch loss 56.1094208 epoch total loss 70.342247\n",
      "Trained batch 438 batch loss 56.5168343 epoch total loss 70.3106842\n",
      "Trained batch 439 batch loss 56.2504539 epoch total loss 70.278656\n",
      "Trained batch 440 batch loss 56.3206635 epoch total loss 70.246933\n",
      "Trained batch 441 batch loss 56.1576653 epoch total loss 70.2149887\n",
      "Trained batch 442 batch loss 56.0111542 epoch total loss 70.1828537\n",
      "Trained batch 443 batch loss 56.2788048 epoch total loss 70.1514664\n",
      "Trained batch 444 batch loss 56.0986404 epoch total loss 70.1198196\n",
      "Trained batch 445 batch loss 55.9663162 epoch total loss 70.0880127\n",
      "Trained batch 446 batch loss 55.8732605 epoch total loss 70.0561447\n",
      "Trained batch 447 batch loss 55.8290863 epoch total loss 70.0243149\n",
      "Trained batch 448 batch loss 56.1849632 epoch total loss 69.9934235\n",
      "Trained batch 449 batch loss 55.8581085 epoch total loss 69.961937\n",
      "Trained batch 450 batch loss 55.7312393 epoch total loss 69.9303131\n",
      "Trained batch 451 batch loss 55.5449677 epoch total loss 69.8984146\n",
      "Trained batch 452 batch loss 55.5956955 epoch total loss 69.8667755\n",
      "Trained batch 453 batch loss 55.552433 epoch total loss 69.8351746\n",
      "Trained batch 454 batch loss 55.5797653 epoch total loss 69.803772\n",
      "Trained batch 455 batch loss 55.4557495 epoch total loss 69.7722397\n",
      "Trained batch 456 batch loss 55.439537 epoch total loss 69.7408066\n",
      "Trained batch 457 batch loss 55.3975143 epoch total loss 69.7094269\n",
      "Trained batch 458 batch loss 55.1926117 epoch total loss 69.6777267\n",
      "Trained batch 459 batch loss 55.0515289 epoch total loss 69.6458664\n",
      "Trained batch 460 batch loss 55.0314178 epoch total loss 69.61409\n",
      "Trained batch 461 batch loss 55.1999969 epoch total loss 69.5828247\n",
      "Trained batch 462 batch loss 54.8365593 epoch total loss 69.5509\n",
      "Trained batch 463 batch loss 55.0665474 epoch total loss 69.5196228\n",
      "Trained batch 464 batch loss 54.1174736 epoch total loss 69.4864273\n",
      "Trained batch 465 batch loss 54.5461121 epoch total loss 69.4543\n",
      "Trained batch 466 batch loss 54.6257706 epoch total loss 69.4224777\n",
      "Trained batch 467 batch loss 54.3927155 epoch total loss 69.3902893\n",
      "Trained batch 468 batch loss 54.7886124 epoch total loss 69.3590927\n",
      "Trained batch 469 batch loss 54.7711792 epoch total loss 69.3279877\n",
      "Trained batch 470 batch loss 54.4467888 epoch total loss 69.2963257\n",
      "Trained batch 471 batch loss 54.6326714 epoch total loss 69.2651901\n",
      "Trained batch 472 batch loss 54.5185776 epoch total loss 69.2339554\n",
      "Trained batch 473 batch loss 54.1546707 epoch total loss 69.2020721\n",
      "Trained batch 474 batch loss 54.3107109 epoch total loss 69.1706543\n",
      "Trained batch 475 batch loss 54.1278915 epoch total loss 69.1389847\n",
      "Trained batch 476 batch loss 54.2395096 epoch total loss 69.1076813\n",
      "Trained batch 477 batch loss 54.3641777 epoch total loss 69.0767746\n",
      "Trained batch 478 batch loss 54.1123734 epoch total loss 69.0454712\n",
      "Trained batch 479 batch loss 53.9443359 epoch total loss 69.0139465\n",
      "Trained batch 480 batch loss 53.7221909 epoch total loss 68.9820862\n",
      "Trained batch 481 batch loss 53.6173096 epoch total loss 68.9501419\n",
      "Trained batch 482 batch loss 54.0594673 epoch total loss 68.9192505\n",
      "Trained batch 483 batch loss 53.8238373 epoch total loss 68.8879929\n",
      "Trained batch 484 batch loss 53.8843842 epoch total loss 68.8569946\n",
      "Trained batch 485 batch loss 53.4642258 epoch total loss 68.8252563\n",
      "Trained batch 486 batch loss 53.8643 epoch total loss 68.7944717\n",
      "Trained batch 487 batch loss 53.2258377 epoch total loss 68.7625046\n",
      "Trained batch 488 batch loss 53.7110367 epoch total loss 68.7316589\n",
      "Trained batch 489 batch loss 53.306076 epoch total loss 68.7001114\n",
      "Trained batch 490 batch loss 53.56147 epoch total loss 68.66922\n",
      "Trained batch 491 batch loss 52.8207359 epoch total loss 68.63694\n",
      "Trained batch 492 batch loss 53.3605194 epoch total loss 68.6058884\n",
      "Trained batch 493 batch loss 52.8832283 epoch total loss 68.574\n",
      "Trained batch 494 batch loss 53.36689 epoch total loss 68.5432129\n",
      "Trained batch 495 batch loss 53.1836739 epoch total loss 68.5121841\n",
      "Trained batch 496 batch loss 52.9725952 epoch total loss 68.4808578\n",
      "Trained batch 497 batch loss 53.0040855 epoch total loss 68.4497147\n",
      "Trained batch 498 batch loss 52.8368874 epoch total loss 68.4183578\n",
      "Trained batch 499 batch loss 53.20961 epoch total loss 68.387886\n",
      "Trained batch 500 batch loss 53.8611374 epoch total loss 68.3588257\n",
      "Trained batch 501 batch loss 53.5584412 epoch total loss 68.3292847\n",
      "Trained batch 502 batch loss 52.6216431 epoch total loss 68.298\n",
      "Trained batch 503 batch loss 52.7704773 epoch total loss 68.2671204\n",
      "Trained batch 504 batch loss 52.8514709 epoch total loss 68.2365341\n",
      "Trained batch 505 batch loss 52.3713493 epoch total loss 68.2051239\n",
      "Trained batch 506 batch loss 52.4364548 epoch total loss 68.1739578\n",
      "Trained batch 507 batch loss 52.256588 epoch total loss 68.1425629\n",
      "Trained batch 508 batch loss 52.5235672 epoch total loss 68.111824\n",
      "Trained batch 509 batch loss 52.6171494 epoch total loss 68.0813751\n",
      "Trained batch 510 batch loss 52.7359238 epoch total loss 68.0512848\n",
      "Trained batch 511 batch loss 52.7196045 epoch total loss 68.0212784\n",
      "Trained batch 512 batch loss 52.5437927 epoch total loss 67.9910507\n",
      "Trained batch 513 batch loss 52.2981758 epoch total loss 67.9604568\n",
      "Trained batch 514 batch loss 52.3923492 epoch total loss 67.9301682\n",
      "Trained batch 515 batch loss 52.2880974 epoch total loss 67.8997955\n",
      "Trained batch 516 batch loss 52.2195 epoch total loss 67.8694077\n",
      "Trained batch 517 batch loss 51.9477768 epoch total loss 67.8386154\n",
      "Trained batch 518 batch loss 51.919632 epoch total loss 67.8078766\n",
      "Trained batch 519 batch loss 51.9008408 epoch total loss 67.7772293\n",
      "Trained batch 520 batch loss 51.6374588 epoch total loss 67.7461929\n",
      "Trained batch 521 batch loss 52.3527908 epoch total loss 67.7166443\n",
      "Trained batch 522 batch loss 52.4680634 epoch total loss 67.6874313\n",
      "Trained batch 523 batch loss 51.8233871 epoch total loss 67.6571045\n",
      "Trained batch 524 batch loss 51.7121162 epoch total loss 67.6266708\n",
      "Trained batch 525 batch loss 52.033535 epoch total loss 67.5969696\n",
      "Trained batch 526 batch loss 52.0500946 epoch total loss 67.5674133\n",
      "Trained batch 527 batch loss 51.5024 epoch total loss 67.5369339\n",
      "Trained batch 528 batch loss 51.2982521 epoch total loss 67.5061798\n",
      "Trained batch 529 batch loss 51.5234146 epoch total loss 67.4759674\n",
      "Trained batch 530 batch loss 51.4337921 epoch total loss 67.445694\n",
      "Trained batch 531 batch loss 51.4089432 epoch total loss 67.4155\n",
      "Trained batch 532 batch loss 51.5523834 epoch total loss 67.3856735\n",
      "Trained batch 533 batch loss 51.3429222 epoch total loss 67.3555756\n",
      "Trained batch 534 batch loss 51.4876213 epoch total loss 67.3258667\n",
      "Trained batch 535 batch loss 51.3580971 epoch total loss 67.2960205\n",
      "Trained batch 536 batch loss 51.3822021 epoch total loss 67.2663345\n",
      "Trained batch 537 batch loss 51.3668442 epoch total loss 67.2367249\n",
      "Trained batch 538 batch loss 51.0557022 epoch total loss 67.2066498\n",
      "Trained batch 539 batch loss 50.5401497 epoch total loss 67.1757202\n",
      "Trained batch 540 batch loss 50.9445686 epoch total loss 67.145668\n",
      "Trained batch 541 batch loss 51.117775 epoch total loss 67.1160431\n",
      "Trained batch 542 batch loss 50.9722137 epoch total loss 67.0862579\n",
      "Trained batch 543 batch loss 50.899231 epoch total loss 67.0564423\n",
      "Trained batch 544 batch loss 50.6448097 epoch total loss 67.0262756\n",
      "Trained batch 545 batch loss 50.6100464 epoch total loss 66.9961548\n",
      "Trained batch 546 batch loss 50.9563522 epoch total loss 66.966774\n",
      "Trained batch 547 batch loss 50.7834663 epoch total loss 66.9371948\n",
      "Trained batch 548 batch loss 50.758522 epoch total loss 66.9076691\n",
      "Trained batch 549 batch loss 50.5534821 epoch total loss 66.8778839\n",
      "Trained batch 550 batch loss 50.2729874 epoch total loss 66.8476944\n",
      "Trained batch 551 batch loss 50.0472908 epoch total loss 66.8172\n",
      "Trained batch 552 batch loss 49.8009491 epoch total loss 66.786377\n",
      "Trained batch 553 batch loss 49.871 epoch total loss 66.7557831\n",
      "Trained batch 554 batch loss 49.6268616 epoch total loss 66.7248611\n",
      "Trained batch 555 batch loss 49.418396 epoch total loss 66.6936798\n",
      "Trained batch 556 batch loss 49.0180664 epoch total loss 66.6618881\n",
      "Trained batch 557 batch loss 48.8587227 epoch total loss 66.6299286\n",
      "Trained batch 558 batch loss 48.76791 epoch total loss 66.5979233\n",
      "Trained batch 559 batch loss 49.7704315 epoch total loss 66.5678177\n",
      "Trained batch 560 batch loss 50.5367279 epoch total loss 66.5391846\n",
      "Trained batch 561 batch loss 50.4360619 epoch total loss 66.5104828\n",
      "Trained batch 562 batch loss 49.9638443 epoch total loss 66.4810486\n",
      "Trained batch 563 batch loss 49.7977867 epoch total loss 66.4514084\n",
      "Trained batch 564 batch loss 49.8271942 epoch total loss 66.421936\n",
      "Trained batch 565 batch loss 49.8573418 epoch total loss 66.3926163\n",
      "Trained batch 566 batch loss 49.7590218 epoch total loss 66.3632278\n",
      "Trained batch 567 batch loss 49.5943451 epoch total loss 66.3336487\n",
      "Trained batch 568 batch loss 49.9267616 epoch total loss 66.3047638\n",
      "Trained batch 569 batch loss 49.6276703 epoch total loss 66.2754593\n",
      "Trained batch 570 batch loss 49.5109634 epoch total loss 66.246048\n",
      "Trained batch 571 batch loss 49.4083 epoch total loss 66.2165604\n",
      "Trained batch 572 batch loss 49.2752571 epoch total loss 66.1869431\n",
      "Trained batch 573 batch loss 49.5302124 epoch total loss 66.1578751\n",
      "Trained batch 574 batch loss 49.5749588 epoch total loss 66.1289825\n",
      "Trained batch 575 batch loss 49.4464722 epoch total loss 66.099968\n",
      "Trained batch 576 batch loss 49.1583939 epoch total loss 66.0705566\n",
      "Trained batch 577 batch loss 49.0316429 epoch total loss 66.0410233\n",
      "Trained batch 578 batch loss 49.4716034 epoch total loss 66.0123596\n",
      "Trained batch 579 batch loss 49.265522 epoch total loss 65.9834366\n",
      "Trained batch 580 batch loss 49.5177536 epoch total loss 65.9550476\n",
      "Trained batch 581 batch loss 49.3471451 epoch total loss 65.9264679\n",
      "Trained batch 582 batch loss 49.2736206 epoch total loss 65.89785\n",
      "Trained batch 583 batch loss 48.9192 epoch total loss 65.8687286\n",
      "Trained batch 584 batch loss 49.0563164 epoch total loss 65.8399353\n",
      "Trained batch 585 batch loss 49.0033722 epoch total loss 65.8111572\n",
      "Trained batch 586 batch loss 48.4227791 epoch total loss 65.7814865\n",
      "Trained batch 587 batch loss 48.3274803 epoch total loss 65.7517471\n",
      "Trained batch 588 batch loss 48.6060677 epoch total loss 65.7225876\n",
      "Trained batch 589 batch loss 48.7567787 epoch total loss 65.6937866\n",
      "Trained batch 590 batch loss 48.6972885 epoch total loss 65.6649857\n",
      "Trained batch 591 batch loss 48.541954 epoch total loss 65.6360092\n",
      "Trained batch 592 batch loss 48.2500763 epoch total loss 65.6066437\n",
      "Trained batch 593 batch loss 48.0666542 epoch total loss 65.5770645\n",
      "Trained batch 594 batch loss 48.2077484 epoch total loss 65.547821\n",
      "Trained batch 595 batch loss 48.2135468 epoch total loss 65.518692\n",
      "Trained batch 596 batch loss 48.6899643 epoch total loss 65.4904556\n",
      "Trained batch 597 batch loss 48.4291687 epoch total loss 65.4618759\n",
      "Trained batch 598 batch loss 48.0190468 epoch total loss 65.4327087\n",
      "Trained batch 599 batch loss 48.2604 epoch total loss 65.4040451\n",
      "Trained batch 600 batch loss 48.1698456 epoch total loss 65.3753204\n",
      "Trained batch 601 batch loss 48.0923157 epoch total loss 65.3465652\n",
      "Trained batch 602 batch loss 48.2060509 epoch total loss 65.3180923\n",
      "Trained batch 603 batch loss 48.2294197 epoch total loss 65.2897568\n",
      "Trained batch 604 batch loss 48.0681725 epoch total loss 65.2612381\n",
      "Trained batch 605 batch loss 48.0691414 epoch total loss 65.2328262\n",
      "Trained batch 606 batch loss 47.7197456 epoch total loss 65.2039261\n",
      "Trained batch 607 batch loss 47.9582443 epoch total loss 65.1755142\n",
      "Trained batch 608 batch loss 47.836235 epoch total loss 65.1469955\n",
      "Trained batch 609 batch loss 47.4380341 epoch total loss 65.1179123\n",
      "Trained batch 610 batch loss 47.1462288 epoch total loss 65.0884476\n",
      "Trained batch 611 batch loss 46.6041832 epoch total loss 65.0582\n",
      "Trained batch 612 batch loss 46.9363365 epoch total loss 65.0285873\n",
      "Trained batch 613 batch loss 47.8409615 epoch total loss 65.0005493\n",
      "Trained batch 614 batch loss 47.5849686 epoch total loss 64.9721832\n",
      "Trained batch 615 batch loss 47.5321121 epoch total loss 64.9438248\n",
      "Trained batch 616 batch loss 47.1726952 epoch total loss 64.914978\n",
      "Trained batch 617 batch loss 47.437458 epoch total loss 64.8866501\n",
      "Trained batch 618 batch loss 47.2867966 epoch total loss 64.8581696\n",
      "Trained batch 619 batch loss 47.3326111 epoch total loss 64.8298569\n",
      "Trained batch 620 batch loss 46.8382111 epoch total loss 64.8008347\n",
      "Trained batch 621 batch loss 46.7730789 epoch total loss 64.7718048\n",
      "Trained batch 622 batch loss 47.1193314 epoch total loss 64.7434311\n",
      "Trained batch 623 batch loss 46.7230721 epoch total loss 64.7145081\n",
      "Trained batch 624 batch loss 47.1444893 epoch total loss 64.686348\n",
      "Trained batch 625 batch loss 46.9771881 epoch total loss 64.6580124\n",
      "Trained batch 626 batch loss 46.8617859 epoch total loss 64.6295853\n",
      "Trained batch 627 batch loss 46.8141632 epoch total loss 64.6011734\n",
      "Trained batch 628 batch loss 46.8424835 epoch total loss 64.5728912\n",
      "Trained batch 629 batch loss 46.8717346 epoch total loss 64.544754\n",
      "Trained batch 630 batch loss 46.9221802 epoch total loss 64.516777\n",
      "Trained batch 631 batch loss 46.8229294 epoch total loss 64.488739\n",
      "Trained batch 632 batch loss 46.2833405 epoch total loss 64.459938\n",
      "Trained batch 633 batch loss 45.9859962 epoch total loss 64.430748\n",
      "Trained batch 634 batch loss 46.3718071 epoch total loss 64.4022675\n",
      "Trained batch 635 batch loss 45.9539032 epoch total loss 64.3732071\n",
      "Trained batch 636 batch loss 46.3017311 epoch total loss 64.3447952\n",
      "Trained batch 637 batch loss 46.399826 epoch total loss 64.3166199\n",
      "Trained batch 638 batch loss 46.6381 epoch total loss 64.2889099\n",
      "Trained batch 639 batch loss 46.8301239 epoch total loss 64.2615891\n",
      "Trained batch 640 batch loss 46.2406197 epoch total loss 64.2334366\n",
      "Trained batch 641 batch loss 46.6419144 epoch total loss 64.205986\n",
      "Trained batch 642 batch loss 46.4 epoch total loss 64.1782532\n",
      "Trained batch 643 batch loss 45.9931679 epoch total loss 64.149971\n",
      "Trained batch 644 batch loss 46.062767 epoch total loss 64.1218796\n",
      "Trained batch 645 batch loss 45.9576721 epoch total loss 64.0937195\n",
      "Trained batch 646 batch loss 45.9304619 epoch total loss 64.0656052\n",
      "Trained batch 647 batch loss 45.6812553 epoch total loss 64.0371857\n",
      "Trained batch 648 batch loss 45.7828598 epoch total loss 64.0090103\n",
      "Trained batch 649 batch loss 45.6706276 epoch total loss 63.9807587\n",
      "Trained batch 650 batch loss 45.355175 epoch total loss 63.9521027\n",
      "Trained batch 651 batch loss 45.7950783 epoch total loss 63.9242134\n",
      "Trained batch 652 batch loss 45.9978714 epoch total loss 63.8967171\n",
      "Trained batch 653 batch loss 45.5778389 epoch total loss 63.8686638\n",
      "Trained batch 654 batch loss 45.398941 epoch total loss 63.8404236\n",
      "Trained batch 655 batch loss 45.5492706 epoch total loss 63.8125\n",
      "Trained batch 656 batch loss 45.5538902 epoch total loss 63.784668\n",
      "Trained batch 657 batch loss 45.2851486 epoch total loss 63.7565117\n",
      "Trained batch 658 batch loss 45.0856628 epoch total loss 63.7281342\n",
      "Trained batch 659 batch loss 45.4436836 epoch total loss 63.7003937\n",
      "Trained batch 660 batch loss 45.5762634 epoch total loss 63.6729355\n",
      "Trained batch 661 batch loss 45.1712379 epoch total loss 63.6449432\n",
      "Trained batch 662 batch loss 45.2823563 epoch total loss 63.6172066\n",
      "Trained batch 663 batch loss 45.2448044 epoch total loss 63.5894966\n",
      "Trained batch 664 batch loss 45.3556633 epoch total loss 63.5620346\n",
      "Trained batch 665 batch loss 45.2800636 epoch total loss 63.5345459\n",
      "Trained batch 666 batch loss 45.6627274 epoch total loss 63.5077133\n",
      "Trained batch 667 batch loss 45.3265915 epoch total loss 63.4804573\n",
      "Trained batch 668 batch loss 45.2075653 epoch total loss 63.4531021\n",
      "Trained batch 669 batch loss 44.9137688 epoch total loss 63.4253883\n",
      "Trained batch 670 batch loss 44.899498 epoch total loss 63.3977394\n",
      "Trained batch 671 batch loss 44.6234856 epoch total loss 63.3697624\n",
      "Trained batch 672 batch loss 44.4407 epoch total loss 63.3415947\n",
      "Trained batch 673 batch loss 44.9051208 epoch total loss 63.3142\n",
      "Trained batch 674 batch loss 44.4792671 epoch total loss 63.2862587\n",
      "Trained batch 675 batch loss 44.4783134 epoch total loss 63.2583923\n",
      "Trained batch 676 batch loss 44.7952271 epoch total loss 63.2310829\n",
      "Trained batch 677 batch loss 43.9988594 epoch total loss 63.2026749\n",
      "Trained batch 678 batch loss 44.1057816 epoch total loss 63.1745071\n",
      "Trained batch 679 batch loss 43.9124184 epoch total loss 63.1461411\n",
      "Trained batch 680 batch loss 43.8947296 epoch total loss 63.1178322\n",
      "Trained batch 681 batch loss 44.2618752 epoch total loss 63.0901413\n",
      "Trained batch 682 batch loss 44.3466377 epoch total loss 63.0626602\n",
      "Trained batch 683 batch loss 44.544487 epoch total loss 63.0355453\n",
      "Trained batch 684 batch loss 44.6518402 epoch total loss 63.0086708\n",
      "Trained batch 685 batch loss 44.1805573 epoch total loss 62.9811821\n",
      "Trained batch 686 batch loss 43.8086319 epoch total loss 62.9532318\n",
      "Trained batch 687 batch loss 43.9005203 epoch total loss 62.9255028\n",
      "Trained batch 688 batch loss 44.4047394 epoch total loss 62.8985863\n",
      "Trained batch 689 batch loss 44.163826 epoch total loss 62.8713951\n",
      "Trained batch 690 batch loss 43.863369 epoch total loss 62.8438454\n",
      "Trained batch 691 batch loss 43.9524689 epoch total loss 62.8165092\n",
      "Trained batch 692 batch loss 43.9547195 epoch total loss 62.7892494\n",
      "Trained batch 693 batch loss 44.0714111 epoch total loss 62.7622375\n",
      "Trained batch 694 batch loss 43.9182396 epoch total loss 62.7350845\n",
      "Trained batch 695 batch loss 43.8193321 epoch total loss 62.7078705\n",
      "Trained batch 696 batch loss 44.0574722 epoch total loss 62.6810722\n",
      "Trained batch 697 batch loss 44.0670319 epoch total loss 62.6543655\n",
      "Trained batch 698 batch loss 44.2934265 epoch total loss 62.6280594\n",
      "Trained batch 699 batch loss 44.0661697 epoch total loss 62.6015053\n",
      "Trained batch 700 batch loss 43.9529152 epoch total loss 62.5748672\n",
      "Trained batch 701 batch loss 43.837677 epoch total loss 62.5481339\n",
      "Trained batch 702 batch loss 43.5344391 epoch total loss 62.5210495\n",
      "Trained batch 703 batch loss 43.9187202 epoch total loss 62.4945869\n",
      "Trained batch 704 batch loss 43.8085327 epoch total loss 62.4680443\n",
      "Trained batch 705 batch loss 43.8276176 epoch total loss 62.4416046\n",
      "Trained batch 706 batch loss 43.6429176 epoch total loss 62.4149818\n",
      "Trained batch 707 batch loss 43.4604492 epoch total loss 62.3881721\n",
      "Trained batch 708 batch loss 43.8123283 epoch total loss 62.3619347\n",
      "Trained batch 709 batch loss 43.564415 epoch total loss 62.3354187\n",
      "Trained batch 710 batch loss 43.2044373 epoch total loss 62.3084717\n",
      "Trained batch 711 batch loss 43.4569931 epoch total loss 62.2819595\n",
      "Trained batch 712 batch loss 43.2041969 epoch total loss 62.2551613\n",
      "Trained batch 713 batch loss 43.2071 epoch total loss 62.228447\n",
      "Trained batch 714 batch loss 42.6905861 epoch total loss 62.2010841\n",
      "Trained batch 715 batch loss 42.4983826 epoch total loss 62.1735306\n",
      "Trained batch 716 batch loss 42.1888885 epoch total loss 62.1456184\n",
      "Trained batch 717 batch loss 42.3991776 epoch total loss 62.1180763\n",
      "Trained batch 718 batch loss 43.1689758 epoch total loss 62.0916824\n",
      "Trained batch 719 batch loss 43.1470261 epoch total loss 62.0653343\n",
      "Trained batch 720 batch loss 43.0580406 epoch total loss 62.0389366\n",
      "Trained batch 721 batch loss 42.8936768 epoch total loss 62.0123863\n",
      "Trained batch 722 batch loss 42.4870758 epoch total loss 61.9853439\n",
      "Trained batch 723 batch loss 42.4095764 epoch total loss 61.9582672\n",
      "Trained batch 724 batch loss 42.5692558 epoch total loss 61.931488\n",
      "Trained batch 725 batch loss 42.3322487 epoch total loss 61.9044571\n",
      "Trained batch 726 batch loss 42.492157 epoch total loss 61.8777161\n",
      "Trained batch 727 batch loss 42.0763 epoch total loss 61.8504829\n",
      "Trained batch 728 batch loss 42.0859413 epoch total loss 61.8233337\n",
      "Trained batch 729 batch loss 42.1538239 epoch total loss 61.7963486\n",
      "Trained batch 730 batch loss 41.8275604 epoch total loss 61.7689972\n",
      "Trained batch 731 batch loss 42.360733 epoch total loss 61.7424431\n",
      "Trained batch 732 batch loss 42.1727638 epoch total loss 61.7157097\n",
      "Trained batch 733 batch loss 42.1947556 epoch total loss 61.6890793\n",
      "Trained batch 734 batch loss 42.2979469 epoch total loss 61.6626587\n",
      "Trained batch 735 batch loss 42.6083832 epoch total loss 61.636734\n",
      "Trained batch 736 batch loss 42.5428963 epoch total loss 61.6107903\n",
      "Trained batch 737 batch loss 42.4015388 epoch total loss 61.5847282\n",
      "Trained batch 738 batch loss 42.035408 epoch total loss 61.558239\n",
      "Trained batch 739 batch loss 41.7843704 epoch total loss 61.5314827\n",
      "Trained batch 740 batch loss 42.1998863 epoch total loss 61.5053596\n",
      "Trained batch 741 batch loss 42.0096092 epoch total loss 61.4790459\n",
      "Trained batch 742 batch loss 41.4713936 epoch total loss 61.4520836\n",
      "Trained batch 743 batch loss 41.5013275 epoch total loss 61.4252281\n",
      "Trained batch 744 batch loss 41.8008041 epoch total loss 61.3988533\n",
      "Trained batch 745 batch loss 41.8042374 epoch total loss 61.372551\n",
      "Trained batch 746 batch loss 41.8942642 epoch total loss 61.3464432\n",
      "Trained batch 747 batch loss 41.8131 epoch total loss 61.3202934\n",
      "Trained batch 748 batch loss 41.874115 epoch total loss 61.2942963\n",
      "Trained batch 749 batch loss 41.7425613 epoch total loss 61.2681923\n",
      "Trained batch 750 batch loss 41.4578743 epoch total loss 61.2417755\n",
      "Trained batch 751 batch loss 41.5929604 epoch total loss 61.2156143\n",
      "Trained batch 752 batch loss 41.7722321 epoch total loss 61.1897583\n",
      "Trained batch 753 batch loss 41.5877151 epoch total loss 61.1637268\n",
      "Trained batch 754 batch loss 41.7288551 epoch total loss 61.1379509\n",
      "Trained batch 755 batch loss 41.79702 epoch total loss 61.1123352\n",
      "Trained batch 756 batch loss 41.4430275 epoch total loss 61.0863152\n",
      "Trained batch 757 batch loss 40.9649849 epoch total loss 61.0597343\n",
      "Trained batch 758 batch loss 41.4699554 epoch total loss 61.0338898\n",
      "Trained batch 759 batch loss 41.1079826 epoch total loss 61.007637\n",
      "Trained batch 760 batch loss 40.8761826 epoch total loss 60.9811478\n",
      "Trained batch 761 batch loss 41.9010925 epoch total loss 60.9560776\n",
      "Trained batch 762 batch loss 41.8392029 epoch total loss 60.9309883\n",
      "Trained batch 763 batch loss 41.6667976 epoch total loss 60.9057426\n",
      "Trained batch 764 batch loss 41.5723 epoch total loss 60.8804398\n",
      "Trained batch 765 batch loss 41.8066597 epoch total loss 60.8555107\n",
      "Trained batch 766 batch loss 41.7409821 epoch total loss 60.8305588\n",
      "Trained batch 767 batch loss 40.8553772 epoch total loss 60.8045158\n",
      "Trained batch 768 batch loss 41.0905876 epoch total loss 60.7788429\n",
      "Trained batch 769 batch loss 41.2360458 epoch total loss 60.7534294\n",
      "Trained batch 770 batch loss 40.8703156 epoch total loss 60.7276077\n",
      "Trained batch 771 batch loss 40.7449036 epoch total loss 60.7016907\n",
      "Trained batch 772 batch loss 40.834034 epoch total loss 60.6759567\n",
      "Trained batch 773 batch loss 40.8683701 epoch total loss 60.6503334\n",
      "Trained batch 774 batch loss 40.6213493 epoch total loss 60.6244545\n",
      "Trained batch 775 batch loss 40.5841827 epoch total loss 60.5986\n",
      "Trained batch 776 batch loss 40.886982 epoch total loss 60.5731964\n",
      "Trained batch 777 batch loss 41.0326614 epoch total loss 60.5480461\n",
      "Trained batch 778 batch loss 40.9674377 epoch total loss 60.5228806\n",
      "Trained batch 779 batch loss 40.8653374 epoch total loss 60.4976463\n",
      "Trained batch 780 batch loss 40.7578125 epoch total loss 60.4723396\n",
      "Trained batch 781 batch loss 40.7904358 epoch total loss 60.4471397\n",
      "Trained batch 782 batch loss 40.4720879 epoch total loss 60.4215965\n",
      "Trained batch 783 batch loss 40.2816772 epoch total loss 60.395874\n",
      "Trained batch 784 batch loss 40.1420708 epoch total loss 60.3700371\n",
      "Trained batch 785 batch loss 40.1958542 epoch total loss 60.3443375\n",
      "Trained batch 786 batch loss 40.1219101 epoch total loss 60.3186073\n",
      "Trained batch 787 batch loss 40.7554855 epoch total loss 60.2937469\n",
      "Trained batch 788 batch loss 40.9540443 epoch total loss 60.2692032\n",
      "Trained batch 789 batch loss 40.5684776 epoch total loss 60.244236\n",
      "Trained batch 790 batch loss 40.4056091 epoch total loss 60.2191277\n",
      "Trained batch 791 batch loss 39.9643364 epoch total loss 60.1935196\n",
      "Trained batch 792 batch loss 40.1661301 epoch total loss 60.1682358\n",
      "Trained batch 793 batch loss 40.2202301 epoch total loss 60.1430779\n",
      "Trained batch 794 batch loss 40.1723366 epoch total loss 60.1179237\n",
      "Trained batch 795 batch loss 40.0102348 epoch total loss 60.0926361\n",
      "Trained batch 796 batch loss 39.9554214 epoch total loss 60.067337\n",
      "Trained batch 797 batch loss 40.1901321 epoch total loss 60.0424\n",
      "Trained batch 798 batch loss 40.3547249 epoch total loss 60.0177307\n",
      "Trained batch 799 batch loss 40.1414566 epoch total loss 59.9928513\n",
      "Trained batch 800 batch loss 39.5129166 epoch total loss 59.9672508\n",
      "Trained batch 801 batch loss 39.5565338 epoch total loss 59.9417686\n",
      "Trained batch 802 batch loss 39.4649353 epoch total loss 59.9162331\n",
      "Trained batch 803 batch loss 38.7129173 epoch total loss 59.8898315\n",
      "Trained batch 804 batch loss 38.290947 epoch total loss 59.8629646\n",
      "Trained batch 805 batch loss 38.5844803 epoch total loss 59.8365326\n",
      "Trained batch 806 batch loss 39.4347038 epoch total loss 59.8112221\n",
      "Trained batch 807 batch loss 40.0941162 epoch total loss 59.7867889\n",
      "Trained batch 808 batch loss 39.9741211 epoch total loss 59.7622643\n",
      "Trained batch 809 batch loss 39.3155365 epoch total loss 59.7369919\n",
      "Trained batch 810 batch loss 38.8575211 epoch total loss 59.711216\n",
      "Trained batch 811 batch loss 39.4857254 epoch total loss 59.6862755\n",
      "Trained batch 812 batch loss 39.5606422 epoch total loss 59.6614952\n",
      "Trained batch 813 batch loss 39.532135 epoch total loss 59.636734\n",
      "Trained batch 814 batch loss 39.4691582 epoch total loss 59.6119576\n",
      "Trained batch 815 batch loss 39.4453926 epoch total loss 59.5872116\n",
      "Trained batch 816 batch loss 39.5277939 epoch total loss 59.5626297\n",
      "Trained batch 817 batch loss 39.4231033 epoch total loss 59.5379753\n",
      "Trained batch 818 batch loss 39.1379 epoch total loss 59.5130386\n",
      "Trained batch 819 batch loss 39.1225777 epoch total loss 59.4881363\n",
      "Trained batch 820 batch loss 39.1793022 epoch total loss 59.4633713\n",
      "Trained batch 821 batch loss 39.2124329 epoch total loss 59.4387054\n",
      "Trained batch 822 batch loss 39.231308 epoch total loss 59.4141197\n",
      "Trained batch 823 batch loss 38.526825 epoch total loss 59.3887405\n",
      "Trained batch 824 batch loss 38.7596092 epoch total loss 59.3637047\n",
      "Trained batch 825 batch loss 38.9679794 epoch total loss 59.3389816\n",
      "Trained batch 826 batch loss 38.256382 epoch total loss 59.3134613\n",
      "Trained batch 827 batch loss 38.3970947 epoch total loss 59.2881699\n",
      "Trained batch 828 batch loss 38.0489731 epoch total loss 59.2625198\n",
      "Trained batch 829 batch loss 38.0876465 epoch total loss 59.2369766\n",
      "Trained batch 830 batch loss 38.7858086 epoch total loss 59.2123337\n",
      "Trained batch 831 batch loss 39.6598969 epoch total loss 59.1888084\n",
      "Trained batch 832 batch loss 39.0366135 epoch total loss 59.1645851\n",
      "Trained batch 833 batch loss 38.958519 epoch total loss 59.1403236\n",
      "Trained batch 834 batch loss 38.9766922 epoch total loss 59.1161461\n",
      "Trained batch 835 batch loss 39.0129776 epoch total loss 59.0920715\n",
      "Trained batch 836 batch loss 38.6532364 epoch total loss 59.0676193\n",
      "Trained batch 837 batch loss 38.7046242 epoch total loss 59.0432892\n",
      "Trained batch 838 batch loss 38.4940186 epoch total loss 59.0187683\n",
      "Trained batch 839 batch loss 38.6147804 epoch total loss 58.9944458\n",
      "Trained batch 840 batch loss 38.34 epoch total loss 58.9698563\n",
      "Trained batch 841 batch loss 38.5028267 epoch total loss 58.9455223\n",
      "Trained batch 842 batch loss 38.5475311 epoch total loss 58.9212952\n",
      "Trained batch 843 batch loss 38.4843 epoch total loss 58.8970528\n",
      "Trained batch 844 batch loss 38.2866402 epoch total loss 58.8726311\n",
      "Trained batch 845 batch loss 38.3638306 epoch total loss 58.8483582\n",
      "Trained batch 846 batch loss 37.642231 epoch total loss 58.8232918\n",
      "Trained batch 847 batch loss 37.2193184 epoch total loss 58.7977829\n",
      "Trained batch 848 batch loss 37.5363617 epoch total loss 58.7727089\n",
      "Trained batch 849 batch loss 38.080719 epoch total loss 58.7483406\n",
      "Trained batch 850 batch loss 38.0846748 epoch total loss 58.7240295\n",
      "Trained batch 851 batch loss 38.2984695 epoch total loss 58.7000275\n",
      "Trained batch 852 batch loss 38.2226295 epoch total loss 58.6759911\n",
      "Trained batch 853 batch loss 38.1377182 epoch total loss 58.6519127\n",
      "Trained batch 854 batch loss 38.1375732 epoch total loss 58.6278915\n",
      "Trained batch 855 batch loss 38.0953445 epoch total loss 58.6038742\n",
      "Trained batch 856 batch loss 38.2243385 epoch total loss 58.5800629\n",
      "Trained batch 857 batch loss 38.3099785 epoch total loss 58.5564117\n",
      "Trained batch 858 batch loss 37.7605 epoch total loss 58.5321732\n",
      "Trained batch 859 batch loss 37.6868095 epoch total loss 58.5079079\n",
      "Trained batch 860 batch loss 37.336113 epoch total loss 58.4832878\n",
      "Trained batch 861 batch loss 37.2130051 epoch total loss 58.4585876\n",
      "Trained batch 862 batch loss 37.5622787 epoch total loss 58.4343452\n",
      "Trained batch 863 batch loss 37.6353683 epoch total loss 58.4102478\n",
      "Trained batch 864 batch loss 37.14534 epoch total loss 58.3856354\n",
      "Trained batch 865 batch loss 37.345253 epoch total loss 58.3613091\n",
      "Trained batch 866 batch loss 37.529747 epoch total loss 58.3372536\n",
      "Trained batch 867 batch loss 36.9802 epoch total loss 58.3126221\n",
      "Trained batch 868 batch loss 37.1683044 epoch total loss 58.2882614\n",
      "Trained batch 869 batch loss 37.2754059 epoch total loss 58.2640839\n",
      "Trained batch 870 batch loss 37.2838478 epoch total loss 58.2399712\n",
      "Trained batch 871 batch loss 37.2833443 epoch total loss 58.2159119\n",
      "Trained batch 872 batch loss 37.0381203 epoch total loss 58.1916275\n",
      "Trained batch 873 batch loss 37.3498383 epoch total loss 58.1677551\n",
      "Trained batch 874 batch loss 37.10952 epoch total loss 58.1436615\n",
      "Trained batch 875 batch loss 36.5985794 epoch total loss 58.1190338\n",
      "Trained batch 876 batch loss 36.3090477 epoch total loss 58.0941391\n",
      "Trained batch 877 batch loss 35.55933 epoch total loss 58.0684433\n",
      "Trained batch 878 batch loss 35.781456 epoch total loss 58.0430565\n",
      "Trained batch 879 batch loss 37.1334915 epoch total loss 58.019268\n",
      "Trained batch 880 batch loss 37.0659294 epoch total loss 57.9954605\n",
      "Trained batch 881 batch loss 36.9408798 epoch total loss 57.9715614\n",
      "Trained batch 882 batch loss 36.5832405 epoch total loss 57.9473114\n",
      "Trained batch 883 batch loss 36.9273186 epoch total loss 57.9235039\n",
      "Trained batch 884 batch loss 36.714386 epoch total loss 57.8995094\n",
      "Trained batch 885 batch loss 36.0495224 epoch total loss 57.8748245\n",
      "Trained batch 886 batch loss 36.500988 epoch total loss 57.8506966\n",
      "Trained batch 887 batch loss 36.3580208 epoch total loss 57.8264694\n",
      "Trained batch 888 batch loss 36.4684486 epoch total loss 57.8024178\n",
      "Trained batch 889 batch loss 36.288559 epoch total loss 57.7782173\n",
      "Trained batch 890 batch loss 36.4445381 epoch total loss 57.7542496\n",
      "Trained batch 891 batch loss 36.3342781 epoch total loss 57.7302094\n",
      "Trained batch 892 batch loss 36.8617821 epoch total loss 57.7068176\n",
      "Trained batch 893 batch loss 37.019 epoch total loss 57.683651\n",
      "Trained batch 894 batch loss 37.4648705 epoch total loss 57.6610336\n",
      "Trained batch 895 batch loss 37.0568047 epoch total loss 57.6380157\n",
      "Trained batch 896 batch loss 36.7507553 epoch total loss 57.6147041\n",
      "Trained batch 897 batch loss 36.8266144 epoch total loss 57.5915298\n",
      "Trained batch 898 batch loss 36.1731033 epoch total loss 57.5676765\n",
      "Trained batch 899 batch loss 35.6848488 epoch total loss 57.543335\n",
      "Trained batch 900 batch loss 36.3396149 epoch total loss 57.5197754\n",
      "Trained batch 901 batch loss 36.4237862 epoch total loss 57.496357\n",
      "Trained batch 902 batch loss 36.4374123 epoch total loss 57.473011\n",
      "Trained batch 903 batch loss 35.7960434 epoch total loss 57.4490051\n",
      "Trained batch 904 batch loss 36.0445862 epoch total loss 57.4253273\n",
      "Trained batch 905 batch loss 36.1350136 epoch total loss 57.4018059\n",
      "Trained batch 906 batch loss 35.8253059 epoch total loss 57.3779869\n",
      "Trained batch 907 batch loss 35.8459511 epoch total loss 57.354248\n",
      "Trained batch 908 batch loss 35.5569191 epoch total loss 57.330246\n",
      "Trained batch 909 batch loss 35.874691 epoch total loss 57.3066444\n",
      "Trained batch 910 batch loss 35.9662094 epoch total loss 57.2831917\n",
      "Trained batch 911 batch loss 35.8737717 epoch total loss 57.2596893\n",
      "Trained batch 912 batch loss 35.688736 epoch total loss 57.2360382\n",
      "Trained batch 913 batch loss 35.8862877 epoch total loss 57.2126541\n",
      "Trained batch 914 batch loss 35.8025703 epoch total loss 57.1892281\n",
      "Trained batch 915 batch loss 36.1169395 epoch total loss 57.1662\n",
      "Trained batch 916 batch loss 35.87323 epoch total loss 57.142952\n",
      "Trained batch 917 batch loss 35.8078423 epoch total loss 57.1196861\n",
      "Trained batch 918 batch loss 35.7462845 epoch total loss 57.096405\n",
      "Trained batch 919 batch loss 35.674324 epoch total loss 57.0730972\n",
      "Trained batch 920 batch loss 35.4167747 epoch total loss 57.0495567\n",
      "Trained batch 921 batch loss 35.863945 epoch total loss 57.0265541\n",
      "Trained batch 922 batch loss 35.6991844 epoch total loss 57.0034218\n",
      "Trained batch 923 batch loss 35.470665 epoch total loss 56.9800911\n",
      "Trained batch 924 batch loss 35.5145874 epoch total loss 56.9568634\n",
      "Trained batch 925 batch loss 35.3222694 epoch total loss 56.9334755\n",
      "Trained batch 926 batch loss 34.9786682 epoch total loss 56.9097672\n",
      "Trained batch 927 batch loss 35.1985931 epoch total loss 56.8863487\n",
      "Trained batch 928 batch loss 35.1488419 epoch total loss 56.8629227\n",
      "Trained batch 929 batch loss 35.2851906 epoch total loss 56.839695\n",
      "Trained batch 930 batch loss 35.5078125 epoch total loss 56.8167572\n",
      "Trained batch 931 batch loss 34.786274 epoch total loss 56.7930946\n",
      "Trained batch 932 batch loss 35.1695518 epoch total loss 56.7698898\n",
      "Trained batch 933 batch loss 34.698967 epoch total loss 56.7462349\n",
      "Trained batch 934 batch loss 34.8835793 epoch total loss 56.7228279\n",
      "Trained batch 935 batch loss 34.9139557 epoch total loss 56.6995\n",
      "Trained batch 936 batch loss 35.1305923 epoch total loss 56.6764565\n",
      "Trained batch 937 batch loss 34.6928864 epoch total loss 56.6529922\n",
      "Trained batch 938 batch loss 34.7887878 epoch total loss 56.6296844\n",
      "Trained batch 939 batch loss 34.5027428 epoch total loss 56.6061211\n",
      "Trained batch 940 batch loss 34.7188873 epoch total loss 56.5828362\n",
      "Trained batch 941 batch loss 35.0291 epoch total loss 56.5599289\n",
      "Trained batch 942 batch loss 34.6022415 epoch total loss 56.5366211\n",
      "Trained batch 943 batch loss 35.1652374 epoch total loss 56.5139542\n",
      "Trained batch 944 batch loss 35.0824127 epoch total loss 56.4912529\n",
      "Trained batch 945 batch loss 34.399025 epoch total loss 56.4678726\n",
      "Trained batch 946 batch loss 34.6381683 epoch total loss 56.4447975\n",
      "Trained batch 947 batch loss 34.8397064 epoch total loss 56.4219818\n",
      "Trained batch 948 batch loss 34.8168182 epoch total loss 56.3991928\n",
      "Trained batch 949 batch loss 34.7832756 epoch total loss 56.3764153\n",
      "Trained batch 950 batch loss 34.6896095 epoch total loss 56.3535881\n",
      "Trained batch 951 batch loss 34.4257622 epoch total loss 56.3305321\n",
      "Trained batch 952 batch loss 34.5261154 epoch total loss 56.3076286\n",
      "Trained batch 953 batch loss 34.4652786 epoch total loss 56.2847099\n",
      "Trained batch 954 batch loss 33.7328224 epoch total loss 56.2610703\n",
      "Trained batch 955 batch loss 34.2157364 epoch total loss 56.2379875\n",
      "Trained batch 956 batch loss 34.2785721 epoch total loss 56.2150154\n",
      "Trained batch 957 batch loss 34.2478027 epoch total loss 56.1920586\n",
      "Trained batch 958 batch loss 34.1600723 epoch total loss 56.1690598\n",
      "Trained batch 959 batch loss 34.128334 epoch total loss 56.14608\n",
      "Trained batch 960 batch loss 33.6568069 epoch total loss 56.122654\n",
      "Trained batch 961 batch loss 33.9967117 epoch total loss 56.0996284\n",
      "Trained batch 962 batch loss 34.0932198 epoch total loss 56.0767517\n",
      "Trained batch 963 batch loss 33.9766388 epoch total loss 56.0538025\n",
      "Trained batch 964 batch loss 33.7089615 epoch total loss 56.0306206\n",
      "Trained batch 965 batch loss 33.5311852 epoch total loss 56.0073051\n",
      "Trained batch 966 batch loss 34.4696312 epoch total loss 55.9850082\n",
      "Trained batch 967 batch loss 34.265213 epoch total loss 55.9625511\n",
      "Trained batch 968 batch loss 34.1546555 epoch total loss 55.9400215\n",
      "Trained batch 969 batch loss 34.0769768 epoch total loss 55.9174614\n",
      "Trained batch 970 batch loss 33.4229851 epoch total loss 55.894268\n",
      "Trained batch 971 batch loss 33.9947128 epoch total loss 55.8717155\n",
      "Trained batch 972 batch loss 33.9514961 epoch total loss 55.8491669\n",
      "Trained batch 973 batch loss 33.6292152 epoch total loss 55.8263321\n",
      "Trained batch 974 batch loss 33.9442062 epoch total loss 55.8038635\n",
      "Trained batch 975 batch loss 33.3936462 epoch total loss 55.78088\n",
      "Trained batch 976 batch loss 33.4673538 epoch total loss 55.7580223\n",
      "Trained batch 977 batch loss 33.6769218 epoch total loss 55.7354202\n",
      "Trained batch 978 batch loss 33.1313286 epoch total loss 55.712307\n",
      "Trained batch 979 batch loss 33.7898331 epoch total loss 55.6899147\n",
      "Trained batch 980 batch loss 33.6528435 epoch total loss 55.6674271\n",
      "Trained batch 981 batch loss 33.2060699 epoch total loss 55.6445312\n",
      "Trained batch 982 batch loss 33.4579697 epoch total loss 55.6219368\n",
      "Trained batch 983 batch loss 33.2581215 epoch total loss 55.5991859\n",
      "Trained batch 984 batch loss 33.9297562 epoch total loss 55.5771637\n",
      "Trained batch 985 batch loss 33.1892548 epoch total loss 55.5544319\n",
      "Trained batch 986 batch loss 32.7143326 epoch total loss 55.5312691\n",
      "Trained batch 987 batch loss 32.3540573 epoch total loss 55.5077896\n",
      "Trained batch 988 batch loss 32.9141083 epoch total loss 55.4849205\n",
      "Trained batch 989 batch loss 32.9597 epoch total loss 55.4621468\n",
      "Trained batch 990 batch loss 33.1662521 epoch total loss 55.4396286\n",
      "Trained batch 991 batch loss 33.0757751 epoch total loss 55.417057\n",
      "Trained batch 992 batch loss 33.2841644 epoch total loss 55.3947487\n",
      "Trained batch 993 batch loss 32.9703217 epoch total loss 55.3721619\n",
      "Trained batch 994 batch loss 33.3108139 epoch total loss 55.3499718\n",
      "Trained batch 995 batch loss 32.9296303 epoch total loss 55.3274384\n",
      "Trained batch 996 batch loss 34.2748718 epoch total loss 55.3063\n",
      "Trained batch 997 batch loss 33.6553497 epoch total loss 55.284584\n",
      "Trained batch 998 batch loss 33.6308632 epoch total loss 55.2628899\n",
      "Trained batch 999 batch loss 33.1680908 epoch total loss 55.2407722\n",
      "Trained batch 1000 batch loss 33.3268204 epoch total loss 55.2188606\n",
      "Trained batch 1001 batch loss 33.0691 epoch total loss 55.1967316\n",
      "Trained batch 1002 batch loss 32.6011581 epoch total loss 55.1741829\n",
      "Trained batch 1003 batch loss 33.0667 epoch total loss 55.1521416\n",
      "Trained batch 1004 batch loss 33.08461 epoch total loss 55.1301613\n",
      "Trained batch 1005 batch loss 32.7815094 epoch total loss 55.1079254\n",
      "Trained batch 1006 batch loss 32.8719826 epoch total loss 55.0858192\n",
      "Trained batch 1007 batch loss 32.5610466 epoch total loss 55.0634537\n",
      "Trained batch 1008 batch loss 32.6226158 epoch total loss 55.0411911\n",
      "Trained batch 1009 batch loss 33.0137596 epoch total loss 55.0193596\n",
      "Trained batch 1010 batch loss 32.8899765 epoch total loss 54.9974518\n",
      "Trained batch 1011 batch loss 33.1689453 epoch total loss 54.9758606\n",
      "Trained batch 1012 batch loss 33.1308327 epoch total loss 54.9542732\n",
      "Trained batch 1013 batch loss 33.0106277 epoch total loss 54.9326096\n",
      "Trained batch 1014 batch loss 32.3326263 epoch total loss 54.9103203\n",
      "Trained batch 1015 batch loss 31.6206913 epoch total loss 54.8873787\n",
      "Trained batch 1016 batch loss 32.3178978 epoch total loss 54.8651619\n",
      "Trained batch 1017 batch loss 32.6433182 epoch total loss 54.8433113\n",
      "Trained batch 1018 batch loss 32.8639565 epoch total loss 54.8217201\n",
      "Trained batch 1019 batch loss 32.6255798 epoch total loss 54.7999382\n",
      "Trained batch 1020 batch loss 32.7411804 epoch total loss 54.7783127\n",
      "Trained batch 1021 batch loss 32.361927 epoch total loss 54.7563591\n",
      "Trained batch 1022 batch loss 32.4291229 epoch total loss 54.7345123\n",
      "Trained batch 1023 batch loss 32.7144928 epoch total loss 54.7129898\n",
      "Trained batch 1024 batch loss 32.52145 epoch total loss 54.6913147\n",
      "Trained batch 1025 batch loss 32.2456207 epoch total loss 54.6694183\n",
      "Trained batch 1026 batch loss 32.2924881 epoch total loss 54.6476059\n",
      "Trained batch 1027 batch loss 32.3449707 epoch total loss 54.6258888\n",
      "Trained batch 1028 batch loss 32.3794136 epoch total loss 54.604248\n",
      "Trained batch 1029 batch loss 32.2860641 epoch total loss 54.5825577\n",
      "Trained batch 1030 batch loss 32.5602608 epoch total loss 54.5611763\n",
      "Trained batch 1031 batch loss 32.6576805 epoch total loss 54.5399284\n",
      "Trained batch 1032 batch loss 32.5049133 epoch total loss 54.5185776\n",
      "Trained batch 1033 batch loss 32.3136368 epoch total loss 54.4970818\n",
      "Trained batch 1034 batch loss 32.4182243 epoch total loss 54.4757271\n",
      "Trained batch 1035 batch loss 32.5367203 epoch total loss 54.4545288\n",
      "Trained batch 1036 batch loss 32.1376877 epoch total loss 54.4329872\n",
      "Trained batch 1037 batch loss 32.2263184 epoch total loss 54.4115715\n",
      "Trained batch 1038 batch loss 32.0475388 epoch total loss 54.3900261\n",
      "Trained batch 1039 batch loss 32.0073204 epoch total loss 54.3684845\n",
      "Trained batch 1040 batch loss 31.9171143 epoch total loss 54.3468971\n",
      "Trained batch 1041 batch loss 31.5854225 epoch total loss 54.3250313\n",
      "Trained batch 1042 batch loss 31.6847534 epoch total loss 54.3033028\n",
      "Trained batch 1043 batch loss 31.720768 epoch total loss 54.2816544\n",
      "Trained batch 1044 batch loss 31.9973793 epoch total loss 54.2603073\n",
      "Trained batch 1045 batch loss 32.0046082 epoch total loss 54.2390099\n",
      "Trained batch 1046 batch loss 31.9354782 epoch total loss 54.2176857\n",
      "Trained batch 1047 batch loss 31.9724846 epoch total loss 54.1964378\n",
      "Trained batch 1048 batch loss 31.6462059 epoch total loss 54.1749191\n",
      "Trained batch 1049 batch loss 31.4855518 epoch total loss 54.1532898\n",
      "Trained batch 1050 batch loss 31.2583 epoch total loss 54.131485\n",
      "Trained batch 1051 batch loss 31.4564896 epoch total loss 54.1099091\n",
      "Trained batch 1052 batch loss 31.7093506 epoch total loss 54.0886192\n",
      "Trained batch 1053 batch loss 31.8480396 epoch total loss 54.0674973\n",
      "Trained batch 1054 batch loss 31.8230934 epoch total loss 54.0463943\n",
      "Trained batch 1055 batch loss 31.8933029 epoch total loss 54.0253944\n",
      "Trained batch 1056 batch loss 32.0564384 epoch total loss 54.0045891\n",
      "Trained batch 1057 batch loss 31.9689274 epoch total loss 53.9837418\n",
      "Trained batch 1058 batch loss 31.9431324 epoch total loss 53.9629097\n",
      "Trained batch 1059 batch loss 32.1287537 epoch total loss 53.9422913\n",
      "Trained batch 1060 batch loss 31.8862782 epoch total loss 53.9214859\n",
      "Trained batch 1061 batch loss 31.5288143 epoch total loss 53.9003792\n",
      "Trained batch 1062 batch loss 31.1056671 epoch total loss 53.8789139\n",
      "Trained batch 1063 batch loss 31.2462082 epoch total loss 53.8576241\n",
      "Trained batch 1064 batch loss 30.9215546 epoch total loss 53.8360672\n",
      "Trained batch 1065 batch loss 30.8116226 epoch total loss 53.8144493\n",
      "Trained batch 1066 batch loss 31.1520348 epoch total loss 53.79319\n",
      "Trained batch 1067 batch loss 31.2905922 epoch total loss 53.7721\n",
      "Trained batch 1068 batch loss 30.4849911 epoch total loss 53.7502937\n",
      "Trained batch 1069 batch loss 30.7065353 epoch total loss 53.7287369\n",
      "Trained batch 1070 batch loss 31.3043098 epoch total loss 53.7077789\n",
      "Trained batch 1071 batch loss 31.0884476 epoch total loss 53.6866608\n",
      "Trained batch 1072 batch loss 31.2945633 epoch total loss 53.6657715\n",
      "Trained batch 1073 batch loss 31.1776161 epoch total loss 53.6448097\n",
      "Trained batch 1074 batch loss 31.0465641 epoch total loss 53.6237717\n",
      "Trained batch 1075 batch loss 30.9389534 epoch total loss 53.6026688\n",
      "Trained batch 1076 batch loss 30.8602734 epoch total loss 53.5815315\n",
      "Trained batch 1077 batch loss 30.5727844 epoch total loss 53.5601692\n",
      "Trained batch 1078 batch loss 30.6256275 epoch total loss 53.5388908\n",
      "Trained batch 1079 batch loss 30.7444897 epoch total loss 53.5177689\n",
      "Trained batch 1080 batch loss 30.5763206 epoch total loss 53.4965286\n",
      "Trained batch 1081 batch loss 30.8425922 epoch total loss 53.4755707\n",
      "Trained batch 1082 batch loss 30.7718048 epoch total loss 53.4545898\n",
      "Trained batch 1083 batch loss 30.5006409 epoch total loss 53.4333954\n",
      "Trained batch 1084 batch loss 30.5474167 epoch total loss 53.412281\n",
      "Trained batch 1085 batch loss 30.3274574 epoch total loss 53.3910065\n",
      "Trained batch 1086 batch loss 30.6569633 epoch total loss 53.3700714\n",
      "Trained batch 1087 batch loss 30.4167213 epoch total loss 53.3489571\n",
      "Trained batch 1088 batch loss 30.5720749 epoch total loss 53.328022\n",
      "Trained batch 1089 batch loss 30.5786018 epoch total loss 53.3071289\n",
      "Trained batch 1090 batch loss 30.7005329 epoch total loss 53.2863884\n",
      "Trained batch 1091 batch loss 30.6168404 epoch total loss 53.2656097\n",
      "Trained batch 1092 batch loss 30.5772285 epoch total loss 53.2448349\n",
      "Trained batch 1093 batch loss 29.9778576 epoch total loss 53.2235451\n",
      "Trained batch 1094 batch loss 30.2319183 epoch total loss 53.2025299\n",
      "Trained batch 1095 batch loss 30.4363804 epoch total loss 53.1817398\n",
      "Trained batch 1096 batch loss 29.9610958 epoch total loss 53.160553\n",
      "Trained batch 1097 batch loss 29.7200527 epoch total loss 53.139183\n",
      "Trained batch 1098 batch loss 29.7372894 epoch total loss 53.1178703\n",
      "Trained batch 1099 batch loss 30.1718788 epoch total loss 53.0969925\n",
      "Trained batch 1100 batch loss 30.1704693 epoch total loss 53.076149\n",
      "Trained batch 1101 batch loss 30.2230339 epoch total loss 53.0553932\n",
      "Trained batch 1102 batch loss 30.4956131 epoch total loss 53.0349236\n",
      "Trained batch 1103 batch loss 30.4834 epoch total loss 53.0144768\n",
      "Trained batch 1104 batch loss 30.181118 epoch total loss 52.9937935\n",
      "Trained batch 1105 batch loss 29.7622986 epoch total loss 52.9727707\n",
      "Trained batch 1106 batch loss 29.3823071 epoch total loss 52.9514389\n",
      "Trained batch 1107 batch loss 29.5382824 epoch total loss 52.9302902\n",
      "Trained batch 1108 batch loss 29.7883549 epoch total loss 52.9094048\n",
      "Trained batch 1109 batch loss 30.0538826 epoch total loss 52.8887978\n",
      "Trained batch 1110 batch loss 29.9730282 epoch total loss 52.8681526\n",
      "Trained batch 1111 batch loss 29.6907043 epoch total loss 52.84729\n",
      "Trained batch 1112 batch loss 29.8236885 epoch total loss 52.8265877\n",
      "Trained batch 1113 batch loss 29.9631615 epoch total loss 52.8060455\n",
      "Trained batch 1114 batch loss 29.9354858 epoch total loss 52.7855148\n",
      "Trained batch 1115 batch loss 30.2376289 epoch total loss 52.7652931\n",
      "Trained batch 1116 batch loss 30.0118866 epoch total loss 52.7449036\n",
      "Trained batch 1117 batch loss 29.9116802 epoch total loss 52.7244606\n",
      "Trained batch 1118 batch loss 29.8248501 epoch total loss 52.7039757\n",
      "Trained batch 1119 batch loss 29.6442509 epoch total loss 52.6833687\n",
      "Trained batch 1120 batch loss 29.461956 epoch total loss 52.6626358\n",
      "Trained batch 1121 batch loss 30.0669518 epoch total loss 52.6424789\n",
      "Trained batch 1122 batch loss 29.8818645 epoch total loss 52.6221924\n",
      "Trained batch 1123 batch loss 29.9311314 epoch total loss 52.6019859\n",
      "Trained batch 1124 batch loss 29.1192 epoch total loss 52.5810966\n",
      "Trained batch 1125 batch loss 29.2223263 epoch total loss 52.5603333\n",
      "Trained batch 1126 batch loss 29.4846401 epoch total loss 52.5398407\n",
      "Trained batch 1127 batch loss 29.0772705 epoch total loss 52.5190201\n",
      "Trained batch 1128 batch loss 29.7724247 epoch total loss 52.4988556\n",
      "Trained batch 1129 batch loss 29.6933422 epoch total loss 52.4786568\n",
      "Trained batch 1130 batch loss 29.4124851 epoch total loss 52.4582443\n",
      "Trained batch 1131 batch loss 28.4762478 epoch total loss 52.4370422\n",
      "Trained batch 1132 batch loss 28.6457291 epoch total loss 52.4160233\n",
      "Trained batch 1133 batch loss 29.0125141 epoch total loss 52.3953667\n",
      "Trained batch 1134 batch loss 28.4428711 epoch total loss 52.3742409\n",
      "Trained batch 1135 batch loss 29.4740067 epoch total loss 52.3540649\n",
      "Trained batch 1136 batch loss 29.0768147 epoch total loss 52.3335762\n",
      "Trained batch 1137 batch loss 29.1486588 epoch total loss 52.3131828\n",
      "Trained batch 1138 batch loss 29.2085209 epoch total loss 52.292881\n",
      "Trained batch 1139 batch loss 29.4770088 epoch total loss 52.2728462\n",
      "Trained batch 1140 batch loss 28.7725048 epoch total loss 52.2522354\n",
      "Trained batch 1141 batch loss 29.2925358 epoch total loss 52.2321129\n",
      "Trained batch 1142 batch loss 29.2691269 epoch total loss 52.2120056\n",
      "Trained batch 1143 batch loss 28.1279278 epoch total loss 52.1909332\n",
      "Trained batch 1144 batch loss 27.5509682 epoch total loss 52.1693954\n",
      "Trained batch 1145 batch loss 28.6397209 epoch total loss 52.1488457\n",
      "Trained batch 1146 batch loss 28.8387585 epoch total loss 52.1285057\n",
      "Trained batch 1147 batch loss 29.8518696 epoch total loss 52.1090851\n",
      "Trained batch 1148 batch loss 29.3137894 epoch total loss 52.0892296\n",
      "Trained batch 1149 batch loss 29.3876095 epoch total loss 52.0694695\n",
      "Trained batch 1150 batch loss 29.0339928 epoch total loss 52.0494385\n",
      "Trained batch 1151 batch loss 29.147192 epoch total loss 52.0295448\n",
      "Trained batch 1152 batch loss 29.2613029 epoch total loss 52.0097809\n",
      "Trained batch 1153 batch loss 28.9919758 epoch total loss 51.9898148\n",
      "Trained batch 1154 batch loss 28.9907112 epoch total loss 51.9698868\n",
      "Trained batch 1155 batch loss 28.9312439 epoch total loss 51.9499397\n",
      "Trained batch 1156 batch loss 28.9399643 epoch total loss 51.9300346\n",
      "Trained batch 1157 batch loss 28.9875355 epoch total loss 51.9102058\n",
      "Trained batch 1158 batch loss 28.7174034 epoch total loss 51.8901787\n",
      "Trained batch 1159 batch loss 28.9514446 epoch total loss 51.870388\n",
      "Trained batch 1160 batch loss 28.9333553 epoch total loss 51.8506165\n",
      "Trained batch 1161 batch loss 28.4892216 epoch total loss 51.8304939\n",
      "Trained batch 1162 batch loss 28.1640511 epoch total loss 51.8101273\n",
      "Trained batch 1163 batch loss 28.7958679 epoch total loss 51.7903404\n",
      "Trained batch 1164 batch loss 28.7638092 epoch total loss 51.7705574\n",
      "Trained batch 1165 batch loss 28.4095917 epoch total loss 51.7505074\n",
      "Trained batch 1166 batch loss 28.4165535 epoch total loss 51.7304955\n",
      "Trained batch 1167 batch loss 28.9725838 epoch total loss 51.7109947\n",
      "Trained batch 1168 batch loss 28.5848751 epoch total loss 51.6911964\n",
      "Trained batch 1169 batch loss 28.5224342 epoch total loss 51.6713753\n",
      "Trained batch 1170 batch loss 28.5470104 epoch total loss 51.6516113\n",
      "Trained batch 1171 batch loss 28.380722 epoch total loss 51.6317368\n",
      "Trained batch 1172 batch loss 28.2033443 epoch total loss 51.6117477\n",
      "Trained batch 1173 batch loss 27.8482227 epoch total loss 51.5914879\n",
      "Trained batch 1174 batch loss 27.6624699 epoch total loss 51.571106\n",
      "Trained batch 1175 batch loss 27.285 epoch total loss 51.5504379\n",
      "Trained batch 1176 batch loss 27.2359962 epoch total loss 51.5297623\n",
      "Trained batch 1177 batch loss 27.8025551 epoch total loss 51.5096\n",
      "Trained batch 1178 batch loss 28.1237736 epoch total loss 51.4897499\n",
      "Trained batch 1179 batch loss 28.385458 epoch total loss 51.4701538\n",
      "Trained batch 1180 batch loss 28.6198177 epoch total loss 51.4507904\n",
      "Trained batch 1181 batch loss 28.7645264 epoch total loss 51.4315834\n",
      "Trained batch 1182 batch loss 28.6457329 epoch total loss 51.4123039\n",
      "Trained batch 1183 batch loss 28.5148067 epoch total loss 51.3929482\n",
      "Trained batch 1184 batch loss 28.2026443 epoch total loss 51.3733635\n",
      "Trained batch 1185 batch loss 28.4817867 epoch total loss 51.3540459\n",
      "Trained batch 1186 batch loss 28.1835461 epoch total loss 51.334507\n",
      "Trained batch 1187 batch loss 28.1942348 epoch total loss 51.3150139\n",
      "Trained batch 1188 batch loss 28.3951416 epoch total loss 51.2957191\n",
      "Trained batch 1189 batch loss 27.9742203 epoch total loss 51.276104\n",
      "Trained batch 1190 batch loss 28.0024281 epoch total loss 51.2565498\n",
      "Trained batch 1191 batch loss 27.6086597 epoch total loss 51.2366943\n",
      "Trained batch 1192 batch loss 27.329319 epoch total loss 51.2166367\n",
      "Trained batch 1193 batch loss 27.6012268 epoch total loss 51.1968422\n",
      "Trained batch 1194 batch loss 27.8032761 epoch total loss 51.1772499\n",
      "Trained batch 1195 batch loss 27.6916428 epoch total loss 51.1575966\n",
      "Trained batch 1196 batch loss 27.1574287 epoch total loss 51.1375275\n",
      "Trained batch 1197 batch loss 27.1188965 epoch total loss 51.1174622\n",
      "Trained batch 1198 batch loss 27.2801895 epoch total loss 51.0975647\n",
      "Trained batch 1199 batch loss 27.4865208 epoch total loss 51.0778732\n",
      "Trained batch 1200 batch loss 27.3087482 epoch total loss 51.0580673\n",
      "Trained batch 1201 batch loss 26.404295 epoch total loss 51.0375366\n",
      "Trained batch 1202 batch loss 26.7010765 epoch total loss 51.0172882\n",
      "Trained batch 1203 batch loss 27.2168522 epoch total loss 50.9975052\n",
      "Trained batch 1204 batch loss 26.8602715 epoch total loss 50.977459\n",
      "Trained batch 1205 batch loss 26.7163544 epoch total loss 50.9573212\n",
      "Trained batch 1206 batch loss 27.1445408 epoch total loss 50.9375763\n",
      "Trained batch 1207 batch loss 27.5229511 epoch total loss 50.9181786\n",
      "Trained batch 1208 batch loss 26.9684238 epoch total loss 50.8983536\n",
      "Trained batch 1209 batch loss 27.0413322 epoch total loss 50.8786201\n",
      "Trained batch 1210 batch loss 26.6616898 epoch total loss 50.8586082\n",
      "Trained batch 1211 batch loss 27.0275955 epoch total loss 50.8389282\n",
      "Trained batch 1212 batch loss 27.486063 epoch total loss 50.8196564\n",
      "Trained batch 1213 batch loss 27.6105747 epoch total loss 50.8005219\n",
      "Trained batch 1214 batch loss 27.6489887 epoch total loss 50.7814522\n",
      "Trained batch 1215 batch loss 27.523674 epoch total loss 50.76231\n",
      "Trained batch 1216 batch loss 26.9621181 epoch total loss 50.7427368\n",
      "Trained batch 1217 batch loss 27.2953453 epoch total loss 50.7234726\n",
      "Trained batch 1218 batch loss 26.9209709 epoch total loss 50.7039299\n",
      "Trained batch 1219 batch loss 26.4332085 epoch total loss 50.684021\n",
      "Trained batch 1220 batch loss 26.6202126 epoch total loss 50.6642952\n",
      "Trained batch 1221 batch loss 26.7469635 epoch total loss 50.6447067\n",
      "Trained batch 1222 batch loss 26.9453812 epoch total loss 50.6253128\n",
      "Trained batch 1223 batch loss 26.8242073 epoch total loss 50.6058502\n",
      "Trained batch 1224 batch loss 27.3283386 epoch total loss 50.586834\n",
      "Trained batch 1225 batch loss 27.3720093 epoch total loss 50.5678825\n",
      "Trained batch 1226 batch loss 27.1315479 epoch total loss 50.5487671\n",
      "Trained batch 1227 batch loss 26.9865494 epoch total loss 50.5295677\n",
      "Trained batch 1228 batch loss 26.9594917 epoch total loss 50.5103722\n",
      "Trained batch 1229 batch loss 26.6199169 epoch total loss 50.4909363\n",
      "Trained batch 1230 batch loss 26.1798229 epoch total loss 50.4711685\n",
      "Trained batch 1231 batch loss 26.9934673 epoch total loss 50.452095\n",
      "Trained batch 1232 batch loss 27.0264244 epoch total loss 50.4330826\n",
      "Trained batch 1233 batch loss 27.1210346 epoch total loss 50.4141769\n",
      "Trained batch 1234 batch loss 27.0420284 epoch total loss 50.395237\n",
      "Trained batch 1235 batch loss 27.0448647 epoch total loss 50.3763275\n",
      "Trained batch 1236 batch loss 26.6946831 epoch total loss 50.3571701\n",
      "Trained batch 1237 batch loss 26.4848289 epoch total loss 50.3378716\n",
      "Trained batch 1238 batch loss 26.4538708 epoch total loss 50.3185768\n",
      "Trained batch 1239 batch loss 27.2139759 epoch total loss 50.2999306\n",
      "Trained batch 1240 batch loss 26.5925713 epoch total loss 50.2808113\n",
      "Trained batch 1241 batch loss 27.0402889 epoch total loss 50.262085\n",
      "Trained batch 1242 batch loss 26.9358101 epoch total loss 50.2433052\n",
      "Trained batch 1243 batch loss 27.0684986 epoch total loss 50.2246628\n",
      "Trained batch 1244 batch loss 26.7286797 epoch total loss 50.2057762\n",
      "Trained batch 1245 batch loss 26.7745 epoch total loss 50.1869545\n",
      "Trained batch 1246 batch loss 26.7555046 epoch total loss 50.168148\n",
      "Trained batch 1247 batch loss 25.9352798 epoch total loss 50.1487122\n",
      "Trained batch 1248 batch loss 26.4911537 epoch total loss 50.1297569\n",
      "Trained batch 1249 batch loss 26.7389717 epoch total loss 50.1110306\n",
      "Trained batch 1250 batch loss 26.3220901 epoch total loss 50.0919952\n",
      "Trained batch 1251 batch loss 26.7515564 epoch total loss 50.0733376\n",
      "Trained batch 1252 batch loss 26.5160809 epoch total loss 50.0545235\n",
      "Trained batch 1253 batch loss 26.2743092 epoch total loss 50.0355415\n",
      "Trained batch 1254 batch loss 26.4353542 epoch total loss 50.0167198\n",
      "Trained batch 1255 batch loss 26.3159218 epoch total loss 49.9978371\n",
      "Trained batch 1256 batch loss 26.6384 epoch total loss 49.9792366\n",
      "Trained batch 1257 batch loss 26.5224533 epoch total loss 49.9605789\n",
      "Trained batch 1258 batch loss 26.0423851 epoch total loss 49.9415665\n",
      "Trained batch 1259 batch loss 26.1582832 epoch total loss 49.9226761\n",
      "Trained batch 1260 batch loss 26.2481346 epoch total loss 49.9038887\n",
      "Trained batch 1261 batch loss 26.1373196 epoch total loss 49.8850403\n",
      "Trained batch 1262 batch loss 26.4956741 epoch total loss 49.8665047\n",
      "Trained batch 1263 batch loss 26.6497784 epoch total loss 49.8481216\n",
      "Trained batch 1264 batch loss 26.7062721 epoch total loss 49.8298149\n",
      "Trained batch 1265 batch loss 26.5468063 epoch total loss 49.811409\n",
      "Trained batch 1266 batch loss 25.3922272 epoch total loss 49.7921219\n",
      "Trained batch 1267 batch loss 26.1866932 epoch total loss 49.7734909\n",
      "Trained batch 1268 batch loss 25.8590851 epoch total loss 49.754631\n",
      "Trained batch 1269 batch loss 25.8077946 epoch total loss 49.7357597\n",
      "Trained batch 1270 batch loss 26.1771011 epoch total loss 49.7172089\n",
      "Trained batch 1271 batch loss 26.1237049 epoch total loss 49.6986465\n",
      "Trained batch 1272 batch loss 26.1632233 epoch total loss 49.6801453\n",
      "Trained batch 1273 batch loss 26.0453911 epoch total loss 49.6615791\n",
      "Trained batch 1274 batch loss 25.9904938 epoch total loss 49.643\n",
      "Trained batch 1275 batch loss 25.9533958 epoch total loss 49.6244202\n",
      "Trained batch 1276 batch loss 25.9684601 epoch total loss 49.6058807\n",
      "Trained batch 1277 batch loss 25.9807625 epoch total loss 49.5873795\n",
      "Trained batch 1278 batch loss 25.8708344 epoch total loss 49.5688248\n",
      "Trained batch 1279 batch loss 25.7648849 epoch total loss 49.5502129\n",
      "Trained batch 1280 batch loss 25.0529423 epoch total loss 49.5310745\n",
      "Trained batch 1281 batch loss 25.4627876 epoch total loss 49.5122871\n",
      "Trained batch 1282 batch loss 25.7412853 epoch total loss 49.4937439\n",
      "Trained batch 1283 batch loss 25.9738 epoch total loss 49.4754105\n",
      "Trained batch 1284 batch loss 25.9088516 epoch total loss 49.457058\n",
      "Trained batch 1285 batch loss 25.9207878 epoch total loss 49.4387436\n",
      "Trained batch 1286 batch loss 25.4632893 epoch total loss 49.4201\n",
      "Trained batch 1287 batch loss 25.5328674 epoch total loss 49.4015388\n",
      "Trained batch 1288 batch loss 25.9805622 epoch total loss 49.3833542\n",
      "Trained batch 1289 batch loss 25.5653076 epoch total loss 49.3648796\n",
      "Trained batch 1290 batch loss 25.5636 epoch total loss 49.3464279\n",
      "Trained batch 1291 batch loss 25.7482681 epoch total loss 49.3281479\n",
      "Trained batch 1292 batch loss 25.5998955 epoch total loss 49.3097839\n",
      "Trained batch 1293 batch loss 25.032259 epoch total loss 49.291008\n",
      "Trained batch 1294 batch loss 25.0921898 epoch total loss 49.2723083\n",
      "Trained batch 1295 batch loss 24.8909912 epoch total loss 49.2534828\n",
      "Trained batch 1296 batch loss 24.6290569 epoch total loss 49.2344818\n",
      "Trained batch 1297 batch loss 24.9019623 epoch total loss 49.2157211\n",
      "Trained batch 1298 batch loss 25.4220448 epoch total loss 49.1973877\n",
      "Trained batch 1299 batch loss 25.4749966 epoch total loss 49.1791267\n",
      "Trained batch 1300 batch loss 25.8103104 epoch total loss 49.1611519\n",
      "Trained batch 1301 batch loss 26.2909851 epoch total loss 49.1435699\n",
      "Trained batch 1302 batch loss 25.7055817 epoch total loss 49.1255684\n",
      "Trained batch 1303 batch loss 25.8200569 epoch total loss 49.1076851\n",
      "Trained batch 1304 batch loss 25.7685852 epoch total loss 49.0897865\n",
      "Trained batch 1305 batch loss 25.9335976 epoch total loss 49.0720444\n",
      "Trained batch 1306 batch loss 25.2146988 epoch total loss 49.0537758\n",
      "Trained batch 1307 batch loss 25.4221268 epoch total loss 49.0356941\n",
      "Trained batch 1308 batch loss 25.1327171 epoch total loss 49.0174217\n",
      "Trained batch 1309 batch loss 25.1917419 epoch total loss 48.999218\n",
      "Trained batch 1310 batch loss 25.0136471 epoch total loss 48.9809074\n",
      "Trained batch 1311 batch loss 24.8649864 epoch total loss 48.9625092\n",
      "Trained batch 1312 batch loss 25.152668 epoch total loss 48.9443626\n",
      "Trained batch 1313 batch loss 24.9729805 epoch total loss 48.9261055\n",
      "Trained batch 1314 batch loss 25.0439854 epoch total loss 48.9079285\n",
      "Trained batch 1315 batch loss 24.5745296 epoch total loss 48.8894234\n",
      "Trained batch 1316 batch loss 24.828804 epoch total loss 48.8711395\n",
      "Trained batch 1317 batch loss 24.9415131 epoch total loss 48.8529701\n",
      "Trained batch 1318 batch loss 24.8080807 epoch total loss 48.8347282\n",
      "Trained batch 1319 batch loss 24.750618 epoch total loss 48.8164673\n",
      "Trained batch 1320 batch loss 24.6100826 epoch total loss 48.79813\n",
      "Trained batch 1321 batch loss 24.4968033 epoch total loss 48.7797318\n",
      "Trained batch 1322 batch loss 24.7072868 epoch total loss 48.7615242\n",
      "Trained batch 1323 batch loss 24.7907162 epoch total loss 48.7434044\n",
      "Trained batch 1324 batch loss 24.6713638 epoch total loss 48.7252235\n",
      "Trained batch 1325 batch loss 24.5892105 epoch total loss 48.7070084\n",
      "Trained batch 1326 batch loss 24.9379902 epoch total loss 48.6890831\n",
      "Trained batch 1327 batch loss 24.5802383 epoch total loss 48.6709137\n",
      "Trained batch 1328 batch loss 24.0527134 epoch total loss 48.6523743\n",
      "Trained batch 1329 batch loss 24.6059189 epoch total loss 48.6342812\n",
      "Trained batch 1330 batch loss 24.8599472 epoch total loss 48.6164055\n",
      "Trained batch 1331 batch loss 24.8683739 epoch total loss 48.5985641\n",
      "Trained batch 1332 batch loss 24.911335 epoch total loss 48.58078\n",
      "Trained batch 1333 batch loss 24.461956 epoch total loss 48.5626831\n",
      "Trained batch 1334 batch loss 24.5218315 epoch total loss 48.5446625\n",
      "Trained batch 1335 batch loss 24.500639 epoch total loss 48.5266533\n",
      "Trained batch 1336 batch loss 24.4037971 epoch total loss 48.5085945\n",
      "Trained batch 1337 batch loss 24.189703 epoch total loss 48.4904099\n",
      "Trained batch 1338 batch loss 24.5277271 epoch total loss 48.4725\n",
      "Trained batch 1339 batch loss 24.6905651 epoch total loss 48.4547386\n",
      "Trained batch 1340 batch loss 25.1774769 epoch total loss 48.4373665\n",
      "Trained batch 1341 batch loss 24.6526222 epoch total loss 48.4196281\n",
      "Trained batch 1342 batch loss 25.1734486 epoch total loss 48.4023056\n",
      "Trained batch 1343 batch loss 24.7890949 epoch total loss 48.3847237\n",
      "Trained batch 1344 batch loss 24.4067593 epoch total loss 48.3668823\n",
      "Trained batch 1345 batch loss 24.5546436 epoch total loss 48.3491783\n",
      "Trained batch 1346 batch loss 24.6484375 epoch total loss 48.3315697\n",
      "Trained batch 1347 batch loss 24.0321026 epoch total loss 48.31353\n",
      "Trained batch 1348 batch loss 24.3595619 epoch total loss 48.2957611\n",
      "Trained batch 1349 batch loss 24.2840633 epoch total loss 48.2779617\n",
      "Trained batch 1350 batch loss 23.8147125 epoch total loss 48.2598419\n",
      "Trained batch 1351 batch loss 23.7579403 epoch total loss 48.241703\n",
      "Trained batch 1352 batch loss 24.2267094 epoch total loss 48.2239418\n",
      "Trained batch 1353 batch loss 24.1184502 epoch total loss 48.2061234\n",
      "Trained batch 1354 batch loss 24.1295319 epoch total loss 48.188343\n",
      "Trained batch 1355 batch loss 24.1132202 epoch total loss 48.1705742\n",
      "Trained batch 1356 batch loss 24.3230782 epoch total loss 48.1529884\n",
      "Trained batch 1357 batch loss 24.3487129 epoch total loss 48.1354446\n",
      "Trained batch 1358 batch loss 23.9299583 epoch total loss 48.1176224\n",
      "Trained batch 1359 batch loss 24.2864685 epoch total loss 48.1000862\n",
      "Trained batch 1360 batch loss 24.1910686 epoch total loss 48.0825043\n",
      "Trained batch 1361 batch loss 23.7925053 epoch total loss 48.0646591\n",
      "Trained batch 1362 batch loss 24.1139221 epoch total loss 48.0470734\n",
      "Trained batch 1363 batch loss 23.9691067 epoch total loss 48.0294075\n",
      "Trained batch 1364 batch loss 23.9930401 epoch total loss 48.0117836\n",
      "Trained batch 1365 batch loss 24.2676449 epoch total loss 47.9943924\n",
      "Trained batch 1366 batch loss 24.4320507 epoch total loss 47.9771385\n",
      "Trained batch 1367 batch loss 23.8213749 epoch total loss 47.9594688\n",
      "Trained batch 1368 batch loss 23.8868027 epoch total loss 47.9418755\n",
      "Trained batch 1369 batch loss 23.8531113 epoch total loss 47.9242783\n",
      "Trained batch 1370 batch loss 24.0405655 epoch total loss 47.9068413\n",
      "Trained batch 1371 batch loss 23.9326725 epoch total loss 47.8893547\n",
      "Trained batch 1372 batch loss 23.9427834 epoch total loss 47.8719025\n",
      "Trained batch 1373 batch loss 23.9064236 epoch total loss 47.8544464\n",
      "Trained batch 1374 batch loss 23.7851 epoch total loss 47.8369255\n",
      "Trained batch 1375 batch loss 23.8286495 epoch total loss 47.8194656\n",
      "Trained batch 1376 batch loss 23.7409859 epoch total loss 47.8019676\n",
      "Trained batch 1377 batch loss 23.8324661 epoch total loss 47.784565\n",
      "Trained batch 1378 batch loss 23.525383 epoch total loss 47.7669563\n",
      "Trained batch 1379 batch loss 23.6399746 epoch total loss 47.7494621\n",
      "Trained batch 1380 batch loss 23.6447506 epoch total loss 47.732\n",
      "Trained batch 1381 batch loss 24.0556526 epoch total loss 47.7148514\n",
      "Trained batch 1382 batch loss 23.9386959 epoch total loss 47.6976471\n",
      "Trained batch 1383 batch loss 24.3526154 epoch total loss 47.6807671\n",
      "Trained batch 1384 batch loss 24.5861053 epoch total loss 47.6640778\n",
      "Trained batch 1385 batch loss 24.2265701 epoch total loss 47.6471558\n",
      "Trained batch 1386 batch loss 24.4902592 epoch total loss 47.6304512\n",
      "Trained batch 1387 batch loss 24.0977707 epoch total loss 47.6134872\n",
      "Trained batch 1388 batch loss 23.3948936 epoch total loss 47.5960426\n",
      "Epoch 2 train loss 47.59604263305664\n",
      "Validated batch 1 batch loss 25.6065845\n",
      "Validated batch 2 batch loss 25.4822083\n",
      "Validated batch 3 batch loss 25.0927582\n",
      "Validated batch 4 batch loss 24.3703613\n",
      "Validated batch 5 batch loss 25.1286392\n",
      "Validated batch 6 batch loss 25.1579323\n",
      "Validated batch 7 batch loss 24.7238617\n",
      "Validated batch 8 batch loss 24.8918476\n",
      "Validated batch 9 batch loss 25.0178146\n",
      "Validated batch 10 batch loss 25.2665215\n",
      "Validated batch 11 batch loss 24.9234123\n",
      "Validated batch 12 batch loss 24.6542702\n",
      "Validated batch 13 batch loss 24.9274464\n",
      "Validated batch 14 batch loss 24.8038597\n",
      "Validated batch 15 batch loss 25.3956108\n",
      "Validated batch 16 batch loss 25.0725937\n",
      "Validated batch 17 batch loss 25.327919\n",
      "Validated batch 18 batch loss 25.3337326\n",
      "Validated batch 19 batch loss 25.3016071\n",
      "Validated batch 20 batch loss 24.5433064\n",
      "Validated batch 21 batch loss 25.5065651\n",
      "Validated batch 22 batch loss 24.0645771\n",
      "Validated batch 23 batch loss 25.772089\n",
      "Validated batch 24 batch loss 25.8276939\n",
      "Validated batch 25 batch loss 25.362484\n",
      "Validated batch 26 batch loss 25.4632492\n",
      "Validated batch 27 batch loss 25.354845\n",
      "Validated batch 28 batch loss 25.633194\n",
      "Validated batch 29 batch loss 25.5538788\n",
      "Validated batch 30 batch loss 25.4778862\n",
      "Validated batch 31 batch loss 24.8419609\n",
      "Validated batch 32 batch loss 25.2243919\n",
      "Validated batch 33 batch loss 25.0272388\n",
      "Validated batch 34 batch loss 25.4705601\n",
      "Validated batch 35 batch loss 24.8510208\n",
      "Validated batch 36 batch loss 25.0184803\n",
      "Validated batch 37 batch loss 24.7379608\n",
      "Validated batch 38 batch loss 25.2498093\n",
      "Validated batch 39 batch loss 25.7097492\n",
      "Validated batch 40 batch loss 25.7088585\n",
      "Validated batch 41 batch loss 25.3799744\n",
      "Validated batch 42 batch loss 25.8450356\n",
      "Validated batch 43 batch loss 25.6410179\n",
      "Validated batch 44 batch loss 25.7777481\n",
      "Validated batch 45 batch loss 24.9308414\n",
      "Validated batch 46 batch loss 24.9236622\n",
      "Validated batch 47 batch loss 24.8369236\n",
      "Validated batch 48 batch loss 24.7307434\n",
      "Validated batch 49 batch loss 25.3074684\n",
      "Validated batch 50 batch loss 25.4373398\n",
      "Validated batch 51 batch loss 25.0738239\n",
      "Validated batch 52 batch loss 25.3082809\n",
      "Validated batch 53 batch loss 25.4936638\n",
      "Validated batch 54 batch loss 25.3951759\n",
      "Validated batch 55 batch loss 25.1267433\n",
      "Validated batch 56 batch loss 25.2481232\n",
      "Validated batch 57 batch loss 24.9248066\n",
      "Validated batch 58 batch loss 25.1996803\n",
      "Validated batch 59 batch loss 25.1300087\n",
      "Validated batch 60 batch loss 25.5657196\n",
      "Validated batch 61 batch loss 25.4574623\n",
      "Validated batch 62 batch loss 25.0705833\n",
      "Validated batch 63 batch loss 25.5742264\n",
      "Validated batch 64 batch loss 24.5747623\n",
      "Validated batch 65 batch loss 25.6416359\n",
      "Validated batch 66 batch loss 25.8450584\n",
      "Validated batch 67 batch loss 25.4541626\n",
      "Validated batch 68 batch loss 25.640831\n",
      "Validated batch 69 batch loss 25.0208626\n",
      "Validated batch 70 batch loss 25.1855698\n",
      "Validated batch 71 batch loss 25.2325935\n",
      "Validated batch 72 batch loss 24.7202015\n",
      "Validated batch 73 batch loss 24.41012\n",
      "Validated batch 74 batch loss 24.836565\n",
      "Validated batch 75 batch loss 25.4733772\n",
      "Validated batch 76 batch loss 24.8586674\n",
      "Validated batch 77 batch loss 25.3601799\n",
      "Validated batch 78 batch loss 25.7437687\n",
      "Validated batch 79 batch loss 25.5032692\n",
      "Validated batch 80 batch loss 25.5038223\n",
      "Validated batch 81 batch loss 24.7775803\n",
      "Validated batch 82 batch loss 25.1919479\n",
      "Validated batch 83 batch loss 25.3352966\n",
      "Validated batch 84 batch loss 25.4160805\n",
      "Validated batch 85 batch loss 25.6631947\n",
      "Validated batch 86 batch loss 25.0818501\n",
      "Validated batch 87 batch loss 25.512867\n",
      "Validated batch 88 batch loss 25.8450565\n",
      "Validated batch 89 batch loss 25.2180824\n",
      "Validated batch 90 batch loss 25.3551979\n",
      "Validated batch 91 batch loss 25.6371803\n",
      "Validated batch 92 batch loss 25.6002808\n",
      "Validated batch 93 batch loss 25.6407967\n",
      "Validated batch 94 batch loss 25.2981491\n",
      "Validated batch 95 batch loss 25.1678638\n",
      "Validated batch 96 batch loss 24.9855461\n",
      "Validated batch 97 batch loss 24.8294468\n",
      "Validated batch 98 batch loss 25.2145329\n",
      "Validated batch 99 batch loss 24.8852787\n",
      "Validated batch 100 batch loss 25.4412766\n",
      "Validated batch 101 batch loss 25.6415272\n",
      "Validated batch 102 batch loss 25.2166519\n",
      "Validated batch 103 batch loss 25.2612\n",
      "Validated batch 104 batch loss 24.817461\n",
      "Validated batch 105 batch loss 25.049448\n",
      "Validated batch 106 batch loss 25.327549\n",
      "Validated batch 107 batch loss 25.4114304\n",
      "Validated batch 108 batch loss 25.3950119\n",
      "Validated batch 109 batch loss 25.0077381\n",
      "Validated batch 110 batch loss 24.5759392\n",
      "Validated batch 111 batch loss 24.8289871\n",
      "Validated batch 112 batch loss 25.3971195\n",
      "Validated batch 113 batch loss 24.8601837\n",
      "Validated batch 114 batch loss 24.9428291\n",
      "Validated batch 115 batch loss 25.0026588\n",
      "Validated batch 116 batch loss 25.2367954\n",
      "Validated batch 117 batch loss 25.2712059\n",
      "Validated batch 118 batch loss 24.9999466\n",
      "Validated batch 119 batch loss 25.5106621\n",
      "Validated batch 120 batch loss 25.8451424\n",
      "Validated batch 121 batch loss 25.7278366\n",
      "Validated batch 122 batch loss 25.4564972\n",
      "Validated batch 123 batch loss 25.3604698\n",
      "Validated batch 124 batch loss 25.5708885\n",
      "Validated batch 125 batch loss 25.5913391\n",
      "Validated batch 126 batch loss 25.8448048\n",
      "Validated batch 127 batch loss 25.6447296\n",
      "Validated batch 128 batch loss 24.5452843\n",
      "Validated batch 129 batch loss 25.5316811\n",
      "Validated batch 130 batch loss 25.2167759\n",
      "Validated batch 131 batch loss 25.1912022\n",
      "Validated batch 132 batch loss 25.2935944\n",
      "Validated batch 133 batch loss 24.5981064\n",
      "Validated batch 134 batch loss 24.5393848\n",
      "Validated batch 135 batch loss 25.8114548\n",
      "Validated batch 136 batch loss 25.141449\n",
      "Validated batch 137 batch loss 25.7540512\n",
      "Validated batch 138 batch loss 25.3268013\n",
      "Validated batch 139 batch loss 24.9092064\n",
      "Validated batch 140 batch loss 25.3481712\n",
      "Validated batch 141 batch loss 25.3207188\n",
      "Validated batch 142 batch loss 25.5049019\n",
      "Validated batch 143 batch loss 25.404026\n",
      "Validated batch 144 batch loss 25.4251328\n",
      "Validated batch 145 batch loss 25.1072655\n",
      "Validated batch 146 batch loss 25.7356\n",
      "Validated batch 147 batch loss 25.1459427\n",
      "Validated batch 148 batch loss 24.877739\n",
      "Validated batch 149 batch loss 25.2468891\n",
      "Validated batch 150 batch loss 24.9868126\n",
      "Validated batch 151 batch loss 24.0439548\n",
      "Validated batch 152 batch loss 25.6466312\n",
      "Validated batch 153 batch loss 25.7010651\n",
      "Validated batch 154 batch loss 25.2082329\n",
      "Validated batch 155 batch loss 25.2723846\n",
      "Validated batch 156 batch loss 25.468811\n",
      "Validated batch 157 batch loss 25.6744461\n",
      "Validated batch 158 batch loss 25.643959\n",
      "Validated batch 159 batch loss 25.7504616\n",
      "Validated batch 160 batch loss 25.5741577\n",
      "Validated batch 161 batch loss 24.4659443\n",
      "Validated batch 162 batch loss 25.4226761\n",
      "Validated batch 163 batch loss 25.0723743\n",
      "Validated batch 164 batch loss 24.9000187\n",
      "Validated batch 165 batch loss 24.8055115\n",
      "Validated batch 166 batch loss 25.0416679\n",
      "Validated batch 167 batch loss 25.404068\n",
      "Validated batch 168 batch loss 24.9850636\n",
      "Validated batch 169 batch loss 25.5629578\n",
      "Validated batch 170 batch loss 25.3961563\n",
      "Validated batch 171 batch loss 25.3538761\n",
      "Validated batch 172 batch loss 25.6750336\n",
      "Validated batch 173 batch loss 25.6797256\n",
      "Validated batch 174 batch loss 24.9100266\n",
      "Validated batch 175 batch loss 25.1432648\n",
      "Validated batch 176 batch loss 25.7619247\n",
      "Validated batch 177 batch loss 24.9979801\n",
      "Validated batch 178 batch loss 25.3674812\n",
      "Validated batch 179 batch loss 24.9110432\n",
      "Validated batch 180 batch loss 25.6697636\n",
      "Validated batch 181 batch loss 24.7994556\n",
      "Validated batch 182 batch loss 25.2997322\n",
      "Validated batch 183 batch loss 25.022831\n",
      "Validated batch 184 batch loss 25.2932663\n",
      "Validated batch 185 batch loss 21.3770084\n",
      "Epoch 2 val loss 25.223907470703125\n",
      "Epoch 2 completed in 764.91 seconds\n",
      "Model ./model_simplebase-epoch-2-loss-25.2239.h5 saved.\n",
      "Start epoch 3 with learning rate 0.0007\n",
      "Trained batch 1 batch loss 23.6508045 epoch total loss 23.6508045\n",
      "Trained batch 2 batch loss 23.5717392 epoch total loss 23.6112709\n",
      "Trained batch 3 batch loss 23.7585526 epoch total loss 23.6603642\n",
      "Trained batch 4 batch loss 23.2515526 epoch total loss 23.5581627\n",
      "Trained batch 5 batch loss 23.5006809 epoch total loss 23.5466652\n",
      "Trained batch 6 batch loss 23.361311 epoch total loss 23.5157719\n",
      "Trained batch 7 batch loss 23.3920517 epoch total loss 23.4980984\n",
      "Trained batch 8 batch loss 23.1207275 epoch total loss 23.4509277\n",
      "Trained batch 9 batch loss 23.2943249 epoch total loss 23.433527\n",
      "Trained batch 10 batch loss 23.2422104 epoch total loss 23.4143944\n",
      "Trained batch 11 batch loss 23.5268574 epoch total loss 23.4246197\n",
      "Trained batch 12 batch loss 22.9937668 epoch total loss 23.3887157\n",
      "Trained batch 13 batch loss 23.5409756 epoch total loss 23.4004288\n",
      "Trained batch 14 batch loss 23.4967346 epoch total loss 23.4073067\n",
      "Trained batch 15 batch loss 23.474102 epoch total loss 23.4117603\n",
      "Trained batch 16 batch loss 23.3900681 epoch total loss 23.4104042\n",
      "Trained batch 17 batch loss 23.370018 epoch total loss 23.4080296\n",
      "Trained batch 18 batch loss 23.2554092 epoch total loss 23.3995495\n",
      "Trained batch 19 batch loss 23.0519218 epoch total loss 23.3812523\n",
      "Trained batch 20 batch loss 22.9593945 epoch total loss 23.3601589\n",
      "Trained batch 21 batch loss 22.9057789 epoch total loss 23.338522\n",
      "Trained batch 22 batch loss 23.1648788 epoch total loss 23.3306293\n",
      "Trained batch 23 batch loss 22.9630184 epoch total loss 23.3146477\n",
      "Trained batch 24 batch loss 22.6673985 epoch total loss 23.2876797\n",
      "Trained batch 25 batch loss 22.9401665 epoch total loss 23.2737789\n",
      "Trained batch 26 batch loss 23.0539474 epoch total loss 23.2653236\n",
      "Trained batch 27 batch loss 23.1783466 epoch total loss 23.2621021\n",
      "Trained batch 28 batch loss 23.0602875 epoch total loss 23.2548962\n",
      "Trained batch 29 batch loss 23.0221443 epoch total loss 23.24687\n",
      "Trained batch 30 batch loss 22.1967525 epoch total loss 23.2118664\n",
      "Trained batch 31 batch loss 23.0092373 epoch total loss 23.2053299\n",
      "Trained batch 32 batch loss 22.971056 epoch total loss 23.1980095\n",
      "Trained batch 33 batch loss 22.5284519 epoch total loss 23.1777191\n",
      "Trained batch 34 batch loss 22.9241943 epoch total loss 23.1702633\n",
      "Trained batch 35 batch loss 22.3323956 epoch total loss 23.1463242\n",
      "Trained batch 36 batch loss 22.1411152 epoch total loss 23.1184\n",
      "Trained batch 37 batch loss 22.7658234 epoch total loss 23.1088715\n",
      "Trained batch 38 batch loss 22.8709984 epoch total loss 23.1026115\n",
      "Trained batch 39 batch loss 22.9423676 epoch total loss 23.0985031\n",
      "Trained batch 40 batch loss 22.9173546 epoch total loss 23.0939751\n",
      "Trained batch 41 batch loss 22.8218269 epoch total loss 23.0873375\n",
      "Trained batch 42 batch loss 22.4499054 epoch total loss 23.0721588\n",
      "Trained batch 43 batch loss 22.2980938 epoch total loss 23.0541573\n",
      "Trained batch 44 batch loss 22.44594 epoch total loss 23.0403347\n",
      "Trained batch 45 batch loss 22.4720936 epoch total loss 23.0277081\n",
      "Trained batch 46 batch loss 22.291357 epoch total loss 23.0117\n",
      "Trained batch 47 batch loss 22.4360886 epoch total loss 22.9994526\n",
      "Trained batch 48 batch loss 22.4212685 epoch total loss 22.9874058\n",
      "Trained batch 49 batch loss 22.3414021 epoch total loss 22.9742241\n",
      "Trained batch 50 batch loss 22.1105576 epoch total loss 22.9569511\n",
      "Trained batch 51 batch loss 22.3981953 epoch total loss 22.9459953\n",
      "Trained batch 52 batch loss 22.2913284 epoch total loss 22.9334068\n",
      "Trained batch 53 batch loss 22.6609612 epoch total loss 22.9282665\n",
      "Trained batch 54 batch loss 22.6877632 epoch total loss 22.9238129\n",
      "Trained batch 55 batch loss 22.5343437 epoch total loss 22.9167309\n",
      "Trained batch 56 batch loss 21.9566517 epoch total loss 22.8995857\n",
      "Trained batch 57 batch loss 22.4383202 epoch total loss 22.8914948\n",
      "Trained batch 58 batch loss 22.7795143 epoch total loss 22.8895645\n",
      "Trained batch 59 batch loss 22.5858116 epoch total loss 22.8844166\n",
      "Trained batch 60 batch loss 22.4300957 epoch total loss 22.8768425\n",
      "Trained batch 61 batch loss 22.2730675 epoch total loss 22.8669453\n",
      "Trained batch 62 batch loss 22.2447643 epoch total loss 22.8569107\n",
      "Trained batch 63 batch loss 22.1291618 epoch total loss 22.8453579\n",
      "Trained batch 64 batch loss 22.1845894 epoch total loss 22.8350334\n",
      "Trained batch 65 batch loss 21.5988331 epoch total loss 22.8160152\n",
      "Trained batch 66 batch loss 22.033989 epoch total loss 22.8041668\n",
      "Trained batch 67 batch loss 22.053896 epoch total loss 22.7929688\n",
      "Trained batch 68 batch loss 22.6033821 epoch total loss 22.7901802\n",
      "Trained batch 69 batch loss 22.0863705 epoch total loss 22.7799816\n",
      "Trained batch 70 batch loss 22.418375 epoch total loss 22.7748146\n",
      "Trained batch 71 batch loss 22.4904346 epoch total loss 22.7708111\n",
      "Trained batch 72 batch loss 22.2087097 epoch total loss 22.7630043\n",
      "Trained batch 73 batch loss 22.2508698 epoch total loss 22.7559891\n",
      "Trained batch 74 batch loss 22.5028248 epoch total loss 22.7525673\n",
      "Trained batch 75 batch loss 22.149765 epoch total loss 22.7445297\n",
      "Trained batch 76 batch loss 21.5872402 epoch total loss 22.7293034\n",
      "Trained batch 77 batch loss 22.1477261 epoch total loss 22.7217503\n",
      "Trained batch 78 batch loss 21.6615105 epoch total loss 22.7081566\n",
      "Trained batch 79 batch loss 22.3012123 epoch total loss 22.7030067\n",
      "Trained batch 80 batch loss 22.0659027 epoch total loss 22.6950417\n",
      "Trained batch 81 batch loss 22.2324028 epoch total loss 22.6893311\n",
      "Trained batch 82 batch loss 21.888504 epoch total loss 22.6795654\n",
      "Trained batch 83 batch loss 21.9125195 epoch total loss 22.6703243\n",
      "Trained batch 84 batch loss 22.0288162 epoch total loss 22.6626854\n",
      "Trained batch 85 batch loss 22.2086868 epoch total loss 22.6573448\n",
      "Trained batch 86 batch loss 21.8381557 epoch total loss 22.6478195\n",
      "Trained batch 87 batch loss 22.1785679 epoch total loss 22.6424274\n",
      "Trained batch 88 batch loss 21.8513069 epoch total loss 22.6334362\n",
      "Trained batch 89 batch loss 21.9662609 epoch total loss 22.6259403\n",
      "Trained batch 90 batch loss 21.4340115 epoch total loss 22.6126976\n",
      "Trained batch 91 batch loss 21.1145973 epoch total loss 22.5962353\n",
      "Trained batch 92 batch loss 21.4492645 epoch total loss 22.5837669\n",
      "Trained batch 93 batch loss 21.193018 epoch total loss 22.5688133\n",
      "Trained batch 94 batch loss 21.4149685 epoch total loss 22.5565395\n",
      "Trained batch 95 batch loss 21.8231411 epoch total loss 22.5488205\n",
      "Trained batch 96 batch loss 21.9878273 epoch total loss 22.5429764\n",
      "Trained batch 97 batch loss 21.7419128 epoch total loss 22.5347176\n",
      "Trained batch 98 batch loss 22.0641098 epoch total loss 22.5299168\n",
      "Trained batch 99 batch loss 21.9778767 epoch total loss 22.5243397\n",
      "Trained batch 100 batch loss 21.9813194 epoch total loss 22.5189095\n",
      "Trained batch 101 batch loss 21.652153 epoch total loss 22.5103264\n",
      "Trained batch 102 batch loss 21.3890629 epoch total loss 22.4993343\n",
      "Trained batch 103 batch loss 21.5571575 epoch total loss 22.4901867\n",
      "Trained batch 104 batch loss 21.2838631 epoch total loss 22.4785881\n",
      "Trained batch 105 batch loss 21.258215 epoch total loss 22.4669666\n",
      "Trained batch 106 batch loss 21.5087299 epoch total loss 22.4579277\n",
      "Trained batch 107 batch loss 21.4640579 epoch total loss 22.4486389\n",
      "Trained batch 108 batch loss 21.0573044 epoch total loss 22.4357567\n",
      "Trained batch 109 batch loss 20.9034157 epoch total loss 22.4216976\n",
      "Trained batch 110 batch loss 21.4793339 epoch total loss 22.4131298\n",
      "Trained batch 111 batch loss 21.4984474 epoch total loss 22.4048901\n",
      "Trained batch 112 batch loss 21.1711407 epoch total loss 22.3938751\n",
      "Trained batch 113 batch loss 21.0740662 epoch total loss 22.3821945\n",
      "Trained batch 114 batch loss 21.5334778 epoch total loss 22.3747501\n",
      "Trained batch 115 batch loss 21.253397 epoch total loss 22.3649979\n",
      "Trained batch 116 batch loss 21.4062328 epoch total loss 22.3567333\n",
      "Trained batch 117 batch loss 21.2861423 epoch total loss 22.3475838\n",
      "Trained batch 118 batch loss 21.0775223 epoch total loss 22.3368206\n",
      "Trained batch 119 batch loss 21.0850868 epoch total loss 22.3263035\n",
      "Trained batch 120 batch loss 21.0900326 epoch total loss 22.3160019\n",
      "Trained batch 121 batch loss 21.2955685 epoch total loss 22.3075695\n",
      "Trained batch 122 batch loss 20.6054554 epoch total loss 22.2936172\n",
      "Trained batch 123 batch loss 21.203373 epoch total loss 22.2847538\n",
      "Trained batch 124 batch loss 21.3475323 epoch total loss 22.277195\n",
      "Trained batch 125 batch loss 21.0702934 epoch total loss 22.26754\n",
      "Trained batch 126 batch loss 21.109869 epoch total loss 22.2583504\n",
      "Trained batch 127 batch loss 21.1273823 epoch total loss 22.2494469\n",
      "Trained batch 128 batch loss 20.8696861 epoch total loss 22.2386665\n",
      "Trained batch 129 batch loss 21.3693199 epoch total loss 22.2319279\n",
      "Trained batch 130 batch loss 20.7898865 epoch total loss 22.2208347\n",
      "Trained batch 131 batch loss 21.1504898 epoch total loss 22.2126637\n",
      "Trained batch 132 batch loss 20.9239 epoch total loss 22.2029\n",
      "Trained batch 133 batch loss 20.1008759 epoch total loss 22.1870937\n",
      "Trained batch 134 batch loss 20.4629784 epoch total loss 22.1742268\n",
      "Trained batch 135 batch loss 20.7097511 epoch total loss 22.1633797\n",
      "Trained batch 136 batch loss 20.9508781 epoch total loss 22.1544647\n",
      "Trained batch 137 batch loss 21.2791481 epoch total loss 22.1480732\n",
      "Trained batch 138 batch loss 20.5263901 epoch total loss 22.136322\n",
      "Trained batch 139 batch loss 20.9839668 epoch total loss 22.1280308\n",
      "Trained batch 140 batch loss 21.1553383 epoch total loss 22.1210842\n",
      "Trained batch 141 batch loss 21.0413723 epoch total loss 22.1134243\n",
      "Trained batch 142 batch loss 20.7849846 epoch total loss 22.1040688\n",
      "Trained batch 143 batch loss 20.5839882 epoch total loss 22.0934391\n",
      "Trained batch 144 batch loss 20.7043 epoch total loss 22.0837936\n",
      "Trained batch 145 batch loss 20.7401237 epoch total loss 22.0745277\n",
      "Trained batch 146 batch loss 20.7147331 epoch total loss 22.0652142\n",
      "Trained batch 147 batch loss 20.8441467 epoch total loss 22.0569077\n",
      "Trained batch 148 batch loss 20.7260132 epoch total loss 22.0479164\n",
      "Trained batch 149 batch loss 20.8333855 epoch total loss 22.0397663\n",
      "Trained batch 150 batch loss 20.7381535 epoch total loss 22.0310879\n",
      "Trained batch 151 batch loss 20.8613014 epoch total loss 22.0233402\n",
      "Trained batch 152 batch loss 20.8082886 epoch total loss 22.0153465\n",
      "Trained batch 153 batch loss 20.5278931 epoch total loss 22.0056248\n",
      "Trained batch 154 batch loss 20.9369698 epoch total loss 21.9986858\n",
      "Trained batch 155 batch loss 20.5815697 epoch total loss 21.989542\n",
      "Trained batch 156 batch loss 20.822485 epoch total loss 21.9820614\n",
      "Trained batch 157 batch loss 20.7739048 epoch total loss 21.9743671\n",
      "Trained batch 158 batch loss 20.6105385 epoch total loss 21.9657345\n",
      "Trained batch 159 batch loss 20.5832863 epoch total loss 21.9570408\n",
      "Trained batch 160 batch loss 20.5773659 epoch total loss 21.9484177\n",
      "Trained batch 161 batch loss 20.6416817 epoch total loss 21.9403\n",
      "Trained batch 162 batch loss 20.6781693 epoch total loss 21.9325104\n",
      "Trained batch 163 batch loss 20.7918987 epoch total loss 21.9255123\n",
      "Trained batch 164 batch loss 20.4151459 epoch total loss 21.9163036\n",
      "Trained batch 165 batch loss 20.3598938 epoch total loss 21.9068699\n",
      "Trained batch 166 batch loss 20.7256088 epoch total loss 21.8997536\n",
      "Trained batch 167 batch loss 20.6608925 epoch total loss 21.8923359\n",
      "Trained batch 168 batch loss 21.1043243 epoch total loss 21.8876438\n",
      "Trained batch 169 batch loss 21.5652771 epoch total loss 21.8857365\n",
      "Trained batch 170 batch loss 21.4374619 epoch total loss 21.8831\n",
      "Trained batch 171 batch loss 21.2271194 epoch total loss 21.8792629\n",
      "Trained batch 172 batch loss 21.3275204 epoch total loss 21.8760567\n",
      "Trained batch 173 batch loss 20.2940254 epoch total loss 21.8669109\n",
      "Trained batch 174 batch loss 20.1909637 epoch total loss 21.8572788\n",
      "Trained batch 175 batch loss 20.1724873 epoch total loss 21.8476524\n",
      "Trained batch 176 batch loss 20.3783493 epoch total loss 21.839304\n",
      "Trained batch 177 batch loss 20.250843 epoch total loss 21.8303299\n",
      "Trained batch 178 batch loss 19.856657 epoch total loss 21.8192406\n",
      "Trained batch 179 batch loss 19.8579807 epoch total loss 21.8082848\n",
      "Trained batch 180 batch loss 20.3415527 epoch total loss 21.8001366\n",
      "Trained batch 181 batch loss 19.9561405 epoch total loss 21.7899475\n",
      "Trained batch 182 batch loss 20.0627384 epoch total loss 21.7804565\n",
      "Trained batch 183 batch loss 19.7709618 epoch total loss 21.7694759\n",
      "Trained batch 184 batch loss 19.5843258 epoch total loss 21.7576\n",
      "Trained batch 185 batch loss 20.1554871 epoch total loss 21.7489395\n",
      "Trained batch 186 batch loss 20.4204445 epoch total loss 21.7417984\n",
      "Trained batch 187 batch loss 20.4628487 epoch total loss 21.7349586\n",
      "Trained batch 188 batch loss 20.0351868 epoch total loss 21.7259178\n",
      "Trained batch 189 batch loss 19.7677307 epoch total loss 21.7155571\n",
      "Trained batch 190 batch loss 19.9709949 epoch total loss 21.7063751\n",
      "Trained batch 191 batch loss 20.2183857 epoch total loss 21.6985855\n",
      "Trained batch 192 batch loss 19.8212147 epoch total loss 21.6888065\n",
      "Trained batch 193 batch loss 20.099102 epoch total loss 21.6805706\n",
      "Trained batch 194 batch loss 19.6830063 epoch total loss 21.6702747\n",
      "Trained batch 195 batch loss 19.9516163 epoch total loss 21.6614609\n",
      "Trained batch 196 batch loss 20.2135277 epoch total loss 21.6540718\n",
      "Trained batch 197 batch loss 19.7778759 epoch total loss 21.6445484\n",
      "Trained batch 198 batch loss 19.8728828 epoch total loss 21.635601\n",
      "Trained batch 199 batch loss 20.0447865 epoch total loss 21.6276073\n",
      "Trained batch 200 batch loss 19.8060875 epoch total loss 21.6185017\n",
      "Trained batch 201 batch loss 19.9763336 epoch total loss 21.6103325\n",
      "Trained batch 202 batch loss 19.9484482 epoch total loss 21.6021042\n",
      "Trained batch 203 batch loss 19.9816208 epoch total loss 21.59412\n",
      "Trained batch 204 batch loss 19.7526569 epoch total loss 21.5850925\n",
      "Trained batch 205 batch loss 19.8462219 epoch total loss 21.5766106\n",
      "Trained batch 206 batch loss 19.9397 epoch total loss 21.5686646\n",
      "Trained batch 207 batch loss 19.6629524 epoch total loss 21.5594597\n",
      "Trained batch 208 batch loss 19.7097588 epoch total loss 21.5505676\n",
      "Trained batch 209 batch loss 19.9271049 epoch total loss 21.5428\n",
      "Trained batch 210 batch loss 19.9978027 epoch total loss 21.5354443\n",
      "Trained batch 211 batch loss 20.0689869 epoch total loss 21.5284939\n",
      "Trained batch 212 batch loss 20.0984 epoch total loss 21.5217495\n",
      "Trained batch 213 batch loss 20.0176067 epoch total loss 21.5146866\n",
      "Trained batch 214 batch loss 19.6305447 epoch total loss 21.5058823\n",
      "Trained batch 215 batch loss 19.8126678 epoch total loss 21.4980068\n",
      "Trained batch 216 batch loss 19.9079533 epoch total loss 21.4906445\n",
      "Trained batch 217 batch loss 19.7100391 epoch total loss 21.4824371\n",
      "Trained batch 218 batch loss 19.7998199 epoch total loss 21.47472\n",
      "Trained batch 219 batch loss 19.5600471 epoch total loss 21.4659767\n",
      "Trained batch 220 batch loss 19.6463585 epoch total loss 21.4577065\n",
      "Trained batch 221 batch loss 19.5823269 epoch total loss 21.4492207\n",
      "Trained batch 222 batch loss 19.6978569 epoch total loss 21.4413319\n",
      "Trained batch 223 batch loss 19.8181324 epoch total loss 21.4340534\n",
      "Trained batch 224 batch loss 19.5817032 epoch total loss 21.4257832\n",
      "Trained batch 225 batch loss 19.5976849 epoch total loss 21.4176579\n",
      "Trained batch 226 batch loss 19.1440678 epoch total loss 21.4075985\n",
      "Trained batch 227 batch loss 19.4476624 epoch total loss 21.3989639\n",
      "Trained batch 228 batch loss 19.540308 epoch total loss 21.3908138\n",
      "Trained batch 229 batch loss 19.4047794 epoch total loss 21.3821411\n",
      "Trained batch 230 batch loss 19.2872086 epoch total loss 21.3730316\n",
      "Trained batch 231 batch loss 19.7004986 epoch total loss 21.3657932\n",
      "Trained batch 232 batch loss 19.3986664 epoch total loss 21.3573132\n",
      "Trained batch 233 batch loss 19.6539669 epoch total loss 21.35\n",
      "Trained batch 234 batch loss 19.0423927 epoch total loss 21.3401394\n",
      "Trained batch 235 batch loss 18.8520126 epoch total loss 21.3295517\n",
      "Trained batch 236 batch loss 19.574131 epoch total loss 21.3221149\n",
      "Trained batch 237 batch loss 19.4345303 epoch total loss 21.3141499\n",
      "Trained batch 238 batch loss 19.5701332 epoch total loss 21.3068237\n",
      "Trained batch 239 batch loss 19.7616348 epoch total loss 21.3003578\n",
      "Trained batch 240 batch loss 19.3723717 epoch total loss 21.292326\n",
      "Trained batch 241 batch loss 19.3448257 epoch total loss 21.2842445\n",
      "Trained batch 242 batch loss 19.3166656 epoch total loss 21.2761154\n",
      "Trained batch 243 batch loss 19.4310894 epoch total loss 21.2685223\n",
      "Trained batch 244 batch loss 19.2820129 epoch total loss 21.2603817\n",
      "Trained batch 245 batch loss 19.3860321 epoch total loss 21.2527332\n",
      "Trained batch 246 batch loss 19.2394295 epoch total loss 21.2445469\n",
      "Trained batch 247 batch loss 18.7767448 epoch total loss 21.2345562\n",
      "Trained batch 248 batch loss 18.5621433 epoch total loss 21.2237797\n",
      "Trained batch 249 batch loss 18.6257248 epoch total loss 21.2133465\n",
      "Trained batch 250 batch loss 18.8202629 epoch total loss 21.2037735\n",
      "Trained batch 251 batch loss 18.4934578 epoch total loss 21.192976\n",
      "Trained batch 252 batch loss 18.2336559 epoch total loss 21.1812344\n",
      "Trained batch 253 batch loss 17.9674606 epoch total loss 21.1685295\n",
      "Trained batch 254 batch loss 17.8232708 epoch total loss 21.1553593\n",
      "Trained batch 255 batch loss 18.2517471 epoch total loss 21.1439743\n",
      "Trained batch 256 batch loss 19.2280197 epoch total loss 21.1364899\n",
      "Trained batch 257 batch loss 19.7919178 epoch total loss 21.131258\n",
      "Trained batch 258 batch loss 19.0537014 epoch total loss 21.1232052\n",
      "Trained batch 259 batch loss 18.9952564 epoch total loss 21.1149902\n",
      "Trained batch 260 batch loss 19.1379223 epoch total loss 21.1073837\n",
      "Trained batch 261 batch loss 19.0610542 epoch total loss 21.0995445\n",
      "Trained batch 262 batch loss 18.9342136 epoch total loss 21.0912781\n",
      "Trained batch 263 batch loss 18.5103512 epoch total loss 21.0814648\n",
      "Trained batch 264 batch loss 19.0175877 epoch total loss 21.0736465\n",
      "Trained batch 265 batch loss 18.638813 epoch total loss 21.0644588\n",
      "Trained batch 266 batch loss 19.0435467 epoch total loss 21.05686\n",
      "Trained batch 267 batch loss 19.054821 epoch total loss 21.0493622\n",
      "Trained batch 268 batch loss 18.7816429 epoch total loss 21.0409012\n",
      "Trained batch 269 batch loss 18.8666592 epoch total loss 21.0328178\n",
      "Trained batch 270 batch loss 18.6587982 epoch total loss 21.024025\n",
      "Trained batch 271 batch loss 19.103796 epoch total loss 21.0169411\n",
      "Trained batch 272 batch loss 19.1625595 epoch total loss 21.0101223\n",
      "Trained batch 273 batch loss 18.6802711 epoch total loss 21.0015888\n",
      "Trained batch 274 batch loss 18.587347 epoch total loss 20.9927769\n",
      "Trained batch 275 batch loss 19.0772057 epoch total loss 20.9858112\n",
      "Trained batch 276 batch loss 19.0495987 epoch total loss 20.978796\n",
      "Trained batch 277 batch loss 18.7471256 epoch total loss 20.9707394\n",
      "Trained batch 278 batch loss 19.0600758 epoch total loss 20.9638672\n",
      "Trained batch 279 batch loss 18.785923 epoch total loss 20.9560623\n",
      "Trained batch 280 batch loss 18.333952 epoch total loss 20.9466972\n",
      "Trained batch 281 batch loss 18.5302887 epoch total loss 20.938097\n",
      "Trained batch 282 batch loss 18.9355183 epoch total loss 20.9309959\n",
      "Trained batch 283 batch loss 19.0330238 epoch total loss 20.9242897\n",
      "Trained batch 284 batch loss 18.9880543 epoch total loss 20.9174728\n",
      "Trained batch 285 batch loss 18.8911724 epoch total loss 20.9103642\n",
      "Trained batch 286 batch loss 18.8980026 epoch total loss 20.9033279\n",
      "Trained batch 287 batch loss 19.3775806 epoch total loss 20.8980103\n",
      "Trained batch 288 batch loss 18.8302975 epoch total loss 20.890831\n",
      "Trained batch 289 batch loss 18.4079647 epoch total loss 20.8822403\n",
      "Trained batch 290 batch loss 18.5536118 epoch total loss 20.8742104\n",
      "Trained batch 291 batch loss 18.0022297 epoch total loss 20.8643417\n",
      "Trained batch 292 batch loss 17.805294 epoch total loss 20.8538647\n",
      "Trained batch 293 batch loss 18.292881 epoch total loss 20.8451252\n",
      "Trained batch 294 batch loss 18.0056438 epoch total loss 20.8354683\n",
      "Trained batch 295 batch loss 17.3222389 epoch total loss 20.8235588\n",
      "Trained batch 296 batch loss 17.2654762 epoch total loss 20.8115387\n",
      "Trained batch 297 batch loss 17.5257683 epoch total loss 20.8004761\n",
      "Trained batch 298 batch loss 17.7592487 epoch total loss 20.7902699\n",
      "Trained batch 299 batch loss 18.3953438 epoch total loss 20.7822609\n",
      "Trained batch 300 batch loss 18.6491394 epoch total loss 20.7751503\n",
      "Trained batch 301 batch loss 18.7876186 epoch total loss 20.7685471\n",
      "Trained batch 302 batch loss 18.8935642 epoch total loss 20.7623386\n",
      "Trained batch 303 batch loss 18.7740974 epoch total loss 20.7557755\n",
      "Trained batch 304 batch loss 18.5052567 epoch total loss 20.748373\n",
      "Trained batch 305 batch loss 18.7315388 epoch total loss 20.7417603\n",
      "Trained batch 306 batch loss 18.5086708 epoch total loss 20.7344627\n",
      "Trained batch 307 batch loss 18.5605812 epoch total loss 20.7273808\n",
      "Trained batch 308 batch loss 18.7379551 epoch total loss 20.7209225\n",
      "Trained batch 309 batch loss 18.6855755 epoch total loss 20.7143345\n",
      "Trained batch 310 batch loss 18.6023178 epoch total loss 20.7075233\n",
      "Trained batch 311 batch loss 18.5568333 epoch total loss 20.7006073\n",
      "Trained batch 312 batch loss 18.5373421 epoch total loss 20.6936722\n",
      "Trained batch 313 batch loss 18.5207729 epoch total loss 20.6867313\n",
      "Trained batch 314 batch loss 18.3710213 epoch total loss 20.6793556\n",
      "Trained batch 315 batch loss 18.4969578 epoch total loss 20.6724281\n",
      "Trained batch 316 batch loss 18.3217201 epoch total loss 20.6649895\n",
      "Trained batch 317 batch loss 18.2659149 epoch total loss 20.657423\n",
      "Trained batch 318 batch loss 18.3049812 epoch total loss 20.6500244\n",
      "Trained batch 319 batch loss 18.3063774 epoch total loss 20.6426773\n",
      "Trained batch 320 batch loss 18.1310978 epoch total loss 20.6348286\n",
      "Trained batch 321 batch loss 18.1302891 epoch total loss 20.6270256\n",
      "Trained batch 322 batch loss 18.2037964 epoch total loss 20.6195\n",
      "Trained batch 323 batch loss 18.2993546 epoch total loss 20.6123161\n",
      "Trained batch 324 batch loss 18.0503 epoch total loss 20.6044102\n",
      "Trained batch 325 batch loss 17.919014 epoch total loss 20.5961456\n",
      "Trained batch 326 batch loss 18.0823517 epoch total loss 20.5884361\n",
      "Trained batch 327 batch loss 18.6223469 epoch total loss 20.5824242\n",
      "Trained batch 328 batch loss 18.5145512 epoch total loss 20.5761204\n",
      "Trained batch 329 batch loss 18.7010784 epoch total loss 20.5704212\n",
      "Trained batch 330 batch loss 18.455946 epoch total loss 20.5640144\n",
      "Trained batch 331 batch loss 18.8855286 epoch total loss 20.5589428\n",
      "Trained batch 332 batch loss 18.6090946 epoch total loss 20.5530701\n",
      "Trained batch 333 batch loss 18.0385818 epoch total loss 20.5455189\n",
      "Trained batch 334 batch loss 17.6807766 epoch total loss 20.5369415\n",
      "Trained batch 335 batch loss 18.2535915 epoch total loss 20.5301247\n",
      "Trained batch 336 batch loss 18.218111 epoch total loss 20.5232449\n",
      "Trained batch 337 batch loss 17.6677055 epoch total loss 20.5147705\n",
      "Trained batch 338 batch loss 18.0090675 epoch total loss 20.5073566\n",
      "Trained batch 339 batch loss 18.0496578 epoch total loss 20.5001087\n",
      "Trained batch 340 batch loss 17.9523048 epoch total loss 20.4926147\n",
      "Trained batch 341 batch loss 17.7337379 epoch total loss 20.4845238\n",
      "Trained batch 342 batch loss 17.9144211 epoch total loss 20.4770088\n",
      "Trained batch 343 batch loss 18.4166737 epoch total loss 20.4710026\n",
      "Trained batch 344 batch loss 17.8216476 epoch total loss 20.4633\n",
      "Trained batch 345 batch loss 18.0348721 epoch total loss 20.4562607\n",
      "Trained batch 346 batch loss 18.2211266 epoch total loss 20.4498024\n",
      "Trained batch 347 batch loss 18.1859741 epoch total loss 20.4432774\n",
      "Trained batch 348 batch loss 17.6926365 epoch total loss 20.4353752\n",
      "Trained batch 349 batch loss 17.7177925 epoch total loss 20.4275875\n",
      "Trained batch 350 batch loss 17.9061813 epoch total loss 20.4203835\n",
      "Trained batch 351 batch loss 17.2103252 epoch total loss 20.4112377\n",
      "Trained batch 352 batch loss 17.8000221 epoch total loss 20.40382\n",
      "Trained batch 353 batch loss 17.9959545 epoch total loss 20.397\n",
      "Trained batch 354 batch loss 18.4079418 epoch total loss 20.3913803\n",
      "Trained batch 355 batch loss 18.1097317 epoch total loss 20.3849525\n",
      "Trained batch 356 batch loss 18.2477093 epoch total loss 20.3789482\n",
      "Trained batch 357 batch loss 17.8480759 epoch total loss 20.3718605\n",
      "Trained batch 358 batch loss 17.6851654 epoch total loss 20.3643551\n",
      "Trained batch 359 batch loss 17.8227882 epoch total loss 20.357275\n",
      "Trained batch 360 batch loss 17.3896751 epoch total loss 20.3490314\n",
      "Trained batch 361 batch loss 17.7437134 epoch total loss 20.341814\n",
      "Trained batch 362 batch loss 17.7487259 epoch total loss 20.33465\n",
      "Trained batch 363 batch loss 17.7229881 epoch total loss 20.3274555\n",
      "Trained batch 364 batch loss 18.1956596 epoch total loss 20.3216\n",
      "Trained batch 365 batch loss 17.9784679 epoch total loss 20.3151817\n",
      "Trained batch 366 batch loss 17.3049946 epoch total loss 20.3069572\n",
      "Trained batch 367 batch loss 17.4684658 epoch total loss 20.299221\n",
      "Trained batch 368 batch loss 17.2341461 epoch total loss 20.2908936\n",
      "Trained batch 369 batch loss 17.6935692 epoch total loss 20.2838535\n",
      "Trained batch 370 batch loss 17.4637108 epoch total loss 20.2762318\n",
      "Trained batch 371 batch loss 17.4832497 epoch total loss 20.2687054\n",
      "Trained batch 372 batch loss 17.6690159 epoch total loss 20.2617168\n",
      "Trained batch 373 batch loss 17.6655731 epoch total loss 20.254755\n",
      "Trained batch 374 batch loss 17.7880325 epoch total loss 20.2481613\n",
      "Trained batch 375 batch loss 17.3455639 epoch total loss 20.2404213\n",
      "Trained batch 376 batch loss 17.5641327 epoch total loss 20.2333031\n",
      "Trained batch 377 batch loss 17.4703064 epoch total loss 20.2259731\n",
      "Trained batch 378 batch loss 17.499691 epoch total loss 20.2187595\n",
      "Trained batch 379 batch loss 17.7389259 epoch total loss 20.2122173\n",
      "Trained batch 380 batch loss 17.7177505 epoch total loss 20.2056522\n",
      "Trained batch 381 batch loss 17.752346 epoch total loss 20.199213\n",
      "Trained batch 382 batch loss 17.7668705 epoch total loss 20.1928463\n",
      "Trained batch 383 batch loss 17.7471962 epoch total loss 20.1864605\n",
      "Trained batch 384 batch loss 17.6969109 epoch total loss 20.1799774\n",
      "Trained batch 385 batch loss 17.6662445 epoch total loss 20.1734486\n",
      "Trained batch 386 batch loss 17.6024265 epoch total loss 20.1667881\n",
      "Trained batch 387 batch loss 17.2007446 epoch total loss 20.1591225\n",
      "Trained batch 388 batch loss 17.4386864 epoch total loss 20.1521111\n",
      "Trained batch 389 batch loss 17.4555855 epoch total loss 20.1451797\n",
      "Trained batch 390 batch loss 17.2383442 epoch total loss 20.1377258\n",
      "Trained batch 391 batch loss 17.249155 epoch total loss 20.1303368\n",
      "Trained batch 392 batch loss 17.5195847 epoch total loss 20.1236763\n",
      "Trained batch 393 batch loss 17.4975319 epoch total loss 20.1169949\n",
      "Trained batch 394 batch loss 17.4773312 epoch total loss 20.1102962\n",
      "Trained batch 395 batch loss 17.2531147 epoch total loss 20.1030617\n",
      "Trained batch 396 batch loss 17.3598785 epoch total loss 20.0961342\n",
      "Trained batch 397 batch loss 17.1536751 epoch total loss 20.0887241\n",
      "Trained batch 398 batch loss 16.7852211 epoch total loss 20.0804234\n",
      "Trained batch 399 batch loss 17.1920395 epoch total loss 20.0731831\n",
      "Trained batch 400 batch loss 16.9336147 epoch total loss 20.0653343\n",
      "Trained batch 401 batch loss 17.2574158 epoch total loss 20.0583324\n",
      "Trained batch 402 batch loss 16.8198719 epoch total loss 20.0502758\n",
      "Trained batch 403 batch loss 16.8031807 epoch total loss 20.0422192\n",
      "Trained batch 404 batch loss 17.1123524 epoch total loss 20.0349674\n",
      "Trained batch 405 batch loss 16.5646725 epoch total loss 20.0263977\n",
      "Trained batch 406 batch loss 16.8449059 epoch total loss 20.0185604\n",
      "Trained batch 407 batch loss 16.8583145 epoch total loss 20.0107956\n",
      "Trained batch 408 batch loss 16.9702606 epoch total loss 20.0033436\n",
      "Trained batch 409 batch loss 16.7025433 epoch total loss 19.9952736\n",
      "Trained batch 410 batch loss 16.7231483 epoch total loss 19.9872932\n",
      "Trained batch 411 batch loss 16.799017 epoch total loss 19.9795341\n",
      "Trained batch 412 batch loss 17.0969734 epoch total loss 19.972538\n",
      "Trained batch 413 batch loss 17.095377 epoch total loss 19.9655724\n",
      "Trained batch 414 batch loss 16.8603382 epoch total loss 19.9580708\n",
      "Trained batch 415 batch loss 16.2366714 epoch total loss 19.9491043\n",
      "Trained batch 416 batch loss 16.2911568 epoch total loss 19.9403095\n",
      "Trained batch 417 batch loss 16.5837135 epoch total loss 19.9322605\n",
      "Trained batch 418 batch loss 16.5569344 epoch total loss 19.9241848\n",
      "Trained batch 419 batch loss 16.5410385 epoch total loss 19.916111\n",
      "Trained batch 420 batch loss 16.5433407 epoch total loss 19.9080791\n",
      "Trained batch 421 batch loss 16.5809937 epoch total loss 19.900177\n",
      "Trained batch 422 batch loss 16.6032944 epoch total loss 19.8923645\n",
      "Trained batch 423 batch loss 16.8587379 epoch total loss 19.8851929\n",
      "Trained batch 424 batch loss 16.9122849 epoch total loss 19.8781815\n",
      "Trained batch 425 batch loss 17.0812569 epoch total loss 19.8716\n",
      "Trained batch 426 batch loss 17.092083 epoch total loss 19.8650742\n",
      "Trained batch 427 batch loss 16.0503712 epoch total loss 19.856142\n",
      "Trained batch 428 batch loss 15.7439938 epoch total loss 19.8465328\n",
      "Trained batch 429 batch loss 15.5427532 epoch total loss 19.8365021\n",
      "Trained batch 430 batch loss 16.7677574 epoch total loss 19.8293648\n",
      "Trained batch 431 batch loss 17.1563778 epoch total loss 19.8231621\n",
      "Trained batch 432 batch loss 17.3348255 epoch total loss 19.8174038\n",
      "Trained batch 433 batch loss 17.3408756 epoch total loss 19.8116837\n",
      "Trained batch 434 batch loss 16.5371704 epoch total loss 19.8041382\n",
      "Trained batch 435 batch loss 16.0550232 epoch total loss 19.7955189\n",
      "Trained batch 436 batch loss 16.61026 epoch total loss 19.7882137\n",
      "Trained batch 437 batch loss 16.6646862 epoch total loss 19.7810669\n",
      "Trained batch 438 batch loss 16.5866966 epoch total loss 19.7737751\n",
      "Trained batch 439 batch loss 16.5867615 epoch total loss 19.7665157\n",
      "Trained batch 440 batch loss 16.9181519 epoch total loss 19.7600403\n",
      "Trained batch 441 batch loss 16.7711697 epoch total loss 19.7532635\n",
      "Trained batch 442 batch loss 16.7544861 epoch total loss 19.7464809\n",
      "Trained batch 443 batch loss 16.7717094 epoch total loss 19.7397652\n",
      "Trained batch 444 batch loss 16.7747097 epoch total loss 19.7330856\n",
      "Trained batch 445 batch loss 16.7484322 epoch total loss 19.7263775\n",
      "Trained batch 446 batch loss 16.574337 epoch total loss 19.7193108\n",
      "Trained batch 447 batch loss 16.8416271 epoch total loss 19.7128735\n",
      "Trained batch 448 batch loss 16.9417915 epoch total loss 19.706686\n",
      "Trained batch 449 batch loss 16.7858562 epoch total loss 19.700182\n",
      "Trained batch 450 batch loss 16.5554943 epoch total loss 19.6931953\n",
      "Trained batch 451 batch loss 16.5387135 epoch total loss 19.6862011\n",
      "Trained batch 452 batch loss 16.5510406 epoch total loss 19.6792641\n",
      "Trained batch 453 batch loss 16.5759735 epoch total loss 19.6724148\n",
      "Trained batch 454 batch loss 16.7047138 epoch total loss 19.6658783\n",
      "Trained batch 455 batch loss 16.6225891 epoch total loss 19.6591911\n",
      "Trained batch 456 batch loss 16.2921028 epoch total loss 19.6518059\n",
      "Trained batch 457 batch loss 16.4231396 epoch total loss 19.6447411\n",
      "Trained batch 458 batch loss 16.3470764 epoch total loss 19.6375389\n",
      "Trained batch 459 batch loss 16.4456825 epoch total loss 19.6305847\n",
      "Trained batch 460 batch loss 16.3664398 epoch total loss 19.6234894\n",
      "Trained batch 461 batch loss 16.5431442 epoch total loss 19.616806\n",
      "Trained batch 462 batch loss 16.1897945 epoch total loss 19.6093884\n",
      "Trained batch 463 batch loss 15.8233967 epoch total loss 19.6012096\n",
      "Trained batch 464 batch loss 15.8502836 epoch total loss 19.5931263\n",
      "Trained batch 465 batch loss 14.9580765 epoch total loss 19.5831585\n",
      "Trained batch 466 batch loss 15.1819201 epoch total loss 19.5737133\n",
      "Trained batch 467 batch loss 16.6847439 epoch total loss 19.5675278\n",
      "Trained batch 468 batch loss 16.7205601 epoch total loss 19.5614433\n",
      "Trained batch 469 batch loss 16.7354984 epoch total loss 19.555418\n",
      "Trained batch 470 batch loss 16.0174866 epoch total loss 19.5478916\n",
      "Trained batch 471 batch loss 15.9512196 epoch total loss 19.5402546\n",
      "Trained batch 472 batch loss 16.2679443 epoch total loss 19.5333214\n",
      "Trained batch 473 batch loss 15.8239479 epoch total loss 19.5254803\n",
      "Trained batch 474 batch loss 15.9775925 epoch total loss 19.5179939\n",
      "Trained batch 475 batch loss 15.902585 epoch total loss 19.5103817\n",
      "Trained batch 476 batch loss 15.6713028 epoch total loss 19.5023155\n",
      "Trained batch 477 batch loss 15.9514542 epoch total loss 19.4948711\n",
      "Trained batch 478 batch loss 15.9855976 epoch total loss 19.4875298\n",
      "Trained batch 479 batch loss 16.1289425 epoch total loss 19.4805183\n",
      "Trained batch 480 batch loss 16.5246429 epoch total loss 19.4743595\n",
      "Trained batch 481 batch loss 16.0823345 epoch total loss 19.4673061\n",
      "Trained batch 482 batch loss 15.501606 epoch total loss 19.4590797\n",
      "Trained batch 483 batch loss 16.1341705 epoch total loss 19.4521942\n",
      "Trained batch 484 batch loss 15.9512672 epoch total loss 19.4449615\n",
      "Trained batch 485 batch loss 16.4386272 epoch total loss 19.4387627\n",
      "Trained batch 486 batch loss 16.1589146 epoch total loss 19.4320145\n",
      "Trained batch 487 batch loss 16.1163826 epoch total loss 19.4252052\n",
      "Trained batch 488 batch loss 16.0842533 epoch total loss 19.4183598\n",
      "Trained batch 489 batch loss 15.9830341 epoch total loss 19.411335\n",
      "Trained batch 490 batch loss 15.8774357 epoch total loss 19.4041214\n",
      "Trained batch 491 batch loss 15.6493168 epoch total loss 19.3964748\n",
      "Trained batch 492 batch loss 15.9018106 epoch total loss 19.38937\n",
      "Trained batch 493 batch loss 16.1845531 epoch total loss 19.3828697\n",
      "Trained batch 494 batch loss 16.5832767 epoch total loss 19.377203\n",
      "Trained batch 495 batch loss 16.4938564 epoch total loss 19.3713779\n",
      "Trained batch 496 batch loss 16.5565929 epoch total loss 19.3657036\n",
      "Trained batch 497 batch loss 16.3995285 epoch total loss 19.3597355\n",
      "Trained batch 498 batch loss 16.0264378 epoch total loss 19.3530407\n",
      "Trained batch 499 batch loss 15.4209747 epoch total loss 19.3451614\n",
      "Trained batch 500 batch loss 15.8189983 epoch total loss 19.33811\n",
      "Trained batch 501 batch loss 15.9972563 epoch total loss 19.33144\n",
      "Trained batch 502 batch loss 15.9508591 epoch total loss 19.324707\n",
      "Trained batch 503 batch loss 16.1397285 epoch total loss 19.3183746\n",
      "Trained batch 504 batch loss 16.2085495 epoch total loss 19.3122063\n",
      "Trained batch 505 batch loss 16.1554966 epoch total loss 19.305954\n",
      "Trained batch 506 batch loss 15.7398052 epoch total loss 19.2989063\n",
      "Trained batch 507 batch loss 15.880621 epoch total loss 19.2921658\n",
      "Trained batch 508 batch loss 15.4356079 epoch total loss 19.2845745\n",
      "Trained batch 509 batch loss 15.5789585 epoch total loss 19.2772942\n",
      "Trained batch 510 batch loss 16.0631599 epoch total loss 19.2709923\n",
      "Trained batch 511 batch loss 16.1730556 epoch total loss 19.2649288\n",
      "Trained batch 512 batch loss 16.3816681 epoch total loss 19.2592983\n",
      "Trained batch 513 batch loss 16.213129 epoch total loss 19.2533607\n",
      "Trained batch 514 batch loss 16.0439472 epoch total loss 19.2471161\n",
      "Trained batch 515 batch loss 16.3397064 epoch total loss 19.2414703\n",
      "Trained batch 516 batch loss 16.6869068 epoch total loss 19.2365189\n",
      "Trained batch 517 batch loss 16.1469402 epoch total loss 19.2305431\n",
      "Trained batch 518 batch loss 16.2556801 epoch total loss 19.2248\n",
      "Trained batch 519 batch loss 15.8954163 epoch total loss 19.2183857\n",
      "Trained batch 520 batch loss 15.6521053 epoch total loss 19.2115269\n",
      "Trained batch 521 batch loss 15.488224 epoch total loss 19.20438\n",
      "Trained batch 522 batch loss 15.5768089 epoch total loss 19.1974316\n",
      "Trained batch 523 batch loss 15.2363701 epoch total loss 19.1898575\n",
      "Trained batch 524 batch loss 14.631649 epoch total loss 19.18116\n",
      "Trained batch 525 batch loss 14.6497097 epoch total loss 19.1725273\n",
      "Trained batch 526 batch loss 15.1514015 epoch total loss 19.1648827\n",
      "Trained batch 527 batch loss 15.2767391 epoch total loss 19.157505\n",
      "Trained batch 528 batch loss 15.9359732 epoch total loss 19.1514015\n",
      "Trained batch 529 batch loss 16.2167988 epoch total loss 19.1458549\n",
      "Trained batch 530 batch loss 16.140604 epoch total loss 19.1401844\n",
      "Trained batch 531 batch loss 15.7791224 epoch total loss 19.1338558\n",
      "Trained batch 532 batch loss 16.2335739 epoch total loss 19.1284027\n",
      "Trained batch 533 batch loss 15.836977 epoch total loss 19.1222286\n",
      "Trained batch 534 batch loss 16.0434647 epoch total loss 19.1164627\n",
      "Trained batch 535 batch loss 15.7515383 epoch total loss 19.1101742\n",
      "Trained batch 536 batch loss 15.8392534 epoch total loss 19.1040707\n",
      "Trained batch 537 batch loss 15.9483 epoch total loss 19.0981941\n",
      "Trained batch 538 batch loss 15.4967594 epoch total loss 19.0915012\n",
      "Trained batch 539 batch loss 15.6755371 epoch total loss 19.0851631\n",
      "Trained batch 540 batch loss 15.3091478 epoch total loss 19.0781727\n",
      "Trained batch 541 batch loss 14.9891396 epoch total loss 19.0706139\n",
      "Trained batch 542 batch loss 15.2029915 epoch total loss 19.0634785\n",
      "Trained batch 543 batch loss 14.9858723 epoch total loss 19.0559692\n",
      "Trained batch 544 batch loss 15.2743273 epoch total loss 19.0490189\n",
      "Trained batch 545 batch loss 15.419713 epoch total loss 19.0423603\n",
      "Trained batch 546 batch loss 15.2286119 epoch total loss 19.0353737\n",
      "Trained batch 547 batch loss 15.5204124 epoch total loss 19.0289478\n",
      "Trained batch 548 batch loss 15.7386866 epoch total loss 19.0229435\n",
      "Trained batch 549 batch loss 15.4673 epoch total loss 19.016468\n",
      "Trained batch 550 batch loss 15.1072788 epoch total loss 19.0093613\n",
      "Trained batch 551 batch loss 15.1799679 epoch total loss 19.0024109\n",
      "Trained batch 552 batch loss 15.089426 epoch total loss 18.9953232\n",
      "Trained batch 553 batch loss 15.5643101 epoch total loss 18.9891186\n",
      "Trained batch 554 batch loss 15.4846153 epoch total loss 18.9827919\n",
      "Trained batch 555 batch loss 15.3923035 epoch total loss 18.9763241\n",
      "Trained batch 556 batch loss 15.675416 epoch total loss 18.9703865\n",
      "Trained batch 557 batch loss 15.7459803 epoch total loss 18.9645977\n",
      "Trained batch 558 batch loss 15.6932888 epoch total loss 18.9587364\n",
      "Trained batch 559 batch loss 15.7718105 epoch total loss 18.9530334\n",
      "Trained batch 560 batch loss 15.4445295 epoch total loss 18.9467678\n",
      "Trained batch 561 batch loss 15.3433952 epoch total loss 18.9403458\n",
      "Trained batch 562 batch loss 15.3001537 epoch total loss 18.9338684\n",
      "Trained batch 563 batch loss 15.2896805 epoch total loss 18.9273968\n",
      "Trained batch 564 batch loss 15.4712458 epoch total loss 18.9212685\n",
      "Trained batch 565 batch loss 15.6152534 epoch total loss 18.9154167\n",
      "Trained batch 566 batch loss 15.6281071 epoch total loss 18.9096088\n",
      "Trained batch 567 batch loss 15.488246 epoch total loss 18.9035759\n",
      "Trained batch 568 batch loss 15.3262615 epoch total loss 18.8972778\n",
      "Trained batch 569 batch loss 15.2392731 epoch total loss 18.8908482\n",
      "Trained batch 570 batch loss 15.1564198 epoch total loss 18.8842964\n",
      "Trained batch 571 batch loss 15.6663742 epoch total loss 18.8786602\n",
      "Trained batch 572 batch loss 15.3898 epoch total loss 18.8725605\n",
      "Trained batch 573 batch loss 15.7292767 epoch total loss 18.867075\n",
      "Trained batch 574 batch loss 15.5365705 epoch total loss 18.8612728\n",
      "Trained batch 575 batch loss 15.377739 epoch total loss 18.8552132\n",
      "Trained batch 576 batch loss 15.4800758 epoch total loss 18.8493557\n",
      "Trained batch 577 batch loss 15.2854652 epoch total loss 18.8431778\n",
      "Trained batch 578 batch loss 15.0542145 epoch total loss 18.8366241\n",
      "Trained batch 579 batch loss 14.7041006 epoch total loss 18.8294868\n",
      "Trained batch 580 batch loss 15.2000256 epoch total loss 18.8232288\n",
      "Trained batch 581 batch loss 15.2577286 epoch total loss 18.8170929\n",
      "Trained batch 582 batch loss 15.4644032 epoch total loss 18.8113327\n",
      "Trained batch 583 batch loss 15.3258419 epoch total loss 18.8053551\n",
      "Trained batch 584 batch loss 15.1730728 epoch total loss 18.7991333\n",
      "Trained batch 585 batch loss 15.1929283 epoch total loss 18.7929707\n",
      "Trained batch 586 batch loss 15.0600567 epoch total loss 18.7866\n",
      "Trained batch 587 batch loss 15.5980272 epoch total loss 18.7811661\n",
      "Trained batch 588 batch loss 15.302906 epoch total loss 18.7752514\n",
      "Trained batch 589 batch loss 15.2745476 epoch total loss 18.7693081\n",
      "Trained batch 590 batch loss 14.6887646 epoch total loss 18.7623901\n",
      "Trained batch 591 batch loss 14.6896057 epoch total loss 18.7554989\n",
      "Trained batch 592 batch loss 15.0713692 epoch total loss 18.7492752\n",
      "Trained batch 593 batch loss 15.226634 epoch total loss 18.7433357\n",
      "Trained batch 594 batch loss 15.0259933 epoch total loss 18.7370777\n",
      "Trained batch 595 batch loss 15.0713472 epoch total loss 18.730917\n",
      "Trained batch 596 batch loss 14.7446604 epoch total loss 18.7242298\n",
      "Trained batch 597 batch loss 14.8553009 epoch total loss 18.7177486\n",
      "Trained batch 598 batch loss 14.9770012 epoch total loss 18.7114925\n",
      "Trained batch 599 batch loss 14.9685469 epoch total loss 18.7052441\n",
      "Trained batch 600 batch loss 15.1778898 epoch total loss 18.6993656\n",
      "Trained batch 601 batch loss 15.1241493 epoch total loss 18.6934166\n",
      "Trained batch 602 batch loss 15.008544 epoch total loss 18.6872959\n",
      "Trained batch 603 batch loss 14.838769 epoch total loss 18.6809139\n",
      "Trained batch 604 batch loss 14.7767544 epoch total loss 18.6744499\n",
      "Trained batch 605 batch loss 14.8429947 epoch total loss 18.6681156\n",
      "Trained batch 606 batch loss 15.0157309 epoch total loss 18.6620884\n",
      "Trained batch 607 batch loss 14.8640509 epoch total loss 18.6558323\n",
      "Trained batch 608 batch loss 15.0149994 epoch total loss 18.6498432\n",
      "Trained batch 609 batch loss 14.8456287 epoch total loss 18.6435966\n",
      "Trained batch 610 batch loss 14.6840296 epoch total loss 18.637104\n",
      "Trained batch 611 batch loss 14.8080912 epoch total loss 18.6308365\n",
      "Trained batch 612 batch loss 14.7806778 epoch total loss 18.6245461\n",
      "Trained batch 613 batch loss 14.6194162 epoch total loss 18.6180115\n",
      "Trained batch 614 batch loss 14.6096897 epoch total loss 18.6114826\n",
      "Trained batch 615 batch loss 14.5609016 epoch total loss 18.6048946\n",
      "Trained batch 616 batch loss 14.6602135 epoch total loss 18.5984917\n",
      "Trained batch 617 batch loss 14.4602566 epoch total loss 18.5917835\n",
      "Trained batch 618 batch loss 14.6030121 epoch total loss 18.5853291\n",
      "Trained batch 619 batch loss 14.8874435 epoch total loss 18.5793552\n",
      "Trained batch 620 batch loss 15.0456667 epoch total loss 18.5736561\n",
      "Trained batch 621 batch loss 15.1339989 epoch total loss 18.5681171\n",
      "Trained batch 622 batch loss 15.1199455 epoch total loss 18.5625744\n",
      "Trained batch 623 batch loss 15.2283 epoch total loss 18.5572224\n",
      "Trained batch 624 batch loss 14.7403717 epoch total loss 18.5511055\n",
      "Trained batch 625 batch loss 14.1880932 epoch total loss 18.5441246\n",
      "Trained batch 626 batch loss 13.8072395 epoch total loss 18.5365582\n",
      "Trained batch 627 batch loss 13.604373 epoch total loss 18.5286922\n",
      "Trained batch 628 batch loss 13.4663143 epoch total loss 18.5206318\n",
      "Trained batch 629 batch loss 13.9017076 epoch total loss 18.5132885\n",
      "Trained batch 630 batch loss 14.4804783 epoch total loss 18.5068874\n",
      "Trained batch 631 batch loss 14.144413 epoch total loss 18.4999733\n",
      "Trained batch 632 batch loss 13.9478683 epoch total loss 18.4927711\n",
      "Trained batch 633 batch loss 14.1321545 epoch total loss 18.4858818\n",
      "Trained batch 634 batch loss 14.000885 epoch total loss 18.4788074\n",
      "Trained batch 635 batch loss 14.8222523 epoch total loss 18.4730492\n",
      "Trained batch 636 batch loss 15.4105883 epoch total loss 18.468235\n",
      "Trained batch 637 batch loss 14.8617668 epoch total loss 18.4625721\n",
      "Trained batch 638 batch loss 14.7927265 epoch total loss 18.4568195\n",
      "Trained batch 639 batch loss 14.872858 epoch total loss 18.4512119\n",
      "Trained batch 640 batch loss 15.0378428 epoch total loss 18.445879\n",
      "Trained batch 641 batch loss 14.6012383 epoch total loss 18.4398804\n",
      "Trained batch 642 batch loss 14.4276199 epoch total loss 18.4336319\n",
      "Trained batch 643 batch loss 14.4857845 epoch total loss 18.4274921\n",
      "Trained batch 644 batch loss 14.7079353 epoch total loss 18.4217167\n",
      "Trained batch 645 batch loss 14.4037304 epoch total loss 18.4154854\n",
      "Trained batch 646 batch loss 14.5558043 epoch total loss 18.4095116\n",
      "Trained batch 647 batch loss 14.6282616 epoch total loss 18.4036655\n",
      "Trained batch 648 batch loss 14.4802856 epoch total loss 18.3976116\n",
      "Trained batch 649 batch loss 14.4686565 epoch total loss 18.3915577\n",
      "Trained batch 650 batch loss 14.5174 epoch total loss 18.3855972\n",
      "Trained batch 651 batch loss 14.3664598 epoch total loss 18.3794231\n",
      "Trained batch 652 batch loss 14.5202694 epoch total loss 18.3735046\n",
      "Trained batch 653 batch loss 14.3024559 epoch total loss 18.3672714\n",
      "Trained batch 654 batch loss 14.2333088 epoch total loss 18.3609505\n",
      "Trained batch 655 batch loss 14.2739887 epoch total loss 18.3547115\n",
      "Trained batch 656 batch loss 13.7374363 epoch total loss 18.3476734\n",
      "Trained batch 657 batch loss 13.9711475 epoch total loss 18.341011\n",
      "Trained batch 658 batch loss 14.1018887 epoch total loss 18.334568\n",
      "Trained batch 659 batch loss 13.9344444 epoch total loss 18.3278904\n",
      "Trained batch 660 batch loss 14.1233597 epoch total loss 18.3215199\n",
      "Trained batch 661 batch loss 13.4904442 epoch total loss 18.3142109\n",
      "Trained batch 662 batch loss 13.6921043 epoch total loss 18.30723\n",
      "Trained batch 663 batch loss 13.3925953 epoch total loss 18.2998161\n",
      "Trained batch 664 batch loss 13.7734051 epoch total loss 18.293\n",
      "Trained batch 665 batch loss 14.1597481 epoch total loss 18.2867851\n",
      "Trained batch 666 batch loss 14.018261 epoch total loss 18.2803764\n",
      "Trained batch 667 batch loss 14.3144073 epoch total loss 18.2744293\n",
      "Trained batch 668 batch loss 14.2786045 epoch total loss 18.2684479\n",
      "Trained batch 669 batch loss 14.4474583 epoch total loss 18.2627354\n",
      "Trained batch 670 batch loss 14.540782 epoch total loss 18.2571812\n",
      "Trained batch 671 batch loss 14.3255405 epoch total loss 18.2513218\n",
      "Trained batch 672 batch loss 14.1439228 epoch total loss 18.2452087\n",
      "Trained batch 673 batch loss 14.2834759 epoch total loss 18.2393208\n",
      "Trained batch 674 batch loss 14.1812458 epoch total loss 18.2333012\n",
      "Trained batch 675 batch loss 13.9280949 epoch total loss 18.226923\n",
      "Trained batch 676 batch loss 14.1514893 epoch total loss 18.2208939\n",
      "Trained batch 677 batch loss 14.2605572 epoch total loss 18.215044\n",
      "Trained batch 678 batch loss 13.6061687 epoch total loss 18.2082462\n",
      "Trained batch 679 batch loss 14.2010813 epoch total loss 18.2023449\n",
      "Trained batch 680 batch loss 14.0556431 epoch total loss 18.1962471\n",
      "Trained batch 681 batch loss 13.8991375 epoch total loss 18.1899376\n",
      "Trained batch 682 batch loss 14.4121103 epoch total loss 18.1843987\n",
      "Trained batch 683 batch loss 14.543541 epoch total loss 18.1790676\n",
      "Trained batch 684 batch loss 14.4362602 epoch total loss 18.1735973\n",
      "Trained batch 685 batch loss 14.2048359 epoch total loss 18.1678028\n",
      "Trained batch 686 batch loss 14.3420601 epoch total loss 18.1622257\n",
      "Trained batch 687 batch loss 14.4403114 epoch total loss 18.1568089\n",
      "Trained batch 688 batch loss 14.2161713 epoch total loss 18.1510811\n",
      "Trained batch 689 batch loss 14.2341299 epoch total loss 18.1453953\n",
      "Trained batch 690 batch loss 14.1420498 epoch total loss 18.1395931\n",
      "Trained batch 691 batch loss 14.3881 epoch total loss 18.1341629\n",
      "Trained batch 692 batch loss 14.2358418 epoch total loss 18.1285305\n",
      "Trained batch 693 batch loss 14.17521 epoch total loss 18.1228256\n",
      "Trained batch 694 batch loss 14.0121689 epoch total loss 18.1169014\n",
      "Trained batch 695 batch loss 14.1060047 epoch total loss 18.1111317\n",
      "Trained batch 696 batch loss 13.897543 epoch total loss 18.1050777\n",
      "Trained batch 697 batch loss 14.192997 epoch total loss 18.0994644\n",
      "Trained batch 698 batch loss 13.9705429 epoch total loss 18.0935497\n",
      "Trained batch 699 batch loss 14.0477276 epoch total loss 18.0877628\n",
      "Trained batch 700 batch loss 13.9278336 epoch total loss 18.0818195\n",
      "Trained batch 701 batch loss 14.0978413 epoch total loss 18.0761356\n",
      "Trained batch 702 batch loss 14.0735149 epoch total loss 18.0704327\n",
      "Trained batch 703 batch loss 13.7739525 epoch total loss 18.0643234\n",
      "Trained batch 704 batch loss 13.7822132 epoch total loss 18.0582409\n",
      "Trained batch 705 batch loss 13.8583069 epoch total loss 18.0522823\n",
      "Trained batch 706 batch loss 13.552372 epoch total loss 18.0459099\n",
      "Trained batch 707 batch loss 13.3542624 epoch total loss 18.0392742\n",
      "Trained batch 708 batch loss 13.1235313 epoch total loss 18.0323296\n",
      "Trained batch 709 batch loss 13.8717308 epoch total loss 18.0264626\n",
      "Trained batch 710 batch loss 13.1624069 epoch total loss 18.0196114\n",
      "Trained batch 711 batch loss 14.2454376 epoch total loss 18.0143032\n",
      "Trained batch 712 batch loss 13.7701292 epoch total loss 18.0083427\n",
      "Trained batch 713 batch loss 13.5400333 epoch total loss 18.0020752\n",
      "Trained batch 714 batch loss 13.1351223 epoch total loss 17.9952583\n",
      "Trained batch 715 batch loss 14.1306114 epoch total loss 17.9898529\n",
      "Trained batch 716 batch loss 13.9444313 epoch total loss 17.9842033\n",
      "Trained batch 717 batch loss 13.9515419 epoch total loss 17.9785786\n",
      "Trained batch 718 batch loss 13.6588612 epoch total loss 17.9725628\n",
      "Trained batch 719 batch loss 13.7525616 epoch total loss 17.9666939\n",
      "Trained batch 720 batch loss 13.7454338 epoch total loss 17.9608307\n",
      "Trained batch 721 batch loss 13.7466717 epoch total loss 17.9549866\n",
      "Trained batch 722 batch loss 14.0128946 epoch total loss 17.9495258\n",
      "Trained batch 723 batch loss 14.1607628 epoch total loss 17.9442863\n",
      "Trained batch 724 batch loss 13.8280468 epoch total loss 17.9386\n",
      "Trained batch 725 batch loss 13.6426992 epoch total loss 17.9326744\n",
      "Trained batch 726 batch loss 13.9614639 epoch total loss 17.927206\n",
      "Trained batch 727 batch loss 13.8029518 epoch total loss 17.9215317\n",
      "Trained batch 728 batch loss 13.7584972 epoch total loss 17.9158154\n",
      "Trained batch 729 batch loss 13.8941288 epoch total loss 17.9102974\n",
      "Trained batch 730 batch loss 13.6323452 epoch total loss 17.904438\n",
      "Trained batch 731 batch loss 13.7252245 epoch total loss 17.8987217\n",
      "Trained batch 732 batch loss 13.9084873 epoch total loss 17.8932705\n",
      "Trained batch 733 batch loss 13.6829376 epoch total loss 17.8875256\n",
      "Trained batch 734 batch loss 13.6436329 epoch total loss 17.8817444\n",
      "Trained batch 735 batch loss 13.5884113 epoch total loss 17.8759041\n",
      "Trained batch 736 batch loss 13.4178019 epoch total loss 17.8698463\n",
      "Trained batch 737 batch loss 13.924984 epoch total loss 17.8644943\n",
      "Trained batch 738 batch loss 14.0507488 epoch total loss 17.8593254\n",
      "Trained batch 739 batch loss 13.4873695 epoch total loss 17.8534107\n",
      "Trained batch 740 batch loss 13.7389517 epoch total loss 17.8478508\n",
      "Trained batch 741 batch loss 14.7477856 epoch total loss 17.8436661\n",
      "Trained batch 742 batch loss 14.1270924 epoch total loss 17.8386574\n",
      "Trained batch 743 batch loss 14.2457829 epoch total loss 17.8338223\n",
      "Trained batch 744 batch loss 14.0075502 epoch total loss 17.82868\n",
      "Trained batch 745 batch loss 14.1152058 epoch total loss 17.8236961\n",
      "Trained batch 746 batch loss 13.4474068 epoch total loss 17.8178291\n",
      "Trained batch 747 batch loss 12.9955254 epoch total loss 17.8113728\n",
      "Trained batch 748 batch loss 13.1797514 epoch total loss 17.8051815\n",
      "Trained batch 749 batch loss 13.7272987 epoch total loss 17.799736\n",
      "Trained batch 750 batch loss 14.1074295 epoch total loss 17.7948132\n",
      "Trained batch 751 batch loss 13.1861601 epoch total loss 17.7886772\n",
      "Trained batch 752 batch loss 13.4117975 epoch total loss 17.7828579\n",
      "Trained batch 753 batch loss 13.285799 epoch total loss 17.776886\n",
      "Trained batch 754 batch loss 13.4513206 epoch total loss 17.7711487\n",
      "Trained batch 755 batch loss 13.0734138 epoch total loss 17.7649269\n",
      "Trained batch 756 batch loss 12.9021587 epoch total loss 17.7584953\n",
      "Trained batch 757 batch loss 13.3920937 epoch total loss 17.7527275\n",
      "Trained batch 758 batch loss 13.4791889 epoch total loss 17.7470894\n",
      "Trained batch 759 batch loss 13.4007845 epoch total loss 17.7413635\n",
      "Trained batch 760 batch loss 13.359148 epoch total loss 17.7355976\n",
      "Trained batch 761 batch loss 13.2997961 epoch total loss 17.7297688\n",
      "Trained batch 762 batch loss 13.3742943 epoch total loss 17.7240524\n",
      "Trained batch 763 batch loss 13.7257824 epoch total loss 17.718811\n",
      "Trained batch 764 batch loss 13.6091194 epoch total loss 17.7134323\n",
      "Trained batch 765 batch loss 13.8328962 epoch total loss 17.7083607\n",
      "Trained batch 766 batch loss 13.881876 epoch total loss 17.7033653\n",
      "Trained batch 767 batch loss 13.0702744 epoch total loss 17.6973248\n",
      "Trained batch 768 batch loss 12.7834072 epoch total loss 17.6909256\n",
      "Trained batch 769 batch loss 13.1204224 epoch total loss 17.6849823\n",
      "Trained batch 770 batch loss 13.3621273 epoch total loss 17.6793671\n",
      "Trained batch 771 batch loss 13.2415104 epoch total loss 17.6736107\n",
      "Trained batch 772 batch loss 13.5221825 epoch total loss 17.6682339\n",
      "Trained batch 773 batch loss 13.349144 epoch total loss 17.6626472\n",
      "Trained batch 774 batch loss 13.4575396 epoch total loss 17.6572151\n",
      "Trained batch 775 batch loss 13.3552647 epoch total loss 17.6516647\n",
      "Trained batch 776 batch loss 13.2975426 epoch total loss 17.6460533\n",
      "Trained batch 777 batch loss 13.3514271 epoch total loss 17.6405277\n",
      "Trained batch 778 batch loss 13.4145374 epoch total loss 17.6350937\n",
      "Trained batch 779 batch loss 13.2445545 epoch total loss 17.6294575\n",
      "Trained batch 780 batch loss 13.090786 epoch total loss 17.6236382\n",
      "Trained batch 781 batch loss 13.660078 epoch total loss 17.6185646\n",
      "Trained batch 782 batch loss 14.2465372 epoch total loss 17.6142521\n",
      "Trained batch 783 batch loss 13.6093645 epoch total loss 17.6091366\n",
      "Trained batch 784 batch loss 13.0424795 epoch total loss 17.6033115\n",
      "Trained batch 785 batch loss 13.2169876 epoch total loss 17.597723\n",
      "Trained batch 786 batch loss 13.1082497 epoch total loss 17.5920124\n",
      "Trained batch 787 batch loss 12.9285507 epoch total loss 17.5860863\n",
      "Trained batch 788 batch loss 13.2706146 epoch total loss 17.5806103\n",
      "Trained batch 789 batch loss 12.7466774 epoch total loss 17.5744839\n",
      "Trained batch 790 batch loss 13.015233 epoch total loss 17.5687122\n",
      "Trained batch 791 batch loss 13.3138924 epoch total loss 17.5633335\n",
      "Trained batch 792 batch loss 13.3311691 epoch total loss 17.5579891\n",
      "Trained batch 793 batch loss 13.2883568 epoch total loss 17.5526047\n",
      "Trained batch 794 batch loss 13.2113152 epoch total loss 17.5471363\n",
      "Trained batch 795 batch loss 12.9244633 epoch total loss 17.5413227\n",
      "Trained batch 796 batch loss 13.085207 epoch total loss 17.5357246\n",
      "Trained batch 797 batch loss 13.1956701 epoch total loss 17.5302792\n",
      "Trained batch 798 batch loss 13.1287498 epoch total loss 17.5247631\n",
      "Trained batch 799 batch loss 12.9738245 epoch total loss 17.5190659\n",
      "Trained batch 800 batch loss 13.0840836 epoch total loss 17.5135231\n",
      "Trained batch 801 batch loss 12.9037018 epoch total loss 17.5077667\n",
      "Trained batch 802 batch loss 12.9322109 epoch total loss 17.5020618\n",
      "Trained batch 803 batch loss 13.163702 epoch total loss 17.4966602\n",
      "Trained batch 804 batch loss 13.1941566 epoch total loss 17.4913101\n",
      "Trained batch 805 batch loss 12.6996117 epoch total loss 17.4853573\n",
      "Trained batch 806 batch loss 12.8859844 epoch total loss 17.4796505\n",
      "Trained batch 807 batch loss 13.020505 epoch total loss 17.4741249\n",
      "Trained batch 808 batch loss 12.564415 epoch total loss 17.4680481\n",
      "Trained batch 809 batch loss 12.7462826 epoch total loss 17.4622116\n",
      "Trained batch 810 batch loss 12.6816511 epoch total loss 17.4563084\n",
      "Trained batch 811 batch loss 12.9236088 epoch total loss 17.4507198\n",
      "Trained batch 812 batch loss 12.7828159 epoch total loss 17.444973\n",
      "Trained batch 813 batch loss 12.9922466 epoch total loss 17.4394951\n",
      "Trained batch 814 batch loss 13.0009089 epoch total loss 17.434042\n",
      "Trained batch 815 batch loss 12.8201847 epoch total loss 17.428381\n",
      "Trained batch 816 batch loss 13.1853924 epoch total loss 17.4231815\n",
      "Trained batch 817 batch loss 13.0180798 epoch total loss 17.4177914\n",
      "Trained batch 818 batch loss 12.7481623 epoch total loss 17.4120827\n",
      "Trained batch 819 batch loss 12.8576813 epoch total loss 17.4065208\n",
      "Trained batch 820 batch loss 12.7355671 epoch total loss 17.4008236\n",
      "Trained batch 821 batch loss 13.1216736 epoch total loss 17.3956127\n",
      "Trained batch 822 batch loss 12.9385576 epoch total loss 17.3901901\n",
      "Trained batch 823 batch loss 13.279768 epoch total loss 17.3851948\n",
      "Trained batch 824 batch loss 13.6145029 epoch total loss 17.380619\n",
      "Trained batch 825 batch loss 13.3806362 epoch total loss 17.3757706\n",
      "Trained batch 826 batch loss 13.1726503 epoch total loss 17.3706818\n",
      "Trained batch 827 batch loss 12.9151726 epoch total loss 17.3652954\n",
      "Trained batch 828 batch loss 13.0622482 epoch total loss 17.3600979\n",
      "Trained batch 829 batch loss 13.0679436 epoch total loss 17.3549213\n",
      "Trained batch 830 batch loss 12.8161097 epoch total loss 17.349453\n",
      "Trained batch 831 batch loss 12.7719269 epoch total loss 17.3439445\n",
      "Trained batch 832 batch loss 12.7660942 epoch total loss 17.3384418\n",
      "Trained batch 833 batch loss 12.8134928 epoch total loss 17.3330097\n",
      "Trained batch 834 batch loss 12.8492279 epoch total loss 17.3276329\n",
      "Trained batch 835 batch loss 12.9348392 epoch total loss 17.3223724\n",
      "Trained batch 836 batch loss 12.9331694 epoch total loss 17.3171215\n",
      "Trained batch 1026 batch loss 11.1841259 epoch total loss 16.3466454\n",
      "Trained batch 1027 batch loss 11.6500015 epoch total loss 16.3420734\n",
      "Trained batch 1028 batch loss 11.2579 epoch total loss 16.3371277\n",
      "Trained batch 1029 batch loss 11.4587135 epoch total loss 16.332386\n",
      "Trained batch 1030 batch loss 11.6397295 epoch total loss 16.3278313\n",
      "Trained batch 1031 batch loss 11.7557049 epoch total loss 16.3233967\n",
      "Trained batch 1032 batch loss 11.6279402 epoch total loss 16.3188477\n",
      "Trained batch 1033 batch loss 11.5582008 epoch total loss 16.3142395\n",
      "Trained batch 1034 batch loss 11.5395823 epoch total loss 16.3096218\n",
      "Trained batch 1035 batch loss 11.4649792 epoch total loss 16.3049412\n",
      "Trained batch 1036 batch loss 11.5611067 epoch total loss 16.3003616\n",
      "Trained batch 1037 batch loss 11.4259739 epoch total loss 16.29566\n",
      "Trained batch 1038 batch loss 11.5235624 epoch total loss 16.2910633\n",
      "Trained batch 1039 batch loss 11.6541319 epoch total loss 16.2866\n",
      "Trained batch 1040 batch loss 11.104557 epoch total loss 16.2816181\n",
      "Trained batch 1041 batch loss 10.6623096 epoch total loss 16.2762203\n",
      "Trained batch 1042 batch loss 10.8221169 epoch total loss 16.2709866\n",
      "Trained batch 1043 batch loss 11.1226006 epoch total loss 16.2660503\n",
      "Trained batch 1044 batch loss 11.0364876 epoch total loss 16.2610416\n",
      "Trained batch 1045 batch loss 11.6046391 epoch total loss 16.2565861\n",
      "Trained batch 1046 batch loss 11.3048286 epoch total loss 16.251852\n",
      "Trained batch 1047 batch loss 11.3752432 epoch total loss 16.2471943\n",
      "Trained batch 1048 batch loss 10.806716 epoch total loss 16.2420025\n",
      "Trained batch 1049 batch loss 10.9765186 epoch total loss 16.2369843\n",
      "Trained batch 1050 batch loss 11.3415718 epoch total loss 16.2323208\n",
      "Trained batch 1051 batch loss 11.4691696 epoch total loss 16.2277889\n",
      "Trained batch 1052 batch loss 11.4200821 epoch total loss 16.2232189\n",
      "Trained batch 1053 batch loss 11.2097797 epoch total loss 16.2184563\n",
      "Trained batch 1054 batch loss 11.2815838 epoch total loss 16.2137718\n",
      "Trained batch 1055 batch loss 10.9603882 epoch total loss 16.2087936\n",
      "Trained batch 1056 batch loss 11.5424881 epoch total loss 16.2043762\n",
      "Trained batch 1057 batch loss 11.688447 epoch total loss 16.2001019\n",
      "Trained batch 1058 batch loss 11.5945225 epoch total loss 16.1957474\n",
      "Trained batch 1059 batch loss 11.4154415 epoch total loss 16.1912346\n",
      "Trained batch 1060 batch loss 11.1688595 epoch total loss 16.1864967\n",
      "Trained batch 1061 batch loss 11.2588692 epoch total loss 16.1818523\n",
      "Trained batch 1062 batch loss 11.0449657 epoch total loss 16.1770153\n",
      "Trained batch 1063 batch loss 11.1842842 epoch total loss 16.1723175\n",
      "Trained batch 1064 batch loss 11.1338472 epoch total loss 16.1675835\n",
      "Trained batch 1065 batch loss 11.1268139 epoch total loss 16.1628494\n",
      "Trained batch 1066 batch loss 11.4978123 epoch total loss 16.158474\n",
      "Trained batch 1067 batch loss 11.4385557 epoch total loss 16.1540508\n",
      "Trained batch 1068 batch loss 11.1189442 epoch total loss 16.1493378\n",
      "Trained batch 1069 batch loss 11.1992235 epoch total loss 16.1447067\n",
      "Trained batch 1070 batch loss 10.9765377 epoch total loss 16.1398773\n",
      "Trained batch 1071 batch loss 10.9344826 epoch total loss 16.1350155\n",
      "Trained batch 1072 batch loss 10.8778019 epoch total loss 16.1301098\n",
      "Trained batch 1073 batch loss 11.2710514 epoch total loss 16.1255817\n",
      "Trained batch 1074 batch loss 11.4940815 epoch total loss 16.1212692\n",
      "Trained batch 1075 batch loss 11.2346725 epoch total loss 16.116724\n",
      "Trained batch 1076 batch loss 11.224472 epoch total loss 16.1121769\n",
      "Trained batch 1077 batch loss 11.7171707 epoch total loss 16.1080971\n",
      "Trained batch 1078 batch loss 11.0543442 epoch total loss 16.1034088\n",
      "Trained batch 1079 batch loss 10.9229374 epoch total loss 16.098608\n",
      "Trained batch 1080 batch loss 11.2480507 epoch total loss 16.0941162\n",
      "Trained batch 1081 batch loss 11.0469351 epoch total loss 16.0894489\n",
      "Trained batch 1082 batch loss 10.8527641 epoch total loss 16.0846081\n",
      "Trained batch 1083 batch loss 10.9834576 epoch total loss 16.0798988\n",
      "Trained batch 1084 batch loss 11.0540895 epoch total loss 16.075264\n",
      "Trained batch 1085 batch loss 11.2579803 epoch total loss 16.0708237\n",
      "Trained batch 1086 batch loss 10.9085579 epoch total loss 16.0660706\n",
      "Trained batch 1087 batch loss 10.7315102 epoch total loss 16.0611629\n",
      "Trained batch 1088 batch loss 11.0098562 epoch total loss 16.0565205\n",
      "Trained batch 1089 batch loss 11.234993 epoch total loss 16.0520916\n",
      "Trained batch 1090 batch loss 10.7966099 epoch total loss 16.0472717\n",
      "Trained batch 1091 batch loss 10.859127 epoch total loss 16.0425167\n",
      "Trained batch 1092 batch loss 11.1749535 epoch total loss 16.0380592\n",
      "Trained batch 1093 batch loss 11.5835361 epoch total loss 16.0339832\n",
      "Trained batch 1094 batch loss 11.2744617 epoch total loss 16.0296345\n",
      "Trained batch 1095 batch loss 11.3693295 epoch total loss 16.0253773\n",
      "Trained batch 1096 batch loss 11.0566597 epoch total loss 16.0208454\n",
      "Trained batch 1097 batch loss 11.0112209 epoch total loss 16.0162792\n",
      "Trained batch 1098 batch loss 10.8934402 epoch total loss 16.0116119\n",
      "Trained batch 1099 batch loss 10.9197254 epoch total loss 16.006979\n",
      "Trained batch 1100 batch loss 10.6414967 epoch total loss 16.0021\n",
      "Trained batch 1101 batch loss 10.6161499 epoch total loss 15.9972076\n",
      "Trained batch 1102 batch loss 10.7500935 epoch total loss 15.9924459\n",
      "Trained batch 1103 batch loss 10.948513 epoch total loss 15.987874\n",
      "Trained batch 1104 batch loss 10.7537632 epoch total loss 15.9831333\n",
      "Trained batch 1105 batch loss 10.87253 epoch total loss 15.9785089\n",
      "Trained batch 1106 batch loss 11.0861664 epoch total loss 15.9740849\n",
      "Trained batch 1107 batch loss 11.6918678 epoch total loss 15.9702158\n",
      "Trained batch 1108 batch loss 11.2131567 epoch total loss 15.9659224\n",
      "Trained batch 1109 batch loss 11.3227224 epoch total loss 15.9617357\n",
      "Trained batch 1110 batch loss 11.5532866 epoch total loss 15.9577637\n",
      "Trained batch 1111 batch loss 11.0094795 epoch total loss 15.95331\n",
      "Trained batch 1112 batch loss 11.0760345 epoch total loss 15.9489241\n",
      "Trained batch 1113 batch loss 10.7356701 epoch total loss 15.9442406\n",
      "Trained batch 1114 batch loss 11.1844349 epoch total loss 15.9399672\n",
      "Trained batch 1115 batch loss 10.7011757 epoch total loss 15.9352684\n",
      "Trained batch 1116 batch loss 10.6188145 epoch total loss 15.9305048\n",
      "Trained batch 1117 batch loss 10.9712868 epoch total loss 15.9260645\n",
      "Trained batch 1118 batch loss 10.7950306 epoch total loss 15.9214754\n",
      "Trained batch 1119 batch loss 10.7363796 epoch total loss 15.9168415\n",
      "Trained batch 1120 batch loss 10.5891495 epoch total loss 15.9120846\n",
      "Trained batch 1121 batch loss 10.915307 epoch total loss 15.9076281\n",
      "Trained batch 1122 batch loss 10.656496 epoch total loss 15.9029474\n",
      "Trained batch 1123 batch loss 10.7754822 epoch total loss 15.8983822\n",
      "Trained batch 1124 batch loss 10.6610088 epoch total loss 15.8937216\n",
      "Trained batch 1125 batch loss 10.5356712 epoch total loss 15.888958\n",
      "Trained batch 1126 batch loss 10.3013945 epoch total loss 15.8839951\n",
      "Trained batch 1127 batch loss 10.2830343 epoch total loss 15.8790255\n",
      "Trained batch 1128 batch loss 10.7397537 epoch total loss 15.8744698\n",
      "Trained batch 1129 batch loss 10.8702984 epoch total loss 15.870038\n",
      "Trained batch 1130 batch loss 10.9021358 epoch total loss 15.8656425\n",
      "Trained batch 1131 batch loss 11.1722221 epoch total loss 15.8614922\n",
      "Trained batch 1132 batch loss 11.1546059 epoch total loss 15.8573341\n",
      "Trained batch 1133 batch loss 11.2249146 epoch total loss 15.8532448\n",
      "Trained batch 1134 batch loss 10.6927624 epoch total loss 15.8486948\n",
      "Trained batch 1135 batch loss 11.2096252 epoch total loss 15.8446074\n",
      "Trained batch 1136 batch loss 10.9729919 epoch total loss 15.8403187\n",
      "Trained batch 1137 batch loss 10.7988281 epoch total loss 15.8358841\n",
      "Trained batch 1138 batch loss 11.1265764 epoch total loss 15.8317461\n",
      "Trained batch 1139 batch loss 11.0319214 epoch total loss 15.8275318\n",
      "Trained batch 1140 batch loss 10.1873627 epoch total loss 15.8225842\n",
      "Trained batch 1141 batch loss 10.7941923 epoch total loss 15.8181782\n",
      "Trained batch 1142 batch loss 11.08354 epoch total loss 15.8140326\n",
      "Trained batch 1143 batch loss 10.841382 epoch total loss 15.8096819\n",
      "Trained batch 1144 batch loss 10.6992416 epoch total loss 15.8052149\n",
      "Trained batch 1145 batch loss 10.8980026 epoch total loss 15.80093\n",
      "Trained batch 1146 batch loss 11.0625076 epoch total loss 15.7967949\n",
      "Trained batch 1147 batch loss 10.9819031 epoch total loss 15.7925978\n",
      "Trained batch 1148 batch loss 11.0208149 epoch total loss 15.7884417\n",
      "Trained batch 1149 batch loss 10.8993587 epoch total loss 15.7841854\n",
      "Trained batch 1150 batch loss 10.9167833 epoch total loss 15.779952\n",
      "Trained batch 1151 batch loss 10.563961 epoch total loss 15.7754211\n",
      "Trained batch 1152 batch loss 10.5037165 epoch total loss 15.7708454\n",
      "Trained batch 1153 batch loss 10.2345839 epoch total loss 15.7660437\n",
      "Trained batch 1154 batch loss 9.99445915 epoch total loss 15.7610416\n",
      "Trained batch 1155 batch loss 10.397624 epoch total loss 15.7563992\n",
      "Trained batch 1156 batch loss 9.846982 epoch total loss 15.7512875\n",
      "Trained batch 1157 batch loss 10.5564117 epoch total loss 15.7467976\n",
      "Trained batch 1158 batch loss 10.6082401 epoch total loss 15.7423592\n",
      "Trained batch 1159 batch loss 10.9448719 epoch total loss 15.7382202\n",
      "Trained batch 1160 batch loss 11.2060242 epoch total loss 15.734313\n",
      "Trained batch 1161 batch loss 10.7587347 epoch total loss 15.7300262\n",
      "Trained batch 1162 batch loss 10.7664394 epoch total loss 15.7257538\n",
      "Trained batch 1163 batch loss 10.736022 epoch total loss 15.7214642\n",
      "Trained batch 1164 batch loss 10.2804565 epoch total loss 15.7167902\n",
      "Trained batch 1165 batch loss 10.0210972 epoch total loss 15.7119017\n",
      "Trained batch 1166 batch loss 10.5332546 epoch total loss 15.7074604\n",
      "Trained batch 1167 batch loss 10.822216 epoch total loss 15.7032738\n",
      "Trained batch 1168 batch loss 10.7996235 epoch total loss 15.6990747\n",
      "Trained batch 1169 batch loss 10.6979685 epoch total loss 15.6947966\n",
      "Trained batch 1170 batch loss 10.7343483 epoch total loss 15.6905565\n",
      "Trained batch 1171 batch loss 10.6001186 epoch total loss 15.6862087\n",
      "Trained batch 1172 batch loss 10.6429653 epoch total loss 15.6819057\n",
      "Trained batch 1173 batch loss 10.5712509 epoch total loss 15.6775475\n",
      "Trained batch 1174 batch loss 10.513401 epoch total loss 15.6731491\n",
      "Trained batch 1175 batch loss 10.8224201 epoch total loss 15.6690207\n",
      "Trained batch 1176 batch loss 10.4674101 epoch total loss 15.6645975\n",
      "Trained batch 1177 batch loss 10.3686895 epoch total loss 15.6600981\n",
      "Trained batch 1178 batch loss 10.0462275 epoch total loss 15.6553335\n",
      "Trained batch 1179 batch loss 10.5753241 epoch total loss 15.6510248\n",
      "Trained batch 1180 batch loss 10.8763332 epoch total loss 15.6469793\n",
      "Trained batch 1181 batch loss 10.9474678 epoch total loss 15.643\n",
      "Trained batch 1182 batch loss 10.6129208 epoch total loss 15.6387444\n",
      "Trained batch 1183 batch loss 10.6682777 epoch total loss 15.6345425\n",
      "Trained batch 1184 batch loss 10.6237831 epoch total loss 15.6303101\n",
      "Trained batch 1185 batch loss 10.4512501 epoch total loss 15.6259394\n",
      "Trained batch 1186 batch loss 10.1977367 epoch total loss 15.6213617\n",
      "Trained batch 1187 batch loss 10.502636 epoch total loss 15.6170492\n",
      "Trained batch 1188 batch loss 10.2592468 epoch total loss 15.6125402\n",
      "Trained batch 1189 batch loss 10.4337311 epoch total loss 15.6081839\n",
      "Trained batch 1190 batch loss 10.7442636 epoch total loss 15.6040964\n",
      "Trained batch 1191 batch loss 10.6501446 epoch total loss 15.5999374\n",
      "Trained batch 1192 batch loss 10.2298584 epoch total loss 15.5954332\n",
      "Trained batch 1193 batch loss 9.69606209 epoch total loss 15.5904875\n",
      "Trained batch 1194 batch loss 9.41928768 epoch total loss 15.5853195\n",
      "Trained batch 1195 batch loss 9.9045639 epoch total loss 15.5805655\n",
      "Trained batch 1196 batch loss 10.0595512 epoch total loss 15.5759478\n",
      "Trained batch 1197 batch loss 10.2314968 epoch total loss 15.5714836\n",
      "Trained batch 1198 batch loss 10.6388435 epoch total loss 15.5673666\n",
      "Trained batch 1199 batch loss 10.5442457 epoch total loss 15.5631781\n",
      "Trained batch 1200 batch loss 10.5973454 epoch total loss 15.5590401\n",
      "Trained batch 1201 batch loss 10.4687376 epoch total loss 15.554801\n",
      "Trained batch 1202 batch loss 10.4363756 epoch total loss 15.5505428\n",
      "Trained batch 1203 batch loss 10.9157476 epoch total loss 15.54669\n",
      "Trained batch 1204 batch loss 10.2204838 epoch total loss 15.5422659\n",
      "Trained batch 1205 batch loss 10.1060104 epoch total loss 15.5377541\n",
      "Trained batch 1206 batch loss 10.2902985 epoch total loss 15.5334044\n",
      "Trained batch 1207 batch loss 10.3954773 epoch total loss 15.5291462\n",
      "Trained batch 1208 batch loss 10.6144791 epoch total loss 15.5250788\n",
      "Trained batch 1209 batch loss 10.4602222 epoch total loss 15.5208902\n",
      "Trained batch 1210 batch loss 11.1583338 epoch total loss 15.5172844\n",
      "Trained batch 1211 batch loss 10.6582251 epoch total loss 15.5132723\n",
      "Trained batch 1212 batch loss 10.0989246 epoch total loss 15.5088053\n",
      "Trained batch 1213 batch loss 9.95754051 epoch total loss 15.5042286\n",
      "Trained batch 1214 batch loss 9.8184967 epoch total loss 15.4995451\n",
      "Trained batch 1215 batch loss 10.2315273 epoch total loss 15.4952097\n",
      "Trained batch 1216 batch loss 10.4574699 epoch total loss 15.491066\n",
      "Trained batch 1217 batch loss 10.4195843 epoch total loss 15.4868994\n",
      "Trained batch 1218 batch loss 10.4173489 epoch total loss 15.4827375\n",
      "Trained batch 1219 batch loss 10.8525372 epoch total loss 15.4789391\n",
      "Trained batch 1220 batch loss 10.7865038 epoch total loss 15.4750929\n",
      "Trained batch 1221 batch loss 10.5721655 epoch total loss 15.471077\n",
      "Trained batch 1222 batch loss 10.4718657 epoch total loss 15.4669867\n",
      "Trained batch 1223 batch loss 10.424901 epoch total loss 15.4628649\n",
      "Trained batch 1224 batch loss 10.4480381 epoch total loss 15.4587669\n",
      "Trained batch 1225 batch loss 10.6534224 epoch total loss 15.4548454\n",
      "Trained batch 1226 batch loss 10.4750185 epoch total loss 15.4507828\n",
      "Trained batch 1227 batch loss 10.294795 epoch total loss 15.4465809\n",
      "Trained batch 1228 batch loss 10.4065447 epoch total loss 15.4424763\n",
      "Trained batch 1229 batch loss 10.1879406 epoch total loss 15.438201\n",
      "Trained batch 1230 batch loss 9.60136223 epoch total loss 15.4334555\n",
      "Trained batch 1231 batch loss 10.0342884 epoch total loss 15.4290705\n",
      "Trained batch 1232 batch loss 10.5230255 epoch total loss 15.4250889\n",
      "Trained batch 1233 batch loss 10.5855207 epoch total loss 15.4211636\n",
      "Trained batch 1234 batch loss 10.390173 epoch total loss 15.4170876\n",
      "Trained batch 1235 batch loss 9.88682842 epoch total loss 15.4126091\n",
      "Trained batch 1236 batch loss 10.4195433 epoch total loss 15.4085693\n",
      "Trained batch 1237 batch loss 10.1990547 epoch total loss 15.4043589\n",
      "Trained batch 1238 batch loss 10.4655876 epoch total loss 15.4003687\n",
      "Trained batch 1239 batch loss 10.5666294 epoch total loss 15.3964672\n",
      "Trained batch 1240 batch loss 10.5569038 epoch total loss 15.3925638\n",
      "Trained batch 1241 batch loss 10.5084438 epoch total loss 15.388628\n",
      "Trained batch 1242 batch loss 10.4745111 epoch total loss 15.3846712\n",
      "Trained batch 1243 batch loss 10.0075817 epoch total loss 15.3803453\n",
      "Trained batch 1244 batch loss 10.1401491 epoch total loss 15.3761339\n",
      "Trained batch 1245 batch loss 10.4335537 epoch total loss 15.3721638\n",
      "Trained batch 1246 batch loss 10.3253851 epoch total loss 15.3681135\n",
      "Trained batch 1247 batch loss 10.2808456 epoch total loss 15.3640347\n",
      "Trained batch 1248 batch loss 9.95652 epoch total loss 15.3597021\n",
      "Trained batch 1249 batch loss 10.1492491 epoch total loss 15.3555298\n",
      "Trained batch 1250 batch loss 10.2684383 epoch total loss 15.3514595\n",
      "Trained batch 1251 batch loss 10.4262877 epoch total loss 15.3475218\n",
      "Trained batch 1252 batch loss 10.1486645 epoch total loss 15.3433695\n",
      "Trained batch 1253 batch loss 10.3449821 epoch total loss 15.3393812\n",
      "Trained batch 1254 batch loss 9.97260952 epoch total loss 15.3351011\n",
      "Trained batch 1255 batch loss 10.4483318 epoch total loss 15.3312082\n",
      "Trained batch 1256 batch loss 10.2881289 epoch total loss 15.3271933\n",
      "Trained batch 1257 batch loss 10.0589361 epoch total loss 15.3230019\n",
      "Trained batch 1258 batch loss 10.2609539 epoch total loss 15.3189793\n",
      "Trained batch 1259 batch loss 9.50122643 epoch total loss 15.3143587\n",
      "Trained batch 1260 batch loss 10.0590935 epoch total loss 15.3101873\n",
      "Trained batch 1261 batch loss 10.143652 epoch total loss 15.3060904\n",
      "Trained batch 1262 batch loss 9.81377506 epoch total loss 15.3017397\n",
      "Trained batch 1263 batch loss 10.1442518 epoch total loss 15.2976561\n",
      "Trained batch 1264 batch loss 10.1279573 epoch total loss 15.2935667\n",
      "Trained batch 1265 batch loss 10.051321 epoch total loss 15.289422\n",
      "Trained batch 1266 batch loss 10.2514782 epoch total loss 15.2854433\n",
      "Trained batch 1267 batch loss 9.71723 epoch total loss 15.2810478\n",
      "Trained batch 1268 batch loss 9.48705101 epoch total loss 15.2764778\n",
      "Trained batch 1269 batch loss 9.34645271 epoch total loss 15.2718048\n",
      "Trained batch 1270 batch loss 9.81188202 epoch total loss 15.2675056\n",
      "Trained batch 1271 batch loss 9.66958523 epoch total loss 15.2631016\n",
      "Trained batch 1272 batch loss 9.96737671 epoch total loss 15.2589378\n",
      "Trained batch 1273 batch loss 9.98959255 epoch total loss 15.2547989\n",
      "Trained batch 1274 batch loss 10.0354757 epoch total loss 15.2507019\n",
      "Trained batch 1275 batch loss 10.3410597 epoch total loss 15.2468519\n",
      "Trained batch 1276 batch loss 9.66579819 epoch total loss 15.2424784\n",
      "Trained batch 1277 batch loss 10.205471 epoch total loss 15.238534\n",
      "Trained batch 1278 batch loss 10.9688015 epoch total loss 15.2351923\n",
      "Trained batch 1279 batch loss 10.8809299 epoch total loss 15.2317877\n",
      "Trained batch 1280 batch loss 10.3489618 epoch total loss 15.2279739\n",
      "Trained batch 1281 batch loss 10.0397272 epoch total loss 15.2239237\n",
      "Trained batch 1282 batch loss 10.0774479 epoch total loss 15.2199097\n",
      "Trained batch 1283 batch loss 9.91798782 epoch total loss 15.2157774\n",
      "Trained batch 1284 batch loss 9.9115591 epoch total loss 15.2116461\n",
      "Trained batch 1285 batch loss 9.18056107 epoch total loss 15.2069521\n",
      "Trained batch 1286 batch loss 9.75341892 epoch total loss 15.2027121\n",
      "Trained batch 1287 batch loss 9.48434448 epoch total loss 15.1982689\n",
      "Trained batch 1288 batch loss 9.75450516 epoch total loss 15.1940422\n",
      "Trained batch 1289 batch loss 9.81574154 epoch total loss 15.1898699\n",
      "Trained batch 1290 batch loss 10.0754194 epoch total loss 15.1859055\n",
      "Trained batch 1291 batch loss 10.3639212 epoch total loss 15.1821699\n",
      "Trained batch 1292 batch loss 9.84805107 epoch total loss 15.1780415\n",
      "Trained batch 1293 batch loss 9.72331 epoch total loss 15.1738224\n",
      "Trained batch 1294 batch loss 9.99457741 epoch total loss 15.1698189\n",
      "Trained batch 1295 batch loss 9.80472374 epoch total loss 15.1656761\n",
      "Trained batch 1296 batch loss 9.98967934 epoch total loss 15.1616831\n",
      "Trained batch 1297 batch loss 9.97397 epoch total loss 15.1576834\n",
      "Trained batch 1298 batch loss 9.89224625 epoch total loss 15.1536274\n",
      "Trained batch 1299 batch loss 9.91225052 epoch total loss 15.1495924\n",
      "Trained batch 1300 batch loss 10.0005875 epoch total loss 15.1456308\n",
      "Trained batch 1301 batch loss 9.85287666 epoch total loss 15.1415634\n",
      "Trained batch 1302 batch loss 9.28879929 epoch total loss 15.1370687\n",
      "Trained batch 1303 batch loss 9.52847481 epoch total loss 15.1327648\n",
      "Trained batch 1304 batch loss 10.0247469 epoch total loss 15.1288481\n",
      "Trained batch 1305 batch loss 9.79079247 epoch total loss 15.1247578\n",
      "Trained batch 1306 batch loss 10.3074512 epoch total loss 15.121068\n",
      "Trained batch 1307 batch loss 10.2765121 epoch total loss 15.117362\n",
      "Trained batch 1308 batch loss 10.3059978 epoch total loss 15.1136847\n",
      "Trained batch 1309 batch loss 10.2535791 epoch total loss 15.109972\n",
      "Trained batch 1310 batch loss 10.3649845 epoch total loss 15.1063499\n",
      "Trained batch 1311 batch loss 10.4730597 epoch total loss 15.1028156\n",
      "Trained batch 1312 batch loss 10.4516087 epoch total loss 15.0992699\n",
      "Trained batch 1313 batch loss 10.3823242 epoch total loss 15.0956774\n",
      "Trained batch 1314 batch loss 10.0071125 epoch total loss 15.0918055\n",
      "Trained batch 1315 batch loss 10.0732527 epoch total loss 15.0879898\n",
      "Trained batch 1316 batch loss 9.13765526 epoch total loss 15.0834675\n",
      "Trained batch 1317 batch loss 9.39011 epoch total loss 15.0791454\n",
      "Trained batch 1318 batch loss 10.1288071 epoch total loss 15.0753899\n",
      "Trained batch 1319 batch loss 10.1965113 epoch total loss 15.0716915\n",
      "Trained batch 1320 batch loss 10.012289 epoch total loss 15.0678577\n",
      "Trained batch 1321 batch loss 9.73576546 epoch total loss 15.0638218\n",
      "Trained batch 1322 batch loss 9.85216713 epoch total loss 15.0598793\n",
      "Trained batch 1323 batch loss 9.69242191 epoch total loss 15.0558224\n",
      "Trained batch 1324 batch loss 9.76947403 epoch total loss 15.0518303\n",
      "Trained batch 1325 batch loss 9.8168478 epoch total loss 15.0478792\n",
      "Trained batch 1326 batch loss 10.0036869 epoch total loss 15.044075\n",
      "Trained batch 1327 batch loss 9.68673801 epoch total loss 15.0400381\n",
      "Trained batch 1328 batch loss 9.21349335 epoch total loss 15.0356503\n",
      "Trained batch 1329 batch loss 9.28820515 epoch total loss 15.0313263\n",
      "Trained batch 1330 batch loss 9.91456795 epoch total loss 15.0274792\n",
      "Trained batch 1331 batch loss 10.0608292 epoch total loss 15.0237474\n",
      "Trained batch 1332 batch loss 10.2246523 epoch total loss 15.0201445\n",
      "Trained batch 1333 batch loss 10.1409597 epoch total loss 15.0164833\n",
      "Trained batch 1334 batch loss 10.605505 epoch total loss 15.0131769\n",
      "Trained batch 1335 batch loss 10.3858681 epoch total loss 15.0097113\n",
      "Trained batch 1336 batch loss 10.2749815 epoch total loss 15.0061674\n",
      "Trained batch 1337 batch loss 10.0857716 epoch total loss 15.0024881\n",
      "Trained batch 1338 batch loss 10.1166315 epoch total loss 14.9988365\n",
      "Trained batch 1339 batch loss 9.7605505 epoch total loss 14.9949236\n",
      "Trained batch 1340 batch loss 9.81242085 epoch total loss 14.9910564\n",
      "Trained batch 1341 batch loss 10.07549 epoch total loss 14.9873915\n",
      "Trained batch 1342 batch loss 10.1983013 epoch total loss 14.9838238\n",
      "Trained batch 1343 batch loss 9.84910202 epoch total loss 14.9800005\n",
      "Trained batch 1344 batch loss 9.85500526 epoch total loss 14.9761877\n",
      "Trained batch 1345 batch loss 9.07184 epoch total loss 14.9717979\n",
      "Trained batch 1346 batch loss 9.7220192 epoch total loss 14.9678984\n",
      "Trained batch 1347 batch loss 9.62578201 epoch total loss 14.963932\n",
      "Trained batch 1348 batch loss 10.20154 epoch total loss 14.9603987\n",
      "Trained batch 1349 batch loss 9.78915882 epoch total loss 14.9565649\n",
      "Trained batch 1350 batch loss 9.92124844 epoch total loss 14.952836\n",
      "Trained batch 1351 batch loss 10.005703 epoch total loss 14.9491739\n",
      "Trained batch 1352 batch loss 9.75849438 epoch total loss 14.9453344\n",
      "Trained batch 1353 batch loss 9.7827282 epoch total loss 14.9415188\n",
      "Trained batch 1354 batch loss 9.80526066 epoch total loss 14.9377251\n",
      "Trained batch 1355 batch loss 9.77765274 epoch total loss 14.933917\n",
      "Trained batch 1356 batch loss 9.96912766 epoch total loss 14.9302549\n",
      "Trained batch 1357 batch loss 9.77701092 epoch total loss 14.9264574\n",
      "Trained batch 1358 batch loss 9.60148811 epoch total loss 14.9225368\n",
      "Trained batch 1359 batch loss 9.27902222 epoch total loss 14.9183846\n",
      "Trained batch 1360 batch loss 9.16563129 epoch total loss 14.9141541\n",
      "Trained batch 1361 batch loss 9.04930592 epoch total loss 14.9098444\n",
      "Trained batch 1362 batch loss 9.31721687 epoch total loss 14.9057379\n",
      "Trained batch 1363 batch loss 9.73140049 epoch total loss 14.9019413\n",
      "Trained batch 1364 batch loss 9.71896648 epoch total loss 14.8981409\n",
      "Trained batch 1365 batch loss 10.475008 epoch total loss 14.8949\n",
      "Trained batch 1366 batch loss 10.2445498 epoch total loss 14.8914957\n",
      "Trained batch 1367 batch loss 10.0709763 epoch total loss 14.887969\n",
      "Trained batch 1368 batch loss 9.6645155 epoch total loss 14.8841505\n",
      "Trained batch 1369 batch loss 9.60010719 epoch total loss 14.88029\n",
      "Trained batch 1370 batch loss 9.30392742 epoch total loss 14.8762207\n",
      "Trained batch 1371 batch loss 9.25603867 epoch total loss 14.8721209\n",
      "Trained batch 1372 batch loss 9.56004143 epoch total loss 14.8682499\n",
      "Trained batch 1373 batch loss 9.54307842 epoch total loss 14.8643713\n",
      "Trained batch 1374 batch loss 8.83029652 epoch total loss 14.8599787\n",
      "Trained batch 1375 batch loss 8.54928684 epoch total loss 14.8553896\n",
      "Trained batch 1376 batch loss 9.07233143 epoch total loss 14.8511868\n",
      "Trained batch 1377 batch loss 8.91317749 epoch total loss 14.8468752\n",
      "Trained batch 1378 batch loss 9.15203857 epoch total loss 14.842742\n",
      "Trained batch 1379 batch loss 9.10550308 epoch total loss 14.838582\n",
      "Trained batch 1380 batch loss 9.48782539 epoch total loss 14.8347044\n",
      "Trained batch 1381 batch loss 9.24004745 epoch total loss 14.8306541\n",
      "Trained batch 1382 batch loss 9.46260166 epoch total loss 14.8267698\n",
      "Trained batch 1383 batch loss 9.30411 epoch total loss 14.8227768\n",
      "Trained batch 1384 batch loss 9.26207352 epoch total loss 14.818759\n",
      "Trained batch 1385 batch loss 9.67970276 epoch total loss 14.8150482\n",
      "Trained batch 1386 batch loss 9.50583744 epoch total loss 14.8112173\n",
      "Trained batch 1387 batch loss 10.1180716 epoch total loss 14.8078337\n",
      "Trained batch 1388 batch loss 10.0850601 epoch total loss 14.804431\n",
      "Epoch 3 train loss 14.804430961608887\n",
      "Validated batch 1 batch loss 10.336957\n",
      "Validated batch 2 batch loss 10.5959\n",
      "Validated batch 3 batch loss 10.0338268\n",
      "Validated batch 4 batch loss 10.2803497\n",
      "Validated batch 5 batch loss 10.3203278\n",
      "Validated batch 6 batch loss 10.5287247\n",
      "Validated batch 7 batch loss 10.5914898\n",
      "Validated batch 8 batch loss 10.6719589\n",
      "Validated batch 9 batch loss 10.9937572\n",
      "Validated batch 10 batch loss 10.8993244\n",
      "Validated batch 11 batch loss 10.5957012\n",
      "Validated batch 12 batch loss 10.4100208\n",
      "Validated batch 13 batch loss 10.7837524\n",
      "Validated batch 14 batch loss 10.5361938\n",
      "Validated batch 15 batch loss 10.9501514\n",
      "Validated batch 16 batch loss 11.0061054\n",
      "Validated batch 17 batch loss 10.5469532\n",
      "Validated batch 18 batch loss 10.9451466\n",
      "Validated batch 19 batch loss 10.7855835\n",
      "Validated batch 20 batch loss 10.6708221\n",
      "Validated batch 21 batch loss 10.6078854\n",
      "Validated batch 22 batch loss 10.9137859\n",
      "Validated batch 23 batch loss 10.9265995\n",
      "Validated batch 24 batch loss 10.4695683\n",
      "Validated batch 25 batch loss 10.0210114\n",
      "Validated batch 26 batch loss 10.7042837\n",
      "Validated batch 27 batch loss 10.7614441\n",
      "Validated batch 28 batch loss 10.4851933\n",
      "Validated batch 29 batch loss 10.7766647\n",
      "Validated batch 30 batch loss 10.7067947\n",
      "Validated batch 31 batch loss 10.8839169\n",
      "Validated batch 32 batch loss 10.5203485\n",
      "Validated batch 33 batch loss 10.6695709\n",
      "Validated batch 34 batch loss 10.3499146\n",
      "Validated batch 35 batch loss 10.5714035\n",
      "Validated batch 36 batch loss 10.5764008\n",
      "Validated batch 37 batch loss 10.9426651\n",
      "Validated batch 38 batch loss 10.8137283\n",
      "Validated batch 39 batch loss 10.4544544\n",
      "Validated batch 40 batch loss 10.9272079\n",
      "Validated batch 41 batch loss 10.0094967\n",
      "Validated batch 42 batch loss 10.7944889\n",
      "Validated batch 43 batch loss 10.9608383\n",
      "Validated batch 44 batch loss 10.7840137\n",
      "Validated batch 45 batch loss 10.9431992\n",
      "Validated batch 46 batch loss 10.3363266\n",
      "Validated batch 47 batch loss 10.9107437\n",
      "Validated batch 48 batch loss 10.5898685\n",
      "Validated batch 49 batch loss 10.4536514\n",
      "Validated batch 50 batch loss 10.3333912\n",
      "Validated batch 51 batch loss 10.2214813\n",
      "Validated batch 52 batch loss 10.6738358\n",
      "Validated batch 53 batch loss 10.4380035\n",
      "Validated batch 54 batch loss 10.7831125\n",
      "Validated batch 55 batch loss 10.9449787\n",
      "Validated batch 56 batch loss 10.5712357\n",
      "Validated batch 57 batch loss 10.5628262\n",
      "Validated batch 58 batch loss 10.0621405\n",
      "Validated batch 59 batch loss 10.7055435\n",
      "Validated batch 60 batch loss 10.4785843\n",
      "Validated batch 61 batch loss 10.8686085\n",
      "Validated batch 62 batch loss 10.510849\n",
      "Validated batch 63 batch loss 10.5881653\n",
      "Validated batch 64 batch loss 9.82642365\n",
      "Validated batch 65 batch loss 10.3005657\n",
      "Validated batch 66 batch loss 10.7431927\n",
      "Validated batch 67 batch loss 10.2208843\n",
      "Validated batch 68 batch loss 10.3710709\n",
      "Validated batch 69 batch loss 10.4451218\n",
      "Validated batch 70 batch loss 10.0002937\n",
      "Validated batch 71 batch loss 11.0975132\n",
      "Validated batch 72 batch loss 10.6654892\n",
      "Validated batch 73 batch loss 10.9581757\n",
      "Validated batch 74 batch loss 10.5820847\n",
      "Validated batch 75 batch loss 11.0494967\n",
      "Validated batch 76 batch loss 10.4672413\n",
      "Validated batch 77 batch loss 11.1042938\n",
      "Validated batch 78 batch loss 10.3753414\n",
      "Validated batch 79 batch loss 10.5203476\n",
      "Validated batch 80 batch loss 10.6039066\n",
      "Validated batch 81 batch loss 10.2064705\n",
      "Validated batch 82 batch loss 9.75022125\n",
      "Validated batch 83 batch loss 11.0540743\n",
      "Validated batch 84 batch loss 10.9425507\n",
      "Validated batch 85 batch loss 10.3440018\n",
      "Validated batch 86 batch loss 10.8910904\n",
      "Validated batch 87 batch loss 10.9157467\n",
      "Validated batch 88 batch loss 10.8465023\n",
      "Validated batch 89 batch loss 11.0992985\n",
      "Validated batch 90 batch loss 10.9698105\n",
      "Validated batch 91 batch loss 10.634552\n",
      "Validated batch 92 batch loss 10.0917015\n",
      "Validated batch 93 batch loss 10.7133408\n",
      "Validated batch 94 batch loss 10.3897781\n",
      "Validated batch 95 batch loss 10.3717518\n",
      "Validated batch 96 batch loss 10.2449265\n",
      "Validated batch 97 batch loss 10.640749\n",
      "Validated batch 98 batch loss 10.6981812\n",
      "Validated batch 99 batch loss 10.5617504\n",
      "Validated batch 100 batch loss 10.8063564\n",
      "Validated batch 101 batch loss 10.3753328\n",
      "Validated batch 102 batch loss 10.9398823\n",
      "Validated batch 103 batch loss 11.1398907\n",
      "Validated batch 104 batch loss 10.6766453\n",
      "Validated batch 105 batch loss 10.7014103\n",
      "Validated batch 106 batch loss 10.5285339\n",
      "Validated batch 107 batch loss 10.8891697\n",
      "Validated batch 108 batch loss 10.3893356\n",
      "Validated batch 109 batch loss 10.9032\n",
      "Validated batch 110 batch loss 10.3014908\n",
      "Validated batch 111 batch loss 10.7361069\n",
      "Validated batch 112 batch loss 10.5313454\n",
      "Validated batch 113 batch loss 10.4637451\n",
      "Validated batch 114 batch loss 10.5975256\n",
      "Validated batch 115 batch loss 10.6922531\n",
      "Validated batch 116 batch loss 10.5823\n",
      "Validated batch 117 batch loss 10.6461315\n",
      "Validated batch 118 batch loss 10.3848553\n",
      "Validated batch 119 batch loss 10.9220181\n",
      "Validated batch 120 batch loss 11.1372223\n",
      "Validated batch 121 batch loss 10.8930416\n",
      "Validated batch 122 batch loss 10.7536545\n",
      "Validated batch 123 batch loss 10.7781067\n",
      "Validated batch 124 batch loss 10.9152908\n",
      "Validated batch 125 batch loss 10.8362217\n",
      "Validated batch 126 batch loss 11.1349316\n",
      "Validated batch 127 batch loss 11.0717087\n",
      "Validated batch 128 batch loss 10.0656815\n",
      "Validated batch 129 batch loss 10.8938065\n",
      "Validated batch 130 batch loss 10.5579376\n",
      "Validated batch 131 batch loss 10.6584854\n",
      "Validated batch 132 batch loss 10.7946167\n",
      "Validated batch 133 batch loss 10.0137939\n",
      "Validated batch 134 batch loss 10.0595016\n",
      "Validated batch 135 batch loss 11.0057621\n",
      "Validated batch 136 batch loss 10.5107117\n",
      "Validated batch 137 batch loss 11.1893005\n",
      "Validated batch 138 batch loss 10.5466\n",
      "Validated batch 139 batch loss 11.1483917\n",
      "Validated batch 140 batch loss 10.7362385\n",
      "Validated batch 141 batch loss 10.8414536\n",
      "Validated batch 142 batch loss 10.7343121\n",
      "Validated batch 143 batch loss 10.7667551\n",
      "Validated batch 144 batch loss 11.0460567\n",
      "Validated batch 145 batch loss 10.7075233\n",
      "Validated batch 146 batch loss 10.240489\n",
      "Validated batch 147 batch loss 10.4137115\n",
      "Validated batch 148 batch loss 10.5276489\n",
      "Validated batch 149 batch loss 10.6596222\n",
      "Validated batch 150 batch loss 10.6466675\n",
      "Validated batch 151 batch loss 10.3345413\n",
      "Validated batch 152 batch loss 10.2291718\n",
      "Validated batch 153 batch loss 10.5003948\n",
      "Validated batch 154 batch loss 10.721283\n",
      "Validated batch 155 batch loss 11.0126781\n",
      "Validated batch 156 batch loss 10.8289413\n",
      "Validated batch 157 batch loss 11.2207108\n",
      "Validated batch 158 batch loss 11.3919086\n",
      "Validated batch 159 batch loss 11.1415634\n",
      "Validated batch 160 batch loss 10.641571\n",
      "Validated batch 161 batch loss 10.1436291\n",
      "Validated batch 162 batch loss 10.7109413\n",
      "Validated batch 163 batch loss 11.1409168\n",
      "Validated batch 164 batch loss 10.6559219\n",
      "Validated batch 165 batch loss 9.84373474\n",
      "Validated batch 166 batch loss 10.5397873\n",
      "Validated batch 167 batch loss 10.3566799\n",
      "Validated batch 168 batch loss 10.2065964\n",
      "Validated batch 169 batch loss 10.2474136\n",
      "Validated batch 170 batch loss 9.98264313\n",
      "Validated batch 171 batch loss 10.9107113\n",
      "Validated batch 172 batch loss 10.6265697\n",
      "Validated batch 173 batch loss 10.0695038\n",
      "Validated batch 174 batch loss 9.94122696\n",
      "Validated batch 175 batch loss 10.8601475\n",
      "Validated batch 176 batch loss 10.1483765\n",
      "Validated batch 177 batch loss 10.5603924\n",
      "Validated batch 178 batch loss 10.422039\n",
      "Validated batch 179 batch loss 10.9490585\n",
      "Validated batch 180 batch loss 10.3994446\n",
      "Validated batch 181 batch loss 10.5180779\n",
      "Validated batch 182 batch loss 10.7319355\n",
      "Validated batch 183 batch loss 9.7555294\n",
      "Validated batch 184 batch loss 10.5252447\n",
      "Validated batch 185 batch loss 7.21722746\n",
      "Epoch 3 val loss 10.598749160766602\n",
      "Epoch 3 completed in 748.40 seconds\n",
      "Model ./model_simplebase-epoch-3-loss-10.5987.h5 saved.\n",
      "Start epoch 4 with learning rate 0.0007\n",
      "Trained batch 1 batch loss 9.57602501 epoch total loss 9.57602501\n",
      "Trained batch 2 batch loss 9.25668716 epoch total loss 9.41635609\n",
      "Trained batch 3 batch loss 9.78011513 epoch total loss 9.5376091\n",
      "Trained batch 4 batch loss 9.7335825 epoch total loss 9.58660221\n",
      "Trained batch 5 batch loss 10.0441227 epoch total loss 9.67810631\n",
      "Trained batch 6 batch loss 9.67085552 epoch total loss 9.676898\n",
      "Trained batch 7 batch loss 9.79153824 epoch total loss 9.69327545\n",
      "Trained batch 8 batch loss 9.80711174 epoch total loss 9.70750523\n",
      "Trained batch 9 batch loss 9.61970711 epoch total loss 9.69775\n",
      "Trained batch 10 batch loss 9.51054668 epoch total loss 9.67902946\n",
      "Trained batch 11 batch loss 9.68051147 epoch total loss 9.67916393\n",
      "Trained batch 12 batch loss 9.32251358 epoch total loss 9.64944363\n",
      "Trained batch 13 batch loss 9.23003483 epoch total loss 9.61718082\n",
      "Trained batch 14 batch loss 8.98378468 epoch total loss 9.57193851\n",
      "Trained batch 15 batch loss 9.45931625 epoch total loss 9.56443119\n",
      "Trained batch 16 batch loss 9.49249 epoch total loss 9.55993462\n",
      "Trained batch 17 batch loss 9.6727066 epoch total loss 9.56656742\n",
      "Trained batch 18 batch loss 9.54229355 epoch total loss 9.56522\n",
      "Trained batch 19 batch loss 9.36054611 epoch total loss 9.55444717\n",
      "Trained batch 20 batch loss 9.18143654 epoch total loss 9.53579712\n",
      "Trained batch 21 batch loss 9.56476212 epoch total loss 9.53717613\n",
      "Trained batch 22 batch loss 8.93077469 epoch total loss 9.50961208\n",
      "Trained batch 23 batch loss 8.21554279 epoch total loss 9.45334816\n",
      "Trained batch 24 batch loss 8.81443214 epoch total loss 9.42672729\n",
      "Trained batch 25 batch loss 8.95928288 epoch total loss 9.40803\n",
      "Trained batch 26 batch loss 9.81763172 epoch total loss 9.4237833\n",
      "Trained batch 27 batch loss 10.5805054 epoch total loss 9.46662521\n",
      "Trained batch 28 batch loss 9.64504528 epoch total loss 9.47299767\n",
      "Trained batch 29 batch loss 9.04191208 epoch total loss 9.45813179\n",
      "Trained batch 30 batch loss 9.25281525 epoch total loss 9.45128822\n",
      "Trained batch 31 batch loss 9.76424217 epoch total loss 9.46138382\n",
      "Trained batch 32 batch loss 9.79400158 epoch total loss 9.47177792\n",
      "Trained batch 33 batch loss 9.83536148 epoch total loss 9.48279572\n",
      "Trained batch 34 batch loss 9.75228405 epoch total loss 9.4907217\n",
      "Trained batch 35 batch loss 9.80415249 epoch total loss 9.4996767\n",
      "Trained batch 36 batch loss 9.7264 epoch total loss 9.50597477\n",
      "Trained batch 37 batch loss 9.70454693 epoch total loss 9.51134205\n",
      "Trained batch 38 batch loss 9.79669 epoch total loss 9.51885128\n",
      "Trained batch 39 batch loss 8.97747135 epoch total loss 9.50497\n",
      "Trained batch 40 batch loss 9.84106064 epoch total loss 9.51337242\n",
      "Trained batch 41 batch loss 9.85762882 epoch total loss 9.52176857\n",
      "Trained batch 42 batch loss 9.56844234 epoch total loss 9.52288055\n",
      "Trained batch 43 batch loss 8.98940849 epoch total loss 9.51047421\n",
      "Trained batch 44 batch loss 9.52021313 epoch total loss 9.51069546\n",
      "Trained batch 45 batch loss 9.4300251 epoch total loss 9.50890255\n",
      "Trained batch 46 batch loss 9.48645592 epoch total loss 9.50841427\n",
      "Trained batch 47 batch loss 9.57186317 epoch total loss 9.50976467\n",
      "Trained batch 48 batch loss 9.78474617 epoch total loss 9.51549339\n",
      "Trained batch 49 batch loss 9.66054 epoch total loss 9.5184536\n",
      "Trained batch 50 batch loss 9.38024 epoch total loss 9.51569\n",
      "Trained batch 51 batch loss 9.54916763 epoch total loss 9.51634598\n",
      "Trained batch 52 batch loss 9.36972809 epoch total loss 9.51352596\n",
      "Trained batch 53 batch loss 9.66949844 epoch total loss 9.516469\n",
      "Trained batch 54 batch loss 9.35409546 epoch total loss 9.51346207\n",
      "Trained batch 55 batch loss 9.3318615 epoch total loss 9.51015949\n",
      "Trained batch 56 batch loss 9.1972208 epoch total loss 9.50457096\n",
      "Trained batch 57 batch loss 9.23347664 epoch total loss 9.49981499\n",
      "Trained batch 58 batch loss 9.63917351 epoch total loss 9.50221729\n",
      "Trained batch 59 batch loss 9.47147655 epoch total loss 9.50169659\n",
      "Trained batch 60 batch loss 9.50099182 epoch total loss 9.50168419\n",
      "Trained batch 61 batch loss 9.31799603 epoch total loss 9.49867344\n",
      "Trained batch 62 batch loss 9.27480888 epoch total loss 9.49506187\n",
      "Trained batch 63 batch loss 9.65529442 epoch total loss 9.49760532\n",
      "Trained batch 64 batch loss 9.42835903 epoch total loss 9.4965229\n",
      "Trained batch 65 batch loss 9.30786705 epoch total loss 9.49362087\n",
      "Trained batch 66 batch loss 9.37490177 epoch total loss 9.49182129\n",
      "Trained batch 67 batch loss 8.82430553 epoch total loss 9.48185825\n",
      "Trained batch 68 batch loss 8.96228409 epoch total loss 9.47421741\n",
      "Trained batch 69 batch loss 8.72635651 epoch total loss 9.46337891\n",
      "Trained batch 70 batch loss 8.90427589 epoch total loss 9.45539188\n",
      "Trained batch 71 batch loss 9.53999 epoch total loss 9.45658302\n",
      "Trained batch 72 batch loss 10.1656122 epoch total loss 9.46643066\n",
      "Trained batch 73 batch loss 10.0727234 epoch total loss 9.47473621\n",
      "Trained batch 74 batch loss 9.80331612 epoch total loss 9.47917747\n",
      "Trained batch 75 batch loss 9.78782749 epoch total loss 9.48329258\n",
      "Trained batch 76 batch loss 9.71318913 epoch total loss 9.48631763\n",
      "Trained batch 77 batch loss 9.53162575 epoch total loss 9.48690605\n",
      "Trained batch 78 batch loss 9.53316593 epoch total loss 9.48749828\n",
      "Trained batch 79 batch loss 9.62703133 epoch total loss 9.48926449\n",
      "Trained batch 80 batch loss 9.36853695 epoch total loss 9.48775578\n",
      "Trained batch 81 batch loss 9.58190918 epoch total loss 9.4889183\n",
      "Trained batch 82 batch loss 9.32013321 epoch total loss 9.48685932\n",
      "Trained batch 83 batch loss 9.64293098 epoch total loss 9.48874\n",
      "Trained batch 84 batch loss 9.31973839 epoch total loss 9.48672867\n",
      "Trained batch 85 batch loss 9.54983425 epoch total loss 9.48747063\n",
      "Trained batch 86 batch loss 9.36390686 epoch total loss 9.48603344\n",
      "Trained batch 87 batch loss 8.92942524 epoch total loss 9.47963619\n",
      "Trained batch 88 batch loss 8.61908722 epoch total loss 9.46985722\n",
      "Trained batch 89 batch loss 8.84561 epoch total loss 9.46284294\n",
      "Trained batch 90 batch loss 9.41890812 epoch total loss 9.46235466\n",
      "Trained batch 91 batch loss 9.2855 epoch total loss 9.46041107\n",
      "Trained batch 92 batch loss 9.64816189 epoch total loss 9.46245098\n",
      "Trained batch 93 batch loss 9.75871 epoch total loss 9.46563721\n",
      "Trained batch 94 batch loss 9.80025 epoch total loss 9.46919632\n",
      "Trained batch 95 batch loss 9.45444679 epoch total loss 9.46904182\n",
      "Trained batch 96 batch loss 9.64658642 epoch total loss 9.470891\n",
      "Trained batch 97 batch loss 9.82841492 epoch total loss 9.47457695\n",
      "Trained batch 98 batch loss 9.70892 epoch total loss 9.47696877\n",
      "Trained batch 99 batch loss 9.4717617 epoch total loss 9.47691536\n",
      "Trained batch 100 batch loss 9.27037525 epoch total loss 9.47485065\n",
      "Trained batch 101 batch loss 9.03102112 epoch total loss 9.47045612\n",
      "Trained batch 102 batch loss 8.73838139 epoch total loss 9.46327877\n",
      "Trained batch 103 batch loss 8.96961689 epoch total loss 9.4584856\n",
      "Trained batch 104 batch loss 9.69957542 epoch total loss 9.46080399\n",
      "Trained batch 105 batch loss 9.49849 epoch total loss 9.46116257\n",
      "Trained batch 106 batch loss 9.12813377 epoch total loss 9.45802116\n",
      "Trained batch 107 batch loss 9.1018858 epoch total loss 9.45469284\n",
      "Trained batch 108 batch loss 8.99968147 epoch total loss 9.45047951\n",
      "Trained batch 109 batch loss 8.98800373 epoch total loss 9.44623661\n",
      "Trained batch 110 batch loss 8.9736414 epoch total loss 9.44194\n",
      "Trained batch 111 batch loss 9.22236633 epoch total loss 9.43996239\n",
      "Trained batch 112 batch loss 9.3031311 epoch total loss 9.43874073\n",
      "Trained batch 113 batch loss 9.16227722 epoch total loss 9.4362936\n",
      "Trained batch 114 batch loss 8.40897655 epoch total loss 9.42728138\n",
      "Trained batch 115 batch loss 9.14003181 epoch total loss 9.42478371\n",
      "Trained batch 116 batch loss 9.24693489 epoch total loss 9.42325\n",
      "Trained batch 117 batch loss 9.34929562 epoch total loss 9.42261791\n",
      "Trained batch 118 batch loss 9.69296169 epoch total loss 9.42491\n",
      "Trained batch 119 batch loss 9.41853142 epoch total loss 9.42485619\n",
      "Trained batch 120 batch loss 9.35136604 epoch total loss 9.42424297\n",
      "Trained batch 121 batch loss 9.29267311 epoch total loss 9.42315578\n",
      "Trained batch 122 batch loss 9.28559494 epoch total loss 9.4220295\n",
      "Trained batch 123 batch loss 8.738554 epoch total loss 9.41647243\n",
      "Trained batch 124 batch loss 9.13245583 epoch total loss 9.41418171\n",
      "Trained batch 125 batch loss 9.08227444 epoch total loss 9.41152668\n",
      "Trained batch 126 batch loss 9.00747871 epoch total loss 9.40831947\n",
      "Trained batch 127 batch loss 9.31702423 epoch total loss 9.4076\n",
      "Trained batch 128 batch loss 9.09307575 epoch total loss 9.40514278\n",
      "Trained batch 129 batch loss 9.21444893 epoch total loss 9.40366459\n",
      "Trained batch 130 batch loss 8.76328659 epoch total loss 9.39873886\n",
      "Trained batch 131 batch loss 8.46681404 epoch total loss 9.39162445\n",
      "Trained batch 132 batch loss 8.70872116 epoch total loss 9.38645172\n",
      "Trained batch 133 batch loss 9.09898472 epoch total loss 9.38429\n",
      "Trained batch 134 batch loss 9.16492558 epoch total loss 9.38265324\n",
      "Trained batch 135 batch loss 9.17696762 epoch total loss 9.38112926\n",
      "Trained batch 136 batch loss 9.31707573 epoch total loss 9.38065815\n",
      "Trained batch 137 batch loss 9.32812214 epoch total loss 9.38027477\n",
      "Trained batch 138 batch loss 9.45884323 epoch total loss 9.38084412\n",
      "Trained batch 139 batch loss 9.25755405 epoch total loss 9.3799572\n",
      "Trained batch 140 batch loss 9.32080841 epoch total loss 9.37953472\n",
      "Trained batch 141 batch loss 9.34352684 epoch total loss 9.37927914\n",
      "Trained batch 142 batch loss 9.11576462 epoch total loss 9.37742329\n",
      "Trained batch 143 batch loss 9.29813576 epoch total loss 9.37686825\n",
      "Trained batch 144 batch loss 9.66403389 epoch total loss 9.37886333\n",
      "Trained batch 145 batch loss 9.28267097 epoch total loss 9.3782\n",
      "Trained batch 146 batch loss 9.37048149 epoch total loss 9.37814713\n",
      "Trained batch 147 batch loss 9.14118195 epoch total loss 9.37653542\n",
      "Trained batch 148 batch loss 9.1524086 epoch total loss 9.37502098\n",
      "Trained batch 149 batch loss 9.25652695 epoch total loss 9.37422562\n",
      "Trained batch 150 batch loss 9.3354 epoch total loss 9.37396717\n",
      "Trained batch 151 batch loss 9.23049831 epoch total loss 9.37301731\n",
      "Trained batch 152 batch loss 9.27101231 epoch total loss 9.37234592\n",
      "Trained batch 153 batch loss 8.94408 epoch total loss 9.36954689\n",
      "Trained batch 154 batch loss 9.36000824 epoch total loss 9.3694849\n",
      "Trained batch 155 batch loss 9.98955345 epoch total loss 9.37348461\n",
      "Trained batch 156 batch loss 9.55289936 epoch total loss 9.37463474\n",
      "Trained batch 157 batch loss 9.73655701 epoch total loss 9.37694\n",
      "Trained batch 158 batch loss 10.0001793 epoch total loss 9.38088417\n",
      "Trained batch 159 batch loss 9.63107109 epoch total loss 9.38245773\n",
      "Trained batch 160 batch loss 9.0763607 epoch total loss 9.38054466\n",
      "Trained batch 161 batch loss 9.1540966 epoch total loss 9.37913799\n",
      "Trained batch 162 batch loss 9.26624298 epoch total loss 9.37844086\n",
      "Trained batch 163 batch loss 9.24110889 epoch total loss 9.37759876\n",
      "Trained batch 164 batch loss 9.02767086 epoch total loss 9.37546539\n",
      "Trained batch 165 batch loss 9.31071 epoch total loss 9.37507248\n",
      "Trained batch 166 batch loss 8.83919048 epoch total loss 9.37184429\n",
      "Trained batch 167 batch loss 9.11765862 epoch total loss 9.37032223\n",
      "Trained batch 168 batch loss 8.90536308 epoch total loss 9.36755562\n",
      "Trained batch 169 batch loss 9.35312557 epoch total loss 9.36747\n",
      "Trained batch 170 batch loss 9.64910412 epoch total loss 9.36912632\n",
      "Trained batch 171 batch loss 9.13767242 epoch total loss 9.36777306\n",
      "Trained batch 172 batch loss 9.61485672 epoch total loss 9.36920929\n",
      "Trained batch 173 batch loss 9.4090786 epoch total loss 9.36944\n",
      "Trained batch 174 batch loss 9.01904488 epoch total loss 9.36742592\n",
      "Trained batch 175 batch loss 9.24512863 epoch total loss 9.36672688\n",
      "Trained batch 176 batch loss 8.84283161 epoch total loss 9.36375\n",
      "Trained batch 177 batch loss 9.28760719 epoch total loss 9.3633194\n",
      "Trained batch 178 batch loss 9.15017128 epoch total loss 9.36212254\n",
      "Trained batch 179 batch loss 9.11707211 epoch total loss 9.36075306\n",
      "Trained batch 180 batch loss 9.15522194 epoch total loss 9.35961151\n",
      "Trained batch 181 batch loss 8.89839554 epoch total loss 9.35706329\n",
      "Trained batch 182 batch loss 8.58488655 epoch total loss 9.35282\n",
      "Trained batch 183 batch loss 8.78285 epoch total loss 9.3497057\n",
      "Trained batch 184 batch loss 9.01245689 epoch total loss 9.34787273\n",
      "Trained batch 185 batch loss 9.34859848 epoch total loss 9.3478775\n",
      "Trained batch 186 batch loss 9.3392067 epoch total loss 9.34783077\n",
      "Trained batch 187 batch loss 9.18810844 epoch total loss 9.34697628\n",
      "Trained batch 188 batch loss 9.28932571 epoch total loss 9.34667\n",
      "Trained batch 189 batch loss 8.74510574 epoch total loss 9.34348679\n",
      "Trained batch 190 batch loss 8.87162 epoch total loss 9.34100342\n",
      "Trained batch 191 batch loss 9.02373791 epoch total loss 9.33934212\n",
      "Trained batch 192 batch loss 8.87428093 epoch total loss 9.33692\n",
      "Trained batch 193 batch loss 8.62282372 epoch total loss 9.33322\n",
      "Trained batch 194 batch loss 9.08053 epoch total loss 9.33191776\n",
      "Trained batch 195 batch loss 8.93723488 epoch total loss 9.32989311\n",
      "Trained batch 196 batch loss 9.15642548 epoch total loss 9.3290081\n",
      "Trained batch 197 batch loss 9.12442112 epoch total loss 9.32797\n",
      "Trained batch 198 batch loss 8.71923923 epoch total loss 9.32489491\n",
      "Trained batch 199 batch loss 9.24683571 epoch total loss 9.32450294\n",
      "Trained batch 200 batch loss 8.92693615 epoch total loss 9.32251453\n",
      "Trained batch 201 batch loss 8.94429398 epoch total loss 9.32063293\n",
      "Trained batch 202 batch loss 9.0890379 epoch total loss 9.31948662\n",
      "Trained batch 203 batch loss 8.77614594 epoch total loss 9.31681\n",
      "Trained batch 204 batch loss 8.09984589 epoch total loss 9.31084442\n",
      "Trained batch 205 batch loss 8.33408165 epoch total loss 9.30608\n",
      "Trained batch 206 batch loss 8.9819355 epoch total loss 9.3045063\n",
      "Trained batch 207 batch loss 8.81031609 epoch total loss 9.3021183\n",
      "Trained batch 208 batch loss 9.2167263 epoch total loss 9.30170822\n",
      "Trained batch 209 batch loss 9.15960312 epoch total loss 9.3010273\n",
      "Trained batch 210 batch loss 8.83144569 epoch total loss 9.29879189\n",
      "Trained batch 211 batch loss 8.95835 epoch total loss 9.29717827\n",
      "Trained batch 212 batch loss 8.62610531 epoch total loss 9.29401302\n",
      "Trained batch 213 batch loss 9.18461323 epoch total loss 9.29349899\n",
      "Trained batch 214 batch loss 8.62893486 epoch total loss 9.29039288\n",
      "Trained batch 215 batch loss 8.73215103 epoch total loss 9.28779697\n",
      "Trained batch 216 batch loss 9.20815754 epoch total loss 9.2874279\n",
      "Trained batch 217 batch loss 9.21658707 epoch total loss 9.28710175\n",
      "Trained batch 218 batch loss 9.12064 epoch total loss 9.28633785\n",
      "Trained batch 219 batch loss 8.3757515 epoch total loss 9.28218\n",
      "Trained batch 220 batch loss 8.89993095 epoch total loss 9.28044224\n",
      "Trained batch 221 batch loss 8.97618866 epoch total loss 9.27906513\n",
      "Trained batch 222 batch loss 9.03175354 epoch total loss 9.27795124\n",
      "Trained batch 223 batch loss 9.03538704 epoch total loss 9.2768631\n",
      "Trained batch 224 batch loss 8.93073845 epoch total loss 9.27531719\n",
      "Trained batch 225 batch loss 8.64129734 epoch total loss 9.2725\n",
      "Trained batch 226 batch loss 8.78526497 epoch total loss 9.27034378\n",
      "Trained batch 227 batch loss 8.90895653 epoch total loss 9.26875114\n",
      "Trained batch 228 batch loss 8.71953297 epoch total loss 9.26634216\n",
      "Trained batch 229 batch loss 8.49036789 epoch total loss 9.26295471\n",
      "Trained batch 230 batch loss 8.71148109 epoch total loss 9.26055622\n",
      "Trained batch 231 batch loss 8.34783363 epoch total loss 9.25660515\n",
      "Trained batch 232 batch loss 8.68113422 epoch total loss 9.2541256\n",
      "Trained batch 233 batch loss 9.0230751 epoch total loss 9.25313377\n",
      "Trained batch 234 batch loss 8.96260643 epoch total loss 9.25189304\n",
      "Trained batch 235 batch loss 9.05241776 epoch total loss 9.25104427\n",
      "Trained batch 236 batch loss 9.01192665 epoch total loss 9.25003147\n",
      "Trained batch 237 batch loss 8.91862202 epoch total loss 9.24863338\n",
      "Trained batch 238 batch loss 8.46408558 epoch total loss 9.24533653\n",
      "Trained batch 239 batch loss 8.43065643 epoch total loss 9.2419281\n",
      "Trained batch 240 batch loss 8.70033264 epoch total loss 9.23967171\n",
      "Trained batch 241 batch loss 8.68119049 epoch total loss 9.23735428\n",
      "Trained batch 242 batch loss 8.71516132 epoch total loss 9.23519611\n",
      "Trained batch 243 batch loss 8.11467934 epoch total loss 9.2305851\n",
      "Trained batch 244 batch loss 8.05052471 epoch total loss 9.22574902\n",
      "Trained batch 245 batch loss 8.4154911 epoch total loss 9.22244167\n",
      "Trained batch 246 batch loss 8.35535622 epoch total loss 9.21891785\n",
      "Trained batch 247 batch loss 8.89732552 epoch total loss 9.21761513\n",
      "Trained batch 248 batch loss 8.85526085 epoch total loss 9.2161541\n",
      "Trained batch 249 batch loss 9.05227852 epoch total loss 9.21549606\n",
      "Trained batch 250 batch loss 8.95100594 epoch total loss 9.21443748\n",
      "Trained batch 251 batch loss 8.85993767 epoch total loss 9.21302509\n",
      "Trained batch 252 batch loss 8.9714222 epoch total loss 9.2120657\n",
      "Trained batch 253 batch loss 8.22747 epoch total loss 9.20817471\n",
      "Trained batch 254 batch loss 8.16454601 epoch total loss 9.20406628\n",
      "Trained batch 255 batch loss 8.61611176 epoch total loss 9.20176\n",
      "Trained batch 256 batch loss 8.86222 epoch total loss 9.20043468\n",
      "Trained batch 257 batch loss 8.86731052 epoch total loss 9.19913864\n",
      "Trained batch 258 batch loss 9.04616165 epoch total loss 9.19854641\n",
      "Trained batch 259 batch loss 9.02337837 epoch total loss 9.19787\n",
      "Trained batch 260 batch loss 8.76497746 epoch total loss 9.19620419\n",
      "Trained batch 261 batch loss 8.92857 epoch total loss 9.19517899\n",
      "Trained batch 262 batch loss 8.75361061 epoch total loss 9.19349384\n",
      "Trained batch 263 batch loss 8.9873457 epoch total loss 9.19271\n",
      "Trained batch 264 batch loss 8.5136795 epoch total loss 9.19013786\n",
      "Trained batch 265 batch loss 8.04353237 epoch total loss 9.18581\n",
      "Trained batch 266 batch loss 8.12683773 epoch total loss 9.18182945\n",
      "Trained batch 267 batch loss 8.53876877 epoch total loss 9.17942142\n",
      "Trained batch 268 batch loss 9.07339668 epoch total loss 9.1790266\n",
      "Trained batch 269 batch loss 8.97484779 epoch total loss 9.17826748\n",
      "Trained batch 270 batch loss 9.10353 epoch total loss 9.17799\n",
      "Trained batch 271 batch loss 9.04580307 epoch total loss 9.17750263\n",
      "Trained batch 272 batch loss 9.25324631 epoch total loss 9.17778111\n",
      "Trained batch 273 batch loss 9.14050674 epoch total loss 9.17764473\n",
      "Trained batch 274 batch loss 9.03982067 epoch total loss 9.17714214\n",
      "Trained batch 275 batch loss 8.69064236 epoch total loss 9.17537308\n",
      "Trained batch 276 batch loss 8.80297279 epoch total loss 9.17402363\n",
      "Trained batch 277 batch loss 9.03532 epoch total loss 9.17352295\n",
      "Trained batch 278 batch loss 8.63314342 epoch total loss 9.17157936\n",
      "Trained batch 279 batch loss 8.89848137 epoch total loss 9.1706\n",
      "Trained batch 280 batch loss 8.99382687 epoch total loss 9.16996861\n",
      "Trained batch 281 batch loss 8.71721268 epoch total loss 9.16835785\n",
      "Trained batch 282 batch loss 8.58228779 epoch total loss 9.16628\n",
      "Trained batch 283 batch loss 8.73749542 epoch total loss 9.1647644\n",
      "Trained batch 284 batch loss 8.68730354 epoch total loss 9.16308308\n",
      "Trained batch 285 batch loss 8.6350975 epoch total loss 9.16123\n",
      "Trained batch 286 batch loss 8.78167534 epoch total loss 9.15990353\n",
      "Trained batch 287 batch loss 8.98735237 epoch total loss 9.15930176\n",
      "Trained batch 288 batch loss 8.73057365 epoch total loss 9.15781307\n",
      "Trained batch 289 batch loss 8.68056297 epoch total loss 9.15616226\n",
      "Trained batch 290 batch loss 8.80437088 epoch total loss 9.15494919\n",
      "Trained batch 291 batch loss 8.76713467 epoch total loss 9.15361691\n",
      "Trained batch 292 batch loss 8.63607597 epoch total loss 9.15184402\n",
      "Trained batch 293 batch loss 8.92993832 epoch total loss 9.15108681\n",
      "Trained batch 294 batch loss 8.60776234 epoch total loss 9.14923763\n",
      "Trained batch 295 batch loss 8.57812405 epoch total loss 9.14730167\n",
      "Trained batch 296 batch loss 9.1614418 epoch total loss 9.14734936\n",
      "Trained batch 297 batch loss 8.81393337 epoch total loss 9.14622688\n",
      "Trained batch 298 batch loss 9.23713112 epoch total loss 9.14653206\n",
      "Trained batch 299 batch loss 9.87075806 epoch total loss 9.14895439\n",
      "Trained batch 300 batch loss 9.4564352 epoch total loss 9.14998\n",
      "Trained batch 301 batch loss 9.96263885 epoch total loss 9.15267944\n",
      "Trained batch 302 batch loss 9.5739994 epoch total loss 9.15407467\n",
      "Trained batch 303 batch loss 8.70767117 epoch total loss 9.15260124\n",
      "Trained batch 304 batch loss 8.73514271 epoch total loss 9.15122795\n",
      "Trained batch 305 batch loss 8.67087078 epoch total loss 9.14965343\n",
      "Trained batch 306 batch loss 8.3961153 epoch total loss 9.14719\n",
      "Trained batch 307 batch loss 8.54880524 epoch total loss 9.14524174\n",
      "Trained batch 308 batch loss 8.83145332 epoch total loss 9.14422321\n",
      "Trained batch 309 batch loss 8.99686337 epoch total loss 9.14374542\n",
      "Trained batch 310 batch loss 8.40104771 epoch total loss 9.14135075\n",
      "Trained batch 311 batch loss 8.43392181 epoch total loss 9.13907528\n",
      "Trained batch 312 batch loss 8.37337685 epoch total loss 9.13662052\n",
      "Trained batch 313 batch loss 8.92308426 epoch total loss 9.13593864\n",
      "Trained batch 314 batch loss 8.99144173 epoch total loss 9.13547897\n",
      "Trained batch 315 batch loss 9.19156075 epoch total loss 9.13565731\n",
      "Trained batch 316 batch loss 8.9166584 epoch total loss 9.13496399\n",
      "Trained batch 317 batch loss 8.96446228 epoch total loss 9.13442612\n",
      "Trained batch 318 batch loss 9.06845093 epoch total loss 9.13421822\n",
      "Trained batch 319 batch loss 8.92752 epoch total loss 9.13357\n",
      "Trained batch 320 batch loss 8.75346 epoch total loss 9.13238239\n",
      "Trained batch 321 batch loss 8.82214832 epoch total loss 9.13141632\n",
      "Trained batch 322 batch loss 9.08993053 epoch total loss 9.13128662\n",
      "Trained batch 323 batch loss 8.9060955 epoch total loss 9.13058949\n",
      "Trained batch 324 batch loss 8.94595146 epoch total loss 9.13002\n",
      "Trained batch 325 batch loss 8.39892673 epoch total loss 9.12777\n",
      "Trained batch 326 batch loss 8.46210098 epoch total loss 9.12572861\n",
      "Trained batch 327 batch loss 8.72334862 epoch total loss 9.12449837\n",
      "Trained batch 328 batch loss 8.76270199 epoch total loss 9.12339497\n",
      "Trained batch 329 batch loss 9.12461758 epoch total loss 9.12339878\n",
      "Trained batch 330 batch loss 8.79645538 epoch total loss 9.12240791\n",
      "Trained batch 331 batch loss 9.08806229 epoch total loss 9.12230396\n",
      "Trained batch 332 batch loss 8.72449493 epoch total loss 9.12110615\n",
      "Trained batch 333 batch loss 8.44773865 epoch total loss 9.11908436\n",
      "Trained batch 334 batch loss 8.87114906 epoch total loss 9.11834145\n",
      "Trained batch 335 batch loss 8.64381695 epoch total loss 9.11692524\n",
      "Trained batch 336 batch loss 8.14152527 epoch total loss 9.11402225\n",
      "Trained batch 337 batch loss 8.89813232 epoch total loss 9.11338234\n",
      "Trained batch 338 batch loss 8.59536934 epoch total loss 9.11185\n",
      "Trained batch 339 batch loss 8.51501846 epoch total loss 9.1100893\n",
      "Trained batch 340 batch loss 8.62558 epoch total loss 9.10866451\n",
      "Validated batch 87 batch loss 7.22642946\n",
      "Validated batch 88 batch loss 7.5281353\n",
      "Validated batch 89 batch loss 7.94053459\n",
      "Validated batch 90 batch loss 7.47755337\n",
      "Validated batch 91 batch loss 6.99949\n",
      "Validated batch 92 batch loss 6.73882198\n",
      "Validated batch 93 batch loss 6.81889105\n",
      "Validated batch 94 batch loss 7.06226301\n",
      "Validated batch 95 batch loss 6.87629032\n",
      "Validated batch 96 batch loss 6.3310008\n",
      "Validated batch 97 batch loss 6.77442312\n",
      "Validated batch 98 batch loss 7.38539791\n",
      "Validated batch 99 batch loss 6.76375961\n",
      "Validated batch 100 batch loss 7.09867668\n",
      "Validated batch 101 batch loss 7.18487215\n",
      "Validated batch 102 batch loss 7.18967867\n",
      "Validated batch 103 batch loss 7.27384853\n",
      "Validated batch 104 batch loss 6.99065208\n",
      "Validated batch 105 batch loss 7.09860134\n",
      "Validated batch 106 batch loss 6.94867039\n",
      "Validated batch 107 batch loss 7.13840914\n",
      "Validated batch 108 batch loss 7.51001787\n",
      "Validated batch 109 batch loss 6.83530283\n",
      "Validated batch 110 batch loss 7.30337191\n",
      "Validated batch 111 batch loss 7.31026268\n",
      "Validated batch 112 batch loss 6.81826591\n",
      "Validated batch 113 batch loss 7.04958248\n",
      "Validated batch 114 batch loss 7.08935356\n",
      "Validated batch 115 batch loss 7.28949404\n",
      "Validated batch 116 batch loss 6.99755955\n",
      "Validated batch 117 batch loss 7.11019\n",
      "Validated batch 118 batch loss 6.84342575\n",
      "Validated batch 119 batch loss 7.39088488\n",
      "Validated batch 120 batch loss 7.43404198\n",
      "Validated batch 121 batch loss 7.18891048\n",
      "Validated batch 122 batch loss 7.15936947\n",
      "Validated batch 123 batch loss 7.14844322\n",
      "Validated batch 124 batch loss 7.26472902\n",
      "Validated batch 125 batch loss 7.14624834\n",
      "Validated batch 126 batch loss 7.462605\n",
      "Validated batch 127 batch loss 7.46050072\n",
      "Validated batch 128 batch loss 6.70636702\n",
      "Validated batch 129 batch loss 7.30231905\n",
      "Validated batch 130 batch loss 6.93592\n",
      "Validated batch 131 batch loss 7.19621325\n",
      "Validated batch 132 batch loss 7.3009243\n",
      "Validated batch 133 batch loss 6.59805059\n",
      "Validated batch 134 batch loss 6.75881815\n",
      "Validated batch 135 batch loss 7.16943264\n",
      "Validated batch 136 batch loss 6.89991808\n",
      "Validated batch 137 batch loss 7.51426\n",
      "Validated batch 138 batch loss 6.95359421\n",
      "Validated batch 139 batch loss 7.24623775\n",
      "Validated batch 140 batch loss 7.05152512\n",
      "Validated batch 141 batch loss 6.97249413\n",
      "Validated batch 142 batch loss 6.87569857\n",
      "Validated batch 143 batch loss 6.83856869\n",
      "Validated batch 144 batch loss 7.06111956\n",
      "Validated batch 145 batch loss 6.75949478\n",
      "Validated batch 146 batch loss 7.01927805\n",
      "Validated batch 147 batch loss 7.15875053\n",
      "Validated batch 148 batch loss 7.15917349\n",
      "Validated batch 149 batch loss 6.91152\n",
      "Validated batch 150 batch loss 6.79919863\n",
      "Validated batch 151 batch loss 6.82445145\n",
      "Validated batch 152 batch loss 7.28359127\n",
      "Validated batch 153 batch loss 6.99424505\n",
      "Validated batch 154 batch loss 6.98449039\n",
      "Validated batch 155 batch loss 6.94269657\n",
      "Validated batch 156 batch loss 6.69092417\n",
      "Validated batch 157 batch loss 6.52029181\n",
      "Validated batch 158 batch loss 7.02978897\n",
      "Validated batch 159 batch loss 6.91707516\n",
      "Validated batch 160 batch loss 6.67567348\n",
      "Validated batch 161 batch loss 7.09277916\n",
      "Validated batch 162 batch loss 7.2522893\n",
      "Validated batch 163 batch loss 7.02608871\n",
      "Validated batch 164 batch loss 6.87447453\n",
      "Validated batch 165 batch loss 6.80937719\n",
      "Validated batch 166 batch loss 6.94246817\n",
      "Validated batch 167 batch loss 7.15327311\n",
      "Validated batch 168 batch loss 6.91703\n",
      "Validated batch 169 batch loss 7.25575304\n",
      "Validated batch 170 batch loss 7.03579092\n",
      "Validated batch 171 batch loss 7.00524521\n",
      "Validated batch 172 batch loss 7.46573544\n",
      "Validated batch 173 batch loss 7.30691957\n",
      "Validated batch 174 batch loss 6.89139462\n",
      "Validated batch 175 batch loss 7.03990746\n",
      "Validated batch 176 batch loss 7.27647209\n",
      "Validated batch 177 batch loss 6.90796566\n",
      "Validated batch 178 batch loss 7.24907589\n",
      "Validated batch 179 batch loss 6.74819899\n",
      "Validated batch 180 batch loss 7.2841382\n",
      "Validated batch 181 batch loss 6.82468319\n",
      "Validated batch 182 batch loss 7.01777744\n",
      "Validated batch 183 batch loss 7.21213675\n",
      "Validated batch 184 batch loss 7.06881332\n",
      "Validated batch 185 batch loss 4.1249\n",
      "Epoch 4 val loss 7.028262138366699\n",
      "Epoch 4 completed in 740.24 seconds\n",
      "Model ./model_simplebase-epoch-4-loss-7.0283.h5 saved.\n",
      "Start epoch 5 with learning rate 0.0007\n",
      "Trained batch 1 batch loss 6.80160141 epoch total loss 6.80160141\n",
      "Trained batch 2 batch loss 7.07049 epoch total loss 6.93604565\n",
      "Trained batch 3 batch loss 7.07501173 epoch total loss 6.98236799\n",
      "Trained batch 4 batch loss 6.95393181 epoch total loss 6.97525883\n",
      "Trained batch 5 batch loss 6.36757565 epoch total loss 6.85372257\n",
      "Trained batch 6 batch loss 6.94059563 epoch total loss 6.86820078\n",
      "Trained batch 7 batch loss 6.86860704 epoch total loss 6.86825895\n",
      "Trained batch 8 batch loss 6.85081768 epoch total loss 6.86607885\n",
      "Trained batch 9 batch loss 6.70392799 epoch total loss 6.84806252\n",
      "Trained batch 10 batch loss 6.47501612 epoch total loss 6.81075764\n",
      "Trained batch 11 batch loss 6.79081488 epoch total loss 6.8089447\n",
      "Trained batch 12 batch loss 7.40171385 epoch total loss 6.85834169\n",
      "Trained batch 13 batch loss 6.92225885 epoch total loss 6.86325836\n",
      "Trained batch 14 batch loss 7.23999929 epoch total loss 6.89016819\n",
      "Trained batch 15 batch loss 7.13365269 epoch total loss 6.90640068\n",
      "Trained batch 16 batch loss 6.55023909 epoch total loss 6.88414049\n",
      "Trained batch 17 batch loss 7.16568899 epoch total loss 6.900702\n",
      "Trained batch 18 batch loss 7.01641798 epoch total loss 6.90713072\n",
      "Trained batch 19 batch loss 6.94706774 epoch total loss 6.90923262\n",
      "Trained batch 20 batch loss 6.91825724 epoch total loss 6.90968418\n",
      "Trained batch 21 batch loss 7.0496068 epoch total loss 6.91634703\n",
      "Trained batch 22 batch loss 7.04296398 epoch total loss 6.92210245\n",
      "Trained batch 23 batch loss 6.64838 epoch total loss 6.91020155\n",
      "Trained batch 24 batch loss 6.96704865 epoch total loss 6.91256952\n",
      "Trained batch 25 batch loss 7.19985867 epoch total loss 6.9240613\n",
      "Trained batch 26 batch loss 7.01883 epoch total loss 6.92770624\n",
      "Trained batch 27 batch loss 7.30928564 epoch total loss 6.94183874\n",
      "Trained batch 28 batch loss 6.98015928 epoch total loss 6.94320726\n",
      "Trained batch 29 batch loss 6.85530758 epoch total loss 6.94017601\n",
      "Trained batch 30 batch loss 6.57745075 epoch total loss 6.92808533\n",
      "Trained batch 31 batch loss 6.89864349 epoch total loss 6.92713594\n",
      "Trained batch 32 batch loss 7.04056787 epoch total loss 6.93068075\n",
      "Trained batch 33 batch loss 7.17449665 epoch total loss 6.93806934\n",
      "Trained batch 34 batch loss 6.98460293 epoch total loss 6.93943787\n",
      "Trained batch 35 batch loss 7.52902842 epoch total loss 6.95628309\n",
      "Trained batch 36 batch loss 7.00269556 epoch total loss 6.95757246\n",
      "Trained batch 37 batch loss 7.28066349 epoch total loss 6.9663043\n",
      "Trained batch 38 batch loss 7.016222 epoch total loss 6.96761847\n",
      "Trained batch 39 batch loss 6.87067604 epoch total loss 6.96513271\n",
      "Trained batch 40 batch loss 7.06621265 epoch total loss 6.96766\n",
      "Trained batch 41 batch loss 6.76561546 epoch total loss 6.96273232\n",
      "Trained batch 42 batch loss 6.86570215 epoch total loss 6.96042156\n",
      "Trained batch 43 batch loss 6.92658234 epoch total loss 6.9596343\n",
      "Trained batch 44 batch loss 6.87388515 epoch total loss 6.95768547\n",
      "Trained batch 45 batch loss 6.97097445 epoch total loss 6.95798063\n",
      "Trained batch 46 batch loss 7.87371111 epoch total loss 6.97788811\n",
      "Trained batch 47 batch loss 7.50156 epoch total loss 6.98903\n",
      "Trained batch 48 batch loss 7.51179123 epoch total loss 6.99992037\n",
      "Trained batch 49 batch loss 7.70074558 epoch total loss 7.0142231\n",
      "Trained batch 50 batch loss 7.7213912 epoch total loss 7.02836657\n",
      "Trained batch 51 batch loss 7.17169285 epoch total loss 7.03117704\n",
      "Trained batch 52 batch loss 6.66021824 epoch total loss 7.02404308\n",
      "Trained batch 53 batch loss 7.62202883 epoch total loss 7.035326\n",
      "Trained batch 54 batch loss 7.0281148 epoch total loss 7.03519249\n",
      "Trained batch 55 batch loss 6.81245947 epoch total loss 7.03114271\n",
      "Trained batch 56 batch loss 7.02535629 epoch total loss 7.03103971\n",
      "Trained batch 57 batch loss 7.09870148 epoch total loss 7.03222656\n",
      "Trained batch 58 batch loss 7.02590752 epoch total loss 7.03211784\n",
      "Trained batch 59 batch loss 7.11932087 epoch total loss 7.03359556\n",
      "Trained batch 60 batch loss 6.91490173 epoch total loss 7.03161716\n",
      "Trained batch 61 batch loss 7.46648645 epoch total loss 7.03874636\n",
      "Trained batch 62 batch loss 7.31386089 epoch total loss 7.0431838\n",
      "Trained batch 63 batch loss 7.25734091 epoch total loss 7.04658318\n",
      "Trained batch 64 batch loss 7.44431448 epoch total loss 7.05279779\n",
      "Trained batch 65 batch loss 7.09063387 epoch total loss 7.05338\n",
      "Trained batch 66 batch loss 6.34549475 epoch total loss 7.04265451\n",
      "Trained batch 67 batch loss 6.36950397 epoch total loss 7.03260756\n",
      "Trained batch 68 batch loss 7.08438492 epoch total loss 7.03336859\n",
      "Trained batch 69 batch loss 7.06054068 epoch total loss 7.03376245\n",
      "Trained batch 70 batch loss 7.42815781 epoch total loss 7.03939676\n",
      "Trained batch 71 batch loss 7.16791296 epoch total loss 7.04120684\n",
      "Trained batch 72 batch loss 7.49928045 epoch total loss 7.0475688\n",
      "Trained batch 73 batch loss 7.45009136 epoch total loss 7.05308294\n",
      "Trained batch 74 batch loss 7.38344049 epoch total loss 7.05754709\n",
      "Trained batch 75 batch loss 7.47284508 epoch total loss 7.06308413\n",
      "Trained batch 76 batch loss 7.16507864 epoch total loss 7.06442642\n",
      "Trained batch 77 batch loss 7.41981316 epoch total loss 7.06904173\n",
      "Trained batch 78 batch loss 6.94740915 epoch total loss 7.06748199\n",
      "Trained batch 79 batch loss 7.16908646 epoch total loss 7.06876802\n",
      "Trained batch 80 batch loss 6.36856747 epoch total loss 7.06001568\n",
      "Trained batch 81 batch loss 6.36098623 epoch total loss 7.0513854\n",
      "Trained batch 82 batch loss 7.40668821 epoch total loss 7.05571842\n",
      "Trained batch 83 batch loss 7.20269775 epoch total loss 7.0574894\n",
      "Trained batch 84 batch loss 6.73308182 epoch total loss 7.05362749\n",
      "Trained batch 85 batch loss 6.94381237 epoch total loss 7.05233526\n",
      "Trained batch 86 batch loss 6.89871359 epoch total loss 7.05054903\n",
      "Trained batch 87 batch loss 7.32702541 epoch total loss 7.05372715\n",
      "Trained batch 88 batch loss 7.16972637 epoch total loss 7.0550456\n",
      "Trained batch 89 batch loss 7.36633 epoch total loss 7.05854321\n",
      "Trained batch 90 batch loss 7.27184772 epoch total loss 7.06091309\n",
      "Trained batch 91 batch loss 7.37529516 epoch total loss 7.06436777\n",
      "Trained batch 92 batch loss 6.86910629 epoch total loss 7.06224537\n",
      "Trained batch 93 batch loss 6.82009268 epoch total loss 7.05964136\n",
      "Trained batch 94 batch loss 7.06249714 epoch total loss 7.0596714\n",
      "Trained batch 95 batch loss 7.07276821 epoch total loss 7.05980921\n",
      "Trained batch 96 batch loss 7.01459789 epoch total loss 7.05933809\n",
      "Trained batch 97 batch loss 7.11006832 epoch total loss 7.05986118\n",
      "Trained batch 98 batch loss 7.12366056 epoch total loss 7.06051207\n",
      "Trained batch 99 batch loss 6.92803192 epoch total loss 7.05917406\n",
      "Trained batch 100 batch loss 7.31581306 epoch total loss 7.06174\n",
      "Trained batch 101 batch loss 7.24170208 epoch total loss 7.06352186\n",
      "Trained batch 102 batch loss 7.09146643 epoch total loss 7.06379604\n",
      "Trained batch 103 batch loss 6.80417728 epoch total loss 7.06127596\n",
      "Trained batch 104 batch loss 7.02343178 epoch total loss 7.06091213\n",
      "Trained batch 105 batch loss 6.85183811 epoch total loss 7.05892086\n",
      "Trained batch 106 batch loss 6.46888447 epoch total loss 7.05335474\n",
      "Trained batch 107 batch loss 6.84901905 epoch total loss 7.05144453\n",
      "Trained batch 108 batch loss 6.4044838 epoch total loss 7.04545403\n",
      "Trained batch 109 batch loss 6.62781525 epoch total loss 7.04162264\n",
      "Trained batch 110 batch loss 7.03865576 epoch total loss 7.04159546\n",
      "Trained batch 111 batch loss 6.44046354 epoch total loss 7.03618\n",
      "Trained batch 112 batch loss 6.51845741 epoch total loss 7.03155756\n",
      "Trained batch 113 batch loss 6.92080545 epoch total loss 7.03057718\n",
      "Trained batch 114 batch loss 7.45627117 epoch total loss 7.03431129\n",
      "Trained batch 115 batch loss 7.53945255 epoch total loss 7.03870392\n",
      "Trained batch 116 batch loss 7.57623053 epoch total loss 7.04333782\n",
      "Trained batch 117 batch loss 7.48869324 epoch total loss 7.04714441\n",
      "Trained batch 118 batch loss 7.06142855 epoch total loss 7.04726505\n",
      "Trained batch 119 batch loss 7.07612467 epoch total loss 7.04750729\n",
      "Trained batch 120 batch loss 7.25846815 epoch total loss 7.04926538\n",
      "Trained batch 121 batch loss 6.85047 epoch total loss 7.04762268\n",
      "Trained batch 122 batch loss 7.27172232 epoch total loss 7.04945946\n",
      "Trained batch 123 batch loss 7.12067318 epoch total loss 7.05003834\n",
      "Trained batch 124 batch loss 6.96994114 epoch total loss 7.0493927\n",
      "Trained batch 125 batch loss 7.40820312 epoch total loss 7.05226326\n",
      "Trained batch 126 batch loss 7.44412374 epoch total loss 7.05537319\n",
      "Trained batch 127 batch loss 6.95992422 epoch total loss 7.0546217\n",
      "Trained batch 128 batch loss 6.67279434 epoch total loss 7.0516386\n",
      "Trained batch 129 batch loss 7.094769 epoch total loss 7.05197287\n",
      "Trained batch 130 batch loss 6.87006378 epoch total loss 7.05057383\n",
      "Trained batch 131 batch loss 7.05991316 epoch total loss 7.05064535\n",
      "Trained batch 132 batch loss 7.33093739 epoch total loss 7.05276871\n",
      "Trained batch 133 batch loss 7.02843 epoch total loss 7.0525856\n",
      "Trained batch 134 batch loss 7.30695868 epoch total loss 7.05448389\n",
      "Trained batch 135 batch loss 7.50442457 epoch total loss 7.05781651\n",
      "Trained batch 136 batch loss 7.41038036 epoch total loss 7.06040907\n",
      "Trained batch 137 batch loss 6.72388124 epoch total loss 7.05795288\n",
      "Trained batch 138 batch loss 6.97319508 epoch total loss 7.05733871\n",
      "Trained batch 139 batch loss 6.35764837 epoch total loss 7.05230474\n",
      "Trained batch 140 batch loss 6.10350943 epoch total loss 7.04552794\n",
      "Trained batch 141 batch loss 6.66801119 epoch total loss 7.04285049\n",
      "Trained batch 142 batch loss 6.64913034 epoch total loss 7.04007769\n",
      "Trained batch 143 batch loss 5.67733717 epoch total loss 7.0305481\n",
      "Trained batch 144 batch loss 5.78475189 epoch total loss 7.02189636\n",
      "Trained batch 145 batch loss 5.92559814 epoch total loss 7.01433563\n",
      "Trained batch 146 batch loss 6.19998264 epoch total loss 7.00875807\n",
      "Trained batch 147 batch loss 6.62250328 epoch total loss 7.0061307\n",
      "Trained batch 148 batch loss 7.02740622 epoch total loss 7.00627518\n",
      "Trained batch 149 batch loss 7.11341333 epoch total loss 7.00699425\n",
      "Trained batch 150 batch loss 7.5203619 epoch total loss 7.01041651\n",
      "Trained batch 151 batch loss 7.31008101 epoch total loss 7.0124011\n",
      "Trained batch 152 batch loss 7.49349308 epoch total loss 7.01556635\n",
      "Trained batch 153 batch loss 6.57538652 epoch total loss 7.01268959\n",
      "Trained batch 154 batch loss 7.04872942 epoch total loss 7.01292372\n",
      "Trained batch 155 batch loss 7.11207247 epoch total loss 7.01356316\n",
      "Trained batch 156 batch loss 7.2644062 epoch total loss 7.01517105\n",
      "Trained batch 157 batch loss 7.4070487 epoch total loss 7.01766777\n",
      "Trained batch 158 batch loss 7.24941444 epoch total loss 7.01913404\n",
      "Trained batch 159 batch loss 6.91139793 epoch total loss 7.01845646\n",
      "Trained batch 160 batch loss 6.6504817 epoch total loss 7.01615667\n",
      "Trained batch 161 batch loss 6.97759295 epoch total loss 7.01591682\n",
      "Trained batch 162 batch loss 6.8951149 epoch total loss 7.01517153\n",
      "Trained batch 163 batch loss 6.97025824 epoch total loss 7.01489544\n",
      "Trained batch 164 batch loss 7.17527 epoch total loss 7.01587343\n",
      "Trained batch 165 batch loss 7.69934511 epoch total loss 7.02001572\n",
      "Trained batch 166 batch loss 7.3202529 epoch total loss 7.02182484\n",
      "Trained batch 167 batch loss 7.4658761 epoch total loss 7.02448368\n",
      "Trained batch 168 batch loss 7.52923536 epoch total loss 7.02748775\n",
      "Trained batch 169 batch loss 7.65282249 epoch total loss 7.03118801\n",
      "Trained batch 170 batch loss 7.51173973 epoch total loss 7.0340147\n",
      "Trained batch 171 batch loss 7.49789238 epoch total loss 7.03672743\n",
      "Trained batch 172 batch loss 7.27456808 epoch total loss 7.03811026\n",
      "Trained batch 173 batch loss 7.51790142 epoch total loss 7.04088354\n",
      "Trained batch 174 batch loss 7.0434227 epoch total loss 7.04089832\n",
      "Trained batch 175 batch loss 7.01129818 epoch total loss 7.04072952\n",
      "Trained batch 176 batch loss 6.99060917 epoch total loss 7.04044485\n",
      "Trained batch 177 batch loss 7.11164665 epoch total loss 7.0408473\n",
      "Trained batch 178 batch loss 6.99403811 epoch total loss 7.04058409\n",
      "Trained batch 179 batch loss 7.10176039 epoch total loss 7.04092646\n",
      "Trained batch 180 batch loss 7.037323 epoch total loss 7.04090643\n",
      "Trained batch 181 batch loss 7.11213207 epoch total loss 7.0413003\n",
      "Trained batch 182 batch loss 6.7808814 epoch total loss 7.03986931\n",
      "Trained batch 183 batch loss 6.46033287 epoch total loss 7.03670263\n",
      "Trained batch 184 batch loss 6.17904139 epoch total loss 7.03204155\n",
      "Trained batch 185 batch loss 6.24224472 epoch total loss 7.02777195\n",
      "Trained batch 186 batch loss 6.28685331 epoch total loss 7.02378845\n",
      "Trained batch 187 batch loss 6.0666213 epoch total loss 7.01867\n",
      "Trained batch 188 batch loss 5.82101297 epoch total loss 7.0123\n",
      "Trained batch 189 batch loss 5.76933479 epoch total loss 7.005723\n",
      "Trained batch 190 batch loss 6.46253157 epoch total loss 7.00286436\n",
      "Trained batch 191 batch loss 6.96722794 epoch total loss 7.00267792\n",
      "Trained batch 192 batch loss 7.12353754 epoch total loss 7.00330734\n",
      "Trained batch 193 batch loss 6.95861149 epoch total loss 7.0030756\n",
      "Trained batch 194 batch loss 6.79946136 epoch total loss 7.00202608\n",
      "Trained batch 195 batch loss 7.07968426 epoch total loss 7.00242472\n",
      "Trained batch 196 batch loss 6.93684769 epoch total loss 7.00209\n",
      "Trained batch 197 batch loss 7.17290115 epoch total loss 7.00295687\n",
      "Trained batch 198 batch loss 7.23100805 epoch total loss 7.00410843\n",
      "Trained batch 199 batch loss 7.28261566 epoch total loss 7.00550795\n",
      "Trained batch 200 batch loss 7.2578764 epoch total loss 7.00677\n",
      "Trained batch 201 batch loss 7.21729517 epoch total loss 7.00781727\n",
      "Trained batch 202 batch loss 6.98208141 epoch total loss 7.00769\n",
      "Trained batch 203 batch loss 7.20162058 epoch total loss 7.00864553\n",
      "Trained batch 204 batch loss 7.04863548 epoch total loss 7.00884104\n",
      "Trained batch 205 batch loss 7.27299833 epoch total loss 7.01012945\n",
      "Trained batch 206 batch loss 7.0915184 epoch total loss 7.01052475\n",
      "Trained batch 207 batch loss 6.55538464 epoch total loss 7.00832605\n",
      "Trained batch 208 batch loss 6.68687534 epoch total loss 7.00678062\n",
      "Trained batch 209 batch loss 7.15546322 epoch total loss 7.00749254\n",
      "Trained batch 210 batch loss 7.19435596 epoch total loss 7.00838232\n",
      "Trained batch 211 batch loss 7.0878067 epoch total loss 7.00875854\n",
      "Trained batch 212 batch loss 6.84668589 epoch total loss 7.0079937\n",
      "Trained batch 213 batch loss 7.14178324 epoch total loss 7.00862169\n",
      "Trained batch 214 batch loss 7.15728 epoch total loss 7.00931597\n",
      "Trained batch 215 batch loss 7.30006742 epoch total loss 7.01066828\n",
      "Trained batch 216 batch loss 6.97182274 epoch total loss 7.01048851\n",
      "Trained batch 217 batch loss 7.18709087 epoch total loss 7.01130247\n",
      "Trained batch 218 batch loss 6.81798601 epoch total loss 7.01041555\n",
      "Trained batch 219 batch loss 7.10047197 epoch total loss 7.01082706\n",
      "Trained batch 220 batch loss 6.79046535 epoch total loss 7.00982523\n",
      "Trained batch 221 batch loss 7.0458 epoch total loss 7.00998783\n",
      "Trained batch 222 batch loss 7.09896851 epoch total loss 7.01038885\n",
      "Trained batch 223 batch loss 7.05825043 epoch total loss 7.01060295\n",
      "Trained batch 224 batch loss 7.00614119 epoch total loss 7.01058292\n",
      "Trained batch 225 batch loss 7.03986406 epoch total loss 7.01071358\n",
      "Trained batch 226 batch loss 7.06073523 epoch total loss 7.01093483\n",
      "Trained batch 227 batch loss 6.95406723 epoch total loss 7.01068449\n",
      "Trained batch 228 batch loss 7.2273016 epoch total loss 7.01163483\n",
      "Trained batch 229 batch loss 6.60200214 epoch total loss 7.00984621\n",
      "Trained batch 230 batch loss 6.83964252 epoch total loss 7.00910568\n",
      "Trained batch 231 batch loss 7.27396 epoch total loss 7.01025248\n",
      "Trained batch 232 batch loss 7.20078087 epoch total loss 7.01107359\n",
      "Trained batch 233 batch loss 7.44918537 epoch total loss 7.01295424\n",
      "Trained batch 234 batch loss 7.92299175 epoch total loss 7.01684332\n",
      "Trained batch 235 batch loss 8.13413143 epoch total loss 7.02159739\n",
      "Trained batch 236 batch loss 7.69099331 epoch total loss 7.02443409\n",
      "Trained batch 237 batch loss 7.72578812 epoch total loss 7.02739382\n",
      "Trained batch 238 batch loss 7.0935173 epoch total loss 7.02767134\n",
      "Trained batch 239 batch loss 7.02430058 epoch total loss 7.02765751\n",
      "Trained batch 240 batch loss 6.99059248 epoch total loss 7.02750301\n",
      "Trained batch 241 batch loss 6.76771879 epoch total loss 7.02642488\n",
      "Trained batch 242 batch loss 6.76734161 epoch total loss 7.02535439\n",
      "Trained batch 243 batch loss 7.13968515 epoch total loss 7.02582455\n",
      "Trained batch 244 batch loss 6.97839689 epoch total loss 7.02563047\n",
      "Trained batch 245 batch loss 6.87535524 epoch total loss 7.02501678\n",
      "Trained batch 246 batch loss 6.77910233 epoch total loss 7.02401686\n",
      "Trained batch 247 batch loss 6.95115662 epoch total loss 7.02372217\n",
      "Trained batch 248 batch loss 6.93055916 epoch total loss 7.02334642\n",
      "Trained batch 249 batch loss 7.33680248 epoch total loss 7.02460527\n",
      "Trained batch 250 batch loss 7.23407269 epoch total loss 7.02544355\n",
      "Trained batch 251 batch loss 7.44884348 epoch total loss 7.02713\n",
      "Trained batch 252 batch loss 7.16641521 epoch total loss 7.02768278\n",
      "Trained batch 253 batch loss 7.21742392 epoch total loss 7.02843285\n",
      "Trained batch 254 batch loss 7.32825756 epoch total loss 7.02961302\n",
      "Trained batch 255 batch loss 6.98390388 epoch total loss 7.02943373\n",
      "Trained batch 256 batch loss 7.00651836 epoch total loss 7.02934408\n",
      "Trained batch 257 batch loss 7.3956356 epoch total loss 7.03076935\n",
      "Trained batch 258 batch loss 7.30352974 epoch total loss 7.03182697\n",
      "Trained batch 259 batch loss 7.17423058 epoch total loss 7.03237629\n",
      "Trained batch 260 batch loss 7.14451408 epoch total loss 7.03280783\n",
      "Trained batch 261 batch loss 7.71015453 epoch total loss 7.03540325\n",
      "Trained batch 262 batch loss 7.68838549 epoch total loss 7.0378952\n",
      "Trained batch 263 batch loss 7.04257059 epoch total loss 7.03791332\n",
      "Trained batch 264 batch loss 6.88682747 epoch total loss 7.03734112\n",
      "Trained batch 265 batch loss 7.09342051 epoch total loss 7.03755236\n",
      "Trained batch 266 batch loss 6.9548831 epoch total loss 7.03724146\n",
      "Trained batch 267 batch loss 6.49033403 epoch total loss 7.03519344\n",
      "Trained batch 268 batch loss 6.84687185 epoch total loss 7.03449059\n",
      "Trained batch 269 batch loss 6.76005411 epoch total loss 7.03347063\n",
      "Trained batch 270 batch loss 7.0690136 epoch total loss 7.03360176\n",
      "Trained batch 271 batch loss 7.2994957 epoch total loss 7.03458261\n",
      "Trained batch 272 batch loss 7.06702948 epoch total loss 7.0347023\n",
      "Trained batch 273 batch loss 7.02484131 epoch total loss 7.03466606\n",
      "Trained batch 274 batch loss 7.00532532 epoch total loss 7.03455925\n",
      "Trained batch 275 batch loss 6.80109739 epoch total loss 7.03371048\n",
      "Trained batch 276 batch loss 7.03147936 epoch total loss 7.03370237\n",
      "Trained batch 277 batch loss 7.23302 epoch total loss 7.03442192\n",
      "Trained batch 278 batch loss 6.93683338 epoch total loss 7.03407145\n",
      "Trained batch 279 batch loss 6.74802113 epoch total loss 7.03304625\n",
      "Trained batch 280 batch loss 6.75560045 epoch total loss 7.03205538\n",
      "Trained batch 281 batch loss 6.6670785 epoch total loss 7.03075647\n",
      "Trained batch 282 batch loss 6.74544191 epoch total loss 7.0297451\n",
      "Trained batch 283 batch loss 6.60259771 epoch total loss 7.02823544\n",
      "Trained batch 284 batch loss 6.53028107 epoch total loss 7.02648211\n",
      "Trained batch 285 batch loss 6.66080809 epoch total loss 7.02519894\n",
      "Trained batch 286 batch loss 6.90390778 epoch total loss 7.02477455\n",
      "Trained batch 287 batch loss 6.50467968 epoch total loss 7.02296257\n",
      "Trained batch 288 batch loss 6.91422415 epoch total loss 7.02258492\n",
      "Trained batch 289 batch loss 6.68401337 epoch total loss 7.02141285\n",
      "Trained batch 290 batch loss 6.99751234 epoch total loss 7.02133083\n",
      "Trained batch 291 batch loss 6.92078066 epoch total loss 7.02098513\n",
      "Trained batch 292 batch loss 5.9414835 epoch total loss 7.01728821\n",
      "Trained batch 293 batch loss 6.0041585 epoch total loss 7.01383\n",
      "Trained batch 294 batch loss 6.44066286 epoch total loss 7.01188087\n",
      "Trained batch 295 batch loss 6.46193 epoch total loss 7.01001644\n",
      "Trained batch 296 batch loss 6.61684418 epoch total loss 7.00868845\n",
      "Trained batch 297 batch loss 6.84296608 epoch total loss 7.00813055\n",
      "Trained batch 298 batch loss 6.92606831 epoch total loss 7.00785494\n",
      "Trained batch 299 batch loss 6.65572929 epoch total loss 7.00667763\n",
      "Trained batch 300 batch loss 7.05121469 epoch total loss 7.0068264\n",
      "Trained batch 301 batch loss 6.95064211 epoch total loss 7.00663948\n",
      "Trained batch 302 batch loss 7.41404963 epoch total loss 7.00798893\n",
      "Trained batch 303 batch loss 7.5890727 epoch total loss 7.00990677\n",
      "Trained batch 304 batch loss 7.08547497 epoch total loss 7.0101552\n",
      "Trained batch 305 batch loss 6.76953459 epoch total loss 7.00936604\n",
      "Trained batch 306 batch loss 7.11975574 epoch total loss 7.00972748\n",
      "Trained batch 307 batch loss 7.00800943 epoch total loss 7.00972176\n",
      "Trained batch 308 batch loss 6.60537434 epoch total loss 7.0084095\n",
      "Trained batch 309 batch loss 6.07179165 epoch total loss 7.00537825\n",
      "Trained batch 310 batch loss 6.59633589 epoch total loss 7.00405884\n",
      "Trained batch 311 batch loss 6.79873848 epoch total loss 7.0033989\n",
      "Trained batch 312 batch loss 7.10611391 epoch total loss 7.00372839\n",
      "Trained batch 313 batch loss 7.14862537 epoch total loss 7.00419188\n",
      "Trained batch 314 batch loss 7.28739786 epoch total loss 7.00509357\n",
      "Trained batch 315 batch loss 7.3579874 epoch total loss 7.00621367\n",
      "Trained batch 316 batch loss 7.29172707 epoch total loss 7.00711727\n",
      "Trained batch 317 batch loss 7.29854631 epoch total loss 7.00803661\n",
      "Trained batch 318 batch loss 7.3237009 epoch total loss 7.00902939\n",
      "Trained batch 319 batch loss 7.05859709 epoch total loss 7.00918484\n",
      "Trained batch 320 batch loss 7.05345535 epoch total loss 7.00932312\n",
      "Trained batch 321 batch loss 6.5901165 epoch total loss 7.00801706\n",
      "Trained batch 322 batch loss 7.18619585 epoch total loss 7.00857067\n",
      "Trained batch 323 batch loss 6.96738911 epoch total loss 7.00844288\n",
      "Trained batch 324 batch loss 6.91516638 epoch total loss 7.00815535\n",
      "Trained batch 325 batch loss 6.82631063 epoch total loss 7.00759602\n",
      "Trained batch 326 batch loss 7.16423607 epoch total loss 7.00807667\n",
      "Trained batch 327 batch loss 7.04859591 epoch total loss 7.00820065\n",
      "Trained batch 328 batch loss 6.05938292 epoch total loss 7.00530767\n",
      "Trained batch 329 batch loss 5.99883366 epoch total loss 7.00224829\n",
      "Trained batch 330 batch loss 6.50391436 epoch total loss 7.00073814\n",
      "Trained batch 331 batch loss 6.84915304 epoch total loss 7.00028038\n",
      "Trained batch 332 batch loss 6.91984177 epoch total loss 7.00003815\n",
      "Trained batch 333 batch loss 6.93173838 epoch total loss 6.99983263\n",
      "Trained batch 334 batch loss 6.74460554 epoch total loss 6.99906874\n",
      "Trained batch 335 batch loss 6.96191502 epoch total loss 6.99895763\n",
      "Trained batch 336 batch loss 7.00906324 epoch total loss 6.99898767\n",
      "Trained batch 337 batch loss 6.83532238 epoch total loss 6.99850178\n",
      "Trained batch 338 batch loss 6.34015465 epoch total loss 6.9965539\n",
      "Trained batch 339 batch loss 6.77217293 epoch total loss 6.99589205\n",
      "Trained batch 340 batch loss 7.04504 epoch total loss 6.99603653\n",
      "Trained batch 341 batch loss 6.57474327 epoch total loss 6.99480057\n",
      "Trained batch 342 batch loss 6.82060099 epoch total loss 6.99429131\n",
      "Trained batch 343 batch loss 6.91180468 epoch total loss 6.99405098\n",
      "Trained batch 344 batch loss 6.95954609 epoch total loss 6.99395037\n",
      "Trained batch 345 batch loss 6.9174366 epoch total loss 6.99372864\n",
      "Trained batch 346 batch loss 7.07129955 epoch total loss 6.99395275\n",
      "Trained batch 347 batch loss 6.82339144 epoch total loss 6.99346161\n",
      "Trained batch 348 batch loss 6.82335281 epoch total loss 6.99297237\n",
      "Trained batch 349 batch loss 6.79111099 epoch total loss 6.99239397\n",
      "Trained batch 350 batch loss 6.95325518 epoch total loss 6.99228239\n",
      "Trained batch 351 batch loss 6.8052063 epoch total loss 6.99174929\n",
      "Trained batch 352 batch loss 6.89490938 epoch total loss 6.99147463\n",
      "Trained batch 353 batch loss 6.85666 epoch total loss 6.99109268\n",
      "Trained batch 354 batch loss 6.81080914 epoch total loss 6.99058342\n",
      "Trained batch 355 batch loss 6.1897049 epoch total loss 6.9883275\n",
      "Trained batch 356 batch loss 5.83800459 epoch total loss 6.98509598\n",
      "Trained batch 357 batch loss 5.59936571 epoch total loss 6.98121405\n",
      "Trained batch 358 batch loss 6.47464609 epoch total loss 6.97979927\n",
      "Trained batch 359 batch loss 6.9855237 epoch total loss 6.97981501\n",
      "Trained batch 360 batch loss 7.07573843 epoch total loss 6.98008156\n",
      "Trained batch 361 batch loss 6.4764452 epoch total loss 6.97868681\n",
      "Trained batch 362 batch loss 6.84437943 epoch total loss 6.97831583\n",
      "Trained batch 363 batch loss 7.13887453 epoch total loss 6.97875834\n",
      "Trained batch 364 batch loss 6.55187893 epoch total loss 6.97758532\n",
      "Trained batch 365 batch loss 6.36135769 epoch total loss 6.97589684\n",
      "Trained batch 366 batch loss 6.32961512 epoch total loss 6.97413111\n",
      "Trained batch 367 batch loss 6.43081903 epoch total loss 6.972651\n",
      "Trained batch 368 batch loss 6.36882877 epoch total loss 6.97101\n",
      "Trained batch 369 batch loss 7.09239054 epoch total loss 6.97133875\n",
      "Trained batch 370 batch loss 6.92801666 epoch total loss 6.97122192\n",
      "Trained batch 371 batch loss 6.5694809 epoch total loss 6.97013903\n",
      "Trained batch 372 batch loss 7.23948479 epoch total loss 6.97086334\n",
      "Trained batch 373 batch loss 6.21624088 epoch total loss 6.96884\n",
      "Trained batch 374 batch loss 6.74559116 epoch total loss 6.9682436\n",
      "Trained batch 375 batch loss 6.80807209 epoch total loss 6.96781635\n",
      "Trained batch 376 batch loss 7.07753134 epoch total loss 6.96810865\n",
      "Trained batch 377 batch loss 7.0921936 epoch total loss 6.96843767\n",
      "Trained batch 378 batch loss 6.87096786 epoch total loss 6.9681797\n",
      "Trained batch 379 batch loss 6.78366041 epoch total loss 6.96769285\n",
      "Trained batch 380 batch loss 6.83425045 epoch total loss 6.9673419\n",
      "Trained batch 381 batch loss 6.85687256 epoch total loss 6.96705198\n",
      "Trained batch 382 batch loss 6.74653292 epoch total loss 6.96647453\n",
      "Trained batch 383 batch loss 6.423347 epoch total loss 6.96505642\n",
      "Trained batch 384 batch loss 6.84004 epoch total loss 6.96473122\n",
      "Trained batch 385 batch loss 7.02572918 epoch total loss 6.96488953\n",
      "Trained batch 386 batch loss 7.29767656 epoch total loss 6.96575117\n",
      "Trained batch 387 batch loss 7.37417459 epoch total loss 6.96680689\n",
      "Trained batch 388 batch loss 7.5494976 epoch total loss 6.96830893\n",
      "Trained batch 389 batch loss 7.1130476 epoch total loss 6.96868086\n",
      "Trained batch 390 batch loss 7.02573204 epoch total loss 6.96882677\n",
      "Trained batch 391 batch loss 6.6903677 epoch total loss 6.96811485\n",
      "Trained batch 392 batch loss 6.39688635 epoch total loss 6.96665812\n",
      "Trained batch 393 batch loss 6.68940496 epoch total loss 6.9659524\n",
      "Trained batch 394 batch loss 6.68336105 epoch total loss 6.96523523\n",
      "Trained batch 395 batch loss 6.76432276 epoch total loss 6.96472692\n",
      "Trained batch 396 batch loss 6.75604153 epoch total loss 6.9642\n",
      "Trained batch 397 batch loss 6.61848974 epoch total loss 6.96332932\n",
      "Trained batch 398 batch loss 6.68128777 epoch total loss 6.96262074\n",
      "Trained batch 399 batch loss 6.75632524 epoch total loss 6.96210384\n",
      "Trained batch 400 batch loss 6.89398289 epoch total loss 6.96193361\n",
      "Trained batch 401 batch loss 7.17982817 epoch total loss 6.96247721\n",
      "Trained batch 402 batch loss 6.97458935 epoch total loss 6.96250725\n",
      "Trained batch 403 batch loss 6.26960802 epoch total loss 6.96078777\n",
      "Trained batch 404 batch loss 7.00538874 epoch total loss 6.9608984\n",
      "Trained batch 405 batch loss 7.03656721 epoch total loss 6.96108532\n",
      "Trained batch 406 batch loss 7.11633158 epoch total loss 6.96146727\n",
      "Trained batch 407 batch loss 7.08678436 epoch total loss 6.96177483\n",
      "Trained batch 408 batch loss 6.85854197 epoch total loss 6.9615221\n",
      "Trained batch 409 batch loss 6.50900364 epoch total loss 6.96041584\n",
      "Trained batch 410 batch loss 6.78332901 epoch total loss 6.9599843\n",
      "Trained batch 411 batch loss 6.4855814 epoch total loss 6.95883\n",
      "Trained batch 412 batch loss 6.38100386 epoch total loss 6.9574275\n",
      "Trained batch 413 batch loss 6.78322315 epoch total loss 6.95700598\n",
      "Trained batch 414 batch loss 6.61243343 epoch total loss 6.9561739\n",
      "Trained batch 415 batch loss 6.35760117 epoch total loss 6.95473146\n",
      "Trained batch 416 batch loss 6.46999884 epoch total loss 6.95356655\n",
      "Trained batch 417 batch loss 6.1964817 epoch total loss 6.95175076\n",
      "Trained batch 418 batch loss 6.58438635 epoch total loss 6.95087242\n",
      "Trained batch 419 batch loss 6.09397078 epoch total loss 6.94882727\n",
      "Trained batch 420 batch loss 6.43941641 epoch total loss 6.94761419\n",
      "Trained batch 421 batch loss 6.98836231 epoch total loss 6.94771099\n",
      "Trained batch 422 batch loss 7.08071423 epoch total loss 6.94802618\n",
      "Trained batch 423 batch loss 7.14997292 epoch total loss 6.94850349\n",
      "Trained batch 424 batch loss 7.03817 epoch total loss 6.94871473\n",
      "Trained batch 425 batch loss 6.84245205 epoch total loss 6.94846487\n",
      "Trained batch 426 batch loss 6.65808105 epoch total loss 6.94778347\n",
      "Trained batch 427 batch loss 6.26612902 epoch total loss 6.9461875\n",
      "Trained batch 428 batch loss 6.65406 epoch total loss 6.94550467\n",
      "Trained batch 429 batch loss 6.75004053 epoch total loss 6.94504881\n",
      "Trained batch 430 batch loss 6.88227272 epoch total loss 6.94490337\n",
      "Trained batch 431 batch loss 7.05731 epoch total loss 6.9451642\n",
      "Trained batch 432 batch loss 6.9507308 epoch total loss 6.94517708\n",
      "Trained batch 433 batch loss 6.96444368 epoch total loss 6.94522095\n",
      "Trained batch 434 batch loss 6.75041246 epoch total loss 6.94477224\n",
      "Trained batch 435 batch loss 6.97718287 epoch total loss 6.94484711\n",
      "Trained batch 436 batch loss 6.78884649 epoch total loss 6.94448948\n",
      "Trained batch 437 batch loss 6.93793 epoch total loss 6.9444747\n",
      "Trained batch 438 batch loss 6.91294718 epoch total loss 6.94440222\n",
      "Trained batch 439 batch loss 6.84628677 epoch total loss 6.94417858\n",
      "Trained batch 440 batch loss 6.77837038 epoch total loss 6.9438014\n",
      "Trained batch 441 batch loss 6.8860054 epoch total loss 6.94367027\n",
      "Trained batch 442 batch loss 7.15214539 epoch total loss 6.94414186\n",
      "Trained batch 443 batch loss 6.98683262 epoch total loss 6.94423819\n",
      "Trained batch 444 batch loss 6.88404608 epoch total loss 6.94410276\n",
      "Trained batch 445 batch loss 6.74977589 epoch total loss 6.94366598\n",
      "Trained batch 446 batch loss 6.04458904 epoch total loss 6.94165039\n",
      "Trained batch 447 batch loss 6.28784895 epoch total loss 6.94018793\n",
      "Trained batch 448 batch loss 6.44805145 epoch total loss 6.9390893\n",
      "Trained batch 449 batch loss 6.53654957 epoch total loss 6.93819284\n",
      "Trained batch 450 batch loss 6.53713608 epoch total loss 6.93730164\n",
      "Trained batch 451 batch loss 6.55735111 epoch total loss 6.93645906\n",
      "Trained batch 452 batch loss 6.38877964 epoch total loss 6.93524694\n",
      "Trained batch 453 batch loss 6.29749489 epoch total loss 6.93383932\n",
      "Trained batch 454 batch loss 6.26162 epoch total loss 6.93235922\n",
      "Trained batch 455 batch loss 6.28078365 epoch total loss 6.9309268\n",
      "Trained batch 456 batch loss 5.24767971 epoch total loss 6.9272356\n",
      "Trained batch 457 batch loss 6.06621408 epoch total loss 6.92535114\n",
      "Trained batch 458 batch loss 6.68187428 epoch total loss 6.92481947\n",
      "Trained batch 459 batch loss 6.15548944 epoch total loss 6.92314339\n",
      "Trained batch 460 batch loss 6.82015562 epoch total loss 6.92291927\n",
      "Trained batch 461 batch loss 6.85008192 epoch total loss 6.92276144\n",
      "Trained batch 462 batch loss 7.13636065 epoch total loss 6.92322397\n",
      "Trained batch 463 batch loss 6.71877098 epoch total loss 6.92278242\n",
      "Trained batch 464 batch loss 6.64196444 epoch total loss 6.92217684\n",
      "Trained batch 465 batch loss 6.78494787 epoch total loss 6.92188168\n",
      "Trained batch 466 batch loss 7.13505316 epoch total loss 6.92233896\n",
      "Trained batch 467 batch loss 6.72689629 epoch total loss 6.9219203\n",
      "Trained batch 468 batch loss 6.42152119 epoch total loss 6.92085171\n",
      "Trained batch 469 batch loss 6.82634783 epoch total loss 6.92065\n",
      "Trained batch 470 batch loss 6.65194607 epoch total loss 6.92007828\n",
      "Trained batch 471 batch loss 7.01472664 epoch total loss 6.92027903\n",
      "Trained batch 472 batch loss 7.21610832 epoch total loss 6.92090559\n",
      "Trained batch 473 batch loss 7.16803741 epoch total loss 6.9214282\n",
      "Trained batch 474 batch loss 7.09653425 epoch total loss 6.92179728\n",
      "Trained batch 475 batch loss 6.46106339 epoch total loss 6.92082739\n",
      "Trained batch 476 batch loss 6.51659346 epoch total loss 6.91997814\n",
      "Trained batch 477 batch loss 6.75740051 epoch total loss 6.9196372\n",
      "Trained batch 478 batch loss 6.97169828 epoch total loss 6.9197464\n",
      "Trained batch 479 batch loss 7.48586798 epoch total loss 6.920928\n",
      "Trained batch 480 batch loss 7.36231804 epoch total loss 6.92184734\n",
      "Trained batch 481 batch loss 6.80371809 epoch total loss 6.92160177\n",
      "Trained batch 482 batch loss 6.62710619 epoch total loss 6.92099094\n",
      "Trained batch 483 batch loss 6.71966314 epoch total loss 6.92057467\n",
      "Trained batch 484 batch loss 6.74543381 epoch total loss 6.92021227\n",
      "Trained batch 485 batch loss 7.25747967 epoch total loss 6.92090797\n",
      "Trained batch 486 batch loss 6.84569788 epoch total loss 6.92075348\n",
      "Trained batch 487 batch loss 7.11855841 epoch total loss 6.92115974\n",
      "Trained batch 488 batch loss 6.64906693 epoch total loss 6.92060232\n",
      "Trained batch 489 batch loss 6.87578249 epoch total loss 6.92051029\n",
      "Trained batch 490 batch loss 6.9187479 epoch total loss 6.92050695\n",
      "Trained batch 491 batch loss 6.89745283 epoch total loss 6.92045975\n",
      "Trained batch 492 batch loss 7.2185092 epoch total loss 6.92106581\n",
      "Trained batch 493 batch loss 7.12238216 epoch total loss 6.92147398\n",
      "Trained batch 494 batch loss 7.06234312 epoch total loss 6.92175865\n",
      "Trained batch 495 batch loss 6.88873577 epoch total loss 6.92169189\n",
      "Trained batch 496 batch loss 6.88718462 epoch total loss 6.92162228\n",
      "Trained batch 497 batch loss 6.77536631 epoch total loss 6.92132807\n",
      "Trained batch 498 batch loss 7.17504263 epoch total loss 6.92183781\n",
      "Trained batch 499 batch loss 7.09043503 epoch total loss 6.92217541\n",
      "Trained batch 500 batch loss 6.87234211 epoch total loss 6.92207575\n",
      "Trained batch 501 batch loss 7.05801964 epoch total loss 6.92234707\n",
      "Trained batch 502 batch loss 7.12656879 epoch total loss 6.92275381\n",
      "Trained batch 503 batch loss 7.07162094 epoch total loss 6.92304945\n",
      "Trained batch 504 batch loss 6.84876204 epoch total loss 6.92290258\n",
      "Trained batch 505 batch loss 7.04656553 epoch total loss 6.9231472\n",
      "Trained batch 506 batch loss 6.58984232 epoch total loss 6.92248869\n",
      "Trained batch 507 batch loss 6.73621321 epoch total loss 6.92212152\n",
      "Trained batch 508 batch loss 6.57168388 epoch total loss 6.92143202\n",
      "Trained batch 509 batch loss 6.72592688 epoch total loss 6.92104769\n",
      "Trained batch 510 batch loss 6.57003689 epoch total loss 6.92035961\n",
      "Trained batch 511 batch loss 6.77253389 epoch total loss 6.92007\n",
      "Trained batch 512 batch loss 6.74265766 epoch total loss 6.91972351\n",
      "Trained batch 513 batch loss 6.53802967 epoch total loss 6.91897964\n",
      "Trained batch 514 batch loss 6.5771966 epoch total loss 6.91831446\n",
      "Trained batch 515 batch loss 6.4820652 epoch total loss 6.91746759\n",
      "Trained batch 516 batch loss 6.41661406 epoch total loss 6.91649675\n",
      "Trained batch 517 batch loss 6.73287773 epoch total loss 6.91614151\n",
      "Trained batch 518 batch loss 6.526443 epoch total loss 6.91538906\n",
      "Trained batch 519 batch loss 6.85475349 epoch total loss 6.91527224\n",
      "Trained batch 520 batch loss 6.90898848 epoch total loss 6.91526031\n",
      "Trained batch 521 batch loss 6.96322346 epoch total loss 6.91535187\n",
      "Trained batch 522 batch loss 6.60119772 epoch total loss 6.91475058\n",
      "Trained batch 523 batch loss 6.64973879 epoch total loss 6.9142437\n",
      "Trained batch 524 batch loss 6.80472946 epoch total loss 6.91403437\n",
      "Trained batch 525 batch loss 6.55855894 epoch total loss 6.91335773\n",
      "Trained batch 526 batch loss 6.6181736 epoch total loss 6.9127965\n",
      "Trained batch 527 batch loss 6.27029228 epoch total loss 6.91157722\n",
      "Trained batch 528 batch loss 6.58195353 epoch total loss 6.91095304\n",
      "Trained batch 529 batch loss 6.87557268 epoch total loss 6.91088581\n",
      "Trained batch 530 batch loss 6.91761494 epoch total loss 6.91089869\n",
      "Trained batch 531 batch loss 6.91803 epoch total loss 6.91091204\n",
      "Trained batch 532 batch loss 7.13729525 epoch total loss 6.91133738\n",
      "Trained batch 533 batch loss 6.49017811 epoch total loss 6.91054726\n",
      "Trained batch 534 batch loss 6.50254345 epoch total loss 6.90978336\n",
      "Trained batch 535 batch loss 6.80334711 epoch total loss 6.90958452\n",
      "Trained batch 536 batch loss 6.9348731 epoch total loss 6.90963173\n",
      "Trained batch 537 batch loss 6.99247217 epoch total loss 6.90978575\n",
      "Trained batch 538 batch loss 7.19572067 epoch total loss 6.91031742\n",
      "Trained batch 539 batch loss 6.92737293 epoch total loss 6.91034937\n",
      "Trained batch 540 batch loss 6.27491331 epoch total loss 6.90917253\n",
      "Trained batch 541 batch loss 6.39489746 epoch total loss 6.9082222\n",
      "Trained batch 542 batch loss 7.03711796 epoch total loss 6.90845966\n",
      "Trained batch 543 batch loss 6.97346926 epoch total loss 6.90857935\n",
      "Trained batch 544 batch loss 6.37361574 epoch total loss 6.90759611\n",
      "Trained batch 545 batch loss 6.16992188 epoch total loss 6.90624237\n",
      "Trained batch 546 batch loss 6.47819567 epoch total loss 6.90545845\n",
      "Trained batch 547 batch loss 6.3546958 epoch total loss 6.90445185\n",
      "Trained batch 548 batch loss 7.19267273 epoch total loss 6.9049778\n",
      "Trained batch 549 batch loss 7.3037734 epoch total loss 6.90570402\n",
      "Trained batch 550 batch loss 7.07008505 epoch total loss 6.90600252\n",
      "Trained batch 551 batch loss 7.20130157 epoch total loss 6.90653896\n",
      "Trained batch 552 batch loss 7.5208 epoch total loss 6.90765142\n",
      "Trained batch 553 batch loss 7.08513 epoch total loss 6.90797281\n",
      "Trained batch 554 batch loss 6.98008394 epoch total loss 6.90810251\n",
      "Trained batch 555 batch loss 6.8452878 epoch total loss 6.9079895\n",
      "Trained batch 556 batch loss 6.92507935 epoch total loss 6.90802\n",
      "Trained batch 557 batch loss 6.98644638 epoch total loss 6.90816069\n",
      "Trained batch 558 batch loss 6.77485 epoch total loss 6.90792179\n",
      "Trained batch 559 batch loss 6.94736862 epoch total loss 6.90799236\n",
      "Trained batch 560 batch loss 6.89489555 epoch total loss 6.90796852\n",
      "Trained batch 561 batch loss 6.7939024 epoch total loss 6.90776539\n",
      "Trained batch 562 batch loss 7.26881886 epoch total loss 6.90840769\n",
      "Trained batch 563 batch loss 6.91832161 epoch total loss 6.90842533\n",
      "Trained batch 564 batch loss 6.75213385 epoch total loss 6.90814829\n",
      "Trained batch 565 batch loss 6.79271412 epoch total loss 6.90794373\n",
      "Trained batch 566 batch loss 6.87243509 epoch total loss 6.90788078\n",
      "Trained batch 567 batch loss 6.92542171 epoch total loss 6.90791225\n",
      "Trained batch 568 batch loss 6.87532759 epoch total loss 6.90785456\n",
      "Trained batch 569 batch loss 7.1290741 epoch total loss 6.90824366\n",
      "Trained batch 570 batch loss 6.95889235 epoch total loss 6.90833235\n",
      "Trained batch 571 batch loss 6.45180416 epoch total loss 6.90753317\n",
      "Trained batch 572 batch loss 6.81120968 epoch total loss 6.90736485\n",
      "Trained batch 573 batch loss 6.98286676 epoch total loss 6.90749693\n",
      "Trained batch 574 batch loss 6.97843456 epoch total loss 6.90762043\n",
      "Trained batch 575 batch loss 6.70026302 epoch total loss 6.90725946\n",
      "Trained batch 576 batch loss 6.54555941 epoch total loss 6.90663195\n",
      "Trained batch 577 batch loss 6.60609436 epoch total loss 6.90611124\n",
      "Trained batch 578 batch loss 6.63054705 epoch total loss 6.9056344\n",
      "Trained batch 579 batch loss 7.00908422 epoch total loss 6.90581322\n",
      "Trained batch 580 batch loss 6.94804621 epoch total loss 6.9058857\n",
      "Trained batch 581 batch loss 6.47741508 epoch total loss 6.90514803\n",
      "Trained batch 582 batch loss 6.71727276 epoch total loss 6.90482521\n",
      "Trained batch 583 batch loss 6.82525253 epoch total loss 6.90468884\n",
      "Trained batch 584 batch loss 6.76480961 epoch total loss 6.90444946\n",
      "Trained batch 585 batch loss 6.55041933 epoch total loss 6.90384436\n",
      "Trained batch 586 batch loss 6.76007175 epoch total loss 6.90359926\n",
      "Trained batch 587 batch loss 6.98222446 epoch total loss 6.90373278\n",
      "Trained batch 588 batch loss 6.45171356 epoch total loss 6.90296412\n",
      "Trained batch 589 batch loss 6.75327826 epoch total loss 6.90271\n",
      "Trained batch 590 batch loss 7.11587286 epoch total loss 6.9030714\n",
      "Trained batch 591 batch loss 6.89527321 epoch total loss 6.90305805\n",
      "Trained batch 592 batch loss 6.888834 epoch total loss 6.90303421\n",
      "Trained batch 593 batch loss 6.72797585 epoch total loss 6.90273905\n",
      "Trained batch 594 batch loss 6.60944223 epoch total loss 6.90224504\n",
      "Trained batch 595 batch loss 6.14903355 epoch total loss 6.90097904\n",
      "Trained batch 596 batch loss 6.72806 epoch total loss 6.90068865\n",
      "Trained batch 597 batch loss 6.91735506 epoch total loss 6.90071678\n",
      "Trained batch 598 batch loss 7.14348698 epoch total loss 6.90112305\n",
      "Trained batch 599 batch loss 7.127388 epoch total loss 6.9015007\n",
      "Trained batch 600 batch loss 7.56651592 epoch total loss 6.90260887\n",
      "Trained batch 601 batch loss 7.13605 epoch total loss 6.90299797\n",
      "Trained batch 602 batch loss 7.32410955 epoch total loss 6.90369749\n",
      "Trained batch 603 batch loss 6.71058559 epoch total loss 6.90337706\n",
      "Trained batch 604 batch loss 5.96071434 epoch total loss 6.90181684\n",
      "Trained batch 605 batch loss 5.81973791 epoch total loss 6.90002823\n",
      "Trained batch 606 batch loss 5.90226316 epoch total loss 6.89838171\n",
      "Trained batch 607 batch loss 6.17516136 epoch total loss 6.89719057\n",
      "Trained batch 608 batch loss 6.53324938 epoch total loss 6.89659214\n",
      "Trained batch 609 batch loss 6.54343271 epoch total loss 6.89601231\n",
      "Trained batch 610 batch loss 6.96800137 epoch total loss 6.89612961\n",
      "Trained batch 611 batch loss 6.81837654 epoch total loss 6.89600229\n",
      "Trained batch 612 batch loss 6.85984278 epoch total loss 6.89594364\n",
      "Trained batch 613 batch loss 6.78203201 epoch total loss 6.89575815\n",
      "Trained batch 614 batch loss 7.18379688 epoch total loss 6.89622688\n",
      "Trained batch 615 batch loss 7.04224968 epoch total loss 6.89646435\n",
      "Trained batch 616 batch loss 6.91268682 epoch total loss 6.89649057\n",
      "Trained batch 617 batch loss 7.0499773 epoch total loss 6.89673901\n",
      "Trained batch 618 batch loss 7.23132944 epoch total loss 6.89728069\n",
      "Trained batch 619 batch loss 6.88779402 epoch total loss 6.89726543\n",
      "Trained batch 620 batch loss 7.09962225 epoch total loss 6.89759159\n",
      "Trained batch 621 batch loss 6.81207848 epoch total loss 6.89745378\n",
      "Trained batch 622 batch loss 6.70834589 epoch total loss 6.89715\n",
      "Trained batch 623 batch loss 7.01854467 epoch total loss 6.89734507\n",
      "Trained batch 624 batch loss 6.95776606 epoch total loss 6.89744234\n",
      "Trained batch 625 batch loss 6.90861034 epoch total loss 6.89746\n",
      "Trained batch 626 batch loss 6.79877949 epoch total loss 6.89730263\n",
      "Trained batch 627 batch loss 6.8055687 epoch total loss 6.89715624\n",
      "Trained batch 628 batch loss 7.13327694 epoch total loss 6.89753246\n",
      "Trained batch 629 batch loss 7.10493946 epoch total loss 6.89786243\n",
      "Trained batch 630 batch loss 6.93702459 epoch total loss 6.89792442\n",
      "Trained batch 631 batch loss 6.82029963 epoch total loss 6.8978014\n",
      "Trained batch 632 batch loss 6.77191877 epoch total loss 6.89760256\n",
      "Trained batch 633 batch loss 6.93732214 epoch total loss 6.8976655\n",
      "Trained batch 634 batch loss 6.70606 epoch total loss 6.89736319\n",
      "Trained batch 635 batch loss 6.83618689 epoch total loss 6.89726734\n",
      "Trained batch 636 batch loss 6.52130604 epoch total loss 6.89667654\n",
      "Trained batch 637 batch loss 6.51720953 epoch total loss 6.89608049\n",
      "Trained batch 638 batch loss 5.97679 epoch total loss 6.89463902\n",
      "Trained batch 639 batch loss 6.57027817 epoch total loss 6.89413166\n",
      "Trained batch 640 batch loss 6.65810966 epoch total loss 6.89376307\n",
      "Trained batch 641 batch loss 6.57420921 epoch total loss 6.89326429\n",
      "Trained batch 642 batch loss 7.02162409 epoch total loss 6.89346409\n",
      "Trained batch 643 batch loss 6.87750244 epoch total loss 6.89343929\n",
      "Trained batch 644 batch loss 6.82572079 epoch total loss 6.89333391\n",
      "Trained batch 645 batch loss 7.03819323 epoch total loss 6.8935585\n",
      "Trained batch 646 batch loss 6.58239 epoch total loss 6.8930769\n",
      "Trained batch 647 batch loss 6.48909092 epoch total loss 6.89245272\n",
      "Trained batch 648 batch loss 6.44241238 epoch total loss 6.89175844\n",
      "Trained batch 649 batch loss 6.80766678 epoch total loss 6.89162874\n",
      "Trained batch 650 batch loss 7.17352486 epoch total loss 6.89206219\n",
      "Trained batch 651 batch loss 6.67088795 epoch total loss 6.8917222\n",
      "Trained batch 652 batch loss 6.73534203 epoch total loss 6.89148235\n",
      "Trained batch 653 batch loss 6.76494 epoch total loss 6.89128923\n",
      "Trained batch 654 batch loss 6.65180063 epoch total loss 6.89092302\n",
      "Trained batch 655 batch loss 6.62256575 epoch total loss 6.89051294\n",
      "Trained batch 656 batch loss 6.62557554 epoch total loss 6.89010906\n",
      "Trained batch 657 batch loss 6.64118958 epoch total loss 6.88973\n",
      "Trained batch 658 batch loss 6.79176092 epoch total loss 6.88958168\n",
      "Trained batch 659 batch loss 6.67844391 epoch total loss 6.88926077\n",
      "Trained batch 660 batch loss 6.69642 epoch total loss 6.88896847\n",
      "Trained batch 661 batch loss 6.65198231 epoch total loss 6.88861\n",
      "Trained batch 662 batch loss 7.0159812 epoch total loss 6.88880253\n",
      "Trained batch 663 batch loss 6.59564924 epoch total loss 6.8883605\n",
      "Trained batch 664 batch loss 6.23474598 epoch total loss 6.88737631\n",
      "Trained batch 665 batch loss 6.52602243 epoch total loss 6.88683271\n",
      "Trained batch 666 batch loss 6.83229446 epoch total loss 6.88675117\n",
      "Trained batch 667 batch loss 6.74405861 epoch total loss 6.88653708\n",
      "Trained batch 668 batch loss 6.47375393 epoch total loss 6.88591909\n",
      "Trained batch 669 batch loss 6.6805048 epoch total loss 6.88561249\n",
      "Trained batch 670 batch loss 6.55828094 epoch total loss 6.88512325\n",
      "Trained batch 671 batch loss 6.72227669 epoch total loss 6.88488054\n",
      "Trained batch 672 batch loss 6.50890398 epoch total loss 6.88432074\n",
      "Trained batch 673 batch loss 6.97231674 epoch total loss 6.88445139\n",
      "Trained batch 674 batch loss 7.2247386 epoch total loss 6.88495636\n",
      "Trained batch 675 batch loss 7.42244101 epoch total loss 6.8857522\n",
      "Trained batch 676 batch loss 7.52882338 epoch total loss 6.88670349\n",
      "Trained batch 677 batch loss 7.50026 epoch total loss 6.88761044\n",
      "Trained batch 678 batch loss 6.91223621 epoch total loss 6.8876462\n",
      "Trained batch 679 batch loss 6.82343 epoch total loss 6.88755131\n",
      "Trained batch 680 batch loss 6.52048779 epoch total loss 6.88701153\n",
      "Trained batch 681 batch loss 6.86184931 epoch total loss 6.88697481\n",
      "Trained batch 682 batch loss 7.24953318 epoch total loss 6.88750648\n",
      "Trained batch 683 batch loss 7.19239426 epoch total loss 6.8879528\n",
      "Trained batch 684 batch loss 7.03044462 epoch total loss 6.88816071\n",
      "Trained batch 685 batch loss 6.924891 epoch total loss 6.88821411\n",
      "Trained batch 686 batch loss 7.13013649 epoch total loss 6.88856745\n",
      "Trained batch 687 batch loss 6.96553326 epoch total loss 6.88867903\n",
      "Trained batch 688 batch loss 7.10597467 epoch total loss 6.88899469\n",
      "Trained batch 689 batch loss 7.41959 epoch total loss 6.88976479\n",
      "Trained batch 690 batch loss 7.48109722 epoch total loss 6.89062166\n",
      "Trained batch 691 batch loss 7.03059483 epoch total loss 6.89082432\n",
      "Trained batch 692 batch loss 6.98537 epoch total loss 6.89096069\n",
      "Trained batch 693 batch loss 7.15802431 epoch total loss 6.89134645\n",
      "Trained batch 694 batch loss 6.83361816 epoch total loss 6.89126301\n",
      "Trained batch 695 batch loss 6.75266123 epoch total loss 6.89106321\n",
      "Trained batch 696 batch loss 6.49150085 epoch total loss 6.89048958\n",
      "Trained batch 697 batch loss 6.16568089 epoch total loss 6.8894496\n",
      "Trained batch 698 batch loss 5.82962942 epoch total loss 6.88793087\n",
      "Trained batch 699 batch loss 5.8771553 epoch total loss 6.88648462\n",
      "Trained batch 700 batch loss 6.52934361 epoch total loss 6.88597441\n",
      "Trained batch 701 batch loss 6.80515766 epoch total loss 6.88585901\n",
      "Trained batch 702 batch loss 6.96760416 epoch total loss 6.88597584\n",
      "Trained batch 703 batch loss 7.2371316 epoch total loss 6.88647556\n",
      "Trained batch 704 batch loss 7.28970385 epoch total loss 6.88704824\n",
      "Trained batch 705 batch loss 6.89341688 epoch total loss 6.8870573\n",
      "Trained batch 706 batch loss 7.14390373 epoch total loss 6.88742161\n",
      "Trained batch 707 batch loss 7.38839149 epoch total loss 6.88812971\n",
      "Trained batch 708 batch loss 7.08607101 epoch total loss 6.88840914\n",
      "Trained batch 709 batch loss 7.13583136 epoch total loss 6.88875818\n",
      "Trained batch 710 batch loss 7.19397926 epoch total loss 6.88918781\n",
      "Trained batch 711 batch loss 7.0240097 epoch total loss 6.88937712\n",
      "Trained batch 712 batch loss 6.75304699 epoch total loss 6.88918543\n",
      "Trained batch 713 batch loss 6.83255482 epoch total loss 6.88910627\n",
      "Trained batch 714 batch loss 6.48474073 epoch total loss 6.88854\n",
      "Trained batch 715 batch loss 6.12232065 epoch total loss 6.88746881\n",
      "Trained batch 716 batch loss 6.41002655 epoch total loss 6.88680172\n",
      "Trained batch 717 batch loss 6.28831387 epoch total loss 6.88596678\n",
      "Trained batch 718 batch loss 6.52406645 epoch total loss 6.88546276\n",
      "Trained batch 719 batch loss 6.79613495 epoch total loss 6.88533831\n",
      "Trained batch 720 batch loss 6.85160542 epoch total loss 6.8852911\n",
      "Trained batch 721 batch loss 6.75763083 epoch total loss 6.88511419\n",
      "Trained batch 722 batch loss 6.51303387 epoch total loss 6.88459921\n",
      "Trained batch 723 batch loss 6.70663357 epoch total loss 6.88435316\n",
      "Trained batch 724 batch loss 6.23123455 epoch total loss 6.88345098\n",
      "Trained batch 725 batch loss 6.46958208 epoch total loss 6.88288069\n",
      "Trained batch 726 batch loss 6.39667034 epoch total loss 6.88221073\n",
      "Trained batch 727 batch loss 6.37816334 epoch total loss 6.88151693\n",
      "Trained batch 728 batch loss 6.27637196 epoch total loss 6.88068581\n",
      "Trained batch 729 batch loss 6.24996614 epoch total loss 6.87982035\n",
      "Trained batch 730 batch loss 6.27717304 epoch total loss 6.87899542\n",
      "Trained batch 731 batch loss 6.57341957 epoch total loss 6.87857676\n",
      "Trained batch 732 batch loss 6.65857553 epoch total loss 6.87827635\n",
      "Trained batch 733 batch loss 6.48795176 epoch total loss 6.87774372\n",
      "Trained batch 734 batch loss 6.8304348 epoch total loss 6.87767935\n",
      "Trained batch 735 batch loss 6.98671722 epoch total loss 6.87782812\n",
      "Trained batch 736 batch loss 6.93765163 epoch total loss 6.87790918\n",
      "Trained batch 737 batch loss 6.76355791 epoch total loss 6.87775421\n",
      "Trained batch 738 batch loss 6.61957502 epoch total loss 6.87740421\n",
      "Trained batch 739 batch loss 6.51832247 epoch total loss 6.87691879\n",
      "Trained batch 740 batch loss 6.66046238 epoch total loss 6.87662649\n",
      "Trained batch 741 batch loss 6.96435308 epoch total loss 6.87674475\n",
      "Trained batch 742 batch loss 6.93215084 epoch total loss 6.87681961\n",
      "Trained batch 743 batch loss 6.94895 epoch total loss 6.87691641\n",
      "Trained batch 744 batch loss 6.87026453 epoch total loss 6.87690735\n",
      "Trained batch 745 batch loss 6.95913315 epoch total loss 6.8770175\n",
      "Trained batch 746 batch loss 7.07011318 epoch total loss 6.87727642\n",
      "Trained batch 747 batch loss 6.49777842 epoch total loss 6.87676811\n",
      "Trained batch 748 batch loss 6.81624794 epoch total loss 6.87668753\n",
      "Trained batch 749 batch loss 6.56570911 epoch total loss 6.87627268\n",
      "Trained batch 750 batch loss 6.23533487 epoch total loss 6.87541819\n",
      "Trained batch 751 batch loss 6.08386517 epoch total loss 6.8743639\n",
      "Trained batch 752 batch loss 6.7131362 epoch total loss 6.87415028\n",
      "Trained batch 753 batch loss 6.45247936 epoch total loss 6.87359047\n",
      "Trained batch 754 batch loss 6.44835234 epoch total loss 6.87302637\n",
      "Trained batch 755 batch loss 7.00961113 epoch total loss 6.87320709\n",
      "Trained batch 756 batch loss 6.62064266 epoch total loss 6.87287331\n",
      "Trained batch 757 batch loss 6.34061241 epoch total loss 6.87217045\n",
      "Trained batch 758 batch loss 6.58289337 epoch total loss 6.87178898\n",
      "Trained batch 759 batch loss 6.57161951 epoch total loss 6.87139368\n",
      "Trained batch 760 batch loss 6.23777533 epoch total loss 6.87055969\n",
      "Trained batch 761 batch loss 5.97309399 epoch total loss 6.86938047\n",
      "Trained batch 762 batch loss 6.77363825 epoch total loss 6.86925459\n",
      "Trained batch 763 batch loss 6.99354172 epoch total loss 6.86941767\n",
      "Trained batch 764 batch loss 6.7455 epoch total loss 6.86925554\n",
      "Trained batch 765 batch loss 6.98852253 epoch total loss 6.86941147\n",
      "Trained batch 766 batch loss 6.21275568 epoch total loss 6.86855412\n",
      "Trained batch 767 batch loss 6.37540102 epoch total loss 6.86791134\n",
      "Trained batch 768 batch loss 6.45415306 epoch total loss 6.86737251\n",
      "Trained batch 769 batch loss 7.15948391 epoch total loss 6.86775255\n",
      "Trained batch 770 batch loss 7.03889036 epoch total loss 6.86797523\n",
      "Trained batch 771 batch loss 6.60603762 epoch total loss 6.86763525\n",
      "Trained batch 772 batch loss 6.73093891 epoch total loss 6.86745834\n",
      "Trained batch 773 batch loss 6.63836527 epoch total loss 6.86716175\n",
      "Trained batch 774 batch loss 6.86627293 epoch total loss 6.86716032\n",
      "Trained batch 775 batch loss 7.09176 epoch total loss 6.86745\n",
      "Trained batch 776 batch loss 6.92271471 epoch total loss 6.86752176\n",
      "Trained batch 777 batch loss 6.64917755 epoch total loss 6.86724091\n",
      "Trained batch 778 batch loss 6.70691824 epoch total loss 6.86703491\n",
      "Trained batch 779 batch loss 6.51131916 epoch total loss 6.8665781\n",
      "Trained batch 780 batch loss 6.79153967 epoch total loss 6.86648178\n",
      "Trained batch 781 batch loss 6.77168751 epoch total loss 6.86636\n",
      "Trained batch 782 batch loss 6.73306561 epoch total loss 6.86618948\n",
      "Trained batch 783 batch loss 6.45296431 epoch total loss 6.8656621\n",
      "Trained batch 784 batch loss 6.79329967 epoch total loss 6.86557\n",
      "Trained batch 785 batch loss 7.05332661 epoch total loss 6.86580896\n",
      "Trained batch 786 batch loss 6.52474356 epoch total loss 6.86537552\n",
      "Trained batch 787 batch loss 6.91046095 epoch total loss 6.86543274\n",
      "Trained batch 788 batch loss 6.60805225 epoch total loss 6.86510611\n",
      "Trained batch 789 batch loss 6.7383914 epoch total loss 6.86494541\n",
      "Trained batch 790 batch loss 6.6375227 epoch total loss 6.86465788\n",
      "Trained batch 791 batch loss 6.843328 epoch total loss 6.8646307\n",
      "Trained batch 792 batch loss 6.85260534 epoch total loss 6.86461544\n",
      "Trained batch 793 batch loss 6.51881123 epoch total loss 6.86417961\n",
      "Trained batch 794 batch loss 6.96304798 epoch total loss 6.86430407\n",
      "Trained batch 795 batch loss 7.0883646 epoch total loss 6.86458588\n",
      "Trained batch 796 batch loss 6.88217545 epoch total loss 6.86460829\n",
      "Trained batch 797 batch loss 7.0695138 epoch total loss 6.86486483\n",
      "Trained batch 798 batch loss 6.84211731 epoch total loss 6.86483669\n",
      "Trained batch 799 batch loss 6.92835712 epoch total loss 6.86491585\n",
      "Trained batch 800 batch loss 6.63720512 epoch total loss 6.86463118\n",
      "Trained batch 801 batch loss 7.08526325 epoch total loss 6.86490679\n",
      "Trained batch 802 batch loss 6.82036352 epoch total loss 6.86485147\n",
      "Trained batch 803 batch loss 6.81123495 epoch total loss 6.86478424\n",
      "Trained batch 804 batch loss 6.76628542 epoch total loss 6.86466169\n",
      "Trained batch 805 batch loss 7.00397921 epoch total loss 6.86483479\n",
      "Trained batch 806 batch loss 7.17287064 epoch total loss 6.86521673\n",
      "Trained batch 807 batch loss 7.81398582 epoch total loss 6.86639261\n",
      "Trained batch 808 batch loss 7.50824881 epoch total loss 6.86718702\n",
      "Trained batch 809 batch loss 7.48057 epoch total loss 6.86794519\n",
      "Trained batch 810 batch loss 7.36723 epoch total loss 6.86856127\n",
      "Trained batch 811 batch loss 6.90717411 epoch total loss 6.86860895\n",
      "Trained batch 812 batch loss 5.86629915 epoch total loss 6.86737442\n",
      "Trained batch 813 batch loss 6.61804724 epoch total loss 6.86706781\n",
      "Trained batch 814 batch loss 7.04509783 epoch total loss 6.86728668\n",
      "Trained batch 815 batch loss 7.23240042 epoch total loss 6.86773443\n",
      "Trained batch 816 batch loss 6.83345938 epoch total loss 6.86769247\n",
      "Trained batch 817 batch loss 6.40816212 epoch total loss 6.86713028\n",
      "Trained batch 818 batch loss 6.54593229 epoch total loss 6.86673737\n",
      "Trained batch 819 batch loss 6.87464523 epoch total loss 6.8667469\n",
      "Trained batch 820 batch loss 6.32091761 epoch total loss 6.86608124\n",
      "Trained batch 821 batch loss 6.58645 epoch total loss 6.8657403\n",
      "Trained batch 822 batch loss 6.65793562 epoch total loss 6.86548758\n",
      "Trained batch 823 batch loss 6.75963068 epoch total loss 6.86535883\n",
      "Trained batch 824 batch loss 6.97895479 epoch total loss 6.86549711\n",
      "Trained batch 825 batch loss 6.87148809 epoch total loss 6.86550426\n",
      "Trained batch 826 batch loss 6.66637135 epoch total loss 6.86526346\n",
      "Trained batch 827 batch loss 6.91449213 epoch total loss 6.86532307\n",
      "Trained batch 828 batch loss 6.96449518 epoch total loss 6.86544275\n",
      "Trained batch 829 batch loss 6.82078695 epoch total loss 6.86538887\n",
      "Trained batch 830 batch loss 6.64122105 epoch total loss 6.8651185\n",
      "Trained batch 831 batch loss 6.93622351 epoch total loss 6.86520386\n",
      "Trained batch 832 batch loss 6.94053745 epoch total loss 6.86529446\n",
      "Trained batch 833 batch loss 6.8481 epoch total loss 6.86527348\n",
      "Trained batch 834 batch loss 7.02431297 epoch total loss 6.86546469\n",
      "Trained batch 835 batch loss 7.0478673 epoch total loss 6.86568308\n",
      "Trained batch 836 batch loss 7.32310963 epoch total loss 6.86623\n",
      "Trained batch 837 batch loss 7.06218386 epoch total loss 6.86646414\n",
      "Trained batch 838 batch loss 7.0787468 epoch total loss 6.86671734\n",
      "Trained batch 839 batch loss 7.11306334 epoch total loss 6.86701107\n",
      "Trained batch 840 batch loss 7.0899272 epoch total loss 6.86727667\n",
      "Trained batch 841 batch loss 6.86210966 epoch total loss 6.86727047\n",
      "Trained batch 842 batch loss 7.15764046 epoch total loss 6.8676157\n",
      "Trained batch 843 batch loss 7.00616 epoch total loss 6.86778\n",
      "Trained batch 844 batch loss 6.87051773 epoch total loss 6.86778355\n",
      "Trained batch 845 batch loss 7.03702927 epoch total loss 6.86798382\n",
      "Trained batch 846 batch loss 7.26016855 epoch total loss 6.8684473\n",
      "Trained batch 847 batch loss 7.12131453 epoch total loss 6.8687458\n",
      "Trained batch 848 batch loss 7.05255222 epoch total loss 6.86896276\n",
      "Trained batch 849 batch loss 6.67037249 epoch total loss 6.86872864\n",
      "Trained batch 850 batch loss 6.75393248 epoch total loss 6.86859369\n",
      "Trained batch 851 batch loss 6.91371536 epoch total loss 6.86864662\n",
      "Trained batch 852 batch loss 7.04444742 epoch total loss 6.86885309\n",
      "Trained batch 853 batch loss 6.72489643 epoch total loss 6.86868429\n",
      "Trained batch 854 batch loss 6.55911922 epoch total loss 6.8683219\n",
      "Trained batch 855 batch loss 6.7322135 epoch total loss 6.86816311\n",
      "Trained batch 856 batch loss 6.94227171 epoch total loss 6.86824942\n",
      "Trained batch 857 batch loss 6.7923646 epoch total loss 6.8681612\n",
      "Trained batch 858 batch loss 6.87937975 epoch total loss 6.86817408\n",
      "Trained batch 859 batch loss 6.90343952 epoch total loss 6.86821508\n",
      "Trained batch 860 batch loss 6.8569541 epoch total loss 6.86820221\n",
      "Trained batch 861 batch loss 7.14771032 epoch total loss 6.86852694\n",
      "Trained batch 862 batch loss 6.95264626 epoch total loss 6.86862469\n",
      "Trained batch 863 batch loss 6.87928295 epoch total loss 6.86863708\n",
      "Trained batch 864 batch loss 6.48139048 epoch total loss 6.86818886\n",
      "Trained batch 865 batch loss 6.21446753 epoch total loss 6.86743307\n",
      "Trained batch 866 batch loss 6.4931488 epoch total loss 6.86700106\n",
      "Trained batch 867 batch loss 6.37094641 epoch total loss 6.86642885\n",
      "Trained batch 868 batch loss 5.99159956 epoch total loss 6.8654213\n",
      "Trained batch 869 batch loss 5.76208162 epoch total loss 6.86415148\n",
      "Trained batch 870 batch loss 5.93356943 epoch total loss 6.86308193\n",
      "Trained batch 871 batch loss 6.38780069 epoch total loss 6.86253643\n",
      "Trained batch 872 batch loss 7.06822 epoch total loss 6.86277246\n",
      "Trained batch 873 batch loss 7.23269939 epoch total loss 6.86319637\n",
      "Trained batch 874 batch loss 6.5282259 epoch total loss 6.862813\n",
      "Trained batch 875 batch loss 6.7470727 epoch total loss 6.86268091\n",
      "Trained batch 876 batch loss 6.27108908 epoch total loss 6.86200523\n",
      "Trained batch 877 batch loss 7.02347946 epoch total loss 6.86218929\n",
      "Trained batch 878 batch loss 6.91889572 epoch total loss 6.86225414\n",
      "Trained batch 879 batch loss 6.87091398 epoch total loss 6.86226416\n",
      "Trained batch 880 batch loss 6.85676813 epoch total loss 6.86225796\n",
      "Trained batch 881 batch loss 6.93598413 epoch total loss 6.86234188\n",
      "Trained batch 882 batch loss 6.85778284 epoch total loss 6.86233664\n",
      "Trained batch 883 batch loss 7.02040148 epoch total loss 6.86251593\n",
      "Trained batch 884 batch loss 6.78829813 epoch total loss 6.86243153\n",
      "Trained batch 885 batch loss 6.75448084 epoch total loss 6.86230946\n",
      "Trained batch 886 batch loss 7.05240059 epoch total loss 6.86252403\n",
      "Trained batch 887 batch loss 6.86351442 epoch total loss 6.86252499\n",
      "Trained batch 888 batch loss 6.62512636 epoch total loss 6.86225748\n",
      "Trained batch 889 batch loss 6.1213212 epoch total loss 6.86142349\n",
      "Trained batch 890 batch loss 6.77984142 epoch total loss 6.86133194\n",
      "Trained batch 891 batch loss 6.65732193 epoch total loss 6.86110306\n",
      "Trained batch 892 batch loss 6.59548092 epoch total loss 6.86080551\n",
      "Trained batch 893 batch loss 6.58961535 epoch total loss 6.86050177\n",
      "Trained batch 894 batch loss 6.7284503 epoch total loss 6.86035442\n",
      "Trained batch 895 batch loss 6.50170517 epoch total loss 6.8599534\n",
      "Trained batch 896 batch loss 6.47049904 epoch total loss 6.859519\n",
      "Trained batch 897 batch loss 6.57495785 epoch total loss 6.85920191\n",
      "Trained batch 898 batch loss 6.52883291 epoch total loss 6.85883379\n",
      "Trained batch 899 batch loss 6.62082672 epoch total loss 6.85856915\n",
      "Trained batch 900 batch loss 6.90940142 epoch total loss 6.85862541\n",
      "Trained batch 901 batch loss 6.81972 epoch total loss 6.85858202\n",
      "Trained batch 902 batch loss 6.69748116 epoch total loss 6.85840321\n",
      "Trained batch 903 batch loss 6.7428112 epoch total loss 6.85827494\n",
      "Trained batch 904 batch loss 7.24751091 epoch total loss 6.858706\n",
      "Trained batch 905 batch loss 6.1723237 epoch total loss 6.85794735\n",
      "Trained batch 906 batch loss 6.78765297 epoch total loss 6.85786963\n",
      "Trained batch 907 batch loss 6.69286442 epoch total loss 6.85768795\n",
      "Trained batch 908 batch loss 6.80262756 epoch total loss 6.85762739\n",
      "Trained batch 909 batch loss 6.86000395 epoch total loss 6.85763\n",
      "Trained batch 910 batch loss 7.20234203 epoch total loss 6.85800838\n",
      "Trained batch 911 batch loss 6.85192204 epoch total loss 6.85800171\n",
      "Trained batch 912 batch loss 5.96737385 epoch total loss 6.85702515\n",
      "Trained batch 913 batch loss 6.54454947 epoch total loss 6.85668278\n",
      "Trained batch 914 batch loss 6.79336357 epoch total loss 6.85661364\n",
      "Trained batch 915 batch loss 6.80575943 epoch total loss 6.85655785\n",
      "Trained batch 916 batch loss 6.89649057 epoch total loss 6.85660172\n",
      "Trained batch 917 batch loss 7.12565041 epoch total loss 6.85689449\n",
      "Trained batch 918 batch loss 6.66819143 epoch total loss 6.85668898\n",
      "Trained batch 919 batch loss 6.60896111 epoch total loss 6.85641909\n",
      "Trained batch 920 batch loss 6.39806557 epoch total loss 6.85592079\n",
      "Trained batch 921 batch loss 6.45741558 epoch total loss 6.8554883\n",
      "Trained batch 922 batch loss 6.93775892 epoch total loss 6.85557795\n",
      "Trained batch 923 batch loss 6.89099073 epoch total loss 6.85561657\n",
      "Trained batch 924 batch loss 6.80310774 epoch total loss 6.85556\n",
      "Trained batch 925 batch loss 6.92766953 epoch total loss 6.85563755\n",
      "Trained batch 926 batch loss 6.95449638 epoch total loss 6.85574436\n",
      "Trained batch 927 batch loss 7.35982895 epoch total loss 6.85628843\n",
      "Trained batch 928 batch loss 7.02313471 epoch total loss 6.85646772\n",
      "Trained batch 929 batch loss 6.73277712 epoch total loss 6.85633516\n",
      "Trained batch 930 batch loss 6.69288254 epoch total loss 6.85615921\n",
      "Trained batch 931 batch loss 6.9164319 epoch total loss 6.85622406\n",
      "Trained batch 932 batch loss 6.71809196 epoch total loss 6.85607576\n",
      "Trained batch 933 batch loss 6.87449217 epoch total loss 6.85609579\n",
      "Trained batch 934 batch loss 6.73519802 epoch total loss 6.85596657\n",
      "Trained batch 935 batch loss 6.73661375 epoch total loss 6.85583878\n",
      "Trained batch 936 batch loss 6.22936583 epoch total loss 6.85517\n",
      "Trained batch 937 batch loss 6.3506012 epoch total loss 6.85463142\n",
      "Trained batch 938 batch loss 6.35927057 epoch total loss 6.85410357\n",
      "Trained batch 939 batch loss 6.46494102 epoch total loss 6.85368872\n",
      "Trained batch 940 batch loss 5.85801077 epoch total loss 6.85262966\n",
      "Trained batch 941 batch loss 5.95767403 epoch total loss 6.85167837\n",
      "Trained batch 942 batch loss 5.80310249 epoch total loss 6.85056543\n",
      "Trained batch 943 batch loss 6.07436657 epoch total loss 6.84974194\n",
      "Trained batch 944 batch loss 6.2793746 epoch total loss 6.84913778\n",
      "Trained batch 945 batch loss 6.06379652 epoch total loss 6.84830666\n",
      "Trained batch 946 batch loss 6.55339861 epoch total loss 6.8479948\n",
      "Trained batch 947 batch loss 6.02534056 epoch total loss 6.84712601\n",
      "Trained batch 948 batch loss 6.63794804 epoch total loss 6.84690571\n",
      "Trained batch 949 batch loss 6.11887407 epoch total loss 6.84613848\n",
      "Trained batch 950 batch loss 6.33502483 epoch total loss 6.8456\n",
      "Trained batch 951 batch loss 6.41975164 epoch total loss 6.84515285\n",
      "Trained batch 952 batch loss 6.82910109 epoch total loss 6.84513569\n",
      "Trained batch 953 batch loss 7.12238932 epoch total loss 6.84542704\n",
      "Trained batch 954 batch loss 7.01177502 epoch total loss 6.84560108\n",
      "Trained batch 955 batch loss 6.74342585 epoch total loss 6.84549475\n",
      "Trained batch 956 batch loss 6.63680553 epoch total loss 6.84527636\n",
      "Trained batch 957 batch loss 6.64650393 epoch total loss 6.84506845\n",
      "Trained batch 958 batch loss 6.80175447 epoch total loss 6.84502316\n",
      "Trained batch 959 batch loss 6.99093914 epoch total loss 6.84517527\n",
      "Trained batch 960 batch loss 7.35367489 epoch total loss 6.84570456\n",
      "Trained batch 961 batch loss 7.24285364 epoch total loss 6.8461175\n",
      "Trained batch 962 batch loss 6.19156313 epoch total loss 6.84543705\n",
      "Trained batch 963 batch loss 6.49535036 epoch total loss 6.84507322\n",
      "Trained batch 964 batch loss 6.79993391 epoch total loss 6.84502649\n",
      "Trained batch 965 batch loss 6.69432116 epoch total loss 6.84487\n",
      "Trained batch 966 batch loss 6.4683857 epoch total loss 6.84448051\n",
      "Trained batch 967 batch loss 6.80704069 epoch total loss 6.84444189\n",
      "Trained batch 968 batch loss 6.8725605 epoch total loss 6.84447098\n",
      "Trained batch 969 batch loss 6.83828926 epoch total loss 6.8444643\n",
      "Trained batch 970 batch loss 6.85069132 epoch total loss 6.84447098\n",
      "Trained batch 971 batch loss 6.73037815 epoch total loss 6.8443532\n",
      "Trained batch 972 batch loss 6.76101494 epoch total loss 6.84426785\n",
      "Trained batch 973 batch loss 6.86399269 epoch total loss 6.84428787\n",
      "Trained batch 974 batch loss 6.71590567 epoch total loss 6.84415627\n",
      "Trained batch 975 batch loss 6.65048933 epoch total loss 6.84395742\n",
      "Trained batch 976 batch loss 6.57349682 epoch total loss 6.84368038\n",
      "Trained batch 977 batch loss 6.41558552 epoch total loss 6.84324217\n",
      "Trained batch 978 batch loss 6.4533329 epoch total loss 6.84284353\n",
      "Trained batch 979 batch loss 6.54006338 epoch total loss 6.84253407\n",
      "Trained batch 980 batch loss 6.77147484 epoch total loss 6.84246159\n",
      "Trained batch 981 batch loss 6.01958656 epoch total loss 6.84162283\n",
      "Trained batch 982 batch loss 6.21118736 epoch total loss 6.84098101\n",
      "Trained batch 983 batch loss 5.96877527 epoch total loss 6.84009361\n",
      "Trained batch 984 batch loss 6.31576967 epoch total loss 6.83956099\n",
      "Trained batch 985 batch loss 6.52056789 epoch total loss 6.83923721\n",
      "Trained batch 986 batch loss 6.80829906 epoch total loss 6.83920527\n",
      "Trained batch 987 batch loss 6.82514477 epoch total loss 6.83919144\n",
      "Trained batch 988 batch loss 6.64346838 epoch total loss 6.83899307\n",
      "Trained batch 989 batch loss 6.4578166 epoch total loss 6.83860779\n",
      "Trained batch 990 batch loss 6.40727901 epoch total loss 6.83817244\n",
      "Trained batch 991 batch loss 6.70148 epoch total loss 6.83803463\n",
      "Trained batch 992 batch loss 6.46606112 epoch total loss 6.83765936\n",
      "Trained batch 993 batch loss 6.67515659 epoch total loss 6.8374958\n",
      "Trained batch 994 batch loss 6.55752754 epoch total loss 6.83721399\n",
      "Trained batch 995 batch loss 6.48962 epoch total loss 6.83686495\n",
      "Trained batch 996 batch loss 6.62888527 epoch total loss 6.83665609\n",
      "Trained batch 997 batch loss 6.53002644 epoch total loss 6.83634853\n",
      "Trained batch 998 batch loss 6.77653217 epoch total loss 6.83628845\n",
      "Trained batch 999 batch loss 6.76261568 epoch total loss 6.83621454\n",
      "Trained batch 1000 batch loss 6.41903734 epoch total loss 6.83579731\n",
      "Trained batch 1001 batch loss 6.52618027 epoch total loss 6.83548832\n",
      "Trained batch 1002 batch loss 6.71959 epoch total loss 6.83537292\n",
      "Trained batch 1003 batch loss 6.7471385 epoch total loss 6.83528471\n",
      "Trained batch 1004 batch loss 6.81094933 epoch total loss 6.83526039\n",
      "Trained batch 1005 batch loss 6.83682442 epoch total loss 6.8352623\n",
      "Trained batch 1006 batch loss 6.32552195 epoch total loss 6.83475542\n",
      "Trained batch 1007 batch loss 6.19979 epoch total loss 6.83412504\n",
      "Trained batch 1008 batch loss 6.66229 epoch total loss 6.83395433\n",
      "Trained batch 1009 batch loss 6.47363281 epoch total loss 6.83359718\n",
      "Trained batch 1010 batch loss 6.6718874 epoch total loss 6.83343697\n",
      "Trained batch 1011 batch loss 5.9880743 epoch total loss 6.83260107\n",
      "Trained batch 1012 batch loss 6.26255512 epoch total loss 6.83203793\n",
      "Trained batch 1013 batch loss 6.63376141 epoch total loss 6.83184242\n",
      "Trained batch 1014 batch loss 6.80266142 epoch total loss 6.83181381\n",
      "Trained batch 1015 batch loss 6.93946409 epoch total loss 6.83191967\n",
      "Trained batch 1016 batch loss 6.94557142 epoch total loss 6.83203173\n",
      "Trained batch 1017 batch loss 6.81085968 epoch total loss 6.83201122\n",
      "Trained batch 1018 batch loss 6.1576252 epoch total loss 6.8313489\n",
      "Trained batch 1019 batch loss 6.4026823 epoch total loss 6.83092833\n",
      "Trained batch 1020 batch loss 6.64973927 epoch total loss 6.83075094\n",
      "Trained batch 1021 batch loss 6.73795795 epoch total loss 6.83066\n",
      "Trained batch 1022 batch loss 5.96223593 epoch total loss 6.82981\n",
      "Trained batch 1023 batch loss 5.9202528 epoch total loss 6.82892132\n",
      "Trained batch 1024 batch loss 6.15248823 epoch total loss 6.82826042\n",
      "Trained batch 1025 batch loss 6.21373 epoch total loss 6.82766104\n",
      "Trained batch 1026 batch loss 6.77531338 epoch total loss 6.82761\n",
      "Trained batch 1027 batch loss 6.68403196 epoch total loss 6.8274703\n",
      "Trained batch 1028 batch loss 6.68842173 epoch total loss 6.82733488\n",
      "Trained batch 1029 batch loss 6.18491268 epoch total loss 6.8267107\n",
      "Trained batch 1030 batch loss 6.56510925 epoch total loss 6.82645655\n",
      "Trained batch 1031 batch loss 6.7514329 epoch total loss 6.82638407\n",
      "Trained batch 1032 batch loss 6.81976271 epoch total loss 6.82637787\n",
      "Trained batch 1033 batch loss 6.77811956 epoch total loss 6.82633114\n",
      "Trained batch 1034 batch loss 6.28556776 epoch total loss 6.82580805\n",
      "Trained batch 1035 batch loss 5.21133852 epoch total loss 6.82424831\n",
      "Trained batch 1036 batch loss 5.31083536 epoch total loss 6.82278776\n",
      "Trained batch 1037 batch loss 6.84163475 epoch total loss 6.82280636\n",
      "Trained batch 1038 batch loss 6.85593414 epoch total loss 6.82283831\n",
      "Trained batch 1039 batch loss 7.74665689 epoch total loss 6.82372713\n",
      "Trained batch 1040 batch loss 7.34879637 epoch total loss 6.8242321\n",
      "Trained batch 1041 batch loss 7.0209322 epoch total loss 6.82442093\n",
      "Trained batch 1042 batch loss 6.82376432 epoch total loss 6.82442045\n",
      "Trained batch 1043 batch loss 6.99597263 epoch total loss 6.82458496\n",
      "Trained batch 1044 batch loss 6.789505 epoch total loss 6.82455111\n",
      "Trained batch 1045 batch loss 6.55539036 epoch total loss 6.82429361\n",
      "Trained batch 1046 batch loss 6.23099232 epoch total loss 6.82372618\n",
      "Trained batch 1047 batch loss 6.51305723 epoch total loss 6.82342958\n",
      "Trained batch 1048 batch loss 6.73314285 epoch total loss 6.82334328\n",
      "Trained batch 1049 batch loss 6.34786749 epoch total loss 6.82289\n",
      "Trained batch 1050 batch loss 6.76119518 epoch total loss 6.82283115\n",
      "Trained batch 1051 batch loss 6.4094348 epoch total loss 6.82243776\n",
      "Trained batch 1052 batch loss 6.83349752 epoch total loss 6.82244873\n",
      "Trained batch 1053 batch loss 6.4528 epoch total loss 6.8220973\n",
      "Trained batch 1054 batch loss 6.93728161 epoch total loss 6.82220697\n",
      "Trained batch 1055 batch loss 6.91202164 epoch total loss 6.82229185\n",
      "Trained batch 1056 batch loss 6.83680677 epoch total loss 6.82230568\n",
      "Trained batch 1057 batch loss 7.02597141 epoch total loss 6.82249832\n",
      "Trained batch 1058 batch loss 7.08324766 epoch total loss 6.82274485\n",
      "Trained batch 1059 batch loss 6.67854595 epoch total loss 6.82260847\n",
      "Trained batch 1060 batch loss 6.44200754 epoch total loss 6.82224941\n",
      "Trained batch 1061 batch loss 6.35313892 epoch total loss 6.82180738\n",
      "Trained batch 1062 batch loss 5.81794786 epoch total loss 6.82086182\n",
      "Trained batch 1063 batch loss 6.7104907 epoch total loss 6.82075787\n",
      "Trained batch 1064 batch loss 6.12638187 epoch total loss 6.82010555\n",
      "Trained batch 1065 batch loss 6.34667587 epoch total loss 6.81966114\n",
      "Trained batch 1066 batch loss 6.92808771 epoch total loss 6.81976271\n",
      "Trained batch 1067 batch loss 6.57121754 epoch total loss 6.81953\n",
      "Trained batch 1068 batch loss 6.5356884 epoch total loss 6.81926394\n",
      "Trained batch 1069 batch loss 6.73890352 epoch total loss 6.81918907\n",
      "Trained batch 1070 batch loss 6.71429968 epoch total loss 6.81909084\n",
      "Trained batch 1071 batch loss 7.10449123 epoch total loss 6.8193574\n",
      "Trained batch 1072 batch loss 7.08522844 epoch total loss 6.81960535\n",
      "Trained batch 1073 batch loss 6.67012644 epoch total loss 6.81946611\n",
      "Trained batch 1074 batch loss 6.86869287 epoch total loss 6.81951189\n",
      "Trained batch 1075 batch loss 6.77317142 epoch total loss 6.8194685\n",
      "Trained batch 1076 batch loss 6.24962282 epoch total loss 6.81893873\n",
      "Trained batch 1077 batch loss 6.88563538 epoch total loss 6.81900072\n",
      "Trained batch 1078 batch loss 7.00912094 epoch total loss 6.81917763\n",
      "Trained batch 1079 batch loss 7.10811806 epoch total loss 6.81944513\n",
      "Trained batch 1080 batch loss 6.91570759 epoch total loss 6.81953382\n",
      "Trained batch 1081 batch loss 6.18514109 epoch total loss 6.81894684\n",
      "Trained batch 1082 batch loss 6.67072535 epoch total loss 6.81881\n",
      "Trained batch 1083 batch loss 6.01721382 epoch total loss 6.81807\n",
      "Trained batch 1084 batch loss 6.86798811 epoch total loss 6.81811619\n",
      "Trained batch 1085 batch loss 6.17817688 epoch total loss 6.81752634\n",
      "Trained batch 1086 batch loss 6.38702631 epoch total loss 6.81713\n",
      "Trained batch 1087 batch loss 6.50932 epoch total loss 6.81684685\n",
      "Trained batch 1088 batch loss 7.11384201 epoch total loss 6.8171196\n",
      "Trained batch 1089 batch loss 6.81089544 epoch total loss 6.81711435\n",
      "Trained batch 1090 batch loss 6.82332325 epoch total loss 6.81712\n",
      "Trained batch 1091 batch loss 6.61530685 epoch total loss 6.81693459\n",
      "Trained batch 1092 batch loss 6.51116705 epoch total loss 6.81665468\n",
      "Trained batch 1093 batch loss 6.83933449 epoch total loss 6.81667566\n",
      "Trained batch 1094 batch loss 6.86260223 epoch total loss 6.81671762\n",
      "Trained batch 1095 batch loss 6.90647 epoch total loss 6.81679964\n",
      "Trained batch 1096 batch loss 6.81702328 epoch total loss 6.81679964\n",
      "Trained batch 1097 batch loss 6.67925835 epoch total loss 6.81667423\n",
      "Trained batch 1098 batch loss 6.06385565 epoch total loss 6.81598854\n",
      "Trained batch 1099 batch loss 6.16680813 epoch total loss 6.81539822\n",
      "Trained batch 1100 batch loss 5.97283459 epoch total loss 6.81463194\n",
      "Trained batch 1101 batch loss 6.14671946 epoch total loss 6.81402493\n",
      "Trained batch 1102 batch loss 6.32664299 epoch total loss 6.8135829\n",
      "Trained batch 1103 batch loss 6.88824654 epoch total loss 6.81365061\n",
      "Trained batch 1104 batch loss 6.84034967 epoch total loss 6.81367493\n",
      "Trained batch 1105 batch loss 6.86077356 epoch total loss 6.81371737\n",
      "Trained batch 1106 batch loss 6.89146376 epoch total loss 6.81378794\n",
      "Trained batch 1107 batch loss 6.57159233 epoch total loss 6.81356907\n",
      "Trained batch 1108 batch loss 6.96493 epoch total loss 6.81370592\n",
      "Trained batch 1109 batch loss 6.67051792 epoch total loss 6.8135767\n",
      "Trained batch 1110 batch loss 6.86156273 epoch total loss 6.81361961\n",
      "Trained batch 1111 batch loss 6.85271597 epoch total loss 6.81365442\n",
      "Trained batch 1112 batch loss 6.83925104 epoch total loss 6.81367779\n",
      "Trained batch 1113 batch loss 6.92110491 epoch total loss 6.81377411\n",
      "Trained batch 1114 batch loss 6.64043808 epoch total loss 6.81361866\n",
      "Trained batch 1115 batch loss 6.73324347 epoch total loss 6.81354666\n",
      "Trained batch 1116 batch loss 6.8503232 epoch total loss 6.81357956\n",
      "Trained batch 1117 batch loss 6.81817245 epoch total loss 6.81358385\n",
      "Trained batch 1118 batch loss 6.70192814 epoch total loss 6.81348419\n",
      "Trained batch 1119 batch loss 6.77376032 epoch total loss 6.81344843\n",
      "Trained batch 1120 batch loss 6.53582191 epoch total loss 6.81320047\n",
      "Trained batch 1121 batch loss 6.26922178 epoch total loss 6.81271505\n",
      "Trained batch 1122 batch loss 6.24131775 epoch total loss 6.81220579\n",
      "Trained batch 1123 batch loss 6.70583105 epoch total loss 6.81211138\n",
      "Trained batch 1124 batch loss 6.57994318 epoch total loss 6.81190491\n",
      "Trained batch 1125 batch loss 6.87619781 epoch total loss 6.81196165\n",
      "Trained batch 1126 batch loss 6.46625805 epoch total loss 6.81165504\n",
      "Trained batch 1127 batch loss 6.55498505 epoch total loss 6.81142712\n",
      "Trained batch 1128 batch loss 6.33941793 epoch total loss 6.81100893\n",
      "Trained batch 1129 batch loss 6.78278923 epoch total loss 6.81098366\n",
      "Trained batch 1130 batch loss 6.74419308 epoch total loss 6.81092453\n",
      "Trained batch 1131 batch loss 6.47426414 epoch total loss 6.81062651\n",
      "Trained batch 1132 batch loss 6.7217741 epoch total loss 6.81054831\n",
      "Trained batch 1133 batch loss 6.46566391 epoch total loss 6.81024408\n",
      "Trained batch 1134 batch loss 6.72750759 epoch total loss 6.81017113\n",
      "Trained batch 1135 batch loss 6.78332806 epoch total loss 6.81014729\n",
      "Trained batch 1136 batch loss 6.536 epoch total loss 6.80990601\n",
      "Trained batch 1137 batch loss 6.32576942 epoch total loss 6.80948\n",
      "Trained batch 1138 batch loss 6.75734377 epoch total loss 6.80943441\n",
      "Trained batch 1139 batch loss 6.5946207 epoch total loss 6.80924559\n",
      "Trained batch 1140 batch loss 7.02029276 epoch total loss 6.80943108\n",
      "Trained batch 1141 batch loss 6.9966917 epoch total loss 6.80959511\n",
      "Trained batch 1142 batch loss 7.12082481 epoch total loss 6.80986738\n",
      "Trained batch 1143 batch loss 6.74871874 epoch total loss 6.80981398\n",
      "Trained batch 1144 batch loss 6.65812302 epoch total loss 6.80968142\n",
      "Trained batch 1145 batch loss 5.75286341 epoch total loss 6.80875826\n",
      "Trained batch 1146 batch loss 6.13389397 epoch total loss 6.80816936\n",
      "Trained batch 1147 batch loss 6.70759344 epoch total loss 6.80808163\n",
      "Trained batch 1148 batch loss 6.92229462 epoch total loss 6.80818129\n",
      "Trained batch 1149 batch loss 6.66269732 epoch total loss 6.80805445\n",
      "Trained batch 1150 batch loss 6.21524858 epoch total loss 6.80753899\n",
      "Trained batch 1151 batch loss 6.73994493 epoch total loss 6.80748034\n",
      "Trained batch 1152 batch loss 7.08261156 epoch total loss 6.80771875\n",
      "Trained batch 1153 batch loss 6.43423653 epoch total loss 6.80739498\n",
      "Trained batch 1154 batch loss 6.89664745 epoch total loss 6.80747223\n",
      "Trained batch 1155 batch loss 6.92631912 epoch total loss 6.80757475\n",
      "Trained batch 1156 batch loss 6.95340443 epoch total loss 6.80770111\n",
      "Trained batch 1157 batch loss 6.58063126 epoch total loss 6.80750513\n",
      "Trained batch 1158 batch loss 6.6647234 epoch total loss 6.80738163\n",
      "Trained batch 1159 batch loss 6.85242081 epoch total loss 6.80742025\n",
      "Trained batch 1160 batch loss 6.4600749 epoch total loss 6.8071208\n",
      "Trained batch 1161 batch loss 6.52539 epoch total loss 6.80687809\n",
      "Trained batch 1162 batch loss 6.39156055 epoch total loss 6.80652094\n",
      "Trained batch 1163 batch loss 6.75993061 epoch total loss 6.80648088\n",
      "Trained batch 1164 batch loss 6.56299257 epoch total loss 6.80627155\n",
      "Trained batch 1165 batch loss 6.93921661 epoch total loss 6.80638599\n",
      "Trained batch 1166 batch loss 6.77775097 epoch total loss 6.8063612\n",
      "Trained batch 1167 batch loss 6.69124603 epoch total loss 6.80626297\n",
      "Trained batch 1168 batch loss 6.3509407 epoch total loss 6.80587292\n",
      "Trained batch 1169 batch loss 7.14118624 epoch total loss 6.80616\n",
      "Trained batch 1170 batch loss 6.67061806 epoch total loss 6.80604362\n",
      "Trained batch 1171 batch loss 6.41612053 epoch total loss 6.80571079\n",
      "Trained batch 1172 batch loss 6.2401557 epoch total loss 6.80522823\n",
      "Trained batch 1173 batch loss 6.18415403 epoch total loss 6.80469894\n",
      "Trained batch 1174 batch loss 6.53723812 epoch total loss 6.80447102\n",
      "Trained batch 1175 batch loss 6.646626 epoch total loss 6.80433655\n",
      "Trained batch 1176 batch loss 6.62813759 epoch total loss 6.80418634\n",
      "Trained batch 1177 batch loss 6.72809935 epoch total loss 6.80412149\n",
      "Trained batch 1178 batch loss 6.92606354 epoch total loss 6.80422544\n",
      "Trained batch 1179 batch loss 6.60893488 epoch total loss 6.80405951\n",
      "Trained batch 1180 batch loss 6.81355524 epoch total loss 6.80406761\n",
      "Trained batch 1181 batch loss 6.85935831 epoch total loss 6.80411434\n",
      "Trained batch 1182 batch loss 6.80179119 epoch total loss 6.80411243\n",
      "Trained batch 1183 batch loss 6.74799204 epoch total loss 6.80406523\n",
      "Trained batch 1184 batch loss 6.680902 epoch total loss 6.8039608\n",
      "Trained batch 1185 batch loss 6.93144846 epoch total loss 6.80406857\n",
      "Trained batch 1186 batch loss 7.05097532 epoch total loss 6.80427647\n",
      "Trained batch 1187 batch loss 6.8986764 epoch total loss 6.8043561\n",
      "Trained batch 1188 batch loss 6.76109934 epoch total loss 6.80432\n",
      "Trained batch 1189 batch loss 6.72351074 epoch total loss 6.80425167\n",
      "Trained batch 1190 batch loss 6.77723503 epoch total loss 6.80422926\n",
      "Trained batch 1191 batch loss 6.94772243 epoch total loss 6.80435\n",
      "Trained batch 1192 batch loss 6.85097742 epoch total loss 6.804389\n",
      "Trained batch 1193 batch loss 6.82059 epoch total loss 6.80440283\n",
      "Trained batch 1194 batch loss 6.61842 epoch total loss 6.8042469\n",
      "Trained batch 1195 batch loss 6.53377724 epoch total loss 6.80402088\n",
      "Trained batch 1196 batch loss 6.80173874 epoch total loss 6.80401897\n",
      "Trained batch 1197 batch loss 6.45565891 epoch total loss 6.80372763\n",
      "Trained batch 1198 batch loss 6.3172636 epoch total loss 6.80332184\n",
      "Trained batch 1199 batch loss 6.49962616 epoch total loss 6.80306816\n",
      "Trained batch 1200 batch loss 6.15786743 epoch total loss 6.80253029\n",
      "Trained batch 1201 batch loss 6.32444763 epoch total loss 6.80213213\n",
      "Trained batch 1202 batch loss 6.27075624 epoch total loss 6.80169058\n",
      "Trained batch 1203 batch loss 6.32197285 epoch total loss 6.80129147\n",
      "Trained batch 1204 batch loss 5.99963284 epoch total loss 6.80062532\n",
      "Trained batch 1205 batch loss 5.96604347 epoch total loss 6.79993296\n",
      "Trained batch 1206 batch loss 6.06005573 epoch total loss 6.79931879\n",
      "Trained batch 1207 batch loss 6.45239925 epoch total loss 6.79903126\n",
      "Trained batch 1208 batch loss 6.61107492 epoch total loss 6.79887581\n",
      "Trained batch 1209 batch loss 6.46458292 epoch total loss 6.79859972\n",
      "Trained batch 1210 batch loss 6.79066467 epoch total loss 6.79859304\n",
      "Trained batch 1211 batch loss 6.93382311 epoch total loss 6.79870462\n",
      "Trained batch 1212 batch loss 6.75878763 epoch total loss 6.79867172\n",
      "Trained batch 1213 batch loss 6.92700624 epoch total loss 6.79877758\n",
      "Trained batch 1214 batch loss 6.80886126 epoch total loss 6.79878569\n",
      "Trained batch 1215 batch loss 6.83061266 epoch total loss 6.79881191\n",
      "Trained batch 1216 batch loss 6.55647755 epoch total loss 6.79861307\n",
      "Trained batch 1217 batch loss 6.54952097 epoch total loss 6.79840851\n",
      "Trained batch 1218 batch loss 6.46428776 epoch total loss 6.79813385\n",
      "Trained batch 1219 batch loss 5.63880968 epoch total loss 6.79718256\n",
      "Trained batch 1220 batch loss 6.4931 epoch total loss 6.79693365\n",
      "Trained batch 1221 batch loss 6.17409372 epoch total loss 6.79642296\n",
      "Trained batch 1222 batch loss 6.51941061 epoch total loss 6.79619646\n",
      "Trained batch 1223 batch loss 6.97454405 epoch total loss 6.79634237\n",
      "Trained batch 1224 batch loss 6.9383626 epoch total loss 6.79645872\n",
      "Trained batch 1225 batch loss 6.6054821 epoch total loss 6.7963028\n",
      "Trained batch 1226 batch loss 7.03829193 epoch total loss 6.79649973\n",
      "Trained batch 1227 batch loss 6.79861832 epoch total loss 6.79650164\n",
      "Trained batch 1228 batch loss 6.76042557 epoch total loss 6.79647255\n",
      "Trained batch 1229 batch loss 6.41500473 epoch total loss 6.79616213\n",
      "Trained batch 1230 batch loss 6.55171585 epoch total loss 6.79596376\n",
      "Trained batch 1231 batch loss 6.37115908 epoch total loss 6.79561853\n",
      "Trained batch 1232 batch loss 6.47057819 epoch total loss 6.79535484\n",
      "Trained batch 1233 batch loss 6.31132174 epoch total loss 6.79496241\n",
      "Trained batch 1234 batch loss 6.6339488 epoch total loss 6.79483175\n",
      "Trained batch 1235 batch loss 6.09721184 epoch total loss 6.79426718\n",
      "Trained batch 1236 batch loss 6.24338341 epoch total loss 6.79382133\n",
      "Trained batch 1237 batch loss 6.43182135 epoch total loss 6.79352856\n",
      "Trained batch 1238 batch loss 6.37666702 epoch total loss 6.79319191\n",
      "Trained batch 1239 batch loss 6.45896721 epoch total loss 6.79292202\n",
      "Trained batch 1240 batch loss 6.92676544 epoch total loss 6.79303026\n",
      "Trained batch 1241 batch loss 6.35317945 epoch total loss 6.79267597\n",
      "Trained batch 1242 batch loss 5.87972212 epoch total loss 6.79194117\n",
      "Trained batch 1243 batch loss 6.63732958 epoch total loss 6.79181719\n",
      "Trained batch 1244 batch loss 6.64811802 epoch total loss 6.79170179\n",
      "Trained batch 1245 batch loss 6.78927231 epoch total loss 6.79169941\n",
      "Trained batch 1246 batch loss 6.80980682 epoch total loss 6.79171419\n",
      "Trained batch 1247 batch loss 6.70909643 epoch total loss 6.79164743\n",
      "Trained batch 1248 batch loss 6.86169386 epoch total loss 6.79170322\n",
      "Trained batch 1249 batch loss 6.65478611 epoch total loss 6.79159403\n",
      "Trained batch 1250 batch loss 6.6671896 epoch total loss 6.79149437\n",
      "Trained batch 1251 batch loss 6.40907097 epoch total loss 6.79118872\n",
      "Trained batch 1252 batch loss 6.42067528 epoch total loss 6.79089308\n",
      "Trained batch 1253 batch loss 6.47143412 epoch total loss 6.79063845\n",
      "Trained batch 1254 batch loss 6.62450171 epoch total loss 6.79050541\n",
      "Trained batch 1255 batch loss 6.7140851 epoch total loss 6.79044437\n",
      "Trained batch 1256 batch loss 6.60673285 epoch total loss 6.79029799\n",
      "Trained batch 1257 batch loss 6.41524792 epoch total loss 6.78999949\n",
      "Trained batch 1258 batch loss 6.76905441 epoch total loss 6.78998327\n",
      "Trained batch 1259 batch loss 6.67412663 epoch total loss 6.78989077\n",
      "Trained batch 1260 batch loss 6.14378 epoch total loss 6.78937817\n",
      "Trained batch 1261 batch loss 6.4724555 epoch total loss 6.78912687\n",
      "Trained batch 1262 batch loss 6.75536537 epoch total loss 6.78909969\n",
      "Trained batch 1263 batch loss 6.43066406 epoch total loss 6.78881598\n",
      "Trained batch 1264 batch loss 6.61433125 epoch total loss 6.78867769\n",
      "Trained batch 1265 batch loss 6.39665651 epoch total loss 6.78836775\n",
      "Trained batch 1266 batch loss 6.85551453 epoch total loss 6.78842068\n",
      "Trained batch 1267 batch loss 7.23427916 epoch total loss 6.78877258\n",
      "Trained batch 1268 batch loss 7.00212669 epoch total loss 6.78894091\n",
      "Trained batch 1269 batch loss 6.83864069 epoch total loss 6.78898\n",
      "Trained batch 1270 batch loss 6.64505339 epoch total loss 6.788867\n",
      "Trained batch 1271 batch loss 6.62386131 epoch total loss 6.7887373\n",
      "Trained batch 1272 batch loss 6.37692976 epoch total loss 6.78841352\n",
      "Trained batch 1273 batch loss 6.32109308 epoch total loss 6.78804684\n",
      "Trained batch 1274 batch loss 6.42637062 epoch total loss 6.78776312\n",
      "Trained batch 1275 batch loss 6.24026203 epoch total loss 6.78733397\n",
      "Trained batch 1276 batch loss 6.24577188 epoch total loss 6.78690958\n",
      "Trained batch 1277 batch loss 6.54079151 epoch total loss 6.78671694\n",
      "Trained batch 1278 batch loss 6.66695595 epoch total loss 6.78662348\n",
      "Trained batch 1279 batch loss 6.40354252 epoch total loss 6.78632355\n",
      "Trained batch 1280 batch loss 7.09938478 epoch total loss 6.78656864\n",
      "Trained batch 1281 batch loss 7.36268187 epoch total loss 6.78701782\n",
      "Trained batch 1282 batch loss 6.90967178 epoch total loss 6.78711414\n",
      "Trained batch 1283 batch loss 7.09117889 epoch total loss 6.78735065\n",
      "Trained batch 1284 batch loss 7.08346844 epoch total loss 6.78758097\n",
      "Trained batch 1285 batch loss 6.98272943 epoch total loss 6.7877326\n",
      "Trained batch 1286 batch loss 6.44159317 epoch total loss 6.78746319\n",
      "Trained batch 1287 batch loss 6.64729643 epoch total loss 6.78735447\n",
      "Trained batch 1288 batch loss 6.7574172 epoch total loss 6.78733158\n",
      "Trained batch 1289 batch loss 6.33333158 epoch total loss 6.7869792\n",
      "Trained batch 1290 batch loss 6.72418976 epoch total loss 6.78693056\n",
      "Trained batch 1291 batch loss 6.58396578 epoch total loss 6.78677368\n",
      "Trained batch 1292 batch loss 6.50126505 epoch total loss 6.78655243\n",
      "Trained batch 1293 batch loss 6.44468 epoch total loss 6.78628778\n",
      "Trained batch 1294 batch loss 6.29116726 epoch total loss 6.78590488\n",
      "Trained batch 1295 batch loss 6.50440598 epoch total loss 6.78568792\n",
      "Trained batch 1296 batch loss 6.37392759 epoch total loss 6.78537035\n",
      "Trained batch 1297 batch loss 6.55902767 epoch total loss 6.78519535\n",
      "Trained batch 1298 batch loss 6.57973766 epoch total loss 6.78503752\n",
      "Trained batch 1299 batch loss 6.26409626 epoch total loss 6.78463602\n",
      "Trained batch 1300 batch loss 6.0877037 epoch total loss 6.7841\n",
      "Trained batch 1301 batch loss 6.01454 epoch total loss 6.78350878\n",
      "Trained batch 1302 batch loss 6.85014153 epoch total loss 6.78356028\n",
      "Trained batch 1303 batch loss 6.71836376 epoch total loss 6.78351\n",
      "Trained batch 1304 batch loss 6.81231499 epoch total loss 6.78353262\n",
      "Trained batch 1305 batch loss 6.89835 epoch total loss 6.78362083\n",
      "Trained batch 1306 batch loss 6.81972551 epoch total loss 6.78364801\n",
      "Trained batch 1307 batch loss 6.86329699 epoch total loss 6.78370905\n",
      "Trained batch 1308 batch loss 6.82168341 epoch total loss 6.78373766\n",
      "Trained batch 1309 batch loss 6.98849678 epoch total loss 6.78389406\n",
      "Trained batch 1310 batch loss 6.87291622 epoch total loss 6.78396177\n",
      "Trained batch 1311 batch loss 6.73822308 epoch total loss 6.78392696\n",
      "Trained batch 1312 batch loss 6.80978 epoch total loss 6.78394651\n",
      "Trained batch 1313 batch loss 6.82716179 epoch total loss 6.78397942\n",
      "Trained batch 1314 batch loss 6.76883125 epoch total loss 6.78396797\n",
      "Trained batch 1315 batch loss 6.66094303 epoch total loss 6.78387451\n",
      "Trained batch 1316 batch loss 6.71702194 epoch total loss 6.78382349\n",
      "Trained batch 1317 batch loss 6.54657364 epoch total loss 6.78364372\n",
      "Trained batch 1318 batch loss 6.49778748 epoch total loss 6.78342676\n",
      "Trained batch 1319 batch loss 6.81406116 epoch total loss 6.7834506\n",
      "Trained batch 1320 batch loss 6.67224932 epoch total loss 6.78336573\n",
      "Trained batch 1321 batch loss 6.68712473 epoch total loss 6.78329325\n",
      "Trained batch 1322 batch loss 6.62481499 epoch total loss 6.78317356\n",
      "Trained batch 1323 batch loss 6.70362139 epoch total loss 6.78311396\n",
      "Trained batch 1324 batch loss 6.5007515 epoch total loss 6.78290081\n",
      "Trained batch 1325 batch loss 6.51778698 epoch total loss 6.78270054\n",
      "Trained batch 1326 batch loss 6.57357836 epoch total loss 6.78254271\n",
      "Trained batch 1327 batch loss 6.75002813 epoch total loss 6.78251791\n",
      "Trained batch 1328 batch loss 6.61431646 epoch total loss 6.78239107\n",
      "Trained batch 1329 batch loss 6.8080883 epoch total loss 6.78241\n",
      "Trained batch 1330 batch loss 6.817101 epoch total loss 6.78243637\n",
      "Trained batch 1331 batch loss 6.53689861 epoch total loss 6.78225231\n",
      "Trained batch 1332 batch loss 6.24913216 epoch total loss 6.78185177\n",
      "Trained batch 1333 batch loss 6.50258255 epoch total loss 6.78164291\n",
      "Trained batch 1334 batch loss 6.62357378 epoch total loss 6.78152466\n",
      "Trained batch 1335 batch loss 6.60932922 epoch total loss 6.78139544\n",
      "Trained batch 1336 batch loss 6.54015541 epoch total loss 6.78121471\n",
      "Trained batch 1337 batch loss 6.80934191 epoch total loss 6.78123617\n",
      "Trained batch 1338 batch loss 6.56088686 epoch total loss 6.78107119\n",
      "Trained batch 1339 batch loss 6.94284964 epoch total loss 6.78119183\n",
      "Trained batch 1340 batch loss 6.30129862 epoch total loss 6.78083372\n",
      "Trained batch 1341 batch loss 6.05920362 epoch total loss 6.78029585\n",
      "Trained batch 1342 batch loss 6.73144293 epoch total loss 6.78025961\n",
      "Trained batch 1343 batch loss 6.81253815 epoch total loss 6.78028345\n",
      "Trained batch 1344 batch loss 6.76098108 epoch total loss 6.78026915\n",
      "Trained batch 1345 batch loss 6.89198589 epoch total loss 6.78035164\n",
      "Trained batch 1346 batch loss 6.46860075 epoch total loss 6.78012037\n",
      "Trained batch 1347 batch loss 6.95276546 epoch total loss 6.78024864\n",
      "Trained batch 1348 batch loss 6.77398491 epoch total loss 6.78024435\n",
      "Trained batch 1349 batch loss 7.00149155 epoch total loss 6.78040886\n",
      "Trained batch 1350 batch loss 7.08112144 epoch total loss 6.78063154\n",
      "Trained batch 1351 batch loss 7.01142788 epoch total loss 6.78080273\n",
      "Trained batch 1352 batch loss 6.67527723 epoch total loss 6.78072405\n",
      "Trained batch 1353 batch loss 6.69313478 epoch total loss 6.78065968\n",
      "Trained batch 1354 batch loss 7.03862381 epoch total loss 6.78085041\n",
      "Trained batch 1355 batch loss 6.99037313 epoch total loss 6.78100491\n",
      "Trained batch 1356 batch loss 6.93888092 epoch total loss 6.78112125\n",
      "Trained batch 1357 batch loss 6.88165236 epoch total loss 6.78119516\n",
      "Trained batch 1358 batch loss 6.91491365 epoch total loss 6.78129387\n",
      "Trained batch 1359 batch loss 7.06726837 epoch total loss 6.78150415\n",
      "Trained batch 1360 batch loss 6.50930882 epoch total loss 6.78130436\n",
      "Trained batch 1361 batch loss 6.93453121 epoch total loss 6.78141737\n",
      "Trained batch 1362 batch loss 6.8029995 epoch total loss 6.78143263\n",
      "Trained batch 1363 batch loss 6.94065619 epoch total loss 6.78154945\n",
      "Trained batch 1364 batch loss 6.64491701 epoch total loss 6.78144884\n",
      "Trained batch 1365 batch loss 6.23050976 epoch total loss 6.78104544\n",
      "Trained batch 1366 batch loss 5.84626532 epoch total loss 6.78036118\n",
      "Trained batch 1367 batch loss 6.46321297 epoch total loss 6.78012896\n",
      "Trained batch 1368 batch loss 6.86392 epoch total loss 6.78019047\n",
      "Trained batch 1369 batch loss 7.11737967 epoch total loss 6.78043699\n",
      "Trained batch 1370 batch loss 6.9075923 epoch total loss 6.7805295\n",
      "Trained batch 1371 batch loss 6.72166443 epoch total loss 6.78048658\n",
      "Trained batch 1372 batch loss 6.77748203 epoch total loss 6.7804842\n",
      "Trained batch 1373 batch loss 6.84017086 epoch total loss 6.78052759\n",
      "Trained batch 1374 batch loss 6.97686243 epoch total loss 6.78067\n",
      "Trained batch 1375 batch loss 7.0251646 epoch total loss 6.78084803\n",
      "Trained batch 1376 batch loss 7.17184639 epoch total loss 6.78113222\n",
      "Trained batch 1377 batch loss 7.29151583 epoch total loss 6.7815032\n",
      "Trained batch 1378 batch loss 6.94767332 epoch total loss 6.78162336\n",
      "Trained batch 1379 batch loss 6.94276905 epoch total loss 6.78174\n",
      "Trained batch 1380 batch loss 6.65961647 epoch total loss 6.78165102\n",
      "Trained batch 1381 batch loss 6.75011253 epoch total loss 6.78162813\n",
      "Trained batch 1382 batch loss 6.66229296 epoch total loss 6.78154182\n",
      "Trained batch 1383 batch loss 6.62853909 epoch total loss 6.78143167\n",
      "Trained batch 1384 batch loss 6.99609089 epoch total loss 6.78158665\n",
      "Trained batch 1385 batch loss 6.87389565 epoch total loss 6.7816534\n",
      "Trained batch 1386 batch loss 6.60030365 epoch total loss 6.78152275\n",
      "Trained batch 1387 batch loss 6.64925 epoch total loss 6.78142738\n",
      "Trained batch 1388 batch loss 6.73714685 epoch total loss 6.78139544\n",
      "Epoch 5 train loss 6.781395435333252\n",
      "Validated batch 1 batch loss 6.58151054\n",
      "Validated batch 2 batch loss 7.32231331\n",
      "Validated batch 3 batch loss 7.07157135\n",
      "Validated batch 4 batch loss 7.16074562\n",
      "Validated batch 5 batch loss 7.13394117\n",
      "Validated batch 6 batch loss 7.36153221\n",
      "Validated batch 7 batch loss 6.94312096\n",
      "Validated batch 8 batch loss 7.27205467\n",
      "Validated batch 9 batch loss 6.80640602\n",
      "Validated batch 10 batch loss 7.06199694\n",
      "Validated batch 11 batch loss 6.94446\n",
      "Validated batch 12 batch loss 6.35511541\n",
      "Validated batch 13 batch loss 6.59906816\n",
      "Validated batch 14 batch loss 7.33592701\n",
      "Validated batch 15 batch loss 7.18630171\n",
      "Validated batch 16 batch loss 6.74975\n",
      "Validated batch 17 batch loss 7.18369246\n",
      "Validated batch 18 batch loss 7.15380144\n",
      "Validated batch 19 batch loss 7.0874691\n",
      "Validated batch 20 batch loss 7.30842733\n",
      "Validated batch 21 batch loss 7.18145943\n",
      "Validated batch 22 batch loss 6.88219404\n",
      "Validated batch 23 batch loss 6.63433123\n",
      "Validated batch 24 batch loss 7.06018782\n",
      "Validated batch 25 batch loss 7.38036203\n",
      "Validated batch 26 batch loss 6.91944933\n",
      "Validated batch 27 batch loss 6.29652166\n",
      "Validated batch 28 batch loss 6.85350752\n",
      "Validated batch 29 batch loss 7.06170082\n",
      "Validated batch 30 batch loss 6.44529247\n",
      "Validated batch 31 batch loss 6.74902534\n",
      "Validated batch 32 batch loss 6.70022535\n",
      "Validated batch 33 batch loss 7.00453\n",
      "Validated batch 34 batch loss 6.70334816\n",
      "Validated batch 35 batch loss 6.57769871\n",
      "Validated batch 36 batch loss 6.80587912\n",
      "Validated batch 37 batch loss 6.83697557\n",
      "Validated batch 38 batch loss 6.89808702\n",
      "Validated batch 39 batch loss 6.93662214\n",
      "Validated batch 40 batch loss 6.95537519\n",
      "Validated batch 41 batch loss 7.11121225\n",
      "Validated batch 42 batch loss 6.93506622\n",
      "Validated batch 43 batch loss 6.69905043\n",
      "Validated batch 44 batch loss 7.23908949\n",
      "Validated batch 45 batch loss 6.03890467\n",
      "Validated batch 46 batch loss 7.47606707\n",
      "Validated batch 47 batch loss 7.0672307\n",
      "Validated batch 48 batch loss 6.92038155\n",
      "Validated batch 49 batch loss 6.68842125\n",
      "Validated batch 50 batch loss 6.7927165\n",
      "Validated batch 51 batch loss 7.09488726\n",
      "Validated batch 52 batch loss 6.97666931\n",
      "Validated batch 53 batch loss 7.00830221\n",
      "Validated batch 54 batch loss 6.92274427\n",
      "Validated batch 55 batch loss 6.89773512\n",
      "Validated batch 56 batch loss 7.30796099\n",
      "Validated batch 57 batch loss 7.3752346\n",
      "Validated batch 58 batch loss 7.06986809\n",
      "Validated batch 59 batch loss 7.02293205\n",
      "Validated batch 60 batch loss 6.90623283\n",
      "Validated batch 61 batch loss 7.07411909\n",
      "Validated batch 62 batch loss 6.94293785\n",
      "Validated batch 63 batch loss 7.10841179\n",
      "Validated batch 64 batch loss 6.91660261\n",
      "Validated batch 65 batch loss 6.78023815\n",
      "Validated batch 66 batch loss 7.11651611\n",
      "Validated batch 67 batch loss 6.75354958\n",
      "Validated batch 68 batch loss 7.17233276\n",
      "Validated batch 69 batch loss 7.24021959\n",
      "Validated batch 70 batch loss 6.95082808\n",
      "Validated batch 71 batch loss 7.02021742\n",
      "Validated batch 72 batch loss 6.78928566\n",
      "Validated batch 73 batch loss 7.39026499\n",
      "Validated batch 74 batch loss 7.33032179\n",
      "Validated batch 75 batch loss 7.09578657\n",
      "Validated batch 76 batch loss 7.12683821\n",
      "Validated batch 77 batch loss 7.08265638\n",
      "Validated batch 78 batch loss 7.13950539\n",
      "Validated batch 79 batch loss 7.24515343\n",
      "Validated batch 80 batch loss 7.43776\n",
      "Validated batch 81 batch loss 7.19881487\n",
      "Validated batch 82 batch loss 6.74420738\n",
      "Validated batch 83 batch loss 7.25457191\n",
      "Validated batch 84 batch loss 6.86999464\n",
      "Validated batch 85 batch loss 7.11556435\n",
      "Validated batch 86 batch loss 7.3428421\n",
      "Validated batch 87 batch loss 6.32670498\n",
      "Validated batch 88 batch loss 6.85756636\n",
      "Validated batch 89 batch loss 7.1196003\n",
      "Validated batch 90 batch loss 6.89902496\n",
      "Validated batch 91 batch loss 7.54010582\n",
      "Validated batch 92 batch loss 6.88656139\n",
      "Validated batch 93 batch loss 7.48134422\n",
      "Validated batch 94 batch loss 7.13649\n",
      "Validated batch 95 batch loss 7.16074324\n",
      "Validated batch 96 batch loss 6.99883318\n",
      "Validated batch 97 batch loss 7.07923937\n",
      "Validated batch 98 batch loss 7.24054527\n",
      "Validated batch 99 batch loss 6.94251966\n",
      "Validated batch 100 batch loss 6.73755598\n",
      "Validated batch 101 batch loss 6.76859331\n",
      "Validated batch 102 batch loss 6.87646675\n",
      "Validated batch 103 batch loss 7.0620985\n",
      "Validated batch 104 batch loss 6.97830391\n",
      "Validated batch 105 batch loss 6.70740318\n",
      "Validated batch 106 batch loss 6.65785789\n",
      "Validated batch 107 batch loss 6.85040474\n",
      "Validated batch 108 batch loss 7.14550734\n",
      "Validated batch 109 batch loss 7.20230293\n",
      "Validated batch 110 batch loss 7.25766563\n",
      "Validated batch 111 batch loss 7.53876686\n",
      "Validated batch 112 batch loss 7.81781769\n",
      "Validated batch 113 batch loss 7.52919197\n",
      "Validated batch 114 batch loss 6.95337105\n",
      "Validated batch 115 batch loss 6.68808365\n",
      "Validated batch 116 batch loss 7.01901722\n",
      "Validated batch 117 batch loss 7.03448296\n",
      "Validated batch 118 batch loss 6.8230896\n",
      "Validated batch 119 batch loss 6.77377367\n",
      "Validated batch 120 batch loss 6.79727745\n",
      "Validated batch 121 batch loss 7.0263896\n",
      "Validated batch 122 batch loss 6.65360832\n",
      "Validated batch 123 batch loss 7.04065\n",
      "Validated batch 124 batch loss 7.13087273\n",
      "Validated batch 125 batch loss 7.00004768\n",
      "Validated batch 126 batch loss 6.87159538\n",
      "Validated batch 127 batch loss 6.66460228\n",
      "Validated batch 128 batch loss 6.8286438\n",
      "Validated batch 129 batch loss 7.26426\n",
      "Validated batch 130 batch loss 6.9085083\n",
      "Validated batch 131 batch loss 6.94284105\n",
      "Validated batch 132 batch loss 6.87901497\n",
      "Validated batch 133 batch loss 6.57213545\n",
      "Validated batch 134 batch loss 6.45627642\n",
      "Validated batch 135 batch loss 7.03343153\n",
      "Validated batch 136 batch loss 6.72269487\n",
      "Validated batch 137 batch loss 6.75277758\n",
      "Validated batch 138 batch loss 6.99107933\n",
      "Validated batch 139 batch loss 6.88470411\n",
      "Validated batch 140 batch loss 6.78038836\n",
      "Validated batch 141 batch loss 6.95970106\n",
      "Validated batch 142 batch loss 7.02965593\n",
      "Validated batch 143 batch loss 6.62769079\n",
      "Validated batch 144 batch loss 7.29253101\n",
      "Validated batch 145 batch loss 7.02496147\n",
      "Validated batch 146 batch loss 6.94268227\n",
      "Validated batch 147 batch loss 7.08006334\n",
      "Validated batch 148 batch loss 7.20854378\n",
      "Validated batch 149 batch loss 6.94216871\n",
      "Validated batch 150 batch loss 6.96732521\n",
      "Validated batch 151 batch loss 6.93581963\n",
      "Validated batch 152 batch loss 7.11688805\n",
      "Validated batch 153 batch loss 7.31666517\n",
      "Validated batch 154 batch loss 7.07647514\n",
      "Validated batch 155 batch loss 6.78030348\n",
      "Validated batch 156 batch loss 6.63343334\n",
      "Validated batch 157 batch loss 7.03927851\n",
      "Validated batch 158 batch loss 7.09587812\n",
      "Validated batch 159 batch loss 7.09726143\n",
      "Validated batch 160 batch loss 7.14488792\n",
      "Validated batch 161 batch loss 6.82636309\n",
      "Validated batch 162 batch loss 6.88742876\n",
      "Validated batch 163 batch loss 7.00975\n",
      "Validated batch 164 batch loss 6.73893\n",
      "Validated batch 165 batch loss 6.19078588\n",
      "Validated batch 166 batch loss 6.96265364\n",
      "Validated batch 167 batch loss 7.21278191\n",
      "Validated batch 168 batch loss 6.5682745\n",
      "Validated batch 169 batch loss 6.97432756\n",
      "Validated batch 170 batch loss 7.07633257\n",
      "Validated batch 171 batch loss 7.13841724\n",
      "Validated batch 172 batch loss 7.32033587\n",
      "Validated batch 173 batch loss 7.00511646\n",
      "Validated batch 174 batch loss 6.72672272\n",
      "Validated batch 175 batch loss 7.05137062\n",
      "Validated batch 176 batch loss 7.0992732\n",
      "Validated batch 177 batch loss 7.37052965\n",
      "Validated batch 178 batch loss 7.03645277\n",
      "Validated batch 179 batch loss 7.11548662\n",
      "Validated batch 180 batch loss 7.16017151\n",
      "Validated batch 181 batch loss 6.68330574\n",
      "Validated batch 182 batch loss 7.0388875\n",
      "Validated batch 183 batch loss 6.94630623\n",
      "Validated batch 184 batch loss 7.27541399\n",
      "Validated batch 185 batch loss 3.75436735\n",
      "Epoch 5 val loss 6.967929363250732\n",
      "Epoch 5 completed in 767.20 seconds\n",
      "Model ./model_simplebase-epoch-5-loss-6.9679.h5 saved.\n",
      "Start epoch 6 with learning rate 0.0007\n",
      "Trained batch 1 batch loss 6.94491625 epoch total loss 6.94491625\n",
      "Trained batch 2 batch loss 6.66467094 epoch total loss 6.80479336\n",
      "Trained batch 3 batch loss 6.74602127 epoch total loss 6.7852025\n",
      "Trained batch 4 batch loss 6.86095715 epoch total loss 6.80414104\n",
      "Trained batch 5 batch loss 6.83186865 epoch total loss 6.80968618\n",
      "Trained batch 6 batch loss 6.85985565 epoch total loss 6.818048\n",
      "Trained batch 7 batch loss 6.71372318 epoch total loss 6.80314398\n",
      "Trained batch 8 batch loss 6.93019247 epoch total loss 6.81902504\n",
      "Trained batch 9 batch loss 6.88495731 epoch total loss 6.82635069\n",
      "Trained batch 10 batch loss 6.74552298 epoch total loss 6.81826782\n",
      "Trained batch 11 batch loss 6.71829033 epoch total loss 6.80917931\n",
      "Trained batch 12 batch loss 6.73005915 epoch total loss 6.8025856\n",
      "Trained batch 13 batch loss 6.67638874 epoch total loss 6.79287815\n",
      "Trained batch 14 batch loss 6.684093 epoch total loss 6.78510761\n",
      "Trained batch 15 batch loss 6.83125114 epoch total loss 6.78818417\n",
      "Trained batch 16 batch loss 6.44689131 epoch total loss 6.76685333\n",
      "Trained batch 17 batch loss 6.54682255 epoch total loss 6.75391\n",
      "Trained batch 18 batch loss 6.72888136 epoch total loss 6.75251961\n",
      "Trained batch 19 batch loss 6.5345459 epoch total loss 6.74104691\n",
      "Trained batch 20 batch loss 6.69608879 epoch total loss 6.7387991\n",
      "Trained batch 21 batch loss 6.83032036 epoch total loss 6.74315739\n",
      "Trained batch 22 batch loss 6.40346861 epoch total loss 6.7277174\n",
      "Trained batch 23 batch loss 6.86278391 epoch total loss 6.73358965\n",
      "Trained batch 24 batch loss 6.53840923 epoch total loss 6.72545671\n",
      "Trained batch 25 batch loss 6.53332472 epoch total loss 6.71777153\n",
      "Trained batch 26 batch loss 6.6706953 epoch total loss 6.71596098\n",
      "Trained batch 27 batch loss 6.51202345 epoch total loss 6.70840788\n",
      "Trained batch 28 batch loss 6.61517096 epoch total loss 6.70507812\n",
      "Trained batch 29 batch loss 6.57799292 epoch total loss 6.70069551\n",
      "Trained batch 30 batch loss 6.35642338 epoch total loss 6.68922\n",
      "Trained batch 31 batch loss 6.69628668 epoch total loss 6.68944836\n",
      "Trained batch 32 batch loss 7.2742815 epoch total loss 6.70772409\n",
      "Trained batch 33 batch loss 7.04256678 epoch total loss 6.71787119\n",
      "Trained batch 34 batch loss 6.7203722 epoch total loss 6.71794462\n",
      "Trained batch 35 batch loss 6.89716101 epoch total loss 6.7230649\n",
      "Trained batch 36 batch loss 7.0205493 epoch total loss 6.73132849\n",
      "Trained batch 37 batch loss 6.83602238 epoch total loss 6.73415804\n",
      "Trained batch 38 batch loss 6.83857298 epoch total loss 6.73690557\n",
      "Trained batch 39 batch loss 6.96563578 epoch total loss 6.74277067\n",
      "Trained batch 40 batch loss 7.08369112 epoch total loss 6.75129318\n",
      "Trained batch 41 batch loss 7.02548838 epoch total loss 6.75798082\n",
      "Trained batch 42 batch loss 6.79986811 epoch total loss 6.75897789\n",
      "Trained batch 43 batch loss 6.81962442 epoch total loss 6.7603879\n",
      "Trained batch 44 batch loss 6.81718397 epoch total loss 6.7616787\n",
      "Trained batch 45 batch loss 6.63395119 epoch total loss 6.75884\n",
      "Trained batch 46 batch loss 6.47746849 epoch total loss 6.75272322\n",
      "Trained batch 47 batch loss 6.68975639 epoch total loss 6.75138378\n",
      "Trained batch 48 batch loss 6.3637948 epoch total loss 6.74330902\n",
      "Trained batch 49 batch loss 6.69211769 epoch total loss 6.74226427\n",
      "Trained batch 50 batch loss 6.58430958 epoch total loss 6.73910522\n",
      "Trained batch 51 batch loss 6.47001839 epoch total loss 6.7338295\n",
      "Trained batch 52 batch loss 6.77992249 epoch total loss 6.73471546\n",
      "Trained batch 53 batch loss 6.30149 epoch total loss 6.72654104\n",
      "Trained batch 54 batch loss 6.69925213 epoch total loss 6.72603559\n",
      "Trained batch 55 batch loss 6.62109327 epoch total loss 6.72412777\n",
      "Trained batch 56 batch loss 7.0254631 epoch total loss 6.7295084\n",
      "Trained batch 57 batch loss 7.15083027 epoch total loss 6.7369\n",
      "Trained batch 58 batch loss 6.80963612 epoch total loss 6.73815393\n",
      "Trained batch 59 batch loss 6.87817144 epoch total loss 6.74052715\n",
      "Trained batch 60 batch loss 6.03171062 epoch total loss 6.72871351\n",
      "Trained batch 61 batch loss 6.1857729 epoch total loss 6.71981239\n",
      "Trained batch 62 batch loss 6.39638138 epoch total loss 6.71459627\n",
      "Trained batch 63 batch loss 6.79443407 epoch total loss 6.71586323\n",
      "Trained batch 64 batch loss 6.81729031 epoch total loss 6.71744823\n",
      "Trained batch 65 batch loss 6.94903755 epoch total loss 6.72101116\n",
      "Trained batch 66 batch loss 6.93129873 epoch total loss 6.72419739\n",
      "Trained batch 67 batch loss 6.8770895 epoch total loss 6.72647905\n",
      "Trained batch 68 batch loss 6.48978138 epoch total loss 6.72299814\n",
      "Trained batch 69 batch loss 6.48762178 epoch total loss 6.71958685\n",
      "Trained batch 70 batch loss 6.47582102 epoch total loss 6.71610451\n",
      "Trained batch 71 batch loss 6.59505177 epoch total loss 6.7144\n",
      "Trained batch 72 batch loss 6.48731899 epoch total loss 6.71124554\n",
      "Trained batch 73 batch loss 6.36090136 epoch total loss 6.70644617\n",
      "Trained batch 74 batch loss 6.38261127 epoch total loss 6.70207\n",
      "Trained batch 75 batch loss 6.39549303 epoch total loss 6.69798279\n",
      "Trained batch 76 batch loss 6.75988674 epoch total loss 6.69879723\n",
      "Trained batch 77 batch loss 6.73276091 epoch total loss 6.69923878\n",
      "Trained batch 78 batch loss 6.58626699 epoch total loss 6.69778967\n",
      "Trained batch 79 batch loss 6.89305258 epoch total loss 6.70026159\n",
      "Trained batch 80 batch loss 6.85255384 epoch total loss 6.70216513\n",
      "Trained batch 81 batch loss 6.74992418 epoch total loss 6.70275497\n",
      "Trained batch 82 batch loss 6.98814869 epoch total loss 6.70623541\n",
      "Trained batch 83 batch loss 6.4806757 epoch total loss 6.70351791\n",
      "Trained batch 84 batch loss 6.69560862 epoch total loss 6.7034235\n",
      "Trained batch 85 batch loss 6.38293457 epoch total loss 6.69965315\n",
      "Trained batch 86 batch loss 6.44519949 epoch total loss 6.69669437\n",
      "Trained batch 87 batch loss 6.85553598 epoch total loss 6.69852\n",
      "Trained batch 88 batch loss 7.0629735 epoch total loss 6.70266151\n",
      "Trained batch 89 batch loss 6.63972425 epoch total loss 6.70195436\n",
      "Trained batch 90 batch loss 7.04385567 epoch total loss 6.7057538\n",
      "Trained batch 91 batch loss 6.49949598 epoch total loss 6.7034874\n",
      "Trained batch 92 batch loss 6.68798494 epoch total loss 6.7033186\n",
      "Trained batch 93 batch loss 6.81887484 epoch total loss 6.70456076\n",
      "Trained batch 94 batch loss 6.44605207 epoch total loss 6.70181084\n",
      "Trained batch 95 batch loss 6.81044912 epoch total loss 6.70295429\n",
      "Trained batch 96 batch loss 6.64762211 epoch total loss 6.7023778\n",
      "Trained batch 97 batch loss 6.61035442 epoch total loss 6.70142937\n",
      "Trained batch 98 batch loss 6.52749491 epoch total loss 6.6996541\n",
      "Trained batch 99 batch loss 6.63467264 epoch total loss 6.6989975\n",
      "Trained batch 100 batch loss 6.87069941 epoch total loss 6.70071459\n",
      "Trained batch 101 batch loss 6.64871597 epoch total loss 6.7002\n",
      "Trained batch 102 batch loss 6.74723101 epoch total loss 6.70066166\n",
      "Trained batch 103 batch loss 6.78030062 epoch total loss 6.70143461\n",
      "Trained batch 104 batch loss 6.48871851 epoch total loss 6.69938898\n",
      "Trained batch 105 batch loss 6.54649115 epoch total loss 6.69793272\n",
      "Trained batch 106 batch loss 6.53513241 epoch total loss 6.6963973\n",
      "Trained batch 107 batch loss 6.00352573 epoch total loss 6.68992186\n",
      "Trained batch 108 batch loss 5.97260427 epoch total loss 6.68328\n",
      "Trained batch 109 batch loss 6.46360922 epoch total loss 6.68126488\n",
      "Trained batch 110 batch loss 6.29356146 epoch total loss 6.67774057\n",
      "Trained batch 111 batch loss 6.50041914 epoch total loss 6.67614317\n",
      "Trained batch 112 batch loss 6.34926033 epoch total loss 6.67322445\n",
      "Trained batch 113 batch loss 6.84423399 epoch total loss 6.67473793\n",
      "Trained batch 114 batch loss 6.32171726 epoch total loss 6.67164087\n",
      "Trained batch 115 batch loss 6.38242 epoch total loss 6.66912651\n",
      "Trained batch 116 batch loss 6.71406078 epoch total loss 6.6695137\n",
      "Trained batch 117 batch loss 6.97447109 epoch total loss 6.67212\n",
      "Trained batch 118 batch loss 6.99278545 epoch total loss 6.67483759\n",
      "Trained batch 119 batch loss 7.04390717 epoch total loss 6.67793894\n",
      "Trained batch 120 batch loss 6.74483299 epoch total loss 6.67849636\n",
      "Trained batch 121 batch loss 6.37280416 epoch total loss 6.67597\n",
      "Trained batch 122 batch loss 6.61277294 epoch total loss 6.67545223\n",
      "Trained batch 123 batch loss 6.15801144 epoch total loss 6.6712451\n",
      "Trained batch 124 batch loss 6.46100426 epoch total loss 6.66955\n",
      "Trained batch 125 batch loss 6.20744658 epoch total loss 6.66585302\n",
      "Trained batch 126 batch loss 6.2608552 epoch total loss 6.66263866\n",
      "Trained batch 127 batch loss 6.46086359 epoch total loss 6.66105032\n",
      "Trained batch 128 batch loss 6.34990215 epoch total loss 6.6586194\n",
      "Trained batch 129 batch loss 6.61682272 epoch total loss 6.65829515\n",
      "Trained batch 130 batch loss 6.69044495 epoch total loss 6.65854263\n",
      "Trained batch 131 batch loss 6.62696123 epoch total loss 6.65830135\n",
      "Trained batch 132 batch loss 6.64422464 epoch total loss 6.65819502\n",
      "Trained batch 133 batch loss 6.53213358 epoch total loss 6.65724659\n",
      "Trained batch 134 batch loss 6.76298475 epoch total loss 6.65803576\n",
      "Trained batch 135 batch loss 6.92309618 epoch total loss 6.65999937\n",
      "Trained batch 136 batch loss 6.62925959 epoch total loss 6.65977335\n",
      "Trained batch 137 batch loss 6.70985 epoch total loss 6.66013908\n",
      "Trained batch 138 batch loss 6.70747137 epoch total loss 6.66048193\n",
      "Trained batch 139 batch loss 6.62607527 epoch total loss 6.66023445\n",
      "Trained batch 140 batch loss 6.65730476 epoch total loss 6.66021347\n",
      "Trained batch 141 batch loss 6.77027798 epoch total loss 6.66099405\n",
      "Trained batch 142 batch loss 6.64700794 epoch total loss 6.66089535\n",
      "Trained batch 143 batch loss 6.38203716 epoch total loss 6.65894556\n",
      "Trained batch 144 batch loss 6.84575605 epoch total loss 6.66024256\n",
      "Trained batch 145 batch loss 7.08284044 epoch total loss 6.66315699\n",
      "Trained batch 146 batch loss 6.85943651 epoch total loss 6.66450167\n",
      "Trained batch 147 batch loss 7.63226843 epoch total loss 6.67108488\n",
      "Trained batch 148 batch loss 7.7629714 epoch total loss 6.67846251\n",
      "Trained batch 149 batch loss 7.85909891 epoch total loss 6.68638611\n",
      "Trained batch 321 batch loss 6.31257629 epoch total loss 6.62895393\n",
      "Trained batch 322 batch loss 6.48162699 epoch total loss 6.62849665\n",
      "Trained batch 323 batch loss 7.00157881 epoch total loss 6.62965155\n",
      "Trained batch 324 batch loss 7.46123838 epoch total loss 6.63221788\n",
      "Trained batch 325 batch loss 6.7329936 epoch total loss 6.63252783\n",
      "Trained batch 326 batch loss 6.12448931 epoch total loss 6.63096952\n",
      "Trained batch 327 batch loss 5.48747778 epoch total loss 6.62747288\n",
      "Trained batch 328 batch loss 6.09738111 epoch total loss 6.62585688\n",
      "Trained batch 329 batch loss 6.31929159 epoch total loss 6.62492514\n",
      "Trained batch 330 batch loss 6.64474297 epoch total loss 6.62498522\n",
      "Trained batch 331 batch loss 6.58732843 epoch total loss 6.62487173\n",
      "Trained batch 332 batch loss 6.70279074 epoch total loss 6.62510681\n",
      "Trained batch 333 batch loss 6.50112677 epoch total loss 6.6247344\n",
      "Trained batch 334 batch loss 6.4825182 epoch total loss 6.62430859\n",
      "Trained batch 335 batch loss 6.59462261 epoch total loss 6.62422037\n",
      "Trained batch 336 batch loss 6.76951027 epoch total loss 6.62465286\n",
      "Trained batch 337 batch loss 6.24357367 epoch total loss 6.62352228\n",
      "Trained batch 338 batch loss 6.29390955 epoch total loss 6.62254715\n",
      "Trained batch 339 batch loss 6.48733187 epoch total loss 6.62214804\n",
      "Trained batch 340 batch loss 6.79485178 epoch total loss 6.62265635\n",
      "Trained batch 341 batch loss 6.43697453 epoch total loss 6.6221118\n",
      "Trained batch 342 batch loss 6.93608618 epoch total loss 6.62302971\n",
      "Trained batch 343 batch loss 6.92675257 epoch total loss 6.6239152\n",
      "Trained batch 344 batch loss 6.7558322 epoch total loss 6.62429857\n",
      "Trained batch 345 batch loss 6.29960966 epoch total loss 6.62335777\n",
      "Trained batch 346 batch loss 6.17355824 epoch total loss 6.62205744\n",
      "Trained batch 347 batch loss 6.30784941 epoch total loss 6.62115192\n",
      "Trained batch 348 batch loss 6.45883465 epoch total loss 6.62068558\n",
      "Trained batch 349 batch loss 6.39591885 epoch total loss 6.62004185\n",
      "Trained batch 350 batch loss 6.57657909 epoch total loss 6.61991787\n",
      "Trained batch 351 batch loss 5.66829205 epoch total loss 6.6172061\n",
      "Trained batch 352 batch loss 5.89546442 epoch total loss 6.61515617\n",
      "Trained batch 353 batch loss 5.98544168 epoch total loss 6.61337185\n",
      "Trained batch 354 batch loss 5.94598532 epoch total loss 6.61148691\n",
      "Trained batch 355 batch loss 6.43231392 epoch total loss 6.61098242\n",
      "Trained batch 356 batch loss 6.83184671 epoch total loss 6.61160231\n",
      "Trained batch 357 batch loss 7.05834913 epoch total loss 6.612854\n",
      "Trained batch 358 batch loss 6.93705273 epoch total loss 6.61375952\n",
      "Trained batch 359 batch loss 6.77994919 epoch total loss 6.61422253\n",
      "Trained batch 360 batch loss 6.68919897 epoch total loss 6.6144309\n",
      "Trained batch 361 batch loss 6.44346809 epoch total loss 6.61395693\n",
      "Trained batch 362 batch loss 6.0018816 epoch total loss 6.61226606\n",
      "Trained batch 363 batch loss 6.3261652 epoch total loss 6.61147833\n",
      "Trained batch 364 batch loss 6.59425068 epoch total loss 6.61143064\n",
      "Trained batch 365 batch loss 6.76091957 epoch total loss 6.61184025\n",
      "Trained batch 366 batch loss 6.67375517 epoch total loss 6.61201\n",
      "Trained batch 367 batch loss 6.72290945 epoch total loss 6.61231184\n",
      "Trained batch 368 batch loss 6.75535059 epoch total loss 6.61270094\n",
      "Trained batch 369 batch loss 6.32255554 epoch total loss 6.61191416\n",
      "Trained batch 370 batch loss 6.69505739 epoch total loss 6.61213923\n",
      "Trained batch 371 batch loss 6.25914049 epoch total loss 6.61118746\n",
      "Trained batch 372 batch loss 6.67706251 epoch total loss 6.61136436\n",
      "Trained batch 373 batch loss 6.85758162 epoch total loss 6.61202478\n",
      "Trained batch 374 batch loss 6.101161 epoch total loss 6.61065817\n",
      "Trained batch 375 batch loss 6.26816225 epoch total loss 6.60974503\n",
      "Trained batch 376 batch loss 6.62468338 epoch total loss 6.6097846\n",
      "Trained batch 377 batch loss 6.49152708 epoch total loss 6.60947084\n",
      "Trained batch 378 batch loss 6.85971737 epoch total loss 6.61013269\n",
      "Trained batch 379 batch loss 6.66871309 epoch total loss 6.61028719\n",
      "Trained batch 380 batch loss 6.63824272 epoch total loss 6.61036062\n",
      "Trained batch 381 batch loss 6.66261625 epoch total loss 6.61049747\n",
      "Trained batch 382 batch loss 6.61582184 epoch total loss 6.6105113\n",
      "Trained batch 383 batch loss 6.31773663 epoch total loss 6.60974646\n",
      "Trained batch 384 batch loss 6.31286812 epoch total loss 6.60897398\n",
      "Trained batch 385 batch loss 6.41095543 epoch total loss 6.60845947\n",
      "Trained batch 386 batch loss 6.48771429 epoch total loss 6.60814667\n",
      "Trained batch 387 batch loss 6.69920349 epoch total loss 6.60838223\n",
      "Trained batch 388 batch loss 6.53240347 epoch total loss 6.60818624\n",
      "Trained batch 389 batch loss 6.44811964 epoch total loss 6.60777473\n",
      "Trained batch 390 batch loss 6.27173519 epoch total loss 6.60691309\n",
      "Trained batch 391 batch loss 6.50430822 epoch total loss 6.60665083\n",
      "Trained batch 392 batch loss 6.67608881 epoch total loss 6.60682774\n",
      "Trained batch 393 batch loss 6.72059965 epoch total loss 6.60711765\n",
      "Trained batch 394 batch loss 6.70925951 epoch total loss 6.60737658\n",
      "Trained batch 395 batch loss 6.24813223 epoch total loss 6.60646677\n",
      "Trained batch 396 batch loss 6.4301219 epoch total loss 6.60602188\n",
      "Trained batch 397 batch loss 6.48765516 epoch total loss 6.60572338\n",
      "Trained batch 398 batch loss 6.59551287 epoch total loss 6.60569763\n",
      "Trained batch 399 batch loss 6.67094851 epoch total loss 6.60586119\n",
      "Trained batch 400 batch loss 6.59889412 epoch total loss 6.60584354\n",
      "Trained batch 401 batch loss 6.68167448 epoch total loss 6.60603237\n",
      "Trained batch 402 batch loss 6.49582815 epoch total loss 6.60575867\n",
      "Trained batch 403 batch loss 6.29590607 epoch total loss 6.60498953\n",
      "Trained batch 404 batch loss 6.4379096 epoch total loss 6.60457611\n",
      "Trained batch 405 batch loss 6.52757454 epoch total loss 6.60438633\n",
      "Trained batch 406 batch loss 6.38619375 epoch total loss 6.60384893\n",
      "Trained batch 407 batch loss 7.0293169 epoch total loss 6.60489416\n",
      "Trained batch 408 batch loss 7.25355768 epoch total loss 6.60648441\n",
      "Trained batch 409 batch loss 7.33015585 epoch total loss 6.60825348\n",
      "Trained batch 410 batch loss 6.97234821 epoch total loss 6.60914135\n",
      "Trained batch 411 batch loss 6.80222464 epoch total loss 6.60961151\n",
      "Trained batch 412 batch loss 6.40861273 epoch total loss 6.60912371\n",
      "Trained batch 413 batch loss 6.36433 epoch total loss 6.608531\n",
      "Trained batch 414 batch loss 6.3391695 epoch total loss 6.60788\n",
      "Trained batch 415 batch loss 6.40835333 epoch total loss 6.60739946\n",
      "Trained batch 416 batch loss 6.59305525 epoch total loss 6.60736513\n",
      "Trained batch 417 batch loss 6.37219095 epoch total loss 6.60680056\n",
      "Trained batch 418 batch loss 6.37546778 epoch total loss 6.60624743\n",
      "Trained batch 419 batch loss 6.35057068 epoch total loss 6.60563707\n",
      "Trained batch 420 batch loss 6.45021105 epoch total loss 6.60526705\n",
      "Trained batch 421 batch loss 6.48955822 epoch total loss 6.60499191\n",
      "Trained batch 422 batch loss 6.68657398 epoch total loss 6.60518503\n",
      "Trained batch 423 batch loss 7.08640718 epoch total loss 6.60632277\n",
      "Trained batch 424 batch loss 6.38661194 epoch total loss 6.60580492\n",
      "Trained batch 425 batch loss 6.31277275 epoch total loss 6.60511541\n",
      "Trained batch 426 batch loss 6.52717638 epoch total loss 6.60493231\n",
      "Trained batch 427 batch loss 6.97513771 epoch total loss 6.6057992\n",
      "Trained batch 428 batch loss 6.74239206 epoch total loss 6.60611868\n",
      "Trained batch 429 batch loss 6.78386545 epoch total loss 6.60653305\n",
      "Trained batch 430 batch loss 6.25964069 epoch total loss 6.60572577\n",
      "Trained batch 431 batch loss 6.48912048 epoch total loss 6.60545492\n",
      "Trained batch 432 batch loss 6.55028677 epoch total loss 6.60532761\n",
      "Trained batch 433 batch loss 6.10658789 epoch total loss 6.60417604\n",
      "Trained batch 434 batch loss 5.66174412 epoch total loss 6.60200453\n",
      "Trained batch 435 batch loss 5.52466822 epoch total loss 6.59952784\n",
      "Trained batch 436 batch loss 5.26387501 epoch total loss 6.59646463\n",
      "Trained batch 437 batch loss 6.02995491 epoch total loss 6.59516859\n",
      "Trained batch 438 batch loss 6.82238626 epoch total loss 6.59568691\n",
      "Trained batch 439 batch loss 7.22929668 epoch total loss 6.5971303\n",
      "Trained batch 440 batch loss 6.80470896 epoch total loss 6.59760189\n",
      "Trained batch 441 batch loss 6.2671442 epoch total loss 6.5968523\n",
      "Trained batch 442 batch loss 6.46362209 epoch total loss 6.59655094\n",
      "Trained batch 443 batch loss 6.51251125 epoch total loss 6.59636116\n",
      "Trained batch 444 batch loss 6.80263186 epoch total loss 6.59682608\n",
      "Trained batch 445 batch loss 6.75637197 epoch total loss 6.59718466\n",
      "Trained batch 446 batch loss 6.6684742 epoch total loss 6.5973444\n",
      "Trained batch 447 batch loss 6.86411619 epoch total loss 6.59794092\n",
      "Trained batch 448 batch loss 6.63716173 epoch total loss 6.59802866\n",
      "Trained batch 449 batch loss 6.95991135 epoch total loss 6.59883451\n",
      "Trained batch 450 batch loss 6.817451 epoch total loss 6.59932041\n",
      "Trained batch 451 batch loss 6.29616642 epoch total loss 6.59864807\n",
      "Trained batch 452 batch loss 6.81945944 epoch total loss 6.59913683\n",
      "Trained batch 453 batch loss 6.72053576 epoch total loss 6.59940481\n",
      "Trained batch 454 batch loss 6.95885801 epoch total loss 6.60019588\n",
      "Trained batch 455 batch loss 5.5130291 epoch total loss 6.59780645\n",
      "Trained batch 456 batch loss 6.38443327 epoch total loss 6.59733868\n",
      "Trained batch 457 batch loss 6.48141146 epoch total loss 6.597085\n",
      "Trained batch 458 batch loss 6.77353621 epoch total loss 6.59747028\n",
      "Trained batch 459 batch loss 6.5534606 epoch total loss 6.59737444\n",
      "Trained batch 460 batch loss 6.89161968 epoch total loss 6.59801388\n",
      "Trained batch 461 batch loss 6.68507862 epoch total loss 6.59820271\n",
      "Trained batch 462 batch loss 6.80848217 epoch total loss 6.59865808\n",
      "Trained batch 463 batch loss 6.526618 epoch total loss 6.59850264\n",
      "Trained batch 464 batch loss 6.35046864 epoch total loss 6.5979681\n",
      "Trained batch 465 batch loss 6.44380236 epoch total loss 6.5976367\n",
      "Trained batch 466 batch loss 6.70914 epoch total loss 6.59787607\n",
      "Trained batch 467 batch loss 6.73109293 epoch total loss 6.5981617\n",
      "Trained batch 468 batch loss 6.86671114 epoch total loss 6.59873581\n",
      "Trained batch 469 batch loss 7.09649801 epoch total loss 6.59979677\n",
      "Trained batch 470 batch loss 7.17959833 epoch total loss 6.60103035\n",
      "Trained batch 471 batch loss 7.07291365 epoch total loss 6.60203266\n",
      "Trained batch 472 batch loss 7.14127254 epoch total loss 6.60317516\n",
      "Trained batch 473 batch loss 7.3690629 epoch total loss 6.6047945\n",
      "Trained batch 474 batch loss 7.11776304 epoch total loss 6.60587645\n",
      "Trained batch 475 batch loss 7.22816849 epoch total loss 6.60718679\n",
      "Trained batch 476 batch loss 6.6529212 epoch total loss 6.60728312\n",
      "Trained batch 477 batch loss 6.39466858 epoch total loss 6.60683727\n",
      "Trained batch 478 batch loss 6.55695152 epoch total loss 6.60673285\n",
      "Trained batch 479 batch loss 6.2008729 epoch total loss 6.60588551\n",
      "Trained batch 480 batch loss 6.84771347 epoch total loss 6.60638952\n",
      "Trained batch 481 batch loss 6.79020071 epoch total loss 6.60677147\n",
      "Trained batch 482 batch loss 6.75880384 epoch total loss 6.60708714\n",
      "Trained batch 483 batch loss 6.50260782 epoch total loss 6.60687113\n",
      "Trained batch 484 batch loss 6.51200676 epoch total loss 6.60667467\n",
      "Trained batch 485 batch loss 6.29961586 epoch total loss 6.60604143\n",
      "Trained batch 486 batch loss 6.1653409 epoch total loss 6.60513449\n",
      "Trained batch 487 batch loss 6.68848848 epoch total loss 6.60530567\n",
      "Trained batch 488 batch loss 6.55656385 epoch total loss 6.60520601\n",
      "Trained batch 489 batch loss 6.87474728 epoch total loss 6.60575724\n",
      "Trained batch 490 batch loss 6.75933123 epoch total loss 6.60607052\n",
      "Trained batch 491 batch loss 7.17875719 epoch total loss 6.60723686\n",
      "Trained batch 492 batch loss 6.38073 epoch total loss 6.60677624\n",
      "Trained batch 493 batch loss 6.57825756 epoch total loss 6.60671854\n",
      "Trained batch 494 batch loss 6.58578682 epoch total loss 6.6066761\n",
      "Trained batch 495 batch loss 6.34348965 epoch total loss 6.60614443\n",
      "Trained batch 496 batch loss 6.39237738 epoch total loss 6.60571337\n",
      "Trained batch 497 batch loss 6.47436237 epoch total loss 6.6054492\n",
      "Trained batch 498 batch loss 6.3552084 epoch total loss 6.60494661\n",
      "Trained batch 499 batch loss 6.69353867 epoch total loss 6.60512447\n",
      "Trained batch 500 batch loss 6.64655685 epoch total loss 6.60520697\n",
      "Trained batch 501 batch loss 6.71099138 epoch total loss 6.60541821\n",
      "Trained batch 502 batch loss 6.60193157 epoch total loss 6.60541153\n",
      "Trained batch 503 batch loss 6.64523363 epoch total loss 6.60549068\n",
      "Trained batch 504 batch loss 6.62787676 epoch total loss 6.60553503\n",
      "Trained batch 505 batch loss 6.82998228 epoch total loss 6.60598\n",
      "Trained batch 506 batch loss 6.97655296 epoch total loss 6.60671234\n",
      "Trained batch 507 batch loss 6.86276054 epoch total loss 6.60721731\n",
      "Trained batch 508 batch loss 6.83348846 epoch total loss 6.60766268\n",
      "Trained batch 509 batch loss 6.69701338 epoch total loss 6.60783815\n",
      "Trained batch 510 batch loss 6.77608871 epoch total loss 6.60816813\n",
      "Trained batch 511 batch loss 6.47229862 epoch total loss 6.60790253\n",
      "Trained batch 512 batch loss 6.9726963 epoch total loss 6.60861492\n",
      "Trained batch 513 batch loss 6.61637878 epoch total loss 6.60863\n",
      "Trained batch 514 batch loss 6.47649765 epoch total loss 6.60837317\n",
      "Trained batch 515 batch loss 7.00426388 epoch total loss 6.60914183\n",
      "Trained batch 516 batch loss 6.68363571 epoch total loss 6.60928583\n",
      "Trained batch 517 batch loss 6.65884066 epoch total loss 6.60938215\n",
      "Trained batch 518 batch loss 6.636096 epoch total loss 6.60943365\n",
      "Trained batch 519 batch loss 6.60912848 epoch total loss 6.6094327\n",
      "Trained batch 520 batch loss 7.16756201 epoch total loss 6.61050606\n",
      "Trained batch 521 batch loss 6.97195101 epoch total loss 6.6112\n",
      "Trained batch 522 batch loss 6.85490894 epoch total loss 6.61166668\n",
      "Trained batch 523 batch loss 6.51448488 epoch total loss 6.61148071\n",
      "Trained batch 524 batch loss 6.72361088 epoch total loss 6.61169481\n",
      "Trained batch 525 batch loss 6.71594095 epoch total loss 6.61189318\n",
      "Trained batch 526 batch loss 6.72650146 epoch total loss 6.61211109\n",
      "Trained batch 527 batch loss 6.58601 epoch total loss 6.6120615\n",
      "Trained batch 528 batch loss 6.557652 epoch total loss 6.6119585\n",
      "Trained batch 529 batch loss 6.28127337 epoch total loss 6.61133337\n",
      "Trained batch 530 batch loss 6.03753948 epoch total loss 6.61025047\n",
      "Trained batch 531 batch loss 5.84241247 epoch total loss 6.6088047\n",
      "Trained batch 532 batch loss 5.96742058 epoch total loss 6.60759926\n",
      "Trained batch 533 batch loss 5.74918318 epoch total loss 6.60598898\n",
      "Trained batch 534 batch loss 5.77790689 epoch total loss 6.6044383\n",
      "Trained batch 535 batch loss 5.53362179 epoch total loss 6.60243702\n",
      "Trained batch 536 batch loss 5.22297573 epoch total loss 6.59986305\n",
      "Trained batch 537 batch loss 6.08543253 epoch total loss 6.59890509\n",
      "Trained batch 538 batch loss 6.75143433 epoch total loss 6.5991888\n",
      "Trained batch 539 batch loss 6.67241573 epoch total loss 6.5993247\n",
      "Trained batch 540 batch loss 6.46158218 epoch total loss 6.5990696\n",
      "Trained batch 541 batch loss 6.79927826 epoch total loss 6.59943962\n",
      "Trained batch 542 batch loss 6.11661 epoch total loss 6.59854889\n",
      "Trained batch 543 batch loss 5.56586742 epoch total loss 6.59664726\n",
      "Trained batch 544 batch loss 5.90633106 epoch total loss 6.5953784\n",
      "Trained batch 545 batch loss 6.4461484 epoch total loss 6.59510422\n",
      "Trained batch 546 batch loss 7.04733181 epoch total loss 6.59593248\n",
      "Trained batch 547 batch loss 6.93121672 epoch total loss 6.59654522\n",
      "Trained batch 548 batch loss 6.77939367 epoch total loss 6.59687901\n",
      "Trained batch 549 batch loss 7.16340303 epoch total loss 6.5979104\n",
      "Trained batch 550 batch loss 7.00525904 epoch total loss 6.59865141\n",
      "Trained batch 551 batch loss 6.91419315 epoch total loss 6.59922457\n",
      "Trained batch 552 batch loss 7.12622261 epoch total loss 6.6001792\n",
      "Trained batch 553 batch loss 7.0195241 epoch total loss 6.60093737\n",
      "Trained batch 554 batch loss 6.90061855 epoch total loss 6.6014781\n",
      "Trained batch 555 batch loss 6.55013895 epoch total loss 6.60138559\n",
      "Trained batch 556 batch loss 6.49570656 epoch total loss 6.60119534\n",
      "Trained batch 557 batch loss 6.07033777 epoch total loss 6.60024214\n",
      "Trained batch 558 batch loss 6.12151814 epoch total loss 6.59938431\n",
      "Trained batch 559 batch loss 6.62935925 epoch total loss 6.59943819\n",
      "Trained batch 560 batch loss 6.84648657 epoch total loss 6.59987926\n",
      "Trained batch 561 batch loss 6.26782274 epoch total loss 6.59928751\n",
      "Trained batch 562 batch loss 6.78716803 epoch total loss 6.59962177\n",
      "Trained batch 563 batch loss 6.55307961 epoch total loss 6.5995388\n",
      "Trained batch 564 batch loss 6.43502855 epoch total loss 6.59924698\n",
      "Trained batch 565 batch loss 6.53497 epoch total loss 6.59913301\n",
      "Trained batch 566 batch loss 6.75591421 epoch total loss 6.59941\n",
      "Trained batch 567 batch loss 6.55665398 epoch total loss 6.59933472\n",
      "Trained batch 568 batch loss 6.63902187 epoch total loss 6.59940434\n",
      "Trained batch 569 batch loss 6.59833479 epoch total loss 6.59940243\n",
      "Trained batch 570 batch loss 6.58271599 epoch total loss 6.59937334\n",
      "Trained batch 571 batch loss 6.37640762 epoch total loss 6.59898281\n",
      "Trained batch 572 batch loss 5.92015696 epoch total loss 6.59779644\n",
      "Trained batch 573 batch loss 5.70903349 epoch total loss 6.59624529\n",
      "Trained batch 574 batch loss 4.90821934 epoch total loss 6.59330416\n",
      "Trained batch 575 batch loss 6.23288774 epoch total loss 6.59267759\n",
      "Trained batch 576 batch loss 6.78527832 epoch total loss 6.59301186\n",
      "Trained batch 577 batch loss 6.72241688 epoch total loss 6.59323597\n",
      "Trained batch 578 batch loss 6.40936613 epoch total loss 6.59291792\n",
      "Trained batch 579 batch loss 6.67696381 epoch total loss 6.59306335\n",
      "Trained batch 580 batch loss 6.58544874 epoch total loss 6.59305\n",
      "Trained batch 581 batch loss 6.19058657 epoch total loss 6.59235764\n",
      "Trained batch 582 batch loss 5.78920269 epoch total loss 6.59097767\n",
      "Trained batch 583 batch loss 6.65856838 epoch total loss 6.59109354\n",
      "Trained batch 584 batch loss 6.18676519 epoch total loss 6.59040117\n",
      "Trained batch 585 batch loss 6.19438314 epoch total loss 6.58972406\n",
      "Trained batch 586 batch loss 6.58448648 epoch total loss 6.589715\n",
      "Trained batch 587 batch loss 6.74213171 epoch total loss 6.58997488\n",
      "Trained batch 588 batch loss 6.48647738 epoch total loss 6.58979893\n",
      "Trained batch 589 batch loss 6.75744724 epoch total loss 6.5900836\n",
      "Trained batch 590 batch loss 6.54450655 epoch total loss 6.59000635\n",
      "Trained batch 591 batch loss 6.75839186 epoch total loss 6.59029102\n",
      "Trained batch 592 batch loss 6.56310797 epoch total loss 6.59024525\n",
      "Trained batch 593 batch loss 6.80742455 epoch total loss 6.59061098\n",
      "Trained batch 594 batch loss 6.73226357 epoch total loss 6.5908494\n",
      "Trained batch 595 batch loss 6.69931555 epoch total loss 6.59103155\n",
      "Trained batch 596 batch loss 6.91048908 epoch total loss 6.59156752\n",
      "Trained batch 597 batch loss 6.80911589 epoch total loss 6.59193182\n",
      "Trained batch 598 batch loss 7.2485404 epoch total loss 6.59303\n",
      "Trained batch 599 batch loss 6.98374462 epoch total loss 6.59368181\n",
      "Trained batch 600 batch loss 6.86115122 epoch total loss 6.59412766\n",
      "Trained batch 601 batch loss 7.06304932 epoch total loss 6.59490776\n",
      "Trained batch 602 batch loss 6.82676554 epoch total loss 6.59529257\n",
      "Trained batch 603 batch loss 6.80945349 epoch total loss 6.59564829\n",
      "Trained batch 604 batch loss 6.8097744 epoch total loss 6.59600258\n",
      "Trained batch 605 batch loss 6.94492388 epoch total loss 6.59657907\n",
      "Trained batch 606 batch loss 6.73564959 epoch total loss 6.59680843\n",
      "Trained batch 607 batch loss 6.50700855 epoch total loss 6.59666061\n",
      "Trained batch 608 batch loss 6.0815 epoch total loss 6.59581375\n",
      "Trained batch 609 batch loss 6.26092434 epoch total loss 6.59526396\n",
      "Trained batch 610 batch loss 6.47369385 epoch total loss 6.59506416\n",
      "Trained batch 611 batch loss 6.63995123 epoch total loss 6.5951376\n",
      "Trained batch 612 batch loss 6.67858887 epoch total loss 6.59527397\n",
      "Trained batch 613 batch loss 6.80171204 epoch total loss 6.59561062\n",
      "Trained batch 614 batch loss 6.1465435 epoch total loss 6.59487915\n",
      "Trained batch 615 batch loss 5.94301176 epoch total loss 6.59381962\n",
      "Trained batch 616 batch loss 6.45511198 epoch total loss 6.59359407\n",
      "Trained batch 617 batch loss 6.78476048 epoch total loss 6.59390402\n",
      "Trained batch 618 batch loss 6.71116924 epoch total loss 6.5940938\n",
      "Trained batch 619 batch loss 6.72151756 epoch total loss 6.59429932\n",
      "Trained batch 620 batch loss 6.2786684 epoch total loss 6.59379\n",
      "Trained batch 621 batch loss 6.44755745 epoch total loss 6.5935545\n",
      "Trained batch 622 batch loss 6.6668663 epoch total loss 6.59367228\n",
      "Trained batch 623 batch loss 6.96472216 epoch total loss 6.59426785\n",
      "Trained batch 624 batch loss 6.98797369 epoch total loss 6.5948987\n",
      "Trained batch 625 batch loss 6.66229296 epoch total loss 6.59500647\n",
      "Trained batch 626 batch loss 6.66023111 epoch total loss 6.59511042\n",
      "Trained batch 627 batch loss 6.47924471 epoch total loss 6.59492493\n",
      "Trained batch 628 batch loss 6.52409077 epoch total loss 6.59481192\n",
      "Trained batch 629 batch loss 6.59938335 epoch total loss 6.59481955\n",
      "Trained batch 630 batch loss 6.43778419 epoch total loss 6.59457064\n",
      "Trained batch 631 batch loss 6.4560442 epoch total loss 6.59435129\n",
      "Trained batch 632 batch loss 6.50875664 epoch total loss 6.59421587\n",
      "Trained batch 633 batch loss 6.73564482 epoch total loss 6.59443951\n",
      "Trained batch 634 batch loss 6.87125063 epoch total loss 6.59487581\n",
      "Trained batch 635 batch loss 6.79958391 epoch total loss 6.59519863\n",
      "Trained batch 636 batch loss 6.71455193 epoch total loss 6.59538603\n",
      "Trained batch 637 batch loss 6.64787579 epoch total loss 6.59546852\n",
      "Trained batch 638 batch loss 6.87382078 epoch total loss 6.5959053\n",
      "Trained batch 639 batch loss 6.44351053 epoch total loss 6.59566641\n",
      "Trained batch 640 batch loss 6.50180149 epoch total loss 6.59552\n",
      "Trained batch 641 batch loss 6.41491604 epoch total loss 6.59523869\n",
      "Trained batch 642 batch loss 6.15298033 epoch total loss 6.59454918\n",
      "Trained batch 643 batch loss 5.92979288 epoch total loss 6.5935154\n",
      "Trained batch 644 batch loss 6.52349186 epoch total loss 6.59340668\n",
      "Trained batch 645 batch loss 6.30015612 epoch total loss 6.59295225\n",
      "Trained batch 646 batch loss 6.92761326 epoch total loss 6.59347\n",
      "Trained batch 647 batch loss 6.3946557 epoch total loss 6.59316301\n",
      "Trained batch 648 batch loss 6.34559 epoch total loss 6.59278107\n",
      "Trained batch 649 batch loss 6.1404686 epoch total loss 6.59208441\n",
      "Trained batch 650 batch loss 6.5316062 epoch total loss 6.59199142\n",
      "Trained batch 651 batch loss 6.53783321 epoch total loss 6.59190798\n",
      "Trained batch 652 batch loss 6.6944375 epoch total loss 6.59206486\n",
      "Trained batch 653 batch loss 6.54639912 epoch total loss 6.59199524\n",
      "Trained batch 654 batch loss 6.44355249 epoch total loss 6.59176779\n",
      "Trained batch 655 batch loss 6.09362888 epoch total loss 6.59100723\n",
      "Trained batch 656 batch loss 5.84228563 epoch total loss 6.58986616\n",
      "Trained batch 657 batch loss 6.16989088 epoch total loss 6.58922672\n",
      "Trained batch 658 batch loss 6.59139824 epoch total loss 6.58923\n",
      "Trained batch 659 batch loss 6.99193811 epoch total loss 6.58984089\n",
      "Trained batch 660 batch loss 6.74601221 epoch total loss 6.5900774\n",
      "Trained batch 661 batch loss 7.03106785 epoch total loss 6.59074497\n",
      "Trained batch 662 batch loss 6.94021463 epoch total loss 6.59127331\n",
      "Trained batch 663 batch loss 7.11198092 epoch total loss 6.59205818\n",
      "Trained batch 664 batch loss 6.93792439 epoch total loss 6.59257936\n",
      "Trained batch 665 batch loss 6.78590059 epoch total loss 6.59287\n",
      "Trained batch 666 batch loss 6.56406212 epoch total loss 6.59282684\n",
      "Trained batch 667 batch loss 6.735394 epoch total loss 6.59304047\n",
      "Trained batch 668 batch loss 6.73497 epoch total loss 6.59325314\n",
      "Trained batch 669 batch loss 6.73958635 epoch total loss 6.593472\n",
      "Trained batch 670 batch loss 6.76516247 epoch total loss 6.59372807\n",
      "Trained batch 671 batch loss 6.60240555 epoch total loss 6.59374142\n",
      "Trained batch 672 batch loss 6.33672857 epoch total loss 6.59335899\n",
      "Trained batch 673 batch loss 6.27743959 epoch total loss 6.59288931\n",
      "Trained batch 674 batch loss 6.40094948 epoch total loss 6.59260464\n",
      "Trained batch 675 batch loss 6.85164976 epoch total loss 6.59298849\n",
      "Trained batch 676 batch loss 6.69161606 epoch total loss 6.59313393\n",
      "Trained batch 677 batch loss 6.99073553 epoch total loss 6.59372091\n",
      "Trained batch 678 batch loss 7.01799488 epoch total loss 6.594347\n",
      "Trained batch 679 batch loss 6.74171257 epoch total loss 6.59456396\n",
      "Trained batch 680 batch loss 6.61046124 epoch total loss 6.59458733\n",
      "Trained batch 681 batch loss 6.5297904 epoch total loss 6.59449196\n",
      "Trained batch 682 batch loss 6.45766068 epoch total loss 6.59429121\n",
      "Trained batch 683 batch loss 5.95229101 epoch total loss 6.59335089\n",
      "Trained batch 684 batch loss 6.37888336 epoch total loss 6.59303761\n",
      "Trained batch 685 batch loss 6.74644756 epoch total loss 6.59326172\n",
      "Trained batch 686 batch loss 6.6193223 epoch total loss 6.59329939\n",
      "Trained batch 687 batch loss 6.93994951 epoch total loss 6.59380388\n",
      "Trained batch 688 batch loss 6.63110304 epoch total loss 6.59385777\n",
      "Trained batch 689 batch loss 6.55364513 epoch total loss 6.59379959\n",
      "Trained batch 690 batch loss 6.5365057 epoch total loss 6.59371662\n",
      "Trained batch 691 batch loss 6.83036566 epoch total loss 6.59405947\n",
      "Trained batch 692 batch loss 6.66858721 epoch total loss 6.59416723\n",
      "Trained batch 693 batch loss 6.72825241 epoch total loss 6.59436035\n",
      "Trained batch 694 batch loss 6.41607046 epoch total loss 6.59410334\n",
      "Trained batch 695 batch loss 6.60896969 epoch total loss 6.59412432\n",
      "Trained batch 696 batch loss 6.50716066 epoch total loss 6.594\n",
      "Trained batch 697 batch loss 6.33461952 epoch total loss 6.59362745\n",
      "Trained batch 698 batch loss 6.65486813 epoch total loss 6.59371519\n",
      "Trained batch 699 batch loss 6.92056084 epoch total loss 6.59418249\n",
      "Trained batch 700 batch loss 6.45876 epoch total loss 6.59398937\n",
      "Trained batch 701 batch loss 6.60422087 epoch total loss 6.59400368\n",
      "Trained batch 702 batch loss 6.32387066 epoch total loss 6.59361839\n",
      "Trained batch 703 batch loss 6.26317024 epoch total loss 6.59314871\n",
      "Trained batch 704 batch loss 6.33965778 epoch total loss 6.5927887\n",
      "Trained batch 705 batch loss 6.6742177 epoch total loss 6.59290457\n",
      "Trained batch 706 batch loss 6.68656826 epoch total loss 6.59303713\n",
      "Trained batch 707 batch loss 6.9429307 epoch total loss 6.59353161\n",
      "Trained batch 708 batch loss 7.33539295 epoch total loss 6.5945797\n",
      "Trained batch 709 batch loss 7.0574646 epoch total loss 6.59523296\n",
      "Trained batch 710 batch loss 7.11292219 epoch total loss 6.59596157\n",
      "Trained batch 711 batch loss 6.78412151 epoch total loss 6.59622622\n",
      "Trained batch 712 batch loss 6.01200199 epoch total loss 6.59540606\n",
      "Trained batch 713 batch loss 5.46721411 epoch total loss 6.59382391\n",
      "Trained batch 714 batch loss 5.75364733 epoch total loss 6.59264708\n",
      "Trained batch 715 batch loss 5.67753029 epoch total loss 6.59136724\n",
      "Trained batch 716 batch loss 6.67903805 epoch total loss 6.59149\n",
      "Trained batch 717 batch loss 6.68895721 epoch total loss 6.59162617\n",
      "Trained batch 718 batch loss 7.04628372 epoch total loss 6.59225941\n",
      "Trained batch 719 batch loss 7.10995054 epoch total loss 6.59297943\n",
      "Trained batch 720 batch loss 6.91500092 epoch total loss 6.5934267\n",
      "Trained batch 721 batch loss 6.57226849 epoch total loss 6.59339714\n",
      "Trained batch 722 batch loss 6.46760368 epoch total loss 6.59322309\n",
      "Trained batch 723 batch loss 6.62414026 epoch total loss 6.59326553\n",
      "Trained batch 724 batch loss 6.62783957 epoch total loss 6.59331369\n",
      "Trained batch 725 batch loss 6.38052225 epoch total loss 6.59302\n",
      "Trained batch 726 batch loss 6.46381092 epoch total loss 6.5928421\n",
      "Trained batch 727 batch loss 6.37399054 epoch total loss 6.59254122\n",
      "Trained batch 728 batch loss 6.5303607 epoch total loss 6.59245539\n",
      "Trained batch 729 batch loss 6.38204861 epoch total loss 6.59216642\n",
      "Trained batch 730 batch loss 6.5562377 epoch total loss 6.59211731\n",
      "Trained batch 731 batch loss 6.30353642 epoch total loss 6.59172297\n",
      "Trained batch 732 batch loss 6.25727892 epoch total loss 6.59126568\n",
      "Trained batch 733 batch loss 6.48780107 epoch total loss 6.59112453\n",
      "Trained batch 734 batch loss 6.30348539 epoch total loss 6.59073305\n",
      "Trained batch 735 batch loss 6.23769522 epoch total loss 6.59025288\n",
      "Trained batch 736 batch loss 6.06406116 epoch total loss 6.5895381\n",
      "Trained batch 737 batch loss 6.07047176 epoch total loss 6.58883333\n",
      "Trained batch 738 batch loss 6.78278589 epoch total loss 6.58909607\n",
      "Trained batch 739 batch loss 6.31147861 epoch total loss 6.58872032\n",
      "Trained batch 740 batch loss 6.29272461 epoch total loss 6.58832073\n",
      "Trained batch 741 batch loss 6.44566 epoch total loss 6.58812857\n",
      "Trained batch 742 batch loss 6.19268322 epoch total loss 6.58759594\n",
      "Trained batch 743 batch loss 5.96107912 epoch total loss 6.58675241\n",
      "Trained batch 744 batch loss 6.4522295 epoch total loss 6.58657169\n",
      "Trained batch 745 batch loss 5.58387375 epoch total loss 6.58522558\n",
      "Trained batch 746 batch loss 5.66655636 epoch total loss 6.58399439\n",
      "Trained batch 747 batch loss 5.84902477 epoch total loss 6.58301\n",
      "Trained batch 748 batch loss 5.99769449 epoch total loss 6.58222771\n",
      "Trained batch 749 batch loss 5.90182829 epoch total loss 6.58131933\n",
      "Trained batch 750 batch loss 5.89591455 epoch total loss 6.58040571\n",
      "Trained batch 751 batch loss 6.40134954 epoch total loss 6.58016729\n",
      "Trained batch 752 batch loss 6.02871037 epoch total loss 6.57943392\n",
      "Trained batch 753 batch loss 6.33230209 epoch total loss 6.57910633\n",
      "Trained batch 754 batch loss 6.16006231 epoch total loss 6.57855034\n",
      "Trained batch 755 batch loss 6.21081686 epoch total loss 6.57806349\n",
      "Trained batch 756 batch loss 6.10454798 epoch total loss 6.57743692\n",
      "Trained batch 757 batch loss 6.83307648 epoch total loss 6.57777452\n",
      "Trained batch 758 batch loss 6.87137651 epoch total loss 6.57816219\n",
      "Trained batch 759 batch loss 6.98436546 epoch total loss 6.57869768\n",
      "Trained batch 760 batch loss 6.55409336 epoch total loss 6.57866526\n",
      "Trained batch 761 batch loss 6.57951689 epoch total loss 6.57866669\n",
      "Trained batch 762 batch loss 6.40599871 epoch total loss 6.57843971\n",
      "Trained batch 763 batch loss 6.20155907 epoch total loss 6.57794571\n",
      "Trained batch 764 batch loss 6.70977974 epoch total loss 6.5781188\n",
      "Trained batch 765 batch loss 6.9877553 epoch total loss 6.57865429\n",
      "Trained batch 766 batch loss 6.66090107 epoch total loss 6.57876158\n",
      "Trained batch 767 batch loss 7.09479 epoch total loss 6.57943439\n",
      "Trained batch 768 batch loss 7.65938616 epoch total loss 6.58084059\n",
      "Trained batch 769 batch loss 6.69758368 epoch total loss 6.5809927\n",
      "Trained batch 770 batch loss 6.70054674 epoch total loss 6.58114767\n",
      "Trained batch 771 batch loss 6.65603876 epoch total loss 6.58124542\n",
      "Trained batch 772 batch loss 6.59004593 epoch total loss 6.58125639\n",
      "Trained batch 773 batch loss 6.56071329 epoch total loss 6.58122969\n",
      "Trained batch 774 batch loss 6.60682583 epoch total loss 6.58126307\n",
      "Trained batch 775 batch loss 6.6477623 epoch total loss 6.5813489\n",
      "Trained batch 776 batch loss 7.08747721 epoch total loss 6.58200121\n",
      "Trained batch 777 batch loss 7.02074718 epoch total loss 6.58256531\n",
      "Trained batch 778 batch loss 6.23238468 epoch total loss 6.58211517\n",
      "Trained batch 779 batch loss 6.55473375 epoch total loss 6.58208036\n",
      "Trained batch 780 batch loss 6.37460566 epoch total loss 6.58181381\n",
      "Trained batch 781 batch loss 6.87306547 epoch total loss 6.5821867\n",
      "Trained batch 782 batch loss 6.69933081 epoch total loss 6.58233643\n",
      "Trained batch 783 batch loss 6.49862862 epoch total loss 6.58222961\n",
      "Trained batch 784 batch loss 6.2896142 epoch total loss 6.58185625\n",
      "Trained batch 785 batch loss 6.78766394 epoch total loss 6.58211851\n",
      "Trained batch 786 batch loss 6.2222271 epoch total loss 6.58166027\n",
      "Trained batch 787 batch loss 6.1391716 epoch total loss 6.58109808\n",
      "Trained batch 788 batch loss 6.36834431 epoch total loss 6.58082771\n",
      "Trained batch 789 batch loss 6.31782484 epoch total loss 6.5804944\n",
      "Trained batch 790 batch loss 6.58424807 epoch total loss 6.58049965\n",
      "Trained batch 791 batch loss 6.53351402 epoch total loss 6.58044052\n",
      "Trained batch 792 batch loss 6.88149405 epoch total loss 6.58082056\n",
      "Trained batch 793 batch loss 6.57406473 epoch total loss 6.58081198\n",
      "Trained batch 794 batch loss 6.64903 epoch total loss 6.58089781\n",
      "Trained batch 795 batch loss 6.83403063 epoch total loss 6.58121634\n",
      "Trained batch 796 batch loss 6.7175 epoch total loss 6.58138704\n",
      "Trained batch 797 batch loss 6.18731356 epoch total loss 6.58089304\n",
      "Trained batch 798 batch loss 6.25021505 epoch total loss 6.58047819\n",
      "Trained batch 799 batch loss 6.77817822 epoch total loss 6.58072567\n",
      "Trained batch 800 batch loss 6.71393299 epoch total loss 6.58089256\n",
      "Trained batch 801 batch loss 7.15544701 epoch total loss 6.58160925\n",
      "Trained batch 802 batch loss 6.83045101 epoch total loss 6.58191967\n",
      "Trained batch 803 batch loss 6.73874331 epoch total loss 6.58211517\n",
      "Trained batch 804 batch loss 6.8983 epoch total loss 6.58250856\n",
      "Trained batch 805 batch loss 6.96626711 epoch total loss 6.5829854\n",
      "Trained batch 806 batch loss 6.65381289 epoch total loss 6.58307314\n",
      "Trained batch 807 batch loss 6.95817232 epoch total loss 6.58353806\n",
      "Trained batch 808 batch loss 7.25307798 epoch total loss 6.58436632\n",
      "Trained batch 809 batch loss 6.383214 epoch total loss 6.58411789\n",
      "Trained batch 810 batch loss 6.38458729 epoch total loss 6.58387184\n",
      "Trained batch 811 batch loss 6.12277079 epoch total loss 6.58330297\n",
      "Trained batch 812 batch loss 5.78220272 epoch total loss 6.5823164\n",
      "Trained batch 813 batch loss 6.14931679 epoch total loss 6.58178377\n",
      "Trained batch 814 batch loss 6.29494333 epoch total loss 6.58143139\n",
      "Trained batch 815 batch loss 5.65084743 epoch total loss 6.58029\n",
      "Trained batch 816 batch loss 5.6517005 epoch total loss 6.57915163\n",
      "Trained batch 817 batch loss 5.40371132 epoch total loss 6.57771301\n",
      "Trained batch 818 batch loss 5.70128059 epoch total loss 6.57664156\n",
      "Trained batch 819 batch loss 6.10787964 epoch total loss 6.57606936\n",
      "Trained batch 820 batch loss 6.51960039 epoch total loss 6.576\n",
      "Trained batch 821 batch loss 6.62644672 epoch total loss 6.57606173\n",
      "Trained batch 822 batch loss 6.95538712 epoch total loss 6.5765233\n",
      "Trained batch 823 batch loss 7.0738492 epoch total loss 6.57712746\n",
      "Trained batch 824 batch loss 6.62951517 epoch total loss 6.57719088\n",
      "Trained batch 825 batch loss 6.66253 epoch total loss 6.57729483\n",
      "Trained batch 826 batch loss 6.48396683 epoch total loss 6.57718134\n",
      "Trained batch 827 batch loss 6.63631344 epoch total loss 6.57725286\n",
      "Trained batch 828 batch loss 7.04939795 epoch total loss 6.57782316\n",
      "Trained batch 829 batch loss 6.91897 epoch total loss 6.57823467\n",
      "Trained batch 830 batch loss 6.81229162 epoch total loss 6.57851696\n",
      "Trained batch 831 batch loss 6.61692333 epoch total loss 6.57856274\n",
      "Trained batch 832 batch loss 6.7172575 epoch total loss 6.57872963\n",
      "Trained batch 833 batch loss 6.59103107 epoch total loss 6.57874393\n",
      "Trained batch 834 batch loss 6.89541912 epoch total loss 6.57912397\n",
      "Trained batch 835 batch loss 6.76713228 epoch total loss 6.57934904\n",
      "Trained batch 836 batch loss 6.58575726 epoch total loss 6.57935667\n",
      "Trained batch 837 batch loss 6.69340897 epoch total loss 6.57949305\n",
      "Trained batch 838 batch loss 6.57535362 epoch total loss 6.5794878\n",
      "Trained batch 839 batch loss 6.22592258 epoch total loss 6.57906675\n",
      "Trained batch 840 batch loss 6.36399937 epoch total loss 6.57881\n",
      "Trained batch 841 batch loss 6.16726732 epoch total loss 6.57832146\n",
      "Trained batch 842 batch loss 6.55247498 epoch total loss 6.57829046\n",
      "Trained batch 843 batch loss 6.82057571 epoch total loss 6.578578\n",
      "Trained batch 844 batch loss 6.38289595 epoch total loss 6.57834578\n",
      "Trained batch 845 batch loss 6.30253315 epoch total loss 6.57801962\n",
      "Trained batch 846 batch loss 6.20951128 epoch total loss 6.57758427\n",
      "Trained batch 847 batch loss 5.87645245 epoch total loss 6.57675648\n",
      "Trained batch 848 batch loss 6.27734613 epoch total loss 6.57640314\n",
      "Trained batch 849 batch loss 6.72176886 epoch total loss 6.57657433\n",
      "Trained batch 850 batch loss 6.493927 epoch total loss 6.57647753\n",
      "Trained batch 851 batch loss 6.50625 epoch total loss 6.57639503\n",
      "Trained batch 852 batch loss 6.60910654 epoch total loss 6.57643318\n",
      "Trained batch 853 batch loss 6.56716633 epoch total loss 6.57642269\n",
      "Trained batch 854 batch loss 6.72832537 epoch total loss 6.57660055\n",
      "Trained batch 855 batch loss 6.86367321 epoch total loss 6.57693672\n",
      "Trained batch 856 batch loss 6.76058292 epoch total loss 6.5771513\n",
      "Trained batch 857 batch loss 6.71252871 epoch total loss 6.57730913\n",
      "Trained batch 858 batch loss 6.61371422 epoch total loss 6.57735157\n",
      "Trained batch 859 batch loss 6.80811882 epoch total loss 6.57762\n",
      "Trained batch 860 batch loss 6.82278872 epoch total loss 6.57790518\n",
      "Trained batch 861 batch loss 6.99868488 epoch total loss 6.57839394\n",
      "Trained batch 862 batch loss 6.58878422 epoch total loss 6.57840586\n",
      "Trained batch 863 batch loss 6.58830833 epoch total loss 6.5784173\n",
      "Trained batch 864 batch loss 6.67277908 epoch total loss 6.57852697\n",
      "Trained batch 865 batch loss 6.64441967 epoch total loss 6.57860327\n",
      "Trained batch 866 batch loss 6.84182835 epoch total loss 6.57890701\n",
      "Trained batch 867 batch loss 6.94270229 epoch total loss 6.57932663\n",
      "Trained batch 868 batch loss 7.3427124 epoch total loss 6.58020639\n",
      "Trained batch 869 batch loss 7.28331709 epoch total loss 6.58101559\n",
      "Trained batch 870 batch loss 6.95748091 epoch total loss 6.58144808\n",
      "Trained batch 871 batch loss 6.57168579 epoch total loss 6.58143711\n",
      "Trained batch 872 batch loss 6.61356783 epoch total loss 6.5814743\n",
      "Trained batch 873 batch loss 6.48827791 epoch total loss 6.58136749\n",
      "Trained batch 874 batch loss 6.51755 epoch total loss 6.58129454\n",
      "Trained batch 875 batch loss 6.44228363 epoch total loss 6.58113575\n",
      "Trained batch 876 batch loss 6.19281292 epoch total loss 6.58069229\n",
      "Trained batch 877 batch loss 6.57766438 epoch total loss 6.58068895\n",
      "Trained batch 878 batch loss 6.87290287 epoch total loss 6.58102179\n",
      "Trained batch 879 batch loss 6.86738396 epoch total loss 6.58134747\n",
      "Trained batch 880 batch loss 6.69389677 epoch total loss 6.58147526\n",
      "Trained batch 881 batch loss 6.59978485 epoch total loss 6.58149576\n",
      "Trained batch 882 batch loss 6.45392895 epoch total loss 6.58135128\n",
      "Trained batch 883 batch loss 6.66921806 epoch total loss 6.58145094\n",
      "Trained batch 884 batch loss 6.64277887 epoch total loss 6.58152056\n",
      "Trained batch 885 batch loss 6.63101101 epoch total loss 6.58157587\n",
      "Trained batch 886 batch loss 6.65553331 epoch total loss 6.58166\n",
      "Trained batch 887 batch loss 6.45003223 epoch total loss 6.5815115\n",
      "Trained batch 888 batch loss 6.22447729 epoch total loss 6.58110952\n",
      "Trained batch 889 batch loss 6.50214577 epoch total loss 6.58102083\n",
      "Trained batch 890 batch loss 6.45352077 epoch total loss 6.5808773\n",
      "Trained batch 891 batch loss 6.78387499 epoch total loss 6.58110523\n",
      "Trained batch 892 batch loss 6.89858532 epoch total loss 6.58146095\n",
      "Trained batch 893 batch loss 6.72090626 epoch total loss 6.58161688\n",
      "Trained batch 894 batch loss 6.60266781 epoch total loss 6.58164024\n",
      "Trained batch 895 batch loss 6.64207411 epoch total loss 6.58170795\n",
      "Trained batch 896 batch loss 6.73062038 epoch total loss 6.58187389\n",
      "Trained batch 897 batch loss 6.25777578 epoch total loss 6.58151245\n",
      "Trained batch 898 batch loss 6.43793154 epoch total loss 6.58135271\n",
      "Trained batch 899 batch loss 6.41225 epoch total loss 6.58116436\n",
      "Trained batch 900 batch loss 6.11645508 epoch total loss 6.58064795\n",
      "Trained batch 901 batch loss 6.56276131 epoch total loss 6.5806284\n",
      "Trained batch 902 batch loss 6.49655724 epoch total loss 6.58053493\n",
      "Trained batch 903 batch loss 6.35423326 epoch total loss 6.58028412\n",
      "Trained batch 904 batch loss 6.24249172 epoch total loss 6.57991076\n",
      "Trained batch 905 batch loss 6.32954741 epoch total loss 6.57963419\n",
      "Trained batch 906 batch loss 6.47869587 epoch total loss 6.57952261\n",
      "Trained batch 907 batch loss 6.2792325 epoch total loss 6.57919168\n",
      "Trained batch 908 batch loss 6.54923582 epoch total loss 6.57915878\n",
      "Trained batch 909 batch loss 6.2978034 epoch total loss 6.57884932\n",
      "Trained batch 910 batch loss 6.52134418 epoch total loss 6.5787859\n",
      "Trained batch 911 batch loss 6.35879612 epoch total loss 6.57854462\n",
      "Trained batch 912 batch loss 6.41601467 epoch total loss 6.57836628\n",
      "Trained batch 913 batch loss 6.68163204 epoch total loss 6.57848\n",
      "Trained batch 914 batch loss 6.3831439 epoch total loss 6.57826614\n",
      "Trained batch 915 batch loss 6.12013817 epoch total loss 6.57776546\n",
      "Trained batch 916 batch loss 6.50380945 epoch total loss 6.57768488\n",
      "Trained batch 917 batch loss 6.75666714 epoch total loss 6.57788\n",
      "Trained batch 918 batch loss 6.81562424 epoch total loss 6.57813883\n",
      "Trained batch 919 batch loss 6.40864086 epoch total loss 6.57795429\n",
      "Trained batch 920 batch loss 6.64427423 epoch total loss 6.57802629\n",
      "Trained batch 921 batch loss 6.60814953 epoch total loss 6.57805872\n",
      "Trained batch 922 batch loss 6.16524744 epoch total loss 6.57761097\n",
      "Trained batch 923 batch loss 6.49078894 epoch total loss 6.57751656\n",
      "Trained batch 924 batch loss 6.22871637 epoch total loss 6.5771389\n",
      "Trained batch 925 batch loss 6.34702921 epoch total loss 6.57689047\n",
      "Trained batch 926 batch loss 6.46064377 epoch total loss 6.57676458\n",
      "Trained batch 927 batch loss 6.5683651 epoch total loss 6.57675552\n",
      "Trained batch 928 batch loss 6.38690186 epoch total loss 6.57655048\n",
      "Trained batch 929 batch loss 6.41739845 epoch total loss 6.5763793\n",
      "Trained batch 930 batch loss 6.59088659 epoch total loss 6.57639503\n",
      "Trained batch 931 batch loss 6.72544432 epoch total loss 6.57655525\n",
      "Trained batch 932 batch loss 6.18757915 epoch total loss 6.57613802\n",
      "Trained batch 933 batch loss 6.08701706 epoch total loss 6.5756135\n",
      "Trained batch 934 batch loss 6.83054829 epoch total loss 6.57588625\n",
      "Trained batch 935 batch loss 6.54189253 epoch total loss 6.57585\n",
      "Trained batch 936 batch loss 7.3302474 epoch total loss 6.57665586\n",
      "Trained batch 937 batch loss 7.48351955 epoch total loss 6.57762384\n",
      "Trained batch 938 batch loss 7.46229935 epoch total loss 6.57856703\n",
      "Trained batch 939 batch loss 7.10608578 epoch total loss 6.57912874\n",
      "Trained batch 940 batch loss 7.07779217 epoch total loss 6.57965899\n",
      "Trained batch 941 batch loss 6.81083 epoch total loss 6.57990503\n",
      "Trained batch 942 batch loss 5.87773371 epoch total loss 6.57915974\n",
      "Trained batch 943 batch loss 6.47186756 epoch total loss 6.57904577\n",
      "Trained batch 944 batch loss 6.80442476 epoch total loss 6.57928419\n",
      "Trained batch 945 batch loss 7.1631279 epoch total loss 6.5799017\n",
      "Trained batch 946 batch loss 6.29422617 epoch total loss 6.57960033\n",
      "Trained batch 947 batch loss 6.58912945 epoch total loss 6.57961035\n",
      "Trained batch 948 batch loss 6.36830235 epoch total loss 6.57938719\n",
      "Trained batch 949 batch loss 6.56081963 epoch total loss 6.57936811\n",
      "Trained batch 950 batch loss 6.12766933 epoch total loss 6.57889223\n",
      "Trained batch 951 batch loss 6.51862288 epoch total loss 6.57882881\n",
      "Trained batch 952 batch loss 6.12741566 epoch total loss 6.57835484\n",
      "Trained batch 953 batch loss 6.62613583 epoch total loss 6.5784049\n",
      "Trained batch 954 batch loss 6.63647842 epoch total loss 6.57846594\n",
      "Trained batch 955 batch loss 6.38367939 epoch total loss 6.57826185\n",
      "Trained batch 956 batch loss 5.91819477 epoch total loss 6.57757139\n",
      "Trained batch 957 batch loss 6.83135796 epoch total loss 6.57783651\n",
      "Trained batch 958 batch loss 6.64300299 epoch total loss 6.5779047\n",
      "Trained batch 959 batch loss 6.68165302 epoch total loss 6.57801294\n",
      "Trained batch 960 batch loss 6.30815935 epoch total loss 6.57773161\n",
      "Trained batch 961 batch loss 6.34916782 epoch total loss 6.57749414\n",
      "Trained batch 962 batch loss 6.26010942 epoch total loss 6.57716417\n",
      "Trained batch 963 batch loss 6.08475 epoch total loss 6.576653\n",
      "Trained batch 964 batch loss 6.27189 epoch total loss 6.57633686\n",
      "Trained batch 965 batch loss 6.4228158 epoch total loss 6.57617807\n",
      "Trained batch 966 batch loss 5.99451113 epoch total loss 6.57557583\n",
      "Trained batch 967 batch loss 6.31015491 epoch total loss 6.57530117\n",
      "Trained batch 968 batch loss 6.11394548 epoch total loss 6.57482433\n",
      "Trained batch 969 batch loss 6.26448536 epoch total loss 6.57450438\n",
      "Trained batch 970 batch loss 6.52680779 epoch total loss 6.57445526\n",
      "Trained batch 971 batch loss 5.98024559 epoch total loss 6.57384348\n",
      "Trained batch 972 batch loss 6.66843271 epoch total loss 6.57394075\n",
      "Trained batch 973 batch loss 6.64849 epoch total loss 6.57401752\n",
      "Trained batch 974 batch loss 6.75424767 epoch total loss 6.57420254\n",
      "Trained batch 975 batch loss 6.56885099 epoch total loss 6.57419729\n",
      "Trained batch 976 batch loss 6.62650585 epoch total loss 6.5742507\n",
      "Trained batch 977 batch loss 6.64304352 epoch total loss 6.57432127\n",
      "Trained batch 978 batch loss 6.66097689 epoch total loss 6.57441\n",
      "Trained batch 979 batch loss 6.34434891 epoch total loss 6.57417488\n",
      "Trained batch 980 batch loss 5.86139536 epoch total loss 6.57344723\n",
      "Trained batch 981 batch loss 5.78337383 epoch total loss 6.57264185\n",
      "Trained batch 982 batch loss 6.37053871 epoch total loss 6.57243633\n",
      "Trained batch 983 batch loss 6.42470217 epoch total loss 6.57228613\n",
      "Trained batch 984 batch loss 6.68653393 epoch total loss 6.572402\n",
      "Trained batch 985 batch loss 6.7442832 epoch total loss 6.57257652\n",
      "Trained batch 986 batch loss 6.48144436 epoch total loss 6.57248402\n",
      "Trained batch 987 batch loss 6.70333576 epoch total loss 6.57261658\n",
      "Trained batch 988 batch loss 6.90898037 epoch total loss 6.57295704\n",
      "Trained batch 989 batch loss 6.75075197 epoch total loss 6.57313681\n",
      "Trained batch 990 batch loss 6.39214563 epoch total loss 6.57295418\n",
      "Trained batch 991 batch loss 6.48948431 epoch total loss 6.57287\n",
      "Trained batch 992 batch loss 6.36953402 epoch total loss 6.57266474\n",
      "Trained batch 993 batch loss 6.34888029 epoch total loss 6.57243967\n",
      "Trained batch 994 batch loss 6.17735529 epoch total loss 6.57204199\n",
      "Trained batch 995 batch loss 6.46096706 epoch total loss 6.57193041\n",
      "Trained batch 996 batch loss 6.1875782 epoch total loss 6.57154465\n",
      "Trained batch 997 batch loss 6.12946033 epoch total loss 6.57110119\n",
      "Trained batch 998 batch loss 6.52370739 epoch total loss 6.5710535\n",
      "Trained batch 999 batch loss 6.37499809 epoch total loss 6.57085752\n",
      "Trained batch 1000 batch loss 6.92051411 epoch total loss 6.57120705\n",
      "Trained batch 1001 batch loss 6.82495451 epoch total loss 6.57146072\n",
      "Trained batch 1002 batch loss 6.86848736 epoch total loss 6.57175732\n",
      "Trained batch 1003 batch loss 6.82107162 epoch total loss 6.57200623\n",
      "Trained batch 1004 batch loss 6.89443874 epoch total loss 6.57232761\n",
      "Trained batch 1005 batch loss 6.5007658 epoch total loss 6.57225657\n",
      "Trained batch 1006 batch loss 6.88536644 epoch total loss 6.57256746\n",
      "Trained batch 1007 batch loss 6.77900791 epoch total loss 6.5727725\n",
      "Trained batch 1008 batch loss 6.87200451 epoch total loss 6.5730691\n",
      "Trained batch 1009 batch loss 6.9351635 epoch total loss 6.57342815\n",
      "Trained batch 1010 batch loss 6.80330896 epoch total loss 6.57365561\n",
      "Trained batch 1011 batch loss 6.82522583 epoch total loss 6.57390451\n",
      "Trained batch 1012 batch loss 6.98736763 epoch total loss 6.57431269\n",
      "Trained batch 1013 batch loss 6.65247822 epoch total loss 6.57439\n",
      "Trained batch 1014 batch loss 6.62776279 epoch total loss 6.57444286\n",
      "Trained batch 1015 batch loss 6.81684208 epoch total loss 6.57468176\n",
      "Trained batch 1016 batch loss 6.77676487 epoch total loss 6.5748806\n",
      "Trained batch 1017 batch loss 6.55222273 epoch total loss 6.57485819\n",
      "Trained batch 1018 batch loss 5.99847603 epoch total loss 6.57429218\n",
      "Trained batch 1019 batch loss 6.05659533 epoch total loss 6.57378435\n",
      "Trained batch 1020 batch loss 6.11223173 epoch total loss 6.57333183\n",
      "Trained batch 1021 batch loss 6.80705309 epoch total loss 6.57356071\n",
      "Trained batch 1022 batch loss 7.34684515 epoch total loss 6.57431698\n",
      "Trained batch 1023 batch loss 7.24069595 epoch total loss 6.57496881\n",
      "Trained batch 1024 batch loss 7.10819864 epoch total loss 6.57548952\n",
      "Trained batch 1025 batch loss 6.94769716 epoch total loss 6.57585287\n",
      "Trained batch 1026 batch loss 6.48136377 epoch total loss 6.57576084\n",
      "Trained batch 1027 batch loss 6.39269972 epoch total loss 6.5755825\n",
      "Trained batch 1028 batch loss 6.74120474 epoch total loss 6.57574368\n",
      "Trained batch 1029 batch loss 7.26653957 epoch total loss 6.57641506\n",
      "Trained batch 1030 batch loss 7.19085121 epoch total loss 6.57701159\n",
      "Trained batch 1031 batch loss 6.50974751 epoch total loss 6.57694626\n",
      "Trained batch 1032 batch loss 6.92152262 epoch total loss 6.57728\n",
      "Trained batch 1033 batch loss 6.72565365 epoch total loss 6.57742357\n",
      "Trained batch 1034 batch loss 6.73651 epoch total loss 6.57757711\n",
      "Trained batch 1035 batch loss 7.31773376 epoch total loss 6.57829237\n",
      "Trained batch 1036 batch loss 7.13910246 epoch total loss 6.57883406\n",
      "Trained batch 1037 batch loss 7.02926731 epoch total loss 6.57926846\n",
      "Trained batch 1038 batch loss 6.96435213 epoch total loss 6.57963943\n",
      "Trained batch 1039 batch loss 6.8077178 epoch total loss 6.57985878\n",
      "Trained batch 1040 batch loss 6.85946703 epoch total loss 6.58012724\n",
      "Trained batch 1041 batch loss 6.84198 epoch total loss 6.58037901\n",
      "Trained batch 1042 batch loss 6.77496052 epoch total loss 6.58056545\n",
      "Trained batch 1043 batch loss 6.81221294 epoch total loss 6.58078718\n",
      "Trained batch 1044 batch loss 6.7820406 epoch total loss 6.5809803\n",
      "Trained batch 1045 batch loss 6.57788706 epoch total loss 6.58097744\n",
      "Trained batch 1046 batch loss 6.66917753 epoch total loss 6.58106184\n",
      "Trained batch 1047 batch loss 6.60912704 epoch total loss 6.58108807\n",
      "Trained batch 1048 batch loss 6.43279505 epoch total loss 6.58094645\n",
      "Trained batch 1049 batch loss 6.38573933 epoch total loss 6.58076048\n",
      "Trained batch 1050 batch loss 6.21824026 epoch total loss 6.58041525\n",
      "Trained batch 1051 batch loss 6.28671122 epoch total loss 6.58013582\n",
      "Trained batch 1052 batch loss 6.07734919 epoch total loss 6.57965755\n",
      "Trained batch 1053 batch loss 6.62286758 epoch total loss 6.57969904\n",
      "Trained batch 1054 batch loss 6.86155081 epoch total loss 6.57996607\n",
      "Trained batch 1055 batch loss 6.44712448 epoch total loss 6.57984\n",
      "Trained batch 1056 batch loss 6.75117397 epoch total loss 6.58000231\n",
      "Trained batch 1057 batch loss 6.45943117 epoch total loss 6.57988834\n",
      "Trained batch 1058 batch loss 6.60491753 epoch total loss 6.57991219\n",
      "Trained batch 1059 batch loss 6.44397688 epoch total loss 6.57978344\n",
      "Trained batch 1060 batch loss 6.43516874 epoch total loss 6.57964706\n",
      "Trained batch 1061 batch loss 6.5190711 epoch total loss 6.57959\n",
      "Trained batch 1062 batch loss 6.98206234 epoch total loss 6.57996893\n",
      "Trained batch 1063 batch loss 6.27760029 epoch total loss 6.57968426\n",
      "Trained batch 1064 batch loss 6.67340422 epoch total loss 6.57977247\n",
      "Trained batch 1065 batch loss 5.94129467 epoch total loss 6.57917309\n",
      "Trained batch 1066 batch loss 5.5348053 epoch total loss 6.57819319\n",
      "Trained batch 1067 batch loss 5.56836319 epoch total loss 6.57724667\n",
      "Trained batch 1068 batch loss 5.96621895 epoch total loss 6.57667494\n",
      "Trained batch 1069 batch loss 6.12659788 epoch total loss 6.57625341\n",
      "Trained batch 1070 batch loss 6.74608278 epoch total loss 6.5764122\n",
      "Trained batch 1071 batch loss 7.171525 epoch total loss 6.57696772\n",
      "Trained batch 1072 batch loss 7.07533693 epoch total loss 6.57743263\n",
      "Trained batch 1073 batch loss 7.14513588 epoch total loss 6.57796144\n",
      "Trained batch 1074 batch loss 7.0281291 epoch total loss 6.57838106\n",
      "Trained batch 1075 batch loss 7.05275822 epoch total loss 6.57882214\n",
      "Trained batch 1076 batch loss 6.72327232 epoch total loss 6.57895613\n",
      "Trained batch 1077 batch loss 6.81331587 epoch total loss 6.57917404\n",
      "Trained batch 1078 batch loss 6.80138826 epoch total loss 6.57938\n",
      "Trained batch 1079 batch loss 6.94174099 epoch total loss 6.57971621\n",
      "Trained batch 1080 batch loss 6.72463036 epoch total loss 6.57985\n",
      "Trained batch 1081 batch loss 6.73133 epoch total loss 6.57999039\n",
      "Trained batch 1082 batch loss 6.51902628 epoch total loss 6.57993412\n",
      "Trained batch 1083 batch loss 5.96049452 epoch total loss 6.57936239\n",
      "Trained batch 1084 batch loss 6.42983103 epoch total loss 6.57922411\n",
      "Trained batch 1085 batch loss 6.49819469 epoch total loss 6.57914925\n",
      "Trained batch 1086 batch loss 6.25586128 epoch total loss 6.5788517\n",
      "Trained batch 1087 batch loss 6.44122028 epoch total loss 6.57872534\n",
      "Trained batch 1088 batch loss 6.67932844 epoch total loss 6.57881737\n",
      "Trained batch 1089 batch loss 6.71513224 epoch total loss 6.57894278\n",
      "Trained batch 1090 batch loss 6.82946253 epoch total loss 6.57917261\n",
      "Trained batch 1091 batch loss 6.67239141 epoch total loss 6.57925844\n",
      "Trained batch 1092 batch loss 6.6407156 epoch total loss 6.57931423\n",
      "Trained batch 1093 batch loss 6.38526154 epoch total loss 6.57913685\n",
      "Trained batch 1094 batch loss 6.17230797 epoch total loss 6.57876492\n",
      "Trained batch 1095 batch loss 6.58441687 epoch total loss 6.57877\n",
      "Trained batch 1096 batch loss 6.43747139 epoch total loss 6.57864141\n",
      "Trained batch 1333 batch loss 6.05741167 epoch total loss 6.57742739\n",
      "Trained batch 1334 batch loss 6.22876453 epoch total loss 6.57716608\n",
      "Trained batch 1335 batch loss 6.91773224 epoch total loss 6.57742119\n",
      "Trained batch 1336 batch loss 6.58873463 epoch total loss 6.57743\n",
      "Trained batch 1337 batch loss 6.72010851 epoch total loss 6.57753611\n",
      "Trained batch 1338 batch loss 6.45092344 epoch total loss 6.57744169\n",
      "Trained batch 1339 batch loss 6.36194754 epoch total loss 6.577281\n",
      "Trained batch 1340 batch loss 6.39718771 epoch total loss 6.57714701\n",
      "Trained batch 1341 batch loss 6.40180159 epoch total loss 6.57701588\n",
      "Trained batch 1342 batch loss 6.56255054 epoch total loss 6.57700491\n",
      "Trained batch 1343 batch loss 6.6456933 epoch total loss 6.57705593\n",
      "Trained batch 1344 batch loss 5.86445427 epoch total loss 6.57652569\n",
      "Trained batch 1345 batch loss 6.29540491 epoch total loss 6.57631636\n",
      "Trained batch 1346 batch loss 6.68822384 epoch total loss 6.5764\n",
      "Trained batch 1347 batch loss 6.96533442 epoch total loss 6.57668877\n",
      "Trained batch 1348 batch loss 6.96026134 epoch total loss 6.57697296\n",
      "Trained batch 1349 batch loss 6.86583853 epoch total loss 6.57718754\n",
      "Trained batch 1350 batch loss 6.76909208 epoch total loss 6.57733\n",
      "Trained batch 1351 batch loss 6.40911913 epoch total loss 6.57720566\n",
      "Trained batch 1352 batch loss 6.64418364 epoch total loss 6.57725525\n",
      "Trained batch 1353 batch loss 6.67626619 epoch total loss 6.57732821\n",
      "Trained batch 1354 batch loss 7.04727221 epoch total loss 6.57767487\n",
      "Trained batch 1355 batch loss 6.5381093 epoch total loss 6.57764578\n",
      "Trained batch 1356 batch loss 6.44138336 epoch total loss 6.57754517\n",
      "Trained batch 1357 batch loss 6.17699909 epoch total loss 6.57725\n",
      "Trained batch 1358 batch loss 6.76947594 epoch total loss 6.57739162\n",
      "Trained batch 1359 batch loss 6.78116322 epoch total loss 6.57754135\n",
      "Trained batch 1360 batch loss 6.71572 epoch total loss 6.57764339\n",
      "Trained batch 1361 batch loss 6.19558 epoch total loss 6.57736206\n",
      "Trained batch 1362 batch loss 6.74299526 epoch total loss 6.57748413\n",
      "Trained batch 1363 batch loss 6.94762945 epoch total loss 6.57775545\n",
      "Trained batch 1364 batch loss 6.878438 epoch total loss 6.57797623\n",
      "Trained batch 1365 batch loss 6.72997952 epoch total loss 6.57808685\n",
      "Trained batch 1366 batch loss 6.68656349 epoch total loss 6.57816648\n",
      "Trained batch 1367 batch loss 6.84119177 epoch total loss 6.57835865\n",
      "Trained batch 1368 batch loss 6.33862 epoch total loss 6.57818365\n",
      "Trained batch 1369 batch loss 6.17504883 epoch total loss 6.57788897\n",
      "Trained batch 1370 batch loss 5.71036291 epoch total loss 6.57725525\n",
      "Trained batch 1371 batch loss 6.00837135 epoch total loss 6.57684088\n",
      "Trained batch 1372 batch loss 6.66466188 epoch total loss 6.57690477\n",
      "Trained batch 1373 batch loss 6.45995712 epoch total loss 6.57682\n",
      "Trained batch 1374 batch loss 6.63746548 epoch total loss 6.57686424\n",
      "Trained batch 1375 batch loss 6.68741083 epoch total loss 6.57694483\n",
      "Trained batch 1376 batch loss 6.43587828 epoch total loss 6.57684183\n",
      "Trained batch 1377 batch loss 6.48136 epoch total loss 6.57677269\n",
      "Trained batch 1378 batch loss 6.55506945 epoch total loss 6.57675648\n",
      "Trained batch 1379 batch loss 6.50771856 epoch total loss 6.57670641\n",
      "Trained batch 1380 batch loss 6.22075224 epoch total loss 6.57644844\n",
      "Trained batch 1381 batch loss 6.36834097 epoch total loss 6.57629776\n",
      "Trained batch 1382 batch loss 6.19568348 epoch total loss 6.57602215\n",
      "Trained batch 1383 batch loss 6.53620625 epoch total loss 6.57599306\n",
      "Trained batch 1384 batch loss 6.56329441 epoch total loss 6.575984\n",
      "Trained batch 1385 batch loss 6.67859077 epoch total loss 6.57605839\n",
      "Trained batch 1386 batch loss 6.51807 epoch total loss 6.5760169\n",
      "Trained batch 1387 batch loss 6.67619896 epoch total loss 6.57608891\n",
      "Trained batch 1388 batch loss 6.75108957 epoch total loss 6.57621479\n",
      "Epoch 6 train loss 6.576214790344238\n",
      "Validated batch 1 batch loss 6.81694603\n",
      "Validated batch 2 batch loss 6.59354305\n",
      "Validated batch 3 batch loss 6.47531557\n",
      "Validated batch 4 batch loss 6.23073101\n",
      "Validated batch 5 batch loss 6.51143217\n",
      "Validated batch 6 batch loss 6.53118038\n",
      "Validated batch 7 batch loss 6.56538963\n",
      "Validated batch 8 batch loss 6.69645262\n",
      "Validated batch 9 batch loss 6.78874493\n",
      "Validated batch 10 batch loss 6.52326059\n",
      "Validated batch 11 batch loss 6.53874302\n",
      "Validated batch 12 batch loss 6.1033721\n",
      "Validated batch 13 batch loss 7.01774025\n",
      "Validated batch 14 batch loss 6.44545412\n",
      "Validated batch 15 batch loss 6.53520298\n",
      "Validated batch 16 batch loss 6.71553373\n",
      "Validated batch 17 batch loss 6.50425148\n",
      "Validated batch 18 batch loss 6.00664377\n",
      "Validated batch 19 batch loss 6.33080578\n",
      "Validated batch 20 batch loss 6.64528322\n",
      "Validated batch 21 batch loss 6.34722328\n",
      "Validated batch 22 batch loss 6.48061848\n",
      "Validated batch 23 batch loss 6.54388094\n",
      "Validated batch 24 batch loss 6.32071257\n",
      "Validated batch 25 batch loss 6.86603975\n",
      "Validated batch 26 batch loss 6.63296127\n",
      "Validated batch 27 batch loss 6.71620512\n",
      "Validated batch 28 batch loss 6.70203781\n",
      "Validated batch 29 batch loss 6.92719841\n",
      "Validated batch 30 batch loss 6.5774579\n",
      "Validated batch 31 batch loss 6.91079283\n",
      "Validated batch 32 batch loss 6.41215\n",
      "Validated batch 33 batch loss 6.70360374\n",
      "Validated batch 34 batch loss 6.60742521\n",
      "Validated batch 35 batch loss 6.12017393\n",
      "Validated batch 36 batch loss 6.09504318\n",
      "Validated batch 37 batch loss 6.81010246\n",
      "Validated batch 38 batch loss 6.72578764\n",
      "Validated batch 39 batch loss 6.32285643\n",
      "Validated batch 40 batch loss 6.75173521\n",
      "Validated batch 41 batch loss 6.80739927\n",
      "Validated batch 42 batch loss 6.77402687\n",
      "Validated batch 43 batch loss 6.86026335\n",
      "Validated batch 44 batch loss 6.81219816\n",
      "Validated batch 45 batch loss 6.48707914\n",
      "Validated batch 46 batch loss 6.30968857\n",
      "Validated batch 47 batch loss 6.29289389\n",
      "Validated batch 48 batch loss 6.26842642\n",
      "Validated batch 49 batch loss 6.56810427\n",
      "Validated batch 50 batch loss 6.51860237\n",
      "Validated batch 51 batch loss 6.47346926\n",
      "Validated batch 52 batch loss 6.80141783\n",
      "Validated batch 53 batch loss 6.72747469\n",
      "Validated batch 54 batch loss 6.73487425\n",
      "Validated batch 55 batch loss 6.64869118\n",
      "Validated batch 56 batch loss 6.7405076\n",
      "Validated batch 57 batch loss 6.38686228\n",
      "Validated batch 58 batch loss 6.57538795\n",
      "Validated batch 59 batch loss 6.47649765\n",
      "Validated batch 60 batch loss 6.87921333\n",
      "Validated batch 61 batch loss 6.73386049\n",
      "Validated batch 62 batch loss 6.46355104\n",
      "Validated batch 63 batch loss 6.76201534\n",
      "Validated batch 64 batch loss 6.10963058\n",
      "Validated batch 65 batch loss 6.6782794\n",
      "Validated batch 66 batch loss 6.73228264\n",
      "Validated batch 67 batch loss 6.59436226\n",
      "Validated batch 68 batch loss 6.74808836\n",
      "Validated batch 69 batch loss 6.4442153\n",
      "Validated batch 70 batch loss 6.51064587\n",
      "Validated batch 71 batch loss 6.67919445\n",
      "Validated batch 72 batch loss 6.32008028\n",
      "Validated batch 73 batch loss 5.9996767\n",
      "Validated batch 74 batch loss 6.39515972\n",
      "Validated batch 75 batch loss 6.87274265\n",
      "Validated batch 76 batch loss 6.37594271\n",
      "Validated batch 77 batch loss 6.61953163\n",
      "Validated batch 78 batch loss 6.71357536\n",
      "Validated batch 79 batch loss 6.82870817\n",
      "Validated batch 80 batch loss 6.72896147\n",
      "Validated batch 81 batch loss 6.53526735\n",
      "Validated batch 82 batch loss 6.66678953\n",
      "Validated batch 83 batch loss 6.54938841\n",
      "Validated batch 84 batch loss 6.81790733\n",
      "Validated batch 85 batch loss 6.91204596\n",
      "Validated batch 86 batch loss 6.47380686\n",
      "Validated batch 87 batch loss 6.90111876\n",
      "Validated batch 88 batch loss 6.74607277\n",
      "Validated batch 89 batch loss 6.35351276\n",
      "Validated batch 90 batch loss 6.56654787\n",
      "Validated batch 91 batch loss 6.72703743\n",
      "Validated batch 92 batch loss 6.89152956\n",
      "Validated batch 93 batch loss 6.71604729\n",
      "Validated batch 94 batch loss 6.51896334\n",
      "Validated batch 95 batch loss 6.38226175\n",
      "Validated batch 96 batch loss 6.2851367\n",
      "Validated batch 97 batch loss 6.70121574\n",
      "Validated batch 98 batch loss 6.58900881\n",
      "Validated batch 99 batch loss 6.60203218\n",
      "Validated batch 100 batch loss 6.62714958\n",
      "Validated batch 101 batch loss 6.37858152\n",
      "Validated batch 102 batch loss 6.79382\n",
      "Validated batch 103 batch loss 7.0516324\n",
      "Validated batch 104 batch loss 6.69144\n",
      "Validated batch 105 batch loss 6.65885496\n",
      "Validated batch 106 batch loss 6.5075469\n",
      "Validated batch 107 batch loss 6.7784\n",
      "Validated batch 108 batch loss 6.43809032\n",
      "Validated batch 109 batch loss 6.9023571\n",
      "Validated batch 110 batch loss 6.38589096\n",
      "Validated batch 111 batch loss 6.59299898\n",
      "Validated batch 112 batch loss 6.58246803\n",
      "Validated batch 113 batch loss 6.41976786\n",
      "Validated batch 114 batch loss 6.78185701\n",
      "Validated batch 115 batch loss 6.76904488\n",
      "Validated batch 116 batch loss 6.78313255\n",
      "Validated batch 117 batch loss 7.16529274\n",
      "Validated batch 118 batch loss 6.60583687\n",
      "Validated batch 119 batch loss 5.9874\n",
      "Validated batch 120 batch loss 6.59500456\n",
      "Validated batch 121 batch loss 6.42611885\n",
      "Validated batch 122 batch loss 6.24352646\n",
      "Validated batch 123 batch loss 6.31607723\n",
      "Validated batch 124 batch loss 6.14834881\n",
      "Validated batch 125 batch loss 6.85146523\n",
      "Validated batch 126 batch loss 6.30942106\n",
      "Validated batch 127 batch loss 6.39757109\n",
      "Validated batch 128 batch loss 6.15316582\n",
      "Validated batch 129 batch loss 6.81889772\n",
      "Validated batch 130 batch loss 6.30426598\n",
      "Validated batch 131 batch loss 6.54236221\n",
      "Validated batch 132 batch loss 6.49212027\n",
      "Validated batch 133 batch loss 6.75333834\n",
      "Validated batch 134 batch loss 6.44730568\n",
      "Validated batch 135 batch loss 6.67261076\n",
      "Validated batch 136 batch loss 6.78024673\n",
      "Validated batch 137 batch loss 5.83356619\n",
      "Validated batch 138 batch loss 6.69231844\n",
      "Validated batch 139 batch loss 6.77954435\n",
      "Validated batch 140 batch loss 6.7274847\n",
      "Validated batch 141 batch loss 6.64680958\n",
      "Validated batch 142 batch loss 6.70109892\n",
      "Validated batch 143 batch loss 7.0643177\n",
      "Validated batch 144 batch loss 6.78261137\n",
      "Validated batch 145 batch loss 6.81857729\n",
      "Validated batch 146 batch loss 6.65053892\n",
      "Validated batch 147 batch loss 6.81458855\n",
      "Validated batch 148 batch loss 6.70593929\n",
      "Validated batch 149 batch loss 7.06315041\n",
      "Validated batch 150 batch loss 7.08629274\n",
      "Validated batch 151 batch loss 6.26215029\n",
      "Validated batch 152 batch loss 6.82289457\n",
      "Validated batch 153 batch loss 6.59691525\n",
      "Validated batch 154 batch loss 6.62164497\n",
      "Validated batch 155 batch loss 6.74238396\n",
      "Validated batch 156 batch loss 6.24730396\n",
      "Validated batch 157 batch loss 6.29023314\n",
      "Validated batch 158 batch loss 6.74772453\n",
      "Validated batch 159 batch loss 6.55369854\n",
      "Validated batch 160 batch loss 7.03558922\n",
      "Validated batch 161 batch loss 6.54968882\n",
      "Validated batch 162 batch loss 7.03875399\n",
      "Validated batch 163 batch loss 6.69357634\n",
      "Validated batch 164 batch loss 6.79217339\n",
      "Validated batch 165 batch loss 6.60935593\n",
      "Validated batch 166 batch loss 6.61893892\n",
      "Validated batch 167 batch loss 6.92258072\n",
      "Validated batch 168 batch loss 6.66903257\n",
      "Validated batch 169 batch loss 6.34424\n",
      "Validated batch 170 batch loss 6.30464792\n",
      "Validated batch 171 batch loss 6.60696745\n",
      "Validated batch 172 batch loss 6.65034151\n",
      "Validated batch 173 batch loss 6.57544374\n",
      "Validated batch 174 batch loss 6.41452456\n",
      "Validated batch 175 batch loss 6.33707142\n",
      "Validated batch 176 batch loss 6.51672792\n",
      "Validated batch 177 batch loss 6.49048471\n",
      "Validated batch 178 batch loss 6.80795717\n",
      "Validated batch 179 batch loss 6.70920134\n",
      "Validated batch 180 batch loss 7.01937\n",
      "Validated batch 181 batch loss 7.45615196\n",
      "Validated batch 182 batch loss 7.24926186\n",
      "Validated batch 183 batch loss 6.62483644\n",
      "Validated batch 184 batch loss 6.1563077\n",
      "Validated batch 185 batch loss 3.34197974\n",
      "Epoch 6 val loss 6.586379528045654\n",
      "Epoch 6 completed in 731.26 seconds\n",
      "Model ./model_simplebase-epoch-6-loss-6.5864.h5 saved.\n",
      "Start epoch 7 with learning rate 0.0007\n",
      "Trained batch 1 batch loss 6.72475 epoch total loss 6.72475\n",
      "Trained batch 2 batch loss 6.05318213 epoch total loss 6.38896608\n",
      "Trained batch 3 batch loss 5.99174404 epoch total loss 6.2565589\n",
      "Trained batch 4 batch loss 6.16623545 epoch total loss 6.23397779\n",
      "Trained batch 5 batch loss 6.01627159 epoch total loss 6.19043636\n",
      "Trained batch 6 batch loss 7.06526756 epoch total loss 6.33624125\n",
      "Trained batch 7 batch loss 6.86541414 epoch total loss 6.41183758\n",
      "Trained batch 8 batch loss 6.94883823 epoch total loss 6.47896242\n",
      "Trained batch 9 batch loss 6.99448633 epoch total loss 6.53624296\n",
      "Trained batch 10 batch loss 7.0335803 epoch total loss 6.5859766\n",
      "Trained batch 11 batch loss 7.07319784 epoch total loss 6.63026905\n",
      "Trained batch 12 batch loss 6.69087219 epoch total loss 6.63531923\n",
      "Trained batch 13 batch loss 6.44414234 epoch total loss 6.62061357\n",
      "Trained batch 14 batch loss 6.85842943 epoch total loss 6.63760042\n",
      "Trained batch 15 batch loss 6.73342323 epoch total loss 6.64398861\n",
      "Trained batch 16 batch loss 6.52920961 epoch total loss 6.63681507\n",
      "Trained batch 17 batch loss 6.78619099 epoch total loss 6.64560223\n",
      "Trained batch 18 batch loss 6.77320957 epoch total loss 6.65269136\n",
      "Trained batch 19 batch loss 6.60125828 epoch total loss 6.64998436\n",
      "Trained batch 20 batch loss 6.82630539 epoch total loss 6.6588006\n",
      "Trained batch 21 batch loss 6.62642765 epoch total loss 6.65725946\n",
      "Trained batch 22 batch loss 6.63474512 epoch total loss 6.65623617\n",
      "Trained batch 23 batch loss 6.60345697 epoch total loss 6.65394115\n",
      "Trained batch 24 batch loss 6.6748147 epoch total loss 6.65481138\n",
      "Trained batch 25 batch loss 6.09894705 epoch total loss 6.63257694\n",
      "Trained batch 26 batch loss 5.80894852 epoch total loss 6.60089874\n",
      "Trained batch 27 batch loss 6.19116 epoch total loss 6.5857234\n",
      "Trained batch 28 batch loss 6.36281395 epoch total loss 6.57776213\n",
      "Trained batch 29 batch loss 6.36220169 epoch total loss 6.57032871\n",
      "Trained batch 30 batch loss 6.71513748 epoch total loss 6.57515574\n",
      "Trained batch 31 batch loss 6.73559141 epoch total loss 6.58033133\n",
      "Trained batch 32 batch loss 6.61033344 epoch total loss 6.58126879\n",
      "Trained batch 33 batch loss 6.64223719 epoch total loss 6.58311653\n",
      "Trained batch 34 batch loss 6.84446192 epoch total loss 6.59080315\n",
      "Trained batch 35 batch loss 6.69472456 epoch total loss 6.59377241\n",
      "Trained batch 36 batch loss 6.44340086 epoch total loss 6.58959532\n",
      "Trained batch 37 batch loss 6.39793301 epoch total loss 6.58441544\n",
      "Trained batch 38 batch loss 6.55073738 epoch total loss 6.583529\n",
      "Trained batch 39 batch loss 6.1727562 epoch total loss 6.57299662\n",
      "Trained batch 40 batch loss 6.00647688 epoch total loss 6.55883312\n",
      "Trained batch 41 batch loss 6.5456214 epoch total loss 6.55851126\n",
      "Trained batch 42 batch loss 5.92035961 epoch total loss 6.54331684\n",
      "Trained batch 43 batch loss 6.5861845 epoch total loss 6.54431343\n",
      "Trained batch 44 batch loss 6.38160324 epoch total loss 6.54061556\n",
      "Trained batch 45 batch loss 6.45314789 epoch total loss 6.53867197\n",
      "Trained batch 46 batch loss 6.44362354 epoch total loss 6.53660583\n",
      "Trained batch 47 batch loss 6.67226028 epoch total loss 6.53949213\n",
      "Trained batch 48 batch loss 6.83141899 epoch total loss 6.54557419\n",
      "Trained batch 49 batch loss 6.39002562 epoch total loss 6.54239941\n",
      "Trained batch 50 batch loss 6.4942317 epoch total loss 6.5414362\n",
      "Trained batch 51 batch loss 6.82852173 epoch total loss 6.54706526\n",
      "Trained batch 52 batch loss 6.6313858 epoch total loss 6.5486865\n",
      "Trained batch 53 batch loss 6.67340899 epoch total loss 6.5510397\n",
      "Trained batch 54 batch loss 6.69199848 epoch total loss 6.55365\n",
      "Trained batch 55 batch loss 6.29457378 epoch total loss 6.5489397\n",
      "Trained batch 56 batch loss 5.97761345 epoch total loss 6.5387373\n",
      "Trained batch 57 batch loss 5.58657646 epoch total loss 6.52203274\n",
      "Trained batch 58 batch loss 6.11810732 epoch total loss 6.51506853\n",
      "Trained batch 59 batch loss 6.45605755 epoch total loss 6.51406813\n",
      "Trained batch 60 batch loss 6.17729473 epoch total loss 6.50845528\n",
      "Trained batch 61 batch loss 5.9862895 epoch total loss 6.49989557\n",
      "Trained batch 62 batch loss 6.01966286 epoch total loss 6.49215\n",
      "Trained batch 63 batch loss 6.26822281 epoch total loss 6.48859501\n",
      "Trained batch 64 batch loss 5.92541 epoch total loss 6.47979546\n",
      "Trained batch 65 batch loss 5.91087675 epoch total loss 6.47104311\n",
      "Trained batch 66 batch loss 6.1512537 epoch total loss 6.46619749\n",
      "Trained batch 67 batch loss 6.45635509 epoch total loss 6.46605062\n",
      "Trained batch 68 batch loss 7.16365671 epoch total loss 6.47631\n",
      "Trained batch 69 batch loss 6.10572433 epoch total loss 6.47093868\n",
      "Trained batch 70 batch loss 6.693923 epoch total loss 6.47412395\n",
      "Trained batch 71 batch loss 7.05953693 epoch total loss 6.48236942\n",
      "Trained batch 72 batch loss 6.6152482 epoch total loss 6.48421478\n",
      "Trained batch 73 batch loss 6.58257103 epoch total loss 6.48556232\n",
      "Trained batch 74 batch loss 6.72445679 epoch total loss 6.48879051\n",
      "Trained batch 75 batch loss 6.57872438 epoch total loss 6.48998976\n",
      "Trained batch 76 batch loss 6.03035831 epoch total loss 6.48394203\n",
      "Trained batch 77 batch loss 6.84303904 epoch total loss 6.48860598\n",
      "Trained batch 78 batch loss 6.15680933 epoch total loss 6.48435211\n",
      "Trained batch 79 batch loss 6.59077358 epoch total loss 6.48569918\n",
      "Trained batch 80 batch loss 6.64655542 epoch total loss 6.48771\n",
      "Trained batch 81 batch loss 6.93047285 epoch total loss 6.49317598\n",
      "Trained batch 82 batch loss 6.58482122 epoch total loss 6.49429417\n",
      "Trained batch 83 batch loss 6.30246735 epoch total loss 6.49198294\n",
      "Trained batch 84 batch loss 6.71186113 epoch total loss 6.49460077\n",
      "Trained batch 85 batch loss 6.70753431 epoch total loss 6.4971056\n",
      "Trained batch 86 batch loss 6.57675552 epoch total loss 6.49803209\n",
      "Trained batch 87 batch loss 5.97402763 epoch total loss 6.49200869\n",
      "Trained batch 88 batch loss 6.32648802 epoch total loss 6.49012756\n",
      "Trained batch 89 batch loss 6.41600227 epoch total loss 6.48929501\n",
      "Trained batch 90 batch loss 6.67154074 epoch total loss 6.49132\n",
      "Trained batch 91 batch loss 6.62837553 epoch total loss 6.49282598\n",
      "Trained batch 92 batch loss 6.78106689 epoch total loss 6.49595928\n",
      "Trained batch 93 batch loss 6.59199572 epoch total loss 6.49699163\n",
      "Trained batch 94 batch loss 6.60871887 epoch total loss 6.49818\n",
      "Trained batch 95 batch loss 6.61406136 epoch total loss 6.4994\n",
      "Trained batch 96 batch loss 6.81616831 epoch total loss 6.50269938\n",
      "Trained batch 97 batch loss 6.46098089 epoch total loss 6.50226974\n",
      "Trained batch 98 batch loss 6.63231754 epoch total loss 6.50359678\n",
      "Trained batch 99 batch loss 6.67027473 epoch total loss 6.50528049\n",
      "Trained batch 100 batch loss 6.55758 epoch total loss 6.50580311\n",
      "Trained batch 101 batch loss 5.95934343 epoch total loss 6.50039291\n",
      "Trained batch 102 batch loss 5.96058607 epoch total loss 6.4951005\n",
      "Trained batch 103 batch loss 5.66464233 epoch total loss 6.48703814\n",
      "Trained batch 104 batch loss 6.16814327 epoch total loss 6.4839716\n",
      "Trained batch 105 batch loss 6.6399765 epoch total loss 6.48545742\n",
      "Trained batch 106 batch loss 6.5750289 epoch total loss 6.48630238\n",
      "Trained batch 107 batch loss 7.37736464 epoch total loss 6.49463\n",
      "Trained batch 108 batch loss 6.79648781 epoch total loss 6.49742508\n",
      "Trained batch 109 batch loss 6.9400506 epoch total loss 6.5014863\n",
      "Trained batch 110 batch loss 6.80001926 epoch total loss 6.50420046\n",
      "Trained batch 111 batch loss 6.28294 epoch total loss 6.50220728\n",
      "Trained batch 112 batch loss 6.39253902 epoch total loss 6.50122786\n",
      "Trained batch 113 batch loss 6.67665339 epoch total loss 6.50278\n",
      "Trained batch 114 batch loss 6.24423552 epoch total loss 6.5005126\n",
      "Trained batch 115 batch loss 6.15996 epoch total loss 6.49755096\n",
      "Trained batch 116 batch loss 6.59677029 epoch total loss 6.49840641\n",
      "Trained batch 117 batch loss 6.22821808 epoch total loss 6.49609709\n",
      "Trained batch 118 batch loss 6.60685492 epoch total loss 6.4970355\n",
      "Trained batch 119 batch loss 6.57855844 epoch total loss 6.49772072\n",
      "Trained batch 120 batch loss 6.59224176 epoch total loss 6.49850798\n",
      "Trained batch 121 batch loss 6.52611876 epoch total loss 6.49873638\n",
      "Trained batch 122 batch loss 6.94860077 epoch total loss 6.50242376\n",
      "Trained batch 123 batch loss 6.39447308 epoch total loss 6.50154638\n",
      "Trained batch 124 batch loss 6.67140198 epoch total loss 6.50291586\n",
      "Trained batch 125 batch loss 6.59658337 epoch total loss 6.50366497\n",
      "Trained batch 126 batch loss 6.4679327 epoch total loss 6.50338173\n",
      "Trained batch 127 batch loss 6.5481925 epoch total loss 6.50373459\n",
      "Trained batch 128 batch loss 6.55045366 epoch total loss 6.5041\n",
      "Trained batch 129 batch loss 7.0164156 epoch total loss 6.50807142\n",
      "Trained batch 130 batch loss 6.79436874 epoch total loss 6.51027346\n",
      "Trained batch 131 batch loss 6.43446302 epoch total loss 6.50969458\n",
      "Trained batch 132 batch loss 6.48153353 epoch total loss 6.50948143\n",
      "Trained batch 133 batch loss 6.40954876 epoch total loss 6.50873\n",
      "Trained batch 134 batch loss 6.38123274 epoch total loss 6.50777817\n",
      "Trained batch 135 batch loss 6.41916943 epoch total loss 6.50712204\n",
      "Trained batch 136 batch loss 6.11190319 epoch total loss 6.50421572\n",
      "Trained batch 137 batch loss 6.20593405 epoch total loss 6.50203848\n",
      "Trained batch 138 batch loss 6.5322504 epoch total loss 6.50225735\n",
      "Trained batch 139 batch loss 6.26018095 epoch total loss 6.50051594\n",
      "Trained batch 140 batch loss 6.71552134 epoch total loss 6.50205183\n",
      "Trained batch 141 batch loss 6.87591314 epoch total loss 6.50470304\n",
      "Trained batch 142 batch loss 6.35820484 epoch total loss 6.50367165\n",
      "Trained batch 143 batch loss 6.26464605 epoch total loss 6.502\n",
      "Trained batch 144 batch loss 6.69339561 epoch total loss 6.50332928\n",
      "Trained batch 145 batch loss 6.94807816 epoch total loss 6.50639629\n",
      "Trained batch 146 batch loss 6.79003429 epoch total loss 6.50833941\n",
      "Trained batch 147 batch loss 6.77149343 epoch total loss 6.51012945\n",
      "Trained batch 148 batch loss 6.44420338 epoch total loss 6.50968409\n",
      "Trained batch 149 batch loss 6.36526728 epoch total loss 6.50871515\n",
      "Trained batch 150 batch loss 6.33514357 epoch total loss 6.50755787\n",
      "Trained batch 151 batch loss 6.26452446 epoch total loss 6.50594807\n",
      "Trained batch 152 batch loss 6.79625559 epoch total loss 6.50785828\n",
      "Trained batch 153 batch loss 7.22510672 epoch total loss 6.51254606\n",
      "Trained batch 154 batch loss 7.18511438 epoch total loss 6.51691341\n",
      "Trained batch 155 batch loss 6.43399334 epoch total loss 6.51637888\n",
      "Trained batch 156 batch loss 6.55621481 epoch total loss 6.51663399\n",
      "Trained batch 157 batch loss 6.47360611 epoch total loss 6.51636028\n",
      "Trained batch 158 batch loss 6.33070898 epoch total loss 6.51518536\n",
      "Trained batch 159 batch loss 6.4988904 epoch total loss 6.51508284\n",
      "Trained batch 160 batch loss 6.38282347 epoch total loss 6.51425648\n",
      "Trained batch 161 batch loss 6.27284384 epoch total loss 6.51275682\n",
      "Trained batch 162 batch loss 6.84851313 epoch total loss 6.51482916\n",
      "Trained batch 163 batch loss 6.84116697 epoch total loss 6.5168314\n",
      "Trained batch 164 batch loss 6.77509403 epoch total loss 6.51840639\n",
      "Trained batch 165 batch loss 6.62061596 epoch total loss 6.5190258\n",
      "Trained batch 166 batch loss 6.57177162 epoch total loss 6.51934385\n",
      "Trained batch 167 batch loss 6.61545897 epoch total loss 6.5199194\n",
      "Trained batch 168 batch loss 6.57407475 epoch total loss 6.52024174\n",
      "Trained batch 169 batch loss 6.72603178 epoch total loss 6.52145958\n",
      "Trained batch 170 batch loss 6.30066586 epoch total loss 6.52016115\n",
      "Trained batch 171 batch loss 6.49121428 epoch total loss 6.51999187\n",
      "Trained batch 172 batch loss 6.34488392 epoch total loss 6.51897335\n",
      "Trained batch 173 batch loss 6.29942226 epoch total loss 6.51770449\n",
      "Trained batch 174 batch loss 6.58698559 epoch total loss 6.51810312\n",
      "Trained batch 175 batch loss 6.55086613 epoch total loss 6.51829052\n",
      "Trained batch 176 batch loss 6.53785753 epoch total loss 6.51840162\n",
      "Trained batch 177 batch loss 6.23524237 epoch total loss 6.51680136\n",
      "Trained batch 178 batch loss 6.44913626 epoch total loss 6.51642132\n",
      "Trained batch 179 batch loss 6.52879477 epoch total loss 6.51649046\n",
      "Trained batch 180 batch loss 6.5153904 epoch total loss 6.51648426\n",
      "Trained batch 181 batch loss 6.14765453 epoch total loss 6.51444674\n",
      "Trained batch 182 batch loss 6.77320147 epoch total loss 6.51586866\n",
      "Trained batch 183 batch loss 6.74606705 epoch total loss 6.51712656\n",
      "Trained batch 184 batch loss 6.18122721 epoch total loss 6.51530123\n",
      "Trained batch 185 batch loss 6.58246899 epoch total loss 6.51566458\n",
      "Trained batch 186 batch loss 6.7110467 epoch total loss 6.51671505\n",
      "Trained batch 187 batch loss 7.01774549 epoch total loss 6.5193944\n",
      "Trained batch 188 batch loss 7.0696106 epoch total loss 6.52232075\n",
      "Trained batch 189 batch loss 6.64907551 epoch total loss 6.52299118\n",
      "Trained batch 190 batch loss 6.29418898 epoch total loss 6.52178717\n",
      "Trained batch 191 batch loss 5.90613461 epoch total loss 6.51856375\n",
      "Trained batch 192 batch loss 6.31801653 epoch total loss 6.517519\n",
      "Trained batch 193 batch loss 6.43311834 epoch total loss 6.51708174\n",
      "Trained batch 194 batch loss 6.69499397 epoch total loss 6.51799822\n",
      "Trained batch 195 batch loss 6.75101566 epoch total loss 6.51919317\n",
      "Trained batch 196 batch loss 6.38826227 epoch total loss 6.5185256\n",
      "Trained batch 197 batch loss 6.84667253 epoch total loss 6.52019119\n",
      "Trained batch 198 batch loss 6.82918453 epoch total loss 6.52175188\n",
      "Trained batch 199 batch loss 6.92287731 epoch total loss 6.52376747\n",
      "Trained batch 200 batch loss 6.71487665 epoch total loss 6.52472305\n",
      "Trained batch 201 batch loss 6.81921673 epoch total loss 6.5261879\n",
      "Trained batch 202 batch loss 6.70552826 epoch total loss 6.52707624\n",
      "Trained batch 203 batch loss 6.83281851 epoch total loss 6.5285821\n",
      "Trained batch 204 batch loss 6.53014898 epoch total loss 6.52858973\n",
      "Trained batch 205 batch loss 6.8725853 epoch total loss 6.53026772\n",
      "Trained batch 206 batch loss 6.73677301 epoch total loss 6.53127\n",
      "Trained batch 207 batch loss 6.46158123 epoch total loss 6.53093338\n",
      "Trained batch 208 batch loss 6.25225735 epoch total loss 6.52959347\n",
      "Trained batch 209 batch loss 6.6320014 epoch total loss 6.53008318\n",
      "Trained batch 210 batch loss 6.69055843 epoch total loss 6.53084707\n",
      "Trained batch 211 batch loss 6.5436635 epoch total loss 6.53090811\n",
      "Trained batch 212 batch loss 6.60093117 epoch total loss 6.53123856\n",
      "Trained batch 213 batch loss 6.86091471 epoch total loss 6.53278637\n",
      "Trained batch 214 batch loss 6.76811743 epoch total loss 6.53388596\n",
      "Trained batch 215 batch loss 6.78894 epoch total loss 6.53507233\n",
      "Trained batch 216 batch loss 6.78968906 epoch total loss 6.53625107\n",
      "Trained batch 217 batch loss 6.37574911 epoch total loss 6.53551102\n",
      "Trained batch 218 batch loss 6.41177511 epoch total loss 6.53494358\n",
      "Trained batch 219 batch loss 6.60821295 epoch total loss 6.53527784\n",
      "Trained batch 220 batch loss 6.52439642 epoch total loss 6.53522825\n",
      "Trained batch 221 batch loss 6.58547783 epoch total loss 6.5354557\n",
      "Trained batch 222 batch loss 6.57867956 epoch total loss 6.53565073\n",
      "Trained batch 223 batch loss 6.70416355 epoch total loss 6.53640652\n",
      "Trained batch 224 batch loss 6.10775185 epoch total loss 6.53449297\n",
      "Trained batch 225 batch loss 5.72621346 epoch total loss 6.53090048\n",
      "Trained batch 226 batch loss 5.64988184 epoch total loss 6.52700233\n",
      "Trained batch 227 batch loss 5.5990715 epoch total loss 6.52291489\n",
      "Trained batch 228 batch loss 5.90668344 epoch total loss 6.52021217\n",
      "Trained batch 229 batch loss 6.88509655 epoch total loss 6.52180576\n",
      "Trained batch 230 batch loss 6.63252831 epoch total loss 6.52228737\n",
      "Trained batch 231 batch loss 6.22746706 epoch total loss 6.52101088\n",
      "Trained batch 232 batch loss 6.71751213 epoch total loss 6.52185774\n",
      "Trained batch 233 batch loss 6.48399973 epoch total loss 6.52169561\n",
      "Trained batch 234 batch loss 6.08157635 epoch total loss 6.51981449\n",
      "Trained batch 235 batch loss 5.94197798 epoch total loss 6.51735592\n",
      "Trained batch 236 batch loss 6.06317759 epoch total loss 6.5154314\n",
      "Trained batch 237 batch loss 6.09546 epoch total loss 6.51365948\n",
      "Trained batch 238 batch loss 6.28590059 epoch total loss 6.51270247\n",
      "Trained batch 239 batch loss 6.68685913 epoch total loss 6.51343107\n",
      "Trained batch 240 batch loss 6.55779457 epoch total loss 6.51361609\n",
      "Trained batch 241 batch loss 6.24826765 epoch total loss 6.51251507\n",
      "Trained batch 242 batch loss 6.54564142 epoch total loss 6.51265192\n",
      "Trained batch 243 batch loss 6.85590219 epoch total loss 6.51406479\n",
      "Trained batch 244 batch loss 6.43845654 epoch total loss 6.51375484\n",
      "Trained batch 245 batch loss 6.43319798 epoch total loss 6.5134263\n",
      "Trained batch 246 batch loss 6.52831078 epoch total loss 6.51348686\n",
      "Trained batch 247 batch loss 6.32014227 epoch total loss 6.51270437\n",
      "Trained batch 248 batch loss 6.27098703 epoch total loss 6.51172972\n",
      "Trained batch 249 batch loss 6.30492496 epoch total loss 6.51089907\n",
      "Trained batch 250 batch loss 6.79235744 epoch total loss 6.51202488\n",
      "Trained batch 251 batch loss 6.81405926 epoch total loss 6.51322842\n",
      "Trained batch 252 batch loss 7.13307095 epoch total loss 6.51568794\n",
      "Trained batch 253 batch loss 6.84824181 epoch total loss 6.51700258\n",
      "Trained batch 254 batch loss 7.09311247 epoch total loss 6.5192709\n",
      "Trained batch 255 batch loss 7.34348631 epoch total loss 6.5225029\n",
      "Trained batch 256 batch loss 6.27944469 epoch total loss 6.52155352\n",
      "Trained batch 257 batch loss 5.77521276 epoch total loss 6.51864958\n",
      "Trained batch 258 batch loss 5.71097898 epoch total loss 6.51551914\n",
      "Trained batch 259 batch loss 5.60425329 epoch total loss 6.51200056\n",
      "Trained batch 260 batch loss 5.5749855 epoch total loss 6.50839663\n",
      "Trained batch 261 batch loss 5.4101634 epoch total loss 6.50418854\n",
      "Trained batch 262 batch loss 5.19076395 epoch total loss 6.49917603\n",
      "Trained batch 263 batch loss 5.61486721 epoch total loss 6.49581337\n",
      "Trained batch 264 batch loss 6.43169451 epoch total loss 6.49557\n",
      "Trained batch 265 batch loss 6.98618412 epoch total loss 6.49742174\n",
      "Trained batch 266 batch loss 6.66380453 epoch total loss 6.49804735\n",
      "Trained batch 267 batch loss 7.14017105 epoch total loss 6.50045204\n",
      "Trained batch 268 batch loss 6.1236577 epoch total loss 6.49904633\n",
      "Trained batch 269 batch loss 6.38729477 epoch total loss 6.498631\n",
      "Trained batch 270 batch loss 6.73781633 epoch total loss 6.49951649\n",
      "Trained batch 271 batch loss 6.71364689 epoch total loss 6.50030661\n",
      "Trained batch 272 batch loss 6.80870342 epoch total loss 6.50144053\n",
      "Trained batch 273 batch loss 6.7571764 epoch total loss 6.50237751\n",
      "Trained batch 274 batch loss 6.83886194 epoch total loss 6.50360537\n",
      "Trained batch 275 batch loss 6.82615519 epoch total loss 6.50477839\n",
      "Trained batch 276 batch loss 6.82387352 epoch total loss 6.50593472\n",
      "Trained batch 277 batch loss 6.66144896 epoch total loss 6.50649595\n",
      "Trained batch 278 batch loss 6.53809595 epoch total loss 6.50661\n",
      "Trained batch 279 batch loss 6.84433174 epoch total loss 6.50782061\n",
      "Trained batch 280 batch loss 6.69645739 epoch total loss 6.5084939\n",
      "Trained batch 281 batch loss 6.26375914 epoch total loss 6.5076232\n",
      "Trained batch 282 batch loss 5.71143103 epoch total loss 6.5048\n",
      "Trained batch 283 batch loss 5.87924671 epoch total loss 6.50258923\n",
      "Trained batch 284 batch loss 6.16232491 epoch total loss 6.50139141\n",
      "Trained batch 285 batch loss 6.63172865 epoch total loss 6.5018487\n",
      "Trained batch 286 batch loss 6.64236879 epoch total loss 6.50234\n",
      "Trained batch 287 batch loss 6.5600338 epoch total loss 6.50254107\n",
      "Trained batch 288 batch loss 6.65950727 epoch total loss 6.50308609\n",
      "Trained batch 289 batch loss 6.01031637 epoch total loss 6.5013814\n",
      "Trained batch 290 batch loss 6.08558273 epoch total loss 6.49994755\n",
      "Trained batch 291 batch loss 6.59872627 epoch total loss 6.50028706\n",
      "Trained batch 292 batch loss 6.59276628 epoch total loss 6.50060368\n",
      "Trained batch 293 batch loss 6.71196795 epoch total loss 6.50132465\n",
      "Trained batch 294 batch loss 6.4375453 epoch total loss 6.50110769\n",
      "Trained batch 295 batch loss 6.39217758 epoch total loss 6.50073862\n",
      "Trained batch 296 batch loss 6.30212545 epoch total loss 6.50006771\n",
      "Trained batch 297 batch loss 7.00769901 epoch total loss 6.5017767\n",
      "Trained batch 298 batch loss 6.70259333 epoch total loss 6.50245094\n",
      "Trained batch 299 batch loss 6.9004097 epoch total loss 6.5037818\n",
      "Trained batch 300 batch loss 6.55507708 epoch total loss 6.5039525\n",
      "Trained batch 301 batch loss 6.600245 epoch total loss 6.50427246\n",
      "Trained batch 302 batch loss 6.57822895 epoch total loss 6.50451756\n",
      "Trained batch 303 batch loss 6.40602112 epoch total loss 6.50419235\n",
      "Trained batch 304 batch loss 6.81726122 epoch total loss 6.50522232\n",
      "Trained batch 305 batch loss 7.15172958 epoch total loss 6.50734186\n",
      "Trained batch 306 batch loss 6.97683811 epoch total loss 6.50887585\n",
      "Trained batch 307 batch loss 6.76664352 epoch total loss 6.50971556\n",
      "Trained batch 308 batch loss 6.30168104 epoch total loss 6.50904\n",
      "Trained batch 309 batch loss 6.31204319 epoch total loss 6.50840235\n",
      "Trained batch 310 batch loss 6.92861891 epoch total loss 6.50975752\n",
      "Trained batch 311 batch loss 6.65736151 epoch total loss 6.51023245\n",
      "Trained batch 312 batch loss 6.57677889 epoch total loss 6.51044559\n",
      "Trained batch 313 batch loss 6.87001038 epoch total loss 6.5115943\n",
      "Trained batch 314 batch loss 6.29992151 epoch total loss 6.51092\n",
      "Trained batch 315 batch loss 6.3355217 epoch total loss 6.5103631\n",
      "Trained batch 316 batch loss 6.78849459 epoch total loss 6.51124382\n",
      "Trained batch 317 batch loss 6.67257261 epoch total loss 6.51175261\n",
      "Trained batch 318 batch loss 6.93959 epoch total loss 6.51309824\n",
      "Trained batch 319 batch loss 6.71717834 epoch total loss 6.51373863\n",
      "Trained batch 320 batch loss 6.63412046 epoch total loss 6.51411438\n",
      "Trained batch 321 batch loss 6.65585661 epoch total loss 6.51455545\n",
      "Trained batch 322 batch loss 6.5117588 epoch total loss 6.51454687\n",
      "Trained batch 323 batch loss 6.68112469 epoch total loss 6.51506281\n",
      "Trained batch 324 batch loss 6.65167856 epoch total loss 6.51548386\n",
      "Trained batch 325 batch loss 6.82189941 epoch total loss 6.51642752\n",
      "Trained batch 326 batch loss 6.67678595 epoch total loss 6.51691914\n",
      "Trained batch 327 batch loss 6.42039585 epoch total loss 6.51662397\n",
      "Trained batch 328 batch loss 6.51476431 epoch total loss 6.51661777\n",
      "Trained batch 329 batch loss 6.47288227 epoch total loss 6.51648521\n",
      "Trained batch 330 batch loss 6.74343586 epoch total loss 6.51717281\n",
      "Trained batch 331 batch loss 6.85223579 epoch total loss 6.51818514\n",
      "Trained batch 332 batch loss 6.73055077 epoch total loss 6.51882458\n",
      "Trained batch 333 batch loss 6.52011347 epoch total loss 6.51882839\n",
      "Trained batch 334 batch loss 6.00520754 epoch total loss 6.51729\n",
      "Trained batch 335 batch loss 6.21447611 epoch total loss 6.51638603\n",
      "Trained batch 336 batch loss 6.59707642 epoch total loss 6.51662636\n",
      "Trained batch 337 batch loss 6.8087225 epoch total loss 6.51749325\n",
      "Trained batch 338 batch loss 6.19803143 epoch total loss 6.51654816\n",
      "Trained batch 339 batch loss 6.807 epoch total loss 6.51740456\n",
      "Trained batch 340 batch loss 6.6919589 epoch total loss 6.51791763\n",
      "Trained batch 341 batch loss 6.58115578 epoch total loss 6.51810312\n",
      "Trained batch 342 batch loss 6.49164915 epoch total loss 6.51802588\n",
      "Trained batch 343 batch loss 6.00756502 epoch total loss 6.51653767\n",
      "Trained batch 344 batch loss 6.43043613 epoch total loss 6.51628733\n",
      "Trained batch 345 batch loss 6.58660316 epoch total loss 6.51649094\n",
      "Trained batch 346 batch loss 6.63526058 epoch total loss 6.51683426\n",
      "Trained batch 347 batch loss 6.75125 epoch total loss 6.51751\n",
      "Trained batch 348 batch loss 6.56949043 epoch total loss 6.51765966\n",
      "Trained batch 349 batch loss 6.27601433 epoch total loss 6.5169673\n",
      "Trained batch 350 batch loss 6.37827396 epoch total loss 6.51657104\n",
      "Trained batch 351 batch loss 6.57194138 epoch total loss 6.51672888\n",
      "Trained batch 352 batch loss 6.43426514 epoch total loss 6.51649475\n",
      "Trained batch 353 batch loss 6.69268274 epoch total loss 6.51699352\n",
      "Trained batch 354 batch loss 6.24199343 epoch total loss 6.51621675\n",
      "Trained batch 355 batch loss 6.62946558 epoch total loss 6.51653576\n",
      "Trained batch 356 batch loss 6.77389908 epoch total loss 6.51725864\n",
      "Trained batch 357 batch loss 7.00695372 epoch total loss 6.51863\n",
      "Trained batch 358 batch loss 6.87266541 epoch total loss 6.51961851\n",
      "Trained batch 359 batch loss 6.28085518 epoch total loss 6.51895332\n",
      "Trained batch 360 batch loss 6.76826382 epoch total loss 6.51964569\n",
      "Trained batch 361 batch loss 6.60428238 epoch total loss 6.51988029\n",
      "Trained batch 362 batch loss 6.16104317 epoch total loss 6.51888943\n",
      "Trained batch 363 batch loss 6.02764463 epoch total loss 6.51753569\n",
      "Trained batch 364 batch loss 6.29895878 epoch total loss 6.51693535\n",
      "Trained batch 365 batch loss 6.23644161 epoch total loss 6.51616669\n",
      "Trained batch 366 batch loss 6.42561245 epoch total loss 6.51591921\n",
      "Trained batch 367 batch loss 6.52826881 epoch total loss 6.51595306\n",
      "Trained batch 368 batch loss 6.53720284 epoch total loss 6.51601028\n",
      "Trained batch 369 batch loss 6.58184481 epoch total loss 6.51618862\n",
      "Trained batch 370 batch loss 7.07670546 epoch total loss 6.51770353\n",
      "Trained batch 371 batch loss 6.54050159 epoch total loss 6.51776505\n",
      "Trained batch 372 batch loss 6.58174181 epoch total loss 6.51793718\n",
      "Trained batch 373 batch loss 6.51829672 epoch total loss 6.51793814\n",
      "Trained batch 374 batch loss 6.78424358 epoch total loss 6.51865\n",
      "Trained batch 375 batch loss 6.72302294 epoch total loss 6.51919508\n",
      "Trained batch 376 batch loss 6.58239746 epoch total loss 6.51936388\n",
      "Trained batch 377 batch loss 6.40736246 epoch total loss 6.51906681\n",
      "Trained batch 378 batch loss 5.99068 epoch total loss 6.5176692\n",
      "Trained batch 379 batch loss 5.83134508 epoch total loss 6.51585817\n",
      "Trained batch 380 batch loss 5.89811182 epoch total loss 6.51423264\n",
      "Trained batch 381 batch loss 5.9272995 epoch total loss 6.51269197\n",
      "Trained batch 382 batch loss 5.48548412 epoch total loss 6.51000357\n",
      "Trained batch 383 batch loss 5.55251884 epoch total loss 6.50750351\n",
      "Trained batch 384 batch loss 5.23359299 epoch total loss 6.50418615\n",
      "Trained batch 385 batch loss 5.71422338 epoch total loss 6.50213385\n",
      "Trained batch 386 batch loss 6.44416523 epoch total loss 6.50198364\n",
      "Trained batch 387 batch loss 6.74306917 epoch total loss 6.50260687\n",
      "Trained batch 388 batch loss 6.35888815 epoch total loss 6.50223637\n",
      "Trained batch 389 batch loss 6.40423298 epoch total loss 6.5019846\n",
      "Trained batch 390 batch loss 6.76731825 epoch total loss 6.50266504\n",
      "Trained batch 391 batch loss 6.97133064 epoch total loss 6.50386381\n",
      "Trained batch 392 batch loss 6.74478769 epoch total loss 6.50447845\n",
      "Trained batch 393 batch loss 6.46179914 epoch total loss 6.50437\n",
      "Trained batch 394 batch loss 6.80836153 epoch total loss 6.50514174\n",
      "Trained batch 395 batch loss 5.90465546 epoch total loss 6.5036211\n",
      "Trained batch 396 batch loss 6.39279652 epoch total loss 6.50334167\n",
      "Trained batch 397 batch loss 6.38343239 epoch total loss 6.50304\n",
      "Trained batch 398 batch loss 6.80419064 epoch total loss 6.50379658\n",
      "Trained batch 399 batch loss 6.78402042 epoch total loss 6.50449848\n",
      "Trained batch 400 batch loss 6.57043123 epoch total loss 6.50466299\n",
      "Trained batch 401 batch loss 6.51974726 epoch total loss 6.50470066\n",
      "Trained batch 402 batch loss 6.26935863 epoch total loss 6.5041151\n",
      "Trained batch 403 batch loss 6.28095341 epoch total loss 6.5035615\n",
      "Trained batch 404 batch loss 6.60523796 epoch total loss 6.50381327\n",
      "Trained batch 405 batch loss 6.19360971 epoch total loss 6.50304747\n",
      "Trained batch 406 batch loss 6.58190107 epoch total loss 6.50324106\n",
      "Trained batch 407 batch loss 6.74860477 epoch total loss 6.50384378\n",
      "Trained batch 408 batch loss 7.40658045 epoch total loss 6.50605631\n",
      "Trained batch 409 batch loss 7.12162209 epoch total loss 6.50756121\n",
      "Trained batch 410 batch loss 7.18593788 epoch total loss 6.50921583\n",
      "Trained batch 411 batch loss 6.70011473 epoch total loss 6.50968075\n",
      "Trained batch 412 batch loss 6.44308805 epoch total loss 6.5095191\n",
      "Trained batch 413 batch loss 6.2366848 epoch total loss 6.5088582\n",
      "Trained batch 414 batch loss 6.70991 epoch total loss 6.5093441\n",
      "Trained batch 415 batch loss 6.35165071 epoch total loss 6.50896358\n",
      "Trained batch 416 batch loss 6.36426878 epoch total loss 6.50861597\n",
      "Trained batch 417 batch loss 6.14215 epoch total loss 6.50773716\n",
      "Trained batch 418 batch loss 6.84614372 epoch total loss 6.50854683\n",
      "Trained batch 419 batch loss 6.53543758 epoch total loss 6.50861073\n",
      "Trained batch 420 batch loss 6.54244709 epoch total loss 6.50869131\n",
      "Trained batch 421 batch loss 6.70547676 epoch total loss 6.50915909\n",
      "Trained batch 422 batch loss 6.18281317 epoch total loss 6.50838566\n",
      "Trained batch 423 batch loss 6.63252687 epoch total loss 6.50867939\n",
      "Trained batch 424 batch loss 6.3793087 epoch total loss 6.50837469\n",
      "Trained batch 425 batch loss 6.17527866 epoch total loss 6.50759077\n",
      "Trained batch 426 batch loss 6.40964031 epoch total loss 6.50736094\n",
      "Trained batch 427 batch loss 6.44087076 epoch total loss 6.50720549\n",
      "Trained batch 428 batch loss 6.25137329 epoch total loss 6.50660753\n",
      "Trained batch 429 batch loss 6.63318968 epoch total loss 6.50690317\n",
      "Trained batch 430 batch loss 6.54793835 epoch total loss 6.50699854\n",
      "Trained batch 431 batch loss 6.4955678 epoch total loss 6.50697184\n",
      "Trained batch 432 batch loss 6.42842293 epoch total loss 6.50679\n",
      "Trained batch 433 batch loss 6.33259487 epoch total loss 6.50638771\n",
      "Trained batch 434 batch loss 6.3016696 epoch total loss 6.50591612\n",
      "Trained batch 435 batch loss 6.59468031 epoch total loss 6.50612\n",
      "Trained batch 436 batch loss 6.62243271 epoch total loss 6.50638676\n",
      "Trained batch 437 batch loss 6.95582485 epoch total loss 6.50741529\n",
      "Trained batch 438 batch loss 7.60142469 epoch total loss 6.50991297\n",
      "Trained batch 439 batch loss 7.22049236 epoch total loss 6.51153135\n",
      "Trained batch 440 batch loss 7.24964666 epoch total loss 6.51320934\n",
      "Trained batch 441 batch loss 6.72833681 epoch total loss 6.51369667\n",
      "Trained batch 442 batch loss 6.75817585 epoch total loss 6.51425\n",
      "Trained batch 443 batch loss 5.99908209 epoch total loss 6.5130868\n",
      "Trained batch 444 batch loss 6.49288845 epoch total loss 6.51304102\n",
      "Trained batch 445 batch loss 6.48828745 epoch total loss 6.51298571\n",
      "Trained batch 446 batch loss 7.0822587 epoch total loss 6.5142622\n",
      "Trained batch 447 batch loss 6.49805355 epoch total loss 6.51422548\n",
      "Trained batch 448 batch loss 6.18226671 epoch total loss 6.51348495\n",
      "Trained batch 449 batch loss 6.60760069 epoch total loss 6.51369476\n",
      "Trained batch 450 batch loss 6.56991816 epoch total loss 6.51381922\n",
      "Trained batch 451 batch loss 6.66170073 epoch total loss 6.51414728\n",
      "Trained batch 452 batch loss 6.05870914 epoch total loss 6.51313925\n",
      "Trained batch 453 batch loss 6.18812 epoch total loss 6.51242208\n",
      "Trained batch 454 batch loss 6.40153503 epoch total loss 6.51217794\n",
      "Trained batch 455 batch loss 6.74539 epoch total loss 6.51269054\n",
      "Trained batch 456 batch loss 5.3076 epoch total loss 6.51004791\n",
      "Trained batch 457 batch loss 5.98151684 epoch total loss 6.50889111\n",
      "Trained batch 458 batch loss 5.96208429 epoch total loss 6.50769758\n",
      "Trained batch 459 batch loss 6.26804066 epoch total loss 6.50717545\n",
      "Trained batch 460 batch loss 6.29647303 epoch total loss 6.50671721\n",
      "Trained batch 461 batch loss 6.7917738 epoch total loss 6.50733519\n",
      "Trained batch 462 batch loss 6.74626303 epoch total loss 6.50785255\n",
      "Trained batch 463 batch loss 6.49054337 epoch total loss 6.50781536\n",
      "Trained batch 464 batch loss 6.49343824 epoch total loss 6.50778389\n",
      "Trained batch 465 batch loss 6.49895334 epoch total loss 6.50776529\n",
      "Trained batch 466 batch loss 6.59694719 epoch total loss 6.5079565\n",
      "Trained batch 467 batch loss 6.31543064 epoch total loss 6.50754452\n",
      "Trained batch 468 batch loss 6.06418705 epoch total loss 6.50659704\n",
      "Trained batch 469 batch loss 6.38558912 epoch total loss 6.5063386\n",
      "Trained batch 470 batch loss 6.37770271 epoch total loss 6.50606489\n",
      "Trained batch 471 batch loss 6.50479221 epoch total loss 6.50606251\n",
      "Trained batch 472 batch loss 6.75102949 epoch total loss 6.50658131\n",
      "Trained batch 473 batch loss 6.9800148 epoch total loss 6.50758219\n",
      "Trained batch 474 batch loss 6.65381241 epoch total loss 6.5078907\n",
      "Trained batch 475 batch loss 6.27979898 epoch total loss 6.50741053\n",
      "Trained batch 476 batch loss 6.13712168 epoch total loss 6.5066328\n",
      "Trained batch 477 batch loss 6.35183525 epoch total loss 6.50630808\n",
      "Trained batch 478 batch loss 5.40574312 epoch total loss 6.50400591\n",
      "Trained batch 479 batch loss 6.07520962 epoch total loss 6.50311041\n",
      "Trained batch 480 batch loss 6.71497488 epoch total loss 6.50355244\n",
      "Trained batch 481 batch loss 6.47043228 epoch total loss 6.5034833\n",
      "Trained batch 482 batch loss 6.48543596 epoch total loss 6.50344563\n",
      "Trained batch 483 batch loss 6.64814091 epoch total loss 6.50374556\n",
      "Trained batch 484 batch loss 6.77895594 epoch total loss 6.50431442\n",
      "Trained batch 485 batch loss 6.71611404 epoch total loss 6.50475073\n",
      "Trained batch 486 batch loss 6.81836653 epoch total loss 6.50539637\n",
      "Trained batch 487 batch loss 6.87106562 epoch total loss 6.50614691\n",
      "Trained batch 488 batch loss 6.85159874 epoch total loss 6.50685501\n",
      "Trained batch 489 batch loss 6.70045137 epoch total loss 6.50725079\n",
      "Trained batch 490 batch loss 6.70464706 epoch total loss 6.50765371\n",
      "Trained batch 491 batch loss 6.18144131 epoch total loss 6.506989\n",
      "Trained batch 492 batch loss 6.19471645 epoch total loss 6.50635481\n",
      "Trained batch 493 batch loss 7.09519 epoch total loss 6.50754881\n",
      "Trained batch 494 batch loss 6.94228029 epoch total loss 6.50842905\n",
      "Trained batch 495 batch loss 6.39320183 epoch total loss 6.50819683\n",
      "Trained batch 496 batch loss 6.29393768 epoch total loss 6.50776482\n",
      "Trained batch 497 batch loss 6.294981 epoch total loss 6.50733662\n",
      "Trained batch 498 batch loss 6.63377905 epoch total loss 6.50759029\n",
      "Trained batch 499 batch loss 6.96230936 epoch total loss 6.50850201\n",
      "Trained batch 500 batch loss 6.22711658 epoch total loss 6.50793886\n",
      "Trained batch 501 batch loss 6.40416193 epoch total loss 6.50773144\n",
      "Trained batch 502 batch loss 6.71839952 epoch total loss 6.50815153\n",
      "Trained batch 503 batch loss 6.54248857 epoch total loss 6.50821972\n",
      "Trained batch 504 batch loss 6.85843515 epoch total loss 6.50891447\n",
      "Trained batch 505 batch loss 6.68140316 epoch total loss 6.50925589\n",
      "Trained batch 506 batch loss 6.56909466 epoch total loss 6.50937414\n",
      "Trained batch 507 batch loss 6.65961218 epoch total loss 6.50967073\n",
      "Trained batch 508 batch loss 6.56307364 epoch total loss 6.50977564\n",
      "Trained batch 509 batch loss 6.70633554 epoch total loss 6.51016188\n",
      "Trained batch 510 batch loss 6.64787102 epoch total loss 6.51043177\n",
      "Trained batch 511 batch loss 6.25688267 epoch total loss 6.50993586\n",
      "Trained batch 512 batch loss 6.53077 epoch total loss 6.50997639\n",
      "Trained batch 513 batch loss 6.87947 epoch total loss 6.51069641\n",
      "Trained batch 514 batch loss 7.02192926 epoch total loss 6.51169109\n",
      "Trained batch 515 batch loss 6.97985268 epoch total loss 6.5126\n",
      "Trained batch 516 batch loss 7.51735926 epoch total loss 6.51454735\n",
      "Trained batch 517 batch loss 7.43799305 epoch total loss 6.5163331\n",
      "Trained batch 518 batch loss 7.21822214 epoch total loss 6.51768827\n",
      "Trained batch 519 batch loss 7.30208683 epoch total loss 6.5192\n",
      "Trained batch 520 batch loss 6.65209627 epoch total loss 6.51945496\n",
      "Trained batch 521 batch loss 6.6938343 epoch total loss 6.5197897\n",
      "Trained batch 522 batch loss 6.71656847 epoch total loss 6.52016687\n",
      "Trained batch 523 batch loss 6.20251131 epoch total loss 6.51955938\n",
      "Trained batch 524 batch loss 5.53784657 epoch total loss 6.51768589\n",
      "Trained batch 525 batch loss 6.31924868 epoch total loss 6.51730776\n",
      "Trained batch 526 batch loss 6.19319868 epoch total loss 6.51669168\n",
      "Trained batch 527 batch loss 6.44928646 epoch total loss 6.51656342\n",
      "Trained batch 528 batch loss 6.05709934 epoch total loss 6.51569366\n",
      "Trained batch 529 batch loss 6.66963816 epoch total loss 6.51598454\n",
      "Trained batch 530 batch loss 6.23904324 epoch total loss 6.51546192\n",
      "Trained batch 531 batch loss 6.33711243 epoch total loss 6.51512623\n",
      "Trained batch 532 batch loss 6.52021646 epoch total loss 6.51513577\n",
      "Trained batch 533 batch loss 6.71542835 epoch total loss 6.51551151\n",
      "Trained batch 534 batch loss 6.76002312 epoch total loss 6.51596928\n",
      "Trained batch 535 batch loss 6.42048025 epoch total loss 6.51579046\n",
      "Trained batch 536 batch loss 5.56636953 epoch total loss 6.51401949\n",
      "Trained batch 537 batch loss 4.80918932 epoch total loss 6.51084471\n",
      "Trained batch 538 batch loss 5.90673494 epoch total loss 6.50972176\n",
      "Trained batch 539 batch loss 7.20413065 epoch total loss 6.51100969\n",
      "Trained batch 540 batch loss 7.26340103 epoch total loss 6.51240301\n",
      "Trained batch 541 batch loss 7.16646957 epoch total loss 6.51361227\n",
      "Trained batch 542 batch loss 6.61719036 epoch total loss 6.51380348\n",
      "Trained batch 543 batch loss 5.76560068 epoch total loss 6.51242542\n",
      "Trained batch 544 batch loss 6.01879025 epoch total loss 6.511518\n",
      "Trained batch 545 batch loss 6.41264105 epoch total loss 6.5113368\n",
      "Trained batch 546 batch loss 6.68046379 epoch total loss 6.51164627\n",
      "Trained batch 547 batch loss 6.88582182 epoch total loss 6.51233\n",
      "Trained batch 548 batch loss 6.64421606 epoch total loss 6.51257086\n",
      "Trained batch 549 batch loss 6.85630417 epoch total loss 6.51319695\n",
      "Trained batch 550 batch loss 7.17968893 epoch total loss 6.51440859\n",
      "Trained batch 551 batch loss 6.76350784 epoch total loss 6.51486063\n",
      "Trained batch 552 batch loss 7.02409315 epoch total loss 6.51578331\n",
      "Trained batch 553 batch loss 7.03325844 epoch total loss 6.51671886\n",
      "Trained batch 554 batch loss 6.82098341 epoch total loss 6.51726818\n",
      "Trained batch 555 batch loss 6.65320253 epoch total loss 6.51751328\n",
      "Trained batch 556 batch loss 6.3800745 epoch total loss 6.51726627\n",
      "Trained batch 557 batch loss 5.95391703 epoch total loss 6.5162549\n",
      "Trained batch 558 batch loss 6.02180719 epoch total loss 6.51536846\n",
      "Trained batch 559 batch loss 6.73096085 epoch total loss 6.51575422\n",
      "Trained batch 560 batch loss 6.64775276 epoch total loss 6.51599\n",
      "Trained batch 561 batch loss 6.23299313 epoch total loss 6.51548529\n",
      "Trained batch 562 batch loss 6.53801298 epoch total loss 6.51552534\n",
      "Trained batch 563 batch loss 6.61809349 epoch total loss 6.51570797\n",
      "Trained batch 564 batch loss 6.67137241 epoch total loss 6.51598406\n",
      "Trained batch 565 batch loss 6.68910503 epoch total loss 6.51629066\n",
      "Trained batch 566 batch loss 6.66406202 epoch total loss 6.51655149\n",
      "Trained batch 567 batch loss 6.3604269 epoch total loss 6.51627588\n",
      "Trained batch 568 batch loss 6.11991167 epoch total loss 6.51557827\n",
      "Trained batch 569 batch loss 6.69020128 epoch total loss 6.51588488\n",
      "Trained batch 570 batch loss 6.42652273 epoch total loss 6.515728\n",
      "Trained batch 571 batch loss 6.22326899 epoch total loss 6.51521635\n",
      "Trained batch 572 batch loss 6.35927153 epoch total loss 6.5149436\n",
      "Trained batch 573 batch loss 6.46583128 epoch total loss 6.51485825\n",
      "Trained batch 574 batch loss 6.39076853 epoch total loss 6.51464224\n",
      "Trained batch 575 batch loss 6.49059868 epoch total loss 6.5146\n",
      "Trained batch 576 batch loss 6.45649624 epoch total loss 6.51449919\n",
      "Trained batch 577 batch loss 6.94474602 epoch total loss 6.51524496\n",
      "Trained batch 578 batch loss 6.58342409 epoch total loss 6.51536322\n",
      "Trained batch 579 batch loss 6.68930626 epoch total loss 6.51566315\n",
      "Trained batch 580 batch loss 6.9040637 epoch total loss 6.5163331\n",
      "Trained batch 581 batch loss 6.82217836 epoch total loss 6.51685953\n",
      "Trained batch 582 batch loss 6.48732758 epoch total loss 6.51680899\n",
      "Trained batch 583 batch loss 6.47174549 epoch total loss 6.51673126\n",
      "Trained batch 584 batch loss 6.55277729 epoch total loss 6.51679325\n",
      "Trained batch 585 batch loss 6.17297554 epoch total loss 6.51620531\n",
      "Trained batch 586 batch loss 6.62400293 epoch total loss 6.51638937\n",
      "Trained batch 587 batch loss 6.76793861 epoch total loss 6.51681805\n",
      "Trained batch 588 batch loss 7.19532728 epoch total loss 6.51797152\n",
      "Trained batch 589 batch loss 6.79770136 epoch total loss 6.51844645\n",
      "Trained batch 590 batch loss 7.48347235 epoch total loss 6.520082\n",
      "Trained batch 591 batch loss 7.22811556 epoch total loss 6.52128\n",
      "Trained batch 592 batch loss 6.42363548 epoch total loss 6.52111483\n",
      "Trained batch 593 batch loss 6.29610538 epoch total loss 6.52073526\n",
      "Trained batch 594 batch loss 6.47506285 epoch total loss 6.52065849\n",
      "Trained batch 595 batch loss 6.96173239 epoch total loss 6.5214\n",
      "Trained batch 596 batch loss 7.21782637 epoch total loss 6.52256823\n",
      "Trained batch 597 batch loss 6.8416729 epoch total loss 6.52310276\n",
      "Trained batch 598 batch loss 6.69636 epoch total loss 6.5233922\n",
      "Trained batch 599 batch loss 6.81829548 epoch total loss 6.52388477\n",
      "Trained batch 600 batch loss 6.75640249 epoch total loss 6.52427197\n",
      "Trained batch 601 batch loss 6.87569904 epoch total loss 6.52485704\n",
      "Trained batch 602 batch loss 6.96170902 epoch total loss 6.52558231\n",
      "Trained batch 603 batch loss 7.27130699 epoch total loss 6.52681923\n",
      "Trained batch 604 batch loss 7.00177193 epoch total loss 6.52760506\n",
      "Trained batch 605 batch loss 6.54382563 epoch total loss 6.52763224\n",
      "Trained batch 606 batch loss 7.13635635 epoch total loss 6.52863693\n",
      "Trained batch 607 batch loss 6.76166964 epoch total loss 6.52902079\n",
      "Trained batch 608 batch loss 6.23807049 epoch total loss 6.52854252\n",
      "Trained batch 609 batch loss 6.16102505 epoch total loss 6.52793884\n",
      "Trained batch 610 batch loss 6.40377235 epoch total loss 6.52773571\n",
      "Trained batch 611 batch loss 6.69471645 epoch total loss 6.52800894\n",
      "Trained batch 612 batch loss 6.52882624 epoch total loss 6.52801037\n",
      "Trained batch 613 batch loss 6.64435673 epoch total loss 6.5282\n",
      "Trained batch 614 batch loss 6.74072218 epoch total loss 6.52854633\n",
      "Trained batch 615 batch loss 6.59583139 epoch total loss 6.52865553\n",
      "Trained batch 616 batch loss 6.72952843 epoch total loss 6.52898169\n",
      "Trained batch 617 batch loss 6.6289258 epoch total loss 6.52914381\n",
      "Trained batch 618 batch loss 6.7623353 epoch total loss 6.52952147\n",
      "Trained batch 619 batch loss 6.69249678 epoch total loss 6.5297842\n",
      "Trained batch 620 batch loss 6.62949467 epoch total loss 6.5299449\n",
      "Trained batch 621 batch loss 6.98704576 epoch total loss 6.53068113\n",
      "Trained batch 622 batch loss 6.64707851 epoch total loss 6.53086805\n",
      "Trained batch 623 batch loss 6.76944923 epoch total loss 6.53125095\n",
      "Trained batch 624 batch loss 6.62047577 epoch total loss 6.531394\n",
      "Trained batch 625 batch loss 6.65523 epoch total loss 6.53159237\n",
      "Trained batch 626 batch loss 6.61406708 epoch total loss 6.53172398\n",
      "Trained batch 627 batch loss 6.59541082 epoch total loss 6.53182554\n",
      "Trained batch 628 batch loss 6.72228527 epoch total loss 6.53212881\n",
      "Trained batch 629 batch loss 7.06986666 epoch total loss 6.5329833\n",
      "Trained batch 630 batch loss 7.10043097 epoch total loss 6.53388453\n",
      "Trained batch 631 batch loss 6.80930328 epoch total loss 6.53432035\n",
      "Trained batch 632 batch loss 6.97636795 epoch total loss 6.53502035\n",
      "Trained batch 633 batch loss 6.80528402 epoch total loss 6.53544712\n",
      "Trained batch 634 batch loss 6.81420565 epoch total loss 6.53588629\n",
      "Trained batch 635 batch loss 6.8971715 epoch total loss 6.53645515\n",
      "Trained batch 636 batch loss 6.44497061 epoch total loss 6.53631115\n",
      "Trained batch 637 batch loss 6.48524618 epoch total loss 6.53623104\n",
      "Trained batch 638 batch loss 6.39652491 epoch total loss 6.5360117\n",
      "Trained batch 639 batch loss 6.51421833 epoch total loss 6.53597784\n",
      "Trained batch 640 batch loss 6.45545626 epoch total loss 6.53585196\n",
      "Trained batch 641 batch loss 6.48207951 epoch total loss 6.53576803\n",
      "Trained batch 642 batch loss 6.32311916 epoch total loss 6.53543711\n",
      "Trained batch 643 batch loss 6.30481195 epoch total loss 6.53507805\n",
      "Trained batch 644 batch loss 6.41163111 epoch total loss 6.53488636\n",
      "Trained batch 645 batch loss 6.26444912 epoch total loss 6.53446722\n",
      "Trained batch 646 batch loss 6.51573849 epoch total loss 6.53443813\n",
      "Trained batch 647 batch loss 6.2616272 epoch total loss 6.53401661\n",
      "Trained batch 648 batch loss 6.26350927 epoch total loss 6.53359938\n",
      "Trained batch 649 batch loss 6.15935373 epoch total loss 6.5330224\n",
      "Trained batch 650 batch loss 6.18884611 epoch total loss 6.53249311\n",
      "Trained batch 651 batch loss 6.93106 epoch total loss 6.53310585\n",
      "Trained batch 652 batch loss 6.59161282 epoch total loss 6.5331955\n",
      "Trained batch 653 batch loss 6.76352739 epoch total loss 6.53354836\n",
      "Trained batch 654 batch loss 7.01938248 epoch total loss 6.53429174\n",
      "Trained batch 655 batch loss 6.55335236 epoch total loss 6.53432035\n",
      "Trained batch 656 batch loss 6.56979656 epoch total loss 6.53437471\n",
      "Trained batch 657 batch loss 6.36364174 epoch total loss 6.53411484\n",
      "Trained batch 658 batch loss 6.74001789 epoch total loss 6.53442812\n",
      "Trained batch 659 batch loss 6.56839 epoch total loss 6.53447962\n",
      "Trained batch 660 batch loss 6.69940138 epoch total loss 6.53472948\n",
      "Trained batch 661 batch loss 6.40351677 epoch total loss 6.53453064\n",
      "Trained batch 662 batch loss 6.52025938 epoch total loss 6.53450871\n",
      "Trained batch 663 batch loss 6.51765776 epoch total loss 6.53448296\n",
      "Trained batch 664 batch loss 6.71369934 epoch total loss 6.53475332\n",
      "Trained batch 665 batch loss 6.6874671 epoch total loss 6.53498316\n",
      "Trained batch 666 batch loss 6.66526508 epoch total loss 6.53517818\n",
      "Trained batch 667 batch loss 6.38603687 epoch total loss 6.53495502\n",
      "Trained batch 668 batch loss 6.56695843 epoch total loss 6.53500271\n",
      "Trained batch 669 batch loss 6.56248713 epoch total loss 6.53504372\n",
      "Trained batch 670 batch loss 6.3380661 epoch total loss 6.53474951\n",
      "Trained batch 671 batch loss 5.91895962 epoch total loss 6.53383207\n",
      "Trained batch 672 batch loss 5.77977514 epoch total loss 6.5327096\n",
      "Trained batch 673 batch loss 6.3514514 epoch total loss 6.53244066\n",
      "Trained batch 674 batch loss 5.90281439 epoch total loss 6.53150654\n",
      "Trained batch 675 batch loss 6.49811506 epoch total loss 6.53145695\n",
      "Trained batch 676 batch loss 6.42280531 epoch total loss 6.53129625\n",
      "Trained batch 677 batch loss 6.53118134 epoch total loss 6.53129625\n",
      "Trained batch 678 batch loss 5.64986134 epoch total loss 6.5299964\n",
      "Trained batch 679 batch loss 5.79836559 epoch total loss 6.52891874\n",
      "Trained batch 680 batch loss 6.03501511 epoch total loss 6.52819252\n",
      "Trained batch 681 batch loss 6.2395134 epoch total loss 6.52776909\n",
      "Trained batch 682 batch loss 6.61302376 epoch total loss 6.52789354\n",
      "Trained batch 683 batch loss 6.77029324 epoch total loss 6.52824879\n",
      "Trained batch 684 batch loss 6.47797918 epoch total loss 6.52817535\n",
      "Trained batch 685 batch loss 6.39628 epoch total loss 6.52798319\n",
      "Trained batch 686 batch loss 6.2233305 epoch total loss 6.52753878\n",
      "Trained batch 687 batch loss 6.53334856 epoch total loss 6.52754688\n",
      "Trained batch 688 batch loss 6.19441509 epoch total loss 6.52706289\n",
      "Trained batch 689 batch loss 6.69451427 epoch total loss 6.5273056\n",
      "Trained batch 690 batch loss 6.34456587 epoch total loss 6.52704096\n",
      "Trained batch 691 batch loss 6.34088802 epoch total loss 6.52677155\n",
      "Trained batch 692 batch loss 6.5041461 epoch total loss 6.52673817\n",
      "Trained batch 693 batch loss 6.53799963 epoch total loss 6.52675486\n",
      "Trained batch 694 batch loss 6.49719095 epoch total loss 6.52671194\n",
      "Trained batch 695 batch loss 6.59488487 epoch total loss 6.52680969\n",
      "Trained batch 696 batch loss 6.66906452 epoch total loss 6.52701426\n",
      "Trained batch 697 batch loss 6.44720793 epoch total loss 6.5269\n",
      "Trained batch 698 batch loss 6.72865915 epoch total loss 6.5271883\n",
      "Trained batch 699 batch loss 6.71480179 epoch total loss 6.52745676\n",
      "Trained batch 700 batch loss 6.47717333 epoch total loss 6.52738476\n",
      "Trained batch 701 batch loss 6.57657719 epoch total loss 6.52745533\n",
      "Trained batch 702 batch loss 5.952209 epoch total loss 6.52663565\n",
      "Trained batch 703 batch loss 5.9543252 epoch total loss 6.52582121\n",
      "Trained batch 704 batch loss 6.28964424 epoch total loss 6.52548552\n",
      "Trained batch 705 batch loss 6.67487144 epoch total loss 6.52569723\n",
      "Trained batch 706 batch loss 6.31834221 epoch total loss 6.52540398\n",
      "Trained batch 707 batch loss 6.66500807 epoch total loss 6.52560139\n",
      "Trained batch 708 batch loss 6.59382963 epoch total loss 6.52569771\n",
      "Trained batch 709 batch loss 6.54092932 epoch total loss 6.52571917\n",
      "Trained batch 710 batch loss 6.5544796 epoch total loss 6.5257597\n",
      "Trained batch 711 batch loss 6.49688721 epoch total loss 6.52571964\n",
      "Trained batch 712 batch loss 6.5311141 epoch total loss 6.52572727\n",
      "Trained batch 713 batch loss 6.0774579 epoch total loss 6.5250988\n",
      "Trained batch 714 batch loss 6.63425589 epoch total loss 6.52525187\n",
      "Trained batch 715 batch loss 6.82587481 epoch total loss 6.52567196\n",
      "Trained batch 716 batch loss 6.74532366 epoch total loss 6.52597857\n",
      "Trained batch 717 batch loss 6.23234415 epoch total loss 6.52556896\n",
      "Trained batch 718 batch loss 6.047369 epoch total loss 6.52490282\n",
      "Trained batch 719 batch loss 6.48585 epoch total loss 6.52484846\n",
      "Trained batch 720 batch loss 6.16217041 epoch total loss 6.52434492\n",
      "Trained batch 721 batch loss 6.08086824 epoch total loss 6.52373028\n",
      "Trained batch 722 batch loss 5.71174097 epoch total loss 6.5226059\n",
      "Trained batch 723 batch loss 5.95180845 epoch total loss 6.52181578\n",
      "Trained batch 724 batch loss 6.46354818 epoch total loss 6.52173519\n",
      "Trained batch 725 batch loss 6.67468357 epoch total loss 6.52194643\n",
      "Trained batch 726 batch loss 6.88723564 epoch total loss 6.52244949\n",
      "Trained batch 727 batch loss 6.64232445 epoch total loss 6.522614\n",
      "Trained batch 728 batch loss 6.48813915 epoch total loss 6.5225668\n",
      "Trained batch 729 batch loss 6.62419176 epoch total loss 6.52270603\n",
      "Trained batch 730 batch loss 6.05395 epoch total loss 6.52206373\n",
      "Trained batch 731 batch loss 5.8912921 epoch total loss 6.52120066\n",
      "Trained batch 732 batch loss 6.36332417 epoch total loss 6.52098465\n",
      "Trained batch 733 batch loss 6.77618456 epoch total loss 6.52133322\n",
      "Trained batch 734 batch loss 6.58703613 epoch total loss 6.52142239\n",
      "Trained batch 735 batch loss 6.67460346 epoch total loss 6.52163124\n",
      "Trained batch 736 batch loss 6.68163729 epoch total loss 6.52184868\n",
      "Trained batch 737 batch loss 6.54575729 epoch total loss 6.5218811\n",
      "Trained batch 738 batch loss 6.39176559 epoch total loss 6.52170467\n",
      "Trained batch 739 batch loss 6.19238377 epoch total loss 6.52125883\n",
      "Trained batch 740 batch loss 5.95878696 epoch total loss 6.52049923\n",
      "Trained batch 741 batch loss 5.74533176 epoch total loss 6.51945305\n",
      "Trained batch 742 batch loss 5.55101585 epoch total loss 6.51814747\n",
      "Trained batch 743 batch loss 6.1290164 epoch total loss 6.51762342\n",
      "Trained batch 744 batch loss 6.0414176 epoch total loss 6.51698351\n",
      "Trained batch 745 batch loss 6.93339252 epoch total loss 6.51754284\n",
      "Trained batch 746 batch loss 7.19894934 epoch total loss 6.51845598\n",
      "Trained batch 747 batch loss 6.67404 epoch total loss 6.51866388\n",
      "Trained batch 748 batch loss 6.96396065 epoch total loss 6.51925898\n",
      "Trained batch 749 batch loss 6.80478859 epoch total loss 6.51964\n",
      "Trained batch 750 batch loss 7.29806805 epoch total loss 6.52067757\n",
      "Trained batch 751 batch loss 6.77042341 epoch total loss 6.5210104\n",
      "Trained batch 752 batch loss 6.83902264 epoch total loss 6.52143288\n",
      "Trained batch 753 batch loss 6.80760527 epoch total loss 6.52181292\n",
      "Trained batch 754 batch loss 7.03710461 epoch total loss 6.5224967\n",
      "Trained batch 755 batch loss 6.535285 epoch total loss 6.52251339\n",
      "Trained batch 756 batch loss 6.7325139 epoch total loss 6.52279091\n",
      "Trained batch 757 batch loss 6.45006 epoch total loss 6.52269506\n",
      "Trained batch 758 batch loss 6.1873126 epoch total loss 6.52225304\n",
      "Trained batch 759 batch loss 6.06575871 epoch total loss 6.52165174\n",
      "Trained batch 760 batch loss 6.13227367 epoch total loss 6.52113914\n",
      "Trained batch 761 batch loss 6.42422056 epoch total loss 6.52101231\n",
      "Trained batch 762 batch loss 6.50121164 epoch total loss 6.5209856\n",
      "Trained batch 763 batch loss 6.65263271 epoch total loss 6.5211587\n",
      "Trained batch 764 batch loss 6.79155493 epoch total loss 6.52151251\n",
      "Trained batch 765 batch loss 6.71096945 epoch total loss 6.52176\n",
      "Trained batch 766 batch loss 6.78701305 epoch total loss 6.52210665\n",
      "Trained batch 767 batch loss 6.48486519 epoch total loss 6.52205801\n",
      "Trained batch 768 batch loss 6.55323648 epoch total loss 6.52209854\n",
      "Trained batch 769 batch loss 6.84466648 epoch total loss 6.52251816\n",
      "Trained batch 770 batch loss 6.66679335 epoch total loss 6.52270555\n",
      "Trained batch 771 batch loss 6.63959789 epoch total loss 6.52285719\n",
      "Trained batch 772 batch loss 6.49033928 epoch total loss 6.52281523\n",
      "Trained batch 773 batch loss 6.46746063 epoch total loss 6.52274323\n",
      "Trained batch 774 batch loss 5.95167065 epoch total loss 6.52200556\n",
      "Trained batch 775 batch loss 6.27003527 epoch total loss 6.52168036\n",
      "Trained batch 776 batch loss 6.24145079 epoch total loss 6.52131891\n",
      "Trained batch 777 batch loss 6.7736969 epoch total loss 6.52164412\n",
      "Trained batch 778 batch loss 6.77378035 epoch total loss 6.52196836\n",
      "Trained batch 779 batch loss 6.33394766 epoch total loss 6.52172709\n",
      "Trained batch 780 batch loss 6.31528664 epoch total loss 6.52146244\n",
      "Trained batch 781 batch loss 6.27290535 epoch total loss 6.52114439\n",
      "Trained batch 782 batch loss 6.56558323 epoch total loss 6.52120113\n",
      "Trained batch 783 batch loss 6.13624382 epoch total loss 6.52070904\n",
      "Trained batch 784 batch loss 6.5904932 epoch total loss 6.52079821\n",
      "Trained batch 785 batch loss 6.29208851 epoch total loss 6.52050638\n",
      "Trained batch 786 batch loss 5.9843235 epoch total loss 6.5198245\n",
      "Trained batch 787 batch loss 6.78262138 epoch total loss 6.52015829\n",
      "Trained batch 788 batch loss 6.0024457 epoch total loss 6.51950169\n",
      "Trained batch 789 batch loss 6.40576839 epoch total loss 6.5193572\n",
      "Trained batch 790 batch loss 6.03173113 epoch total loss 6.51874\n",
      "Trained batch 791 batch loss 5.9635005 epoch total loss 6.5180378\n",
      "Trained batch 792 batch loss 6.45473909 epoch total loss 6.51795769\n",
      "Trained batch 793 batch loss 6.96386242 epoch total loss 6.51852036\n",
      "Trained batch 794 batch loss 6.27997208 epoch total loss 6.51821947\n",
      "Trained batch 795 batch loss 6.46074963 epoch total loss 6.51814747\n",
      "Trained batch 796 batch loss 6.45518112 epoch total loss 6.51806831\n",
      "Trained batch 797 batch loss 6.46931601 epoch total loss 6.5180068\n",
      "Trained batch 798 batch loss 6.43182755 epoch total loss 6.51789856\n",
      "Trained batch 799 batch loss 6.67424345 epoch total loss 6.51809454\n",
      "Trained batch 800 batch loss 6.40252876 epoch total loss 6.51795\n",
      "Trained batch 801 batch loss 6.02732372 epoch total loss 6.51733732\n",
      "Trained batch 802 batch loss 6.62228537 epoch total loss 6.51746798\n",
      "Trained batch 803 batch loss 6.02745533 epoch total loss 6.51685762\n",
      "Trained batch 804 batch loss 6.15635633 epoch total loss 6.51640892\n",
      "Trained batch 805 batch loss 6.67714167 epoch total loss 6.51660872\n",
      "Trained batch 806 batch loss 6.43927431 epoch total loss 6.51651335\n",
      "Trained batch 807 batch loss 6.60135078 epoch total loss 6.51661873\n",
      "Trained batch 808 batch loss 6.47651672 epoch total loss 6.51656914\n",
      "Trained batch 809 batch loss 5.9463315 epoch total loss 6.5158639\n",
      "Trained batch 810 batch loss 6.52207088 epoch total loss 6.51587152\n",
      "Trained batch 811 batch loss 6.26703787 epoch total loss 6.51556492\n",
      "Trained batch 812 batch loss 6.22302771 epoch total loss 6.51520491\n",
      "Trained batch 813 batch loss 6.21280622 epoch total loss 6.51483297\n",
      "Trained batch 814 batch loss 6.13656807 epoch total loss 6.51436853\n",
      "Trained batch 815 batch loss 6.17835855 epoch total loss 6.51395607\n",
      "Trained batch 816 batch loss 6.14357948 epoch total loss 6.51350212\n",
      "Trained batch 817 batch loss 6.0871048 epoch total loss 6.51298\n",
      "Trained batch 818 batch loss 6.35100746 epoch total loss 6.5127821\n",
      "Trained batch 819 batch loss 6.34926224 epoch total loss 6.51258183\n",
      "Trained batch 820 batch loss 6.3864193 epoch total loss 6.51242781\n",
      "Trained batch 821 batch loss 6.68298244 epoch total loss 6.51263571\n",
      "Trained batch 822 batch loss 6.64780283 epoch total loss 6.5128\n",
      "Trained batch 823 batch loss 6.58657169 epoch total loss 6.51289\n",
      "Trained batch 824 batch loss 6.57423878 epoch total loss 6.51296425\n",
      "Trained batch 825 batch loss 5.82519817 epoch total loss 6.51213074\n",
      "Trained batch 826 batch loss 6.50136518 epoch total loss 6.51211786\n",
      "Trained batch 827 batch loss 6.38187551 epoch total loss 6.51196\n",
      "Trained batch 828 batch loss 6.49533749 epoch total loss 6.51194\n",
      "Trained batch 829 batch loss 6.83558846 epoch total loss 6.51233\n",
      "Trained batch 830 batch loss 6.72955751 epoch total loss 6.51259184\n",
      "Trained batch 831 batch loss 6.62251568 epoch total loss 6.51272392\n",
      "Trained batch 832 batch loss 6.52873945 epoch total loss 6.51274347\n",
      "Trained batch 833 batch loss 6.35791445 epoch total loss 6.51255751\n",
      "Trained batch 834 batch loss 6.23731518 epoch total loss 6.51222754\n",
      "Trained batch 835 batch loss 6.53956 epoch total loss 6.51226044\n",
      "Trained batch 836 batch loss 6.52466249 epoch total loss 6.51227522\n",
      "Trained batch 837 batch loss 6.74145412 epoch total loss 6.51254892\n",
      "Trained batch 838 batch loss 6.62479 epoch total loss 6.51268291\n",
      "Trained batch 839 batch loss 7.02917147 epoch total loss 6.51329899\n",
      "Trained batch 840 batch loss 7.00166178 epoch total loss 6.51388\n",
      "Trained batch 841 batch loss 6.89677668 epoch total loss 6.51433563\n",
      "Trained batch 842 batch loss 6.73486042 epoch total loss 6.51459742\n",
      "Trained batch 843 batch loss 6.67051268 epoch total loss 6.51478243\n",
      "Trained batch 844 batch loss 6.66868734 epoch total loss 6.5149641\n",
      "Trained batch 845 batch loss 6.63697147 epoch total loss 6.51510906\n",
      "Trained batch 846 batch loss 6.44689369 epoch total loss 6.515028\n",
      "Trained batch 847 batch loss 6.7677989 epoch total loss 6.5153265\n",
      "Trained batch 848 batch loss 6.68034887 epoch total loss 6.51552057\n",
      "Trained batch 849 batch loss 6.63590097 epoch total loss 6.51566219\n",
      "Trained batch 850 batch loss 6.71824694 epoch total loss 6.51590061\n",
      "Trained batch 851 batch loss 6.72843456 epoch total loss 6.51615047\n",
      "Trained batch 852 batch loss 6.78583908 epoch total loss 6.51646709\n",
      "Trained batch 853 batch loss 6.87604237 epoch total loss 6.51688814\n",
      "Trained batch 854 batch loss 6.95084095 epoch total loss 6.51739645\n",
      "Trained batch 855 batch loss 6.69385576 epoch total loss 6.51760244\n",
      "Trained batch 856 batch loss 6.71787453 epoch total loss 6.51783657\n",
      "Trained batch 857 batch loss 6.70833969 epoch total loss 6.51805878\n",
      "Trained batch 858 batch loss 6.67969084 epoch total loss 6.5182476\n",
      "Trained batch 859 batch loss 6.60902739 epoch total loss 6.51835299\n",
      "Trained batch 860 batch loss 6.53644 epoch total loss 6.51837397\n",
      "Trained batch 861 batch loss 6.69948292 epoch total loss 6.51858473\n",
      "Trained batch 862 batch loss 6.41231823 epoch total loss 6.51846123\n",
      "Trained batch 863 batch loss 6.71507883 epoch total loss 6.51868868\n",
      "Trained batch 864 batch loss 6.50988436 epoch total loss 6.51867867\n",
      "Trained batch 865 batch loss 6.29520798 epoch total loss 6.51842\n",
      "Trained batch 866 batch loss 6.34450436 epoch total loss 6.51822\n",
      "Trained batch 867 batch loss 6.7019124 epoch total loss 6.51843214\n",
      "Trained batch 868 batch loss 6.35605431 epoch total loss 6.51824474\n",
      "Trained batch 869 batch loss 6.80533 epoch total loss 6.51857471\n",
      "Trained batch 870 batch loss 7.19016123 epoch total loss 6.51934671\n",
      "Trained batch 871 batch loss 7.08703804 epoch total loss 6.51999807\n",
      "Trained batch 872 batch loss 6.98954153 epoch total loss 6.5205369\n",
      "Trained batch 873 batch loss 7.29876709 epoch total loss 6.52142859\n",
      "Trained batch 874 batch loss 7.25935793 epoch total loss 6.52227259\n",
      "Trained batch 875 batch loss 6.76885462 epoch total loss 6.52255487\n",
      "Trained batch 876 batch loss 6.61160946 epoch total loss 6.52265644\n",
      "Trained batch 877 batch loss 6.50388432 epoch total loss 6.52263498\n",
      "Trained batch 878 batch loss 6.82013035 epoch total loss 6.52297401\n",
      "Trained batch 879 batch loss 6.37012 epoch total loss 6.52280045\n",
      "Trained batch 880 batch loss 6.61058092 epoch total loss 6.52289963\n",
      "Trained batch 881 batch loss 6.66631889 epoch total loss 6.52306271\n",
      "Trained batch 882 batch loss 6.51970291 epoch total loss 6.52305889\n",
      "Trained batch 883 batch loss 6.42218 epoch total loss 6.52294493\n",
      "Trained batch 884 batch loss 6.43625355 epoch total loss 6.5228467\n",
      "Trained batch 885 batch loss 7.19434786 epoch total loss 6.52360535\n",
      "Trained batch 886 batch loss 6.82219172 epoch total loss 6.52394247\n",
      "Trained batch 887 batch loss 6.79417562 epoch total loss 6.52424669\n",
      "Trained batch 888 batch loss 6.70203447 epoch total loss 6.52444696\n",
      "Trained batch 889 batch loss 6.90632 epoch total loss 6.52487659\n",
      "Trained batch 890 batch loss 6.64735603 epoch total loss 6.5250144\n",
      "Trained batch 891 batch loss 6.56417322 epoch total loss 6.52505779\n",
      "Trained batch 892 batch loss 6.9557023 epoch total loss 6.52554083\n",
      "Trained batch 893 batch loss 6.66971874 epoch total loss 6.52570248\n",
      "Trained batch 894 batch loss 6.86410713 epoch total loss 6.52608109\n",
      "Trained batch 895 batch loss 7.16271496 epoch total loss 6.52679205\n",
      "Trained batch 896 batch loss 6.92885494 epoch total loss 6.52724075\n",
      "Trained batch 897 batch loss 6.55313444 epoch total loss 6.52727\n",
      "Trained batch 898 batch loss 6.95723629 epoch total loss 6.52774811\n",
      "Trained batch 899 batch loss 6.76749897 epoch total loss 6.52801514\n",
      "Trained batch 900 batch loss 6.8512888 epoch total loss 6.5283742\n",
      "Trained batch 901 batch loss 6.8971324 epoch total loss 6.52878332\n",
      "Trained batch 902 batch loss 6.81201839 epoch total loss 6.52909708\n",
      "Trained batch 903 batch loss 6.917768 epoch total loss 6.52952766\n",
      "Trained batch 904 batch loss 6.86666632 epoch total loss 6.52990055\n",
      "Trained batch 905 batch loss 6.5041604 epoch total loss 6.52987242\n",
      "Trained batch 906 batch loss 6.52013302 epoch total loss 6.52986145\n",
      "Trained batch 907 batch loss 6.88474083 epoch total loss 6.53025293\n",
      "Trained batch 908 batch loss 6.57763481 epoch total loss 6.53030539\n",
      "Trained batch 909 batch loss 5.93470335 epoch total loss 6.52964973\n",
      "Trained batch 910 batch loss 6.07104778 epoch total loss 6.52914619\n",
      "Trained batch 911 batch loss 6.02211857 epoch total loss 6.52858925\n",
      "Trained batch 912 batch loss 6.55491543 epoch total loss 6.52861786\n",
      "Trained batch 913 batch loss 6.49768686 epoch total loss 6.528584\n",
      "Trained batch 914 batch loss 6.40945 epoch total loss 6.52845383\n",
      "Trained batch 915 batch loss 6.66249371 epoch total loss 6.52860069\n",
      "Trained batch 916 batch loss 6.3715806 epoch total loss 6.52842903\n",
      "Trained batch 917 batch loss 6.68725872 epoch total loss 6.5286026\n",
      "Trained batch 918 batch loss 6.51110792 epoch total loss 6.52858353\n",
      "Trained batch 919 batch loss 6.32149 epoch total loss 6.52835798\n",
      "Trained batch 920 batch loss 6.59717274 epoch total loss 6.52843285\n",
      "Trained batch 921 batch loss 6.91693497 epoch total loss 6.52885485\n",
      "Trained batch 922 batch loss 6.50982523 epoch total loss 6.52883387\n",
      "Trained batch 923 batch loss 6.48213243 epoch total loss 6.52878332\n",
      "Trained batch 924 batch loss 6.65434885 epoch total loss 6.52891922\n",
      "Trained batch 925 batch loss 6.58849239 epoch total loss 6.52898312\n",
      "Trained batch 926 batch loss 6.83855438 epoch total loss 6.52931738\n",
      "Trained batch 927 batch loss 6.96343231 epoch total loss 6.52978563\n",
      "Trained batch 928 batch loss 6.80909729 epoch total loss 6.53008652\n",
      "Trained batch 929 batch loss 6.4252634 epoch total loss 6.52997398\n",
      "Trained batch 930 batch loss 6.67128229 epoch total loss 6.53012609\n",
      "Trained batch 931 batch loss 6.77445555 epoch total loss 6.53038836\n",
      "Trained batch 932 batch loss 6.88965416 epoch total loss 6.53077364\n",
      "Trained batch 933 batch loss 6.6006 epoch total loss 6.5308485\n",
      "Trained batch 934 batch loss 6.30703688 epoch total loss 6.53060913\n",
      "Trained batch 935 batch loss 6.82406902 epoch total loss 6.53092289\n",
      "Trained batch 936 batch loss 6.42896461 epoch total loss 6.53081417\n",
      "Trained batch 937 batch loss 6.74176359 epoch total loss 6.53103924\n",
      "Trained batch 938 batch loss 6.85498285 epoch total loss 6.53138494\n",
      "Trained batch 939 batch loss 6.61748362 epoch total loss 6.5314765\n",
      "Trained batch 940 batch loss 6.81356525 epoch total loss 6.53177691\n",
      "Trained batch 941 batch loss 6.79341221 epoch total loss 6.5320549\n",
      "Trained batch 942 batch loss 6.51954174 epoch total loss 6.53204155\n",
      "Trained batch 943 batch loss 6.32245 epoch total loss 6.53181887\n",
      "Trained batch 944 batch loss 6.30777693 epoch total loss 6.5315814\n",
      "Trained batch 945 batch loss 6.06558228 epoch total loss 6.53108835\n",
      "Trained batch 946 batch loss 6.06216049 epoch total loss 6.53059244\n",
      "Trained batch 947 batch loss 6.39558029 epoch total loss 6.53045\n",
      "Trained batch 948 batch loss 6.48249149 epoch total loss 6.53039932\n",
      "Trained batch 949 batch loss 6.31083584 epoch total loss 6.53016806\n",
      "Trained batch 950 batch loss 6.60131359 epoch total loss 6.53024244\n",
      "Trained batch 951 batch loss 6.65039444 epoch total loss 6.5303688\n",
      "Trained batch 952 batch loss 6.30605936 epoch total loss 6.53013325\n",
      "Trained batch 953 batch loss 6.26124 epoch total loss 6.52985144\n",
      "Trained batch 954 batch loss 6.55970812 epoch total loss 6.52988243\n",
      "Trained batch 955 batch loss 6.58409262 epoch total loss 6.52993917\n",
      "Trained batch 956 batch loss 6.21926308 epoch total loss 6.52961397\n",
      "Trained batch 957 batch loss 6.34064054 epoch total loss 6.52941656\n",
      "Trained batch 958 batch loss 6.52979279 epoch total loss 6.52941704\n",
      "Trained batch 959 batch loss 6.2153511 epoch total loss 6.52908945\n",
      "Trained batch 960 batch loss 6.19217587 epoch total loss 6.52873898\n",
      "Trained batch 961 batch loss 6.19439793 epoch total loss 6.52839088\n",
      "Trained batch 962 batch loss 6.59375668 epoch total loss 6.52845907\n",
      "Trained batch 963 batch loss 6.57294607 epoch total loss 6.52850485\n",
      "Trained batch 964 batch loss 6.65769672 epoch total loss 6.52863884\n",
      "Trained batch 965 batch loss 6.89046 epoch total loss 6.52901411\n",
      "Trained batch 966 batch loss 6.61059189 epoch total loss 6.52909803\n",
      "Trained batch 967 batch loss 6.05871773 epoch total loss 6.52861166\n",
      "Trained batch 968 batch loss 6.45722818 epoch total loss 6.52853775\n",
      "Trained batch 969 batch loss 6.84656096 epoch total loss 6.52886581\n",
      "Trained batch 970 batch loss 6.76745224 epoch total loss 6.52911234\n",
      "Trained batch 971 batch loss 6.554245 epoch total loss 6.52913809\n",
      "Trained batch 972 batch loss 6.73175335 epoch total loss 6.52934647\n",
      "Trained batch 973 batch loss 6.36449671 epoch total loss 6.52917671\n",
      "Trained batch 974 batch loss 6.00956488 epoch total loss 6.52864361\n",
      "Trained batch 975 batch loss 6.49467468 epoch total loss 6.5286088\n",
      "Trained batch 976 batch loss 7.11016703 epoch total loss 6.52920485\n",
      "Trained batch 977 batch loss 6.62423 epoch total loss 6.52930164\n",
      "Trained batch 978 batch loss 6.83511353 epoch total loss 6.52961445\n",
      "Trained batch 979 batch loss 6.61003256 epoch total loss 6.52969646\n",
      "Trained batch 980 batch loss 6.67852688 epoch total loss 6.52984858\n",
      "Trained batch 981 batch loss 7.38456535 epoch total loss 6.53071976\n",
      "Trained batch 982 batch loss 6.73391962 epoch total loss 6.5309267\n",
      "Trained batch 983 batch loss 6.22524548 epoch total loss 6.53061581\n",
      "Trained batch 984 batch loss 6.54292297 epoch total loss 6.5306282\n",
      "Trained batch 985 batch loss 5.96516848 epoch total loss 6.53005409\n",
      "Trained batch 986 batch loss 5.82592535 epoch total loss 6.52934\n",
      "Trained batch 987 batch loss 6.21677732 epoch total loss 6.52902317\n",
      "Trained batch 988 batch loss 6.01226282 epoch total loss 6.5285\n",
      "Trained batch 989 batch loss 5.17959738 epoch total loss 6.52713633\n",
      "Trained batch 990 batch loss 5.54007673 epoch total loss 6.52613926\n",
      "Trained batch 991 batch loss 5.49885225 epoch total loss 6.52510309\n",
      "Trained batch 992 batch loss 5.96375704 epoch total loss 6.52453709\n",
      "Trained batch 993 batch loss 6.42901707 epoch total loss 6.52444124\n",
      "Trained batch 994 batch loss 6.4135623 epoch total loss 6.52432966\n",
      "Trained batch 995 batch loss 6.83278942 epoch total loss 6.52463961\n",
      "Trained batch 996 batch loss 7.16490078 epoch total loss 6.52528286\n",
      "Trained batch 997 batch loss 6.87803602 epoch total loss 6.52563667\n",
      "Trained batch 998 batch loss 6.50309277 epoch total loss 6.52561378\n",
      "Trained batch 999 batch loss 6.45189762 epoch total loss 6.52554\n",
      "Trained batch 1000 batch loss 6.69462061 epoch total loss 6.52570915\n",
      "Trained batch 1001 batch loss 6.67720699 epoch total loss 6.52586031\n",
      "Trained batch 1002 batch loss 6.93665409 epoch total loss 6.52627039\n",
      "Trained batch 1003 batch loss 6.81038427 epoch total loss 6.52655363\n",
      "Trained batch 1004 batch loss 6.66164064 epoch total loss 6.5266881\n",
      "Trained batch 1005 batch loss 6.43931246 epoch total loss 6.52660131\n",
      "Trained batch 1006 batch loss 6.16808558 epoch total loss 6.52624464\n",
      "Trained batch 1007 batch loss 6.60587 epoch total loss 6.5263238\n",
      "Trained batch 1008 batch loss 6.52897549 epoch total loss 6.52632666\n",
      "Trained batch 1009 batch loss 6.65231466 epoch total loss 6.52645159\n",
      "Trained batch 1010 batch loss 6.89372396 epoch total loss 6.52681494\n",
      "Trained batch 1011 batch loss 7.11324167 epoch total loss 6.52739477\n",
      "Trained batch 1012 batch loss 7.13546 epoch total loss 6.52799559\n",
      "Trained batch 1013 batch loss 6.93309832 epoch total loss 6.52839565\n",
      "Trained batch 1014 batch loss 7.35922861 epoch total loss 6.52921486\n",
      "Trained batch 1015 batch loss 6.89972305 epoch total loss 6.52958\n",
      "Trained batch 1016 batch loss 7.30860186 epoch total loss 6.53034687\n",
      "Trained batch 1017 batch loss 6.91436481 epoch total loss 6.53072453\n",
      "Trained batch 1018 batch loss 7.00421381 epoch total loss 6.53119\n",
      "Trained batch 1019 batch loss 6.56248426 epoch total loss 6.53122091\n",
      "Trained batch 1020 batch loss 6.15823793 epoch total loss 6.53085518\n",
      "Trained batch 1021 batch loss 6.57522726 epoch total loss 6.53089857\n",
      "Trained batch 1022 batch loss 6.64294386 epoch total loss 6.53100824\n",
      "Trained batch 1023 batch loss 6.58898926 epoch total loss 6.53106499\n",
      "Trained batch 1024 batch loss 6.16881704 epoch total loss 6.53071117\n",
      "Trained batch 1025 batch loss 5.86356449 epoch total loss 6.53006029\n",
      "Trained batch 1026 batch loss 6.47187281 epoch total loss 6.53000355\n",
      "Trained batch 1027 batch loss 6.46158648 epoch total loss 6.52993679\n",
      "Trained batch 1028 batch loss 6.55178261 epoch total loss 6.52995825\n",
      "Trained batch 1029 batch loss 6.81251192 epoch total loss 6.53023243\n",
      "Trained batch 1030 batch loss 6.65827179 epoch total loss 6.53035688\n",
      "Trained batch 1031 batch loss 6.60139799 epoch total loss 6.53042603\n",
      "Trained batch 1032 batch loss 6.61288 epoch total loss 6.53050566\n",
      "Trained batch 1033 batch loss 6.55990601 epoch total loss 6.53053427\n",
      "Trained batch 1034 batch loss 6.48762226 epoch total loss 6.53049326\n",
      "Trained batch 1035 batch loss 6.03436327 epoch total loss 6.53001356\n",
      "Trained batch 1036 batch loss 6.41741943 epoch total loss 6.52990484\n",
      "Trained batch 1037 batch loss 6.38223457 epoch total loss 6.52976274\n",
      "Trained batch 1038 batch loss 6.55528212 epoch total loss 6.52978706\n",
      "Trained batch 1039 batch loss 6.47374249 epoch total loss 6.52973318\n",
      "Trained batch 1040 batch loss 6.55561304 epoch total loss 6.52975798\n",
      "Trained batch 1041 batch loss 6.0628252 epoch total loss 6.52930975\n",
      "Trained batch 1042 batch loss 6.39248 epoch total loss 6.52917814\n",
      "Trained batch 1043 batch loss 6.06300926 epoch total loss 6.52873135\n",
      "Trained batch 1044 batch loss 6.41646957 epoch total loss 6.52862406\n",
      "Trained batch 1045 batch loss 6.08192968 epoch total loss 6.52819633\n",
      "Trained batch 1046 batch loss 6.21813059 epoch total loss 6.5279\n",
      "Trained batch 1047 batch loss 6.45063066 epoch total loss 6.52782631\n",
      "Trained batch 1048 batch loss 6.48095798 epoch total loss 6.52778149\n",
      "Trained batch 1049 batch loss 6.86654568 epoch total loss 6.52810478\n",
      "Trained batch 1050 batch loss 6.06879 epoch total loss 6.52766752\n",
      "Trained batch 1051 batch loss 5.69511366 epoch total loss 6.5268755\n",
      "Trained batch 1052 batch loss 5.50283194 epoch total loss 6.52590227\n",
      "Trained batch 1053 batch loss 6.12817526 epoch total loss 6.52552462\n",
      "Trained batch 1054 batch loss 6.46756792 epoch total loss 6.52547\n",
      "Trained batch 1055 batch loss 6.06343651 epoch total loss 6.52503204\n",
      "Trained batch 1056 batch loss 6.38795948 epoch total loss 6.52490234\n",
      "Trained batch 1057 batch loss 6.47657061 epoch total loss 6.52485657\n",
      "Trained batch 1058 batch loss 6.6378231 epoch total loss 6.52496338\n",
      "Trained batch 1059 batch loss 6.26680231 epoch total loss 6.52471924\n",
      "Trained batch 1060 batch loss 6.60513735 epoch total loss 6.52479506\n",
      "Trained batch 1061 batch loss 7.20270061 epoch total loss 6.52543402\n",
      "Trained batch 1062 batch loss 7.36890411 epoch total loss 6.52622843\n",
      "Trained batch 1063 batch loss 6.62086868 epoch total loss 6.5263176\n",
      "Trained batch 1064 batch loss 6.76155567 epoch total loss 6.52653885\n",
      "Trained batch 1065 batch loss 6.82300234 epoch total loss 6.52681732\n",
      "Trained batch 1066 batch loss 6.45949554 epoch total loss 6.52675438\n",
      "Trained batch 1067 batch loss 6.07979441 epoch total loss 6.52633524\n",
      "Trained batch 1068 batch loss 6.12604761 epoch total loss 6.52596045\n",
      "Trained batch 1069 batch loss 6.42279673 epoch total loss 6.52586365\n",
      "Trained batch 1070 batch loss 6.26686096 epoch total loss 6.52562189\n",
      "Trained batch 1071 batch loss 6.01183891 epoch total loss 6.52514219\n",
      "Trained batch 1072 batch loss 6.07390118 epoch total loss 6.52472115\n",
      "Trained batch 1073 batch loss 6.09860229 epoch total loss 6.52432394\n",
      "Trained batch 1074 batch loss 5.92310953 epoch total loss 6.52376461\n",
      "Trained batch 1075 batch loss 5.93609 epoch total loss 6.52321768\n",
      "Trained batch 1076 batch loss 5.77405739 epoch total loss 6.5225215\n",
      "Trained batch 1077 batch loss 6.5078 epoch total loss 6.52250767\n",
      "Trained batch 1078 batch loss 6.43065786 epoch total loss 6.52242231\n",
      "Trained batch 1079 batch loss 6.47220325 epoch total loss 6.52237606\n",
      "Trained batch 1080 batch loss 6.61106586 epoch total loss 6.5224576\n",
      "Trained batch 1081 batch loss 6.6710887 epoch total loss 6.52259493\n",
      "Trained batch 1082 batch loss 6.84590769 epoch total loss 6.52289391\n",
      "Trained batch 1083 batch loss 6.90404415 epoch total loss 6.52324533\n",
      "Trained batch 1084 batch loss 6.55965424 epoch total loss 6.52327871\n",
      "Trained batch 1085 batch loss 7.04525661 epoch total loss 6.52376032\n",
      "Trained batch 1086 batch loss 6.3561492 epoch total loss 6.52360582\n",
      "Trained batch 1087 batch loss 5.88743305 epoch total loss 6.52302027\n",
      "Trained batch 1088 batch loss 5.5052886 epoch total loss 6.52208471\n",
      "Trained batch 1089 batch loss 6.04061413 epoch total loss 6.52164268\n",
      "Trained batch 1090 batch loss 6.6561904 epoch total loss 6.52176619\n",
      "Trained batch 1091 batch loss 6.50381899 epoch total loss 6.52175\n",
      "Trained batch 1092 batch loss 6.54481316 epoch total loss 6.52177095\n",
      "Trained batch 1093 batch loss 6.54328442 epoch total loss 6.52179098\n",
      "Trained batch 1094 batch loss 6.60715771 epoch total loss 6.52186871\n",
      "Trained batch 1095 batch loss 6.6369009 epoch total loss 6.52197361\n",
      "Trained batch 1096 batch loss 6.42440271 epoch total loss 6.52188444\n",
      "Trained batch 1097 batch loss 5.85079718 epoch total loss 6.52127266\n",
      "Trained batch 1098 batch loss 6.74556303 epoch total loss 6.52147675\n",
      "Trained batch 1099 batch loss 6.48437786 epoch total loss 6.52144289\n",
      "Trained batch 1100 batch loss 6.21125698 epoch total loss 6.52116108\n",
      "Trained batch 1101 batch loss 6.69276619 epoch total loss 6.52131701\n",
      "Trained batch 1102 batch loss 6.50369453 epoch total loss 6.52130127\n",
      "Trained batch 1103 batch loss 6.47849274 epoch total loss 6.52126265\n",
      "Trained batch 1104 batch loss 6.61443615 epoch total loss 6.52134705\n",
      "Trained batch 1105 batch loss 6.61119223 epoch total loss 6.52142811\n",
      "Trained batch 1106 batch loss 6.80594063 epoch total loss 6.5216856\n",
      "Trained batch 1107 batch loss 6.33787823 epoch total loss 6.52151966\n",
      "Trained batch 1108 batch loss 6.67592669 epoch total loss 6.5216589\n",
      "Trained batch 1109 batch loss 6.6540103 epoch total loss 6.52177811\n",
      "Trained batch 1110 batch loss 6.62630272 epoch total loss 6.52187252\n",
      "Trained batch 1111 batch loss 6.83820868 epoch total loss 6.52215719\n",
      "Trained batch 1112 batch loss 6.78630209 epoch total loss 6.52239466\n",
      "Trained batch 1113 batch loss 6.1667738 epoch total loss 6.52207518\n",
      "Trained batch 1114 batch loss 6.47856951 epoch total loss 6.52203608\n",
      "Trained batch 1115 batch loss 6.66513348 epoch total loss 6.52216434\n",
      "Trained batch 1116 batch loss 6.81745481 epoch total loss 6.52242899\n",
      "Trained batch 1117 batch loss 6.65595675 epoch total loss 6.5225482\n",
      "Trained batch 1118 batch loss 6.42186642 epoch total loss 6.52245855\n",
      "Trained batch 1119 batch loss 6.12181616 epoch total loss 6.5221\n",
      "Trained batch 1120 batch loss 6.56319 epoch total loss 6.52213669\n",
      "Trained batch 1121 batch loss 6.35500097 epoch total loss 6.52198744\n",
      "Trained batch 1122 batch loss 6.76550102 epoch total loss 6.5222044\n",
      "Trained batch 1123 batch loss 6.51040602 epoch total loss 6.52219391\n",
      "Trained batch 1124 batch loss 6.28933477 epoch total loss 6.52198696\n",
      "Trained batch 1125 batch loss 6.61006212 epoch total loss 6.52206516\n",
      "Trained batch 1126 batch loss 6.54346609 epoch total loss 6.52208424\n",
      "Trained batch 1127 batch loss 6.20520353 epoch total loss 6.5218029\n",
      "Trained batch 1128 batch loss 6.73876905 epoch total loss 6.52199507\n",
      "Trained batch 1129 batch loss 6.42244577 epoch total loss 6.52190685\n",
      "Trained batch 1130 batch loss 6.3869009 epoch total loss 6.52178717\n",
      "Trained batch 1131 batch loss 6.55625629 epoch total loss 6.52181768\n",
      "Trained batch 1132 batch loss 6.5915761 epoch total loss 6.52187967\n",
      "Trained batch 1133 batch loss 6.41372299 epoch total loss 6.52178383\n",
      "Trained batch 1134 batch loss 6.64094543 epoch total loss 6.52188921\n",
      "Trained batch 1135 batch loss 6.54071093 epoch total loss 6.52190542\n",
      "Trained batch 1136 batch loss 6.46602297 epoch total loss 6.52185631\n",
      "Trained batch 1137 batch loss 6.27778912 epoch total loss 6.52164173\n",
      "Trained batch 1138 batch loss 6.31541252 epoch total loss 6.52146053\n",
      "Trained batch 1139 batch loss 6.44694519 epoch total loss 6.52139473\n",
      "Trained batch 1140 batch loss 6.21420765 epoch total loss 6.52112532\n",
      "Trained batch 1141 batch loss 6.17721701 epoch total loss 6.52082396\n",
      "Trained batch 1142 batch loss 6.34742165 epoch total loss 6.52067232\n",
      "Trained batch 1143 batch loss 6.18744 epoch total loss 6.52038097\n",
      "Trained batch 1144 batch loss 6.40658665 epoch total loss 6.52028179\n",
      "Trained batch 1145 batch loss 6.92307615 epoch total loss 6.52063322\n",
      "Trained batch 1146 batch loss 6.80646467 epoch total loss 6.52088261\n",
      "Trained batch 1147 batch loss 6.65662575 epoch total loss 6.52100134\n",
      "Trained batch 1148 batch loss 6.41321516 epoch total loss 6.5209074\n",
      "Trained batch 1149 batch loss 6.28249884 epoch total loss 6.5207\n",
      "Trained batch 1150 batch loss 6.25855923 epoch total loss 6.52047205\n",
      "Trained batch 1151 batch loss 6.31110144 epoch total loss 6.52029037\n",
      "Trained batch 1152 batch loss 6.37073421 epoch total loss 6.52016\n",
      "Trained batch 1153 batch loss 6.73744917 epoch total loss 6.52034855\n",
      "Trained batch 1154 batch loss 6.705369 epoch total loss 6.52050924\n",
      "Trained batch 1155 batch loss 7.30256 epoch total loss 6.52118635\n",
      "Trained batch 1156 batch loss 6.85668325 epoch total loss 6.52147627\n",
      "Trained batch 1157 batch loss 6.40053701 epoch total loss 6.52137184\n",
      "Trained batch 1158 batch loss 6.33531046 epoch total loss 6.52121115\n",
      "Trained batch 1159 batch loss 6.28119135 epoch total loss 6.5210042\n",
      "Trained batch 1160 batch loss 6.66907835 epoch total loss 6.52113152\n",
      "Trained batch 1161 batch loss 6.61702681 epoch total loss 6.52121449\n",
      "Trained batch 1162 batch loss 6.43131351 epoch total loss 6.52113676\n",
      "Trained batch 1163 batch loss 6.61628723 epoch total loss 6.52121878\n",
      "Trained batch 1164 batch loss 6.49927378 epoch total loss 6.5212\n",
      "Trained batch 1165 batch loss 6.69730377 epoch total loss 6.52135086\n",
      "Trained batch 1166 batch loss 6.56444645 epoch total loss 6.52138805\n",
      "Trained batch 1167 batch loss 6.53708458 epoch total loss 6.52140141\n",
      "Trained batch 1168 batch loss 6.54946 epoch total loss 6.52142525\n",
      "Trained batch 1169 batch loss 6.5899353 epoch total loss 6.5214839\n",
      "Trained batch 1170 batch loss 6.57375813 epoch total loss 6.52152872\n",
      "Trained batch 1171 batch loss 6.45273781 epoch total loss 6.52146959\n",
      "Trained batch 1172 batch loss 6.39017963 epoch total loss 6.52135754\n",
      "Trained batch 1173 batch loss 6.34448814 epoch total loss 6.52120733\n",
      "Trained batch 1174 batch loss 6.5370965 epoch total loss 6.52122068\n",
      "Trained batch 1175 batch loss 6.57060051 epoch total loss 6.52126265\n",
      "Trained batch 1176 batch loss 6.81573534 epoch total loss 6.52151346\n",
      "Trained batch 1177 batch loss 6.39664412 epoch total loss 6.52140713\n",
      "Trained batch 1178 batch loss 6.49013424 epoch total loss 6.5213809\n",
      "Trained batch 1179 batch loss 6.37550974 epoch total loss 6.52125692\n",
      "Trained batch 1180 batch loss 6.15534973 epoch total loss 6.52094698\n",
      "Trained batch 1181 batch loss 6.53451109 epoch total loss 6.52095842\n",
      "Trained batch 1182 batch loss 6.41566658 epoch total loss 6.52086926\n",
      "Trained batch 1183 batch loss 6.87365437 epoch total loss 6.52116728\n",
      "Trained batch 1184 batch loss 6.69486809 epoch total loss 6.52131414\n",
      "Trained batch 1185 batch loss 6.9836216 epoch total loss 6.5217042\n",
      "Trained batch 1186 batch loss 6.34596586 epoch total loss 6.5215559\n",
      "Trained batch 1187 batch loss 6.56396818 epoch total loss 6.52159166\n",
      "Trained batch 1188 batch loss 6.5536561 epoch total loss 6.52161884\n",
      "Trained batch 1189 batch loss 6.46975136 epoch total loss 6.52157497\n",
      "Trained batch 1190 batch loss 6.43766212 epoch total loss 6.5215044\n",
      "Trained batch 1191 batch loss 6.35908508 epoch total loss 6.52136803\n",
      "Trained batch 1192 batch loss 6.23644066 epoch total loss 6.52112865\n",
      "Trained batch 1193 batch loss 6.69551563 epoch total loss 6.52127504\n",
      "Trained batch 1194 batch loss 6.58605433 epoch total loss 6.52132893\n",
      "Trained batch 1195 batch loss 6.69280386 epoch total loss 6.52147245\n",
      "Trained batch 1196 batch loss 6.40370846 epoch total loss 6.52137423\n",
      "Trained batch 1197 batch loss 6.4905591 epoch total loss 6.52134848\n",
      "Trained batch 1198 batch loss 6.47196388 epoch total loss 6.52130747\n",
      "Trained batch 1199 batch loss 6.76499414 epoch total loss 6.52151108\n",
      "Trained batch 1200 batch loss 6.94292688 epoch total loss 6.52186203\n",
      "Trained batch 1201 batch loss 6.78690481 epoch total loss 6.52208281\n",
      "Trained batch 1202 batch loss 6.59288692 epoch total loss 6.52214146\n",
      "Trained batch 1203 batch loss 6.81087732 epoch total loss 6.52238178\n",
      "Trained batch 1204 batch loss 6.8756485 epoch total loss 6.52267504\n",
      "Trained batch 1205 batch loss 6.79863787 epoch total loss 6.5229044\n",
      "Trained batch 1206 batch loss 6.8111515 epoch total loss 6.52314329\n",
      "Trained batch 1207 batch loss 6.46110535 epoch total loss 6.52309179\n",
      "Trained batch 1208 batch loss 6.70491743 epoch total loss 6.52324247\n",
      "Trained batch 1209 batch loss 6.7362442 epoch total loss 6.52341843\n",
      "Trained batch 1210 batch loss 6.59244871 epoch total loss 6.52347565\n",
      "Trained batch 1211 batch loss 6.50460958 epoch total loss 6.52346\n",
      "Trained batch 1212 batch loss 6.79816 epoch total loss 6.52368641\n",
      "Trained batch 1213 batch loss 6.59071398 epoch total loss 6.52374172\n",
      "Trained batch 1214 batch loss 6.71740341 epoch total loss 6.52390146\n",
      "Trained batch 1215 batch loss 6.73569632 epoch total loss 6.52407551\n",
      "Trained batch 1216 batch loss 6.74811363 epoch total loss 6.52426\n",
      "Trained batch 1217 batch loss 6.84544373 epoch total loss 6.52452374\n",
      "Trained batch 1218 batch loss 6.80588579 epoch total loss 6.52475452\n",
      "Trained batch 1219 batch loss 6.66145134 epoch total loss 6.52486658\n",
      "Trained batch 1220 batch loss 6.44725275 epoch total loss 6.52480316\n",
      "Trained batch 1221 batch loss 6.66536236 epoch total loss 6.52491856\n",
      "Trained batch 1222 batch loss 6.3676796 epoch total loss 6.52479\n",
      "Trained batch 1223 batch loss 6.33589077 epoch total loss 6.52463531\n",
      "Trained batch 1224 batch loss 6.36320162 epoch total loss 6.52450323\n",
      "Trained batch 1225 batch loss 6.33194447 epoch total loss 6.52434635\n",
      "Trained batch 1226 batch loss 6.50392818 epoch total loss 6.52432966\n",
      "Trained batch 1227 batch loss 6.27776289 epoch total loss 6.52412891\n",
      "Trained batch 1228 batch loss 6.48393869 epoch total loss 6.52409601\n",
      "Trained batch 1229 batch loss 6.17787266 epoch total loss 6.5238142\n",
      "Trained batch 1230 batch loss 6.38772488 epoch total loss 6.52370358\n",
      "Trained batch 1231 batch loss 6.19018698 epoch total loss 6.52343273\n",
      "Trained batch 1232 batch loss 6.28261948 epoch total loss 6.52323723\n",
      "Trained batch 1233 batch loss 6.4774766 epoch total loss 6.52320051\n",
      "Trained batch 1234 batch loss 6.20182657 epoch total loss 6.52293968\n",
      "Trained batch 1235 batch loss 6.54397678 epoch total loss 6.52295685\n",
      "Trained batch 1236 batch loss 6.74718428 epoch total loss 6.52313805\n",
      "Trained batch 1237 batch loss 6.52521133 epoch total loss 6.52314\n",
      "Trained batch 1238 batch loss 6.3896203 epoch total loss 6.52303219\n",
      "Trained batch 1239 batch loss 6.24990225 epoch total loss 6.52281189\n",
      "Trained batch 1240 batch loss 6.16147709 epoch total loss 6.52252054\n",
      "Trained batch 1241 batch loss 6.78512907 epoch total loss 6.52273226\n",
      "Trained batch 1242 batch loss 6.39618206 epoch total loss 6.52263\n",
      "Trained batch 1243 batch loss 7.47917461 epoch total loss 6.52339935\n",
      "Trained batch 1244 batch loss 6.85195112 epoch total loss 6.52366352\n",
      "Trained batch 1245 batch loss 7.2039237 epoch total loss 6.52421\n",
      "Trained batch 1246 batch loss 6.60947514 epoch total loss 6.52427864\n",
      "Trained batch 1247 batch loss 6.55923367 epoch total loss 6.5243063\n",
      "Trained batch 1248 batch loss 6.81986046 epoch total loss 6.52454329\n",
      "Trained batch 1249 batch loss 6.52118683 epoch total loss 6.52454042\n",
      "Trained batch 1250 batch loss 6.46668339 epoch total loss 6.52449417\n",
      "Trained batch 1251 batch loss 6.51080656 epoch total loss 6.5244832\n",
      "Trained batch 1252 batch loss 6.75899458 epoch total loss 6.52467\n",
      "Trained batch 1253 batch loss 7.18245745 epoch total loss 6.5251956\n",
      "Trained batch 1254 batch loss 6.78790951 epoch total loss 6.52540493\n",
      "Trained batch 1255 batch loss 6.42642975 epoch total loss 6.52532625\n",
      "Trained batch 1256 batch loss 6.3456316 epoch total loss 6.5251832\n",
      "Trained batch 1257 batch loss 6.42296791 epoch total loss 6.52510166\n",
      "Trained batch 1258 batch loss 6.36808968 epoch total loss 6.52497673\n",
      "Trained batch 1259 batch loss 6.41382074 epoch total loss 6.52488899\n",
      "Trained batch 1260 batch loss 6.50393152 epoch total loss 6.5248723\n",
      "Trained batch 1261 batch loss 6.70143461 epoch total loss 6.52501202\n",
      "Trained batch 1262 batch loss 6.27498198 epoch total loss 6.52481413\n",
      "Trained batch 1263 batch loss 5.74801207 epoch total loss 6.52419901\n",
      "Trained batch 1264 batch loss 5.75385 epoch total loss 6.52358961\n",
      "Trained batch 1265 batch loss 6.6927309 epoch total loss 6.52372313\n",
      "Trained batch 1266 batch loss 6.78706217 epoch total loss 6.52393103\n",
      "Trained batch 1267 batch loss 6.72071934 epoch total loss 6.52408648\n",
      "Trained batch 1268 batch loss 7.15841055 epoch total loss 6.52458668\n",
      "Trained batch 1269 batch loss 7.02400875 epoch total loss 6.52498055\n",
      "Trained batch 1270 batch loss 6.76385403 epoch total loss 6.52516842\n",
      "Trained batch 1271 batch loss 6.94991493 epoch total loss 6.52550268\n",
      "Trained batch 1272 batch loss 6.80899906 epoch total loss 6.52572536\n",
      "Trained batch 1273 batch loss 6.39950371 epoch total loss 6.52562618\n",
      "Trained batch 1274 batch loss 6.71999168 epoch total loss 6.52577829\n",
      "Trained batch 1275 batch loss 6.71017933 epoch total loss 6.52592278\n",
      "Trained batch 1276 batch loss 6.61056471 epoch total loss 6.52598906\n",
      "Trained batch 1277 batch loss 6.96973658 epoch total loss 6.52633667\n",
      "Trained batch 1278 batch loss 6.50503206 epoch total loss 6.52632\n",
      "Trained batch 1279 batch loss 6.46488714 epoch total loss 6.52627182\n",
      "Trained batch 1280 batch loss 6.77328 epoch total loss 6.52646494\n",
      "Trained batch 1281 batch loss 6.26518393 epoch total loss 6.52626133\n",
      "Trained batch 1282 batch loss 6.65669584 epoch total loss 6.52636242\n",
      "Trained batch 1283 batch loss 6.84123421 epoch total loss 6.52660751\n",
      "Trained batch 1284 batch loss 6.67373085 epoch total loss 6.52672243\n",
      "Trained batch 1285 batch loss 6.56932402 epoch total loss 6.52675533\n",
      "Trained batch 1286 batch loss 6.67343712 epoch total loss 6.52687\n",
      "Trained batch 1287 batch loss 6.53706026 epoch total loss 6.52687788\n",
      "Trained batch 1288 batch loss 6.68785286 epoch total loss 6.52700233\n",
      "Trained batch 1289 batch loss 6.71716452 epoch total loss 6.52714968\n",
      "Trained batch 1290 batch loss 6.77403688 epoch total loss 6.52734137\n",
      "Trained batch 1291 batch loss 6.85852861 epoch total loss 6.5275979\n",
      "Trained batch 1292 batch loss 7.14911938 epoch total loss 6.52807903\n",
      "Trained batch 1293 batch loss 7.01478243 epoch total loss 6.52845573\n",
      "Trained batch 1294 batch loss 6.92285776 epoch total loss 6.52876043\n",
      "Trained batch 1295 batch loss 7.04051924 epoch total loss 6.52915525\n",
      "Trained batch 1296 batch loss 6.8964119 epoch total loss 6.5294385\n",
      "Trained batch 1297 batch loss 6.62593031 epoch total loss 6.52951288\n",
      "Trained batch 1298 batch loss 6.84100199 epoch total loss 6.52975273\n",
      "Trained batch 1299 batch loss 6.63378 epoch total loss 6.52983284\n",
      "Trained batch 1300 batch loss 6.87833214 epoch total loss 6.53010082\n",
      "Trained batch 1301 batch loss 6.58251572 epoch total loss 6.5301404\n",
      "Trained batch 1302 batch loss 6.9593 epoch total loss 6.53047\n",
      "Trained batch 1303 batch loss 6.82858706 epoch total loss 6.5306983\n",
      "Trained batch 1304 batch loss 6.96095 epoch total loss 6.53102827\n",
      "Trained batch 1305 batch loss 6.88406897 epoch total loss 6.53129864\n",
      "Trained batch 1306 batch loss 6.42818832 epoch total loss 6.53121948\n",
      "Trained batch 1307 batch loss 6.53757954 epoch total loss 6.53122377\n",
      "Trained batch 1308 batch loss 6.68697548 epoch total loss 6.53134251\n",
      "Trained batch 1309 batch loss 6.80706167 epoch total loss 6.53155279\n",
      "Trained batch 1310 batch loss 6.952034 epoch total loss 6.53187418\n",
      "Trained batch 1311 batch loss 7.21845293 epoch total loss 6.53239775\n",
      "Trained batch 1312 batch loss 7.0409708 epoch total loss 6.53278542\n",
      "Trained batch 1313 batch loss 6.83513832 epoch total loss 6.53301573\n",
      "Trained batch 1314 batch loss 6.90993643 epoch total loss 6.53330278\n",
      "Trained batch 1315 batch loss 6.487463 epoch total loss 6.5332675\n",
      "Trained batch 1316 batch loss 6.58073282 epoch total loss 6.53330421\n",
      "Trained batch 1317 batch loss 6.57832909 epoch total loss 6.53333807\n",
      "Trained batch 1318 batch loss 6.63054895 epoch total loss 6.53341198\n",
      "Trained batch 1319 batch loss 6.90000343 epoch total loss 6.53369045\n",
      "Trained batch 1320 batch loss 6.65085649 epoch total loss 6.53377867\n",
      "Trained batch 1321 batch loss 6.72471046 epoch total loss 6.53392315\n",
      "Trained batch 1322 batch loss 6.38662434 epoch total loss 6.53381205\n",
      "Trained batch 1323 batch loss 6.82564211 epoch total loss 6.53403187\n",
      "Trained batch 1324 batch loss 6.50477362 epoch total loss 6.53401\n",
      "Trained batch 1325 batch loss 6.40292883 epoch total loss 6.53391123\n",
      "Trained batch 1326 batch loss 6.18793535 epoch total loss 6.5336504\n",
      "Trained batch 1327 batch loss 6.03068304 epoch total loss 6.53327084\n",
      "Trained batch 1328 batch loss 6.17988729 epoch total loss 6.53300476\n",
      "Trained batch 1329 batch loss 6.55710793 epoch total loss 6.5330224\n",
      "Trained batch 1330 batch loss 5.65622711 epoch total loss 6.53236294\n",
      "Trained batch 1331 batch loss 5.940835 epoch total loss 6.53191853\n",
      "Trained batch 1332 batch loss 5.68316555 epoch total loss 6.53128147\n",
      "Trained batch 1333 batch loss 5.83131266 epoch total loss 6.530756\n",
      "Trained batch 1334 batch loss 6.30101252 epoch total loss 6.53058386\n",
      "Trained batch 1335 batch loss 5.60876274 epoch total loss 6.52989292\n",
      "Trained batch 1336 batch loss 6.2693429 epoch total loss 6.52969837\n",
      "Trained batch 1337 batch loss 6.01775 epoch total loss 6.52931499\n",
      "Trained batch 1338 batch loss 6.40800905 epoch total loss 6.5292244\n",
      "Trained batch 1339 batch loss 6.16412878 epoch total loss 6.52895212\n",
      "Trained batch 1340 batch loss 5.98049402 epoch total loss 6.52854252\n",
      "Trained batch 1341 batch loss 6.32297659 epoch total loss 6.52838945\n",
      "Trained batch 1342 batch loss 6.42604351 epoch total loss 6.52831316\n",
      "Trained batch 1343 batch loss 6.91067648 epoch total loss 6.52859831\n",
      "Trained batch 1344 batch loss 7.02592278 epoch total loss 6.52896833\n",
      "Trained batch 1345 batch loss 6.75281239 epoch total loss 6.52913475\n",
      "Trained batch 1346 batch loss 6.68027878 epoch total loss 6.52924776\n",
      "Trained batch 1347 batch loss 6.49875212 epoch total loss 6.52922487\n",
      "Trained batch 1348 batch loss 6.54685831 epoch total loss 6.52923822\n",
      "Trained batch 1349 batch loss 6.64723873 epoch total loss 6.52932596\n",
      "Trained batch 1350 batch loss 6.39762068 epoch total loss 6.52922821\n",
      "Trained batch 1351 batch loss 6.65686703 epoch total loss 6.5293231\n",
      "Trained batch 1352 batch loss 6.21888542 epoch total loss 6.52909327\n",
      "Trained batch 1353 batch loss 6.29214096 epoch total loss 6.52891779\n",
      "Trained batch 1354 batch loss 6.85814857 epoch total loss 6.52916145\n",
      "Trained batch 1355 batch loss 6.81011963 epoch total loss 6.52936888\n",
      "Trained batch 1356 batch loss 6.87227345 epoch total loss 6.5296216\n",
      "Trained batch 1357 batch loss 6.91298 epoch total loss 6.52990437\n",
      "Trained batch 1358 batch loss 6.85955191 epoch total loss 6.53014708\n",
      "Trained batch 1359 batch loss 6.87137127 epoch total loss 6.53039789\n",
      "Trained batch 1360 batch loss 6.9521842 epoch total loss 6.53070784\n",
      "Trained batch 1361 batch loss 6.70790577 epoch total loss 6.53083801\n",
      "Trained batch 1362 batch loss 6.63931608 epoch total loss 6.53091812\n",
      "Trained batch 1363 batch loss 7.0224185 epoch total loss 6.53127861\n",
      "Trained batch 1364 batch loss 6.79026699 epoch total loss 6.53146839\n",
      "Trained batch 1365 batch loss 6.87484026 epoch total loss 6.53172\n",
      "Trained batch 1366 batch loss 6.95437527 epoch total loss 6.53202915\n",
      "Trained batch 1367 batch loss 7.37793064 epoch total loss 6.53264809\n",
      "Trained batch 1368 batch loss 6.83819771 epoch total loss 6.53287125\n",
      "Trained batch 1369 batch loss 6.37943554 epoch total loss 6.53275919\n",
      "Trained batch 1370 batch loss 6.76494551 epoch total loss 6.53292847\n",
      "Trained batch 1371 batch loss 6.82740688 epoch total loss 6.53314352\n",
      "Trained batch 1372 batch loss 6.46933889 epoch total loss 6.53309727\n",
      "Trained batch 1373 batch loss 6.70852566 epoch total loss 6.53322506\n",
      "Trained batch 1374 batch loss 6.65314388 epoch total loss 6.5333128\n",
      "Trained batch 1375 batch loss 6.37738609 epoch total loss 6.53319883\n",
      "Trained batch 1376 batch loss 6.70068359 epoch total loss 6.5333209\n",
      "Trained batch 1377 batch loss 6.67365074 epoch total loss 6.53342295\n",
      "Trained batch 1378 batch loss 6.62774134 epoch total loss 6.53349161\n",
      "Trained batch 1379 batch loss 6.69070148 epoch total loss 6.53360558\n",
      "Trained batch 1380 batch loss 6.54846239 epoch total loss 6.53361654\n",
      "Trained batch 1381 batch loss 6.76123238 epoch total loss 6.53378153\n",
      "Trained batch 1382 batch loss 6.7883029 epoch total loss 6.53396559\n",
      "Trained batch 1383 batch loss 6.65786 epoch total loss 6.53405523\n",
      "Trained batch 1384 batch loss 6.31639194 epoch total loss 6.53389835\n",
      "Trained batch 1385 batch loss 6.30750132 epoch total loss 6.5337348\n",
      "Trained batch 1386 batch loss 6.21463633 epoch total loss 6.53350449\n",
      "Trained batch 1387 batch loss 6.41871405 epoch total loss 6.53342199\n",
      "Trained batch 1388 batch loss 6.55694962 epoch total loss 6.53343868\n",
      "Epoch 7 train loss 6.533438682556152\n",
      "Validated batch 1 batch loss 6.34473467\n",
      "Validated batch 2 batch loss 6.46434975\n",
      "Validated batch 3 batch loss 6.03642607\n",
      "Validated batch 4 batch loss 6.20724821\n",
      "Validated batch 5 batch loss 6.34037876\n",
      "Validated batch 6 batch loss 6.47010088\n",
      "Validated batch 7 batch loss 6.4682045\n",
      "Validated batch 8 batch loss 6.50939512\n",
      "Validated batch 9 batch loss 6.58789396\n",
      "Validated batch 10 batch loss 6.67853\n",
      "Validated batch 11 batch loss 6.49612713\n",
      "Validated batch 12 batch loss 6.51977873\n",
      "Validated batch 13 batch loss 6.67849207\n",
      "Validated batch 14 batch loss 6.35838652\n",
      "Validated batch 15 batch loss 6.73180103\n",
      "Validated batch 16 batch loss 6.76737309\n",
      "Validated batch 17 batch loss 6.45170164\n",
      "Validated batch 18 batch loss 6.77816296\n",
      "Validated batch 19 batch loss 6.42996597\n",
      "Validated batch 20 batch loss 6.30764484\n",
      "Validated batch 21 batch loss 6.39741\n",
      "Validated batch 22 batch loss 6.65594196\n",
      "Validated batch 23 batch loss 6.71357203\n",
      "Validated batch 24 batch loss 6.64364195\n",
      "Validated batch 25 batch loss 6.8916955\n",
      "Validated batch 26 batch loss 6.41842699\n",
      "Validated batch 27 batch loss 5.89622259\n",
      "Validated batch 28 batch loss 6.35148191\n",
      "Validated batch 29 batch loss 6.55196619\n",
      "Validated batch 30 batch loss 6.05221033\n",
      "Validated batch 31 batch loss 6.26492786\n",
      "Validated batch 32 batch loss 6.22301817\n",
      "Validated batch 33 batch loss 6.55372143\n",
      "Validated batch 34 batch loss 6.24355412\n",
      "Validated batch 35 batch loss 6.11692238\n",
      "Validated batch 36 batch loss 6.37341642\n",
      "Validated batch 37 batch loss 6.4087882\n",
      "Validated batch 38 batch loss 6.39467859\n",
      "Validated batch 39 batch loss 6.44705534\n",
      "Validated batch 40 batch loss 6.48039293\n",
      "Validated batch 41 batch loss 6.61838055\n",
      "Validated batch 42 batch loss 6.45463037\n",
      "Validated batch 43 batch loss 6.30407286\n",
      "Validated batch 44 batch loss 6.74682951\n",
      "Validated batch 45 batch loss 5.67436\n",
      "Validated batch 46 batch loss 6.93593168\n",
      "Validated batch 47 batch loss 6.58542967\n",
      "Validated batch 48 batch loss 6.46189928\n",
      "Validated batch 49 batch loss 6.25122261\n",
      "Validated batch 50 batch loss 6.30110455\n",
      "Validated batch 51 batch loss 6.64276886\n",
      "Validated batch 52 batch loss 6.44352961\n",
      "Validated batch 53 batch loss 6.54992151\n",
      "Validated batch 54 batch loss 6.39099932\n",
      "Validated batch 55 batch loss 6.39531946\n",
      "Validated batch 56 batch loss 6.81516504\n",
      "Validated batch 57 batch loss 6.82713413\n",
      "Validated batch 58 batch loss 6.58414555\n",
      "Validated batch 59 batch loss 6.54884434\n",
      "Validated batch 60 batch loss 6.41591644\n",
      "Validated batch 61 batch loss 6.55563831\n",
      "Validated batch 62 batch loss 6.46062469\n",
      "Validated batch 63 batch loss 6.63874292\n",
      "Validated batch 64 batch loss 6.45351171\n",
      "Validated batch 65 batch loss 6.27624846\n",
      "Validated batch 66 batch loss 6.6234622\n",
      "Validated batch 67 batch loss 6.28104448\n",
      "Validated batch 68 batch loss 6.7386508\n",
      "Validated batch 69 batch loss 6.76334095\n",
      "Validated batch 70 batch loss 6.157166\n",
      "Validated batch 71 batch loss 6.91426945\n",
      "Validated batch 72 batch loss 6.52608871\n",
      "Validated batch 73 batch loss 6.71324492\n",
      "Validated batch 74 batch loss 6.53820133\n",
      "Validated batch 75 batch loss 6.83365154\n",
      "Validated batch 76 batch loss 6.33958673\n",
      "Validated batch 77 batch loss 6.8540225\n",
      "Validated batch 78 batch loss 6.32348919\n",
      "Validated batch 79 batch loss 6.63149357\n",
      "Validated batch 80 batch loss 6.59926939\n",
      "Validated batch 81 batch loss 6.16495\n",
      "Validated batch 82 batch loss 5.83315039\n",
      "Validated batch 83 batch loss 6.764081\n",
      "Validated batch 84 batch loss 6.66257954\n",
      "Validated batch 85 batch loss 6.33427191\n",
      "Validated batch 86 batch loss 6.60010242\n",
      "Validated batch 87 batch loss 6.67928553\n",
      "Validated batch 88 batch loss 6.63552094\n",
      "Validated batch 89 batch loss 6.74006462\n",
      "Validated batch 90 batch loss 6.58335972\n",
      "Validated batch 91 batch loss 6.45934\n",
      "Validated batch 92 batch loss 6.20344114\n",
      "Validated batch 93 batch loss 6.86576176\n",
      "Validated batch 94 batch loss 6.62607861\n",
      "Validated batch 95 batch loss 6.63994312\n",
      "Validated batch 96 batch loss 6.47980595\n",
      "Validated batch 97 batch loss 6.552279\n",
      "Validated batch 98 batch loss 6.69408512\n",
      "Validated batch 99 batch loss 6.42045116\n",
      "Validated batch 100 batch loss 6.27321196\n",
      "Validated batch 101 batch loss 6.29063\n",
      "Validated batch 102 batch loss 6.4499774\n",
      "Validated batch 103 batch loss 6.5418582\n",
      "Validated batch 104 batch loss 6.48024511\n",
      "Validated batch 105 batch loss 6.2841115\n",
      "Validated batch 106 batch loss 6.22457218\n",
      "Validated batch 107 batch loss 6.33970976\n",
      "Validated batch 108 batch loss 6.60768366\n",
      "Validated batch 109 batch loss 6.66839123\n",
      "Validated batch 110 batch loss 6.75035334\n",
      "Validated batch 111 batch loss 6.99869108\n",
      "Validated batch 112 batch loss 7.37269831\n",
      "Validated batch 113 batch loss 7.02222443\n",
      "Validated batch 114 batch loss 6.47801542\n",
      "Validated batch 115 batch loss 6.21391821\n",
      "Validated batch 116 batch loss 6.50884819\n",
      "Validated batch 117 batch loss 6.55233479\n",
      "Validated batch 118 batch loss 6.36201811\n",
      "Validated batch 119 batch loss 6.25637388\n",
      "Validated batch 120 batch loss 6.34910917\n",
      "Validated batch 121 batch loss 6.55342436\n",
      "Validated batch 122 batch loss 6.21491718\n",
      "Validated batch 123 batch loss 6.50645733\n",
      "Validated batch 124 batch loss 6.58636713\n",
      "Validated batch 125 batch loss 6.53251839\n",
      "Validated batch 126 batch loss 6.35239935\n",
      "Validated batch 127 batch loss 6.19604206\n",
      "Validated batch 128 batch loss 6.39402819\n",
      "Validated batch 129 batch loss 6.83428764\n",
      "Validated batch 130 batch loss 6.38936329\n",
      "Validated batch 131 batch loss 6.4160924\n",
      "Validated batch 132 batch loss 6.38006401\n",
      "Validated batch 133 batch loss 6.14467049\n",
      "Validated batch 134 batch loss 5.9950366\n",
      "Validated batch 135 batch loss 6.53071117\n",
      "Validated batch 136 batch loss 6.25808954\n",
      "Validated batch 137 batch loss 6.29981613\n",
      "Validated batch 138 batch loss 6.56386805\n",
      "Validated batch 139 batch loss 6.53272486\n",
      "Validated batch 140 batch loss 6.62484217\n",
      "Validated batch 141 batch loss 6.51300573\n",
      "Validated batch 142 batch loss 6.52707291\n",
      "Validated batch 143 batch loss 6.9112711\n",
      "Validated batch 144 batch loss 6.60809469\n",
      "Validated batch 145 batch loss 6.66035557\n",
      "Validated batch 146 batch loss 6.52050352\n",
      "Validated batch 147 batch loss 6.72027826\n",
      "Validated batch 148 batch loss 6.57677174\n",
      "Validated batch 149 batch loss 6.85205412\n",
      "Validated batch 150 batch loss 6.98563623\n",
      "Validated batch 151 batch loss 6.1730423\n",
      "Validated batch 152 batch loss 6.70316029\n",
      "Validated batch 153 batch loss 6.46774817\n",
      "Validated batch 154 batch loss 6.59485197\n",
      "Validated batch 155 batch loss 6.70649242\n",
      "Validated batch 156 batch loss 6.15934324\n",
      "Validated batch 157 batch loss 6.2566123\n",
      "Validated batch 158 batch loss 6.63841486\n",
      "Validated batch 159 batch loss 6.43200922\n",
      "Validated batch 160 batch loss 6.91378498\n",
      "Validated batch 161 batch loss 6.43925714\n",
      "Validated batch 162 batch loss 6.51706743\n",
      "Validated batch 163 batch loss 6.12363768\n",
      "Validated batch 164 batch loss 6.51414824\n",
      "Validated batch 165 batch loss 6.42783689\n",
      "Validated batch 166 batch loss 6.18809319\n",
      "Validated batch 167 batch loss 6.77032948\n",
      "Validated batch 168 batch loss 6.55602598\n",
      "Validated batch 169 batch loss 6.37117338\n",
      "Validated batch 170 batch loss 6.69637632\n",
      "Validated batch 171 batch loss 6.77715635\n",
      "Validated batch 172 batch loss 6.3646822\n",
      "Validated batch 173 batch loss 6.46651745\n",
      "Validated batch 174 batch loss 6.42089176\n",
      "Validated batch 175 batch loss 6.62382841\n",
      "Validated batch 176 batch loss 6.77380085\n",
      "Validated batch 177 batch loss 6.5461297\n",
      "Validated batch 178 batch loss 6.32559252\n",
      "Validated batch 179 batch loss 6.28092384\n",
      "Validated batch 180 batch loss 6.40886736\n",
      "Validated batch 181 batch loss 6.51940632\n",
      "Validated batch 182 batch loss 6.63349438\n",
      "Validated batch 183 batch loss 6.48374557\n",
      "Validated batch 184 batch loss 6.40737438\n",
      "Validated batch 185 batch loss 3.31427193\n",
      "Epoch 7 val loss 6.481165409088135\n",
      "Epoch 7 completed in 728.48 seconds\n",
      "Model ./model_simplebase-epoch-7-loss-6.4812.h5 saved.\n",
      "Start epoch 8 with learning rate 0.0007\n",
      "Trained batch 1 batch loss 6.72794437 epoch total loss 6.72794437\n",
      "Trained batch 2 batch loss 6.43485832 epoch total loss 6.58140135\n",
      "Trained batch 3 batch loss 6.69173717 epoch total loss 6.61818\n",
      "Trained batch 4 batch loss 6.52565813 epoch total loss 6.59504938\n",
      "Trained batch 5 batch loss 6.78450632 epoch total loss 6.63294077\n",
      "Trained batch 6 batch loss 6.77829695 epoch total loss 6.65716696\n",
      "Trained batch 7 batch loss 6.54581451 epoch total loss 6.64125919\n",
      "Trained batch 8 batch loss 6.21910143 epoch total loss 6.58848953\n",
      "Trained batch 9 batch loss 6.6866374 epoch total loss 6.5993948\n",
      "Trained batch 10 batch loss 6.71028376 epoch total loss 6.61048365\n",
      "Trained batch 11 batch loss 6.59254456 epoch total loss 6.60885286\n",
      "Trained batch 12 batch loss 6.45163774 epoch total loss 6.59575129\n",
      "Trained batch 13 batch loss 6.28888464 epoch total loss 6.57214642\n",
      "Trained batch 14 batch loss 6.25879 epoch total loss 6.54976368\n",
      "Trained batch 15 batch loss 6.59300756 epoch total loss 6.55264711\n",
      "Trained batch 16 batch loss 6.56959152 epoch total loss 6.55370617\n",
      "Trained batch 17 batch loss 6.47605324 epoch total loss 6.54913807\n",
      "Trained batch 18 batch loss 6.4710722 epoch total loss 6.54480124\n",
      "Trained batch 19 batch loss 6.56863689 epoch total loss 6.54605532\n",
      "Trained batch 20 batch loss 6.48337269 epoch total loss 6.54292154\n",
      "Trained batch 21 batch loss 6.08936071 epoch total loss 6.5213232\n",
      "Trained batch 22 batch loss 6.43383694 epoch total loss 6.51734638\n",
      "Trained batch 23 batch loss 6.65369892 epoch total loss 6.5232749\n",
      "Trained batch 24 batch loss 6.3967495 epoch total loss 6.51800299\n",
      "Trained batch 25 batch loss 6.20150805 epoch total loss 6.50534296\n",
      "Trained batch 26 batch loss 6.35429621 epoch total loss 6.49953365\n",
      "Trained batch 27 batch loss 6.71364546 epoch total loss 6.50746346\n",
      "Trained batch 28 batch loss 6.96177769 epoch total loss 6.52368879\n",
      "Trained batch 29 batch loss 7.02077 epoch total loss 6.54082918\n",
      "Trained batch 30 batch loss 7.17469692 epoch total loss 6.56195831\n",
      "Trained batch 31 batch loss 7.02022123 epoch total loss 6.57674074\n",
      "Trained batch 32 batch loss 6.49059248 epoch total loss 6.57404852\n",
      "Trained batch 33 batch loss 6.80901814 epoch total loss 6.58116913\n",
      "Trained batch 34 batch loss 6.35884762 epoch total loss 6.57463\n",
      "Trained batch 35 batch loss 6.73148108 epoch total loss 6.5791111\n",
      "Trained batch 36 batch loss 6.26583195 epoch total loss 6.5704093\n",
      "Trained batch 37 batch loss 6.72891665 epoch total loss 6.5746932\n",
      "Trained batch 38 batch loss 7.00443792 epoch total loss 6.58600235\n",
      "Trained batch 39 batch loss 6.92850828 epoch total loss 6.59478426\n",
      "Trained batch 40 batch loss 7.09797478 epoch total loss 6.6073637\n",
      "Trained batch 41 batch loss 5.95095062 epoch total loss 6.59135389\n",
      "Trained batch 42 batch loss 6.47810268 epoch total loss 6.58865738\n",
      "Trained batch 43 batch loss 6.37338972 epoch total loss 6.58365059\n",
      "Trained batch 44 batch loss 6.74739599 epoch total loss 6.5873723\n",
      "Trained batch 45 batch loss 6.86697626 epoch total loss 6.59358597\n",
      "Trained batch 46 batch loss 6.79883 epoch total loss 6.59804773\n",
      "Trained batch 47 batch loss 6.99558115 epoch total loss 6.60650587\n",
      "Trained batch 48 batch loss 6.49847651 epoch total loss 6.6042552\n",
      "Trained batch 49 batch loss 6.76008606 epoch total loss 6.6074357\n",
      "Trained batch 50 batch loss 6.71223545 epoch total loss 6.60953188\n",
      "Trained batch 51 batch loss 6.76701975 epoch total loss 6.61262\n",
      "Trained batch 52 batch loss 7.09067345 epoch total loss 6.6218133\n",
      "Trained batch 53 batch loss 7.08248234 epoch total loss 6.63050508\n",
      "Trained batch 54 batch loss 6.98223829 epoch total loss 6.63701868\n",
      "Trained batch 55 batch loss 6.94192743 epoch total loss 6.64256239\n",
      "Trained batch 56 batch loss 6.78807831 epoch total loss 6.64516115\n",
      "Trained batch 57 batch loss 6.79292059 epoch total loss 6.64775324\n",
      "Trained batch 58 batch loss 6.23309278 epoch total loss 6.64060402\n",
      "Trained batch 59 batch loss 6.46336794 epoch total loss 6.6376\n",
      "Trained batch 60 batch loss 6.85819292 epoch total loss 6.64127636\n",
      "Trained batch 61 batch loss 6.73093843 epoch total loss 6.64274645\n",
      "Trained batch 62 batch loss 6.86988163 epoch total loss 6.64640951\n",
      "Trained batch 63 batch loss 6.74007416 epoch total loss 6.64789629\n",
      "Trained batch 64 batch loss 6.60263681 epoch total loss 6.64718914\n",
      "Trained batch 65 batch loss 6.58840513 epoch total loss 6.64628506\n",
      "Trained batch 66 batch loss 6.50548077 epoch total loss 6.64415169\n",
      "Trained batch 67 batch loss 6.48365498 epoch total loss 6.64175606\n",
      "Trained batch 68 batch loss 6.38254642 epoch total loss 6.63794374\n",
      "Trained batch 69 batch loss 6.64511919 epoch total loss 6.6380477\n",
      "Trained batch 70 batch loss 6.02680922 epoch total loss 6.62931585\n",
      "Trained batch 71 batch loss 6.34847403 epoch total loss 6.62536\n",
      "Trained batch 72 batch loss 6.40698814 epoch total loss 6.62232733\n",
      "Trained batch 73 batch loss 6.28088379 epoch total loss 6.61765\n",
      "Trained batch 74 batch loss 6.46840239 epoch total loss 6.61563301\n",
      "Trained batch 75 batch loss 5.84836817 epoch total loss 6.60540295\n",
      "Trained batch 76 batch loss 6.22815752 epoch total loss 6.60043907\n",
      "Trained batch 77 batch loss 6.65727091 epoch total loss 6.60117674\n",
      "Trained batch 78 batch loss 6.85867548 epoch total loss 6.60447788\n",
      "Trained batch 79 batch loss 6.6350112 epoch total loss 6.6048646\n",
      "Trained batch 80 batch loss 6.28341055 epoch total loss 6.60084629\n",
      "Trained batch 81 batch loss 6.08775 epoch total loss 6.59451199\n",
      "Trained batch 82 batch loss 6.63314772 epoch total loss 6.59498262\n",
      "Trained batch 83 batch loss 6.62824249 epoch total loss 6.59538317\n",
      "Trained batch 84 batch loss 6.1641717 epoch total loss 6.59025\n",
      "Trained batch 85 batch loss 6.58345 epoch total loss 6.59017\n",
      "Trained batch 86 batch loss 6.46000576 epoch total loss 6.58865643\n",
      "Trained batch 87 batch loss 6.44393969 epoch total loss 6.58699322\n",
      "Trained batch 88 batch loss 6.53286266 epoch total loss 6.5863781\n",
      "Trained batch 89 batch loss 6.28576326 epoch total loss 6.583\n",
      "Trained batch 90 batch loss 5.57739115 epoch total loss 6.57182693\n",
      "Trained batch 91 batch loss 6.04862261 epoch total loss 6.56607771\n",
      "Trained batch 92 batch loss 6.22909737 epoch total loss 6.56241512\n",
      "Trained batch 93 batch loss 6.0507741 epoch total loss 6.55691385\n",
      "Trained batch 94 batch loss 6.15412521 epoch total loss 6.55262852\n",
      "Trained batch 95 batch loss 6.49047661 epoch total loss 6.5519743\n",
      "Trained batch 96 batch loss 6.37448788 epoch total loss 6.5501256\n",
      "Trained batch 97 batch loss 6.37035561 epoch total loss 6.54827261\n",
      "Trained batch 98 batch loss 6.79032516 epoch total loss 6.55074263\n",
      "Trained batch 99 batch loss 6.46750879 epoch total loss 6.54990196\n",
      "Trained batch 100 batch loss 6.6531291 epoch total loss 6.55093431\n",
      "Trained batch 101 batch loss 6.63420343 epoch total loss 6.55175924\n",
      "Trained batch 102 batch loss 5.10281372 epoch total loss 6.53755331\n",
      "Trained batch 103 batch loss 5.62746525 epoch total loss 6.52871752\n",
      "Trained batch 104 batch loss 5.76675272 epoch total loss 6.52139044\n",
      "Trained batch 105 batch loss 7.01575756 epoch total loss 6.52609873\n",
      "Trained batch 106 batch loss 6.90209436 epoch total loss 6.52964592\n",
      "Trained batch 107 batch loss 7.34258556 epoch total loss 6.53724337\n",
      "Trained batch 108 batch loss 6.55866623 epoch total loss 6.53744173\n",
      "Trained batch 109 batch loss 6.33693552 epoch total loss 6.53560209\n",
      "Trained batch 110 batch loss 6.34469414 epoch total loss 6.53386641\n",
      "Trained batch 111 batch loss 6.85876894 epoch total loss 6.53679323\n",
      "Trained batch 112 batch loss 6.68611097 epoch total loss 6.53812647\n",
      "Trained batch 113 batch loss 6.79389858 epoch total loss 6.54038954\n",
      "Trained batch 114 batch loss 6.69169903 epoch total loss 6.54171705\n",
      "Trained batch 115 batch loss 6.69011593 epoch total loss 6.54300737\n",
      "Trained batch 116 batch loss 6.49115562 epoch total loss 6.54256058\n",
      "Trained batch 117 batch loss 6.72845745 epoch total loss 6.5441494\n",
      "Trained batch 118 batch loss 6.66469812 epoch total loss 6.54517078\n",
      "Trained batch 119 batch loss 5.95666885 epoch total loss 6.54022503\n",
      "Trained batch 120 batch loss 6.38334608 epoch total loss 6.53891802\n",
      "Trained batch 121 batch loss 6.36839104 epoch total loss 6.53750896\n",
      "Trained batch 122 batch loss 6.66702414 epoch total loss 6.53857088\n",
      "Trained batch 123 batch loss 6.92538786 epoch total loss 6.54171562\n",
      "Trained batch 124 batch loss 6.59246635 epoch total loss 6.54212523\n",
      "Trained batch 125 batch loss 6.63413048 epoch total loss 6.54286146\n",
      "Trained batch 126 batch loss 6.47748899 epoch total loss 6.54234219\n",
      "Trained batch 127 batch loss 6.31944 epoch total loss 6.54058743\n",
      "Trained batch 128 batch loss 7.01357746 epoch total loss 6.54428244\n",
      "Trained batch 129 batch loss 6.66414 epoch total loss 6.54521132\n",
      "Trained batch 130 batch loss 6.48712349 epoch total loss 6.54476452\n",
      "Trained batch 131 batch loss 6.80682564 epoch total loss 6.54676485\n",
      "Trained batch 132 batch loss 7.00481558 epoch total loss 6.55023527\n",
      "Trained batch 133 batch loss 6.8448782 epoch total loss 6.55245\n",
      "Trained batch 134 batch loss 6.97640562 epoch total loss 6.55561399\n",
      "Trained batch 135 batch loss 6.23396873 epoch total loss 6.55323124\n",
      "Trained batch 136 batch loss 6.51129103 epoch total loss 6.55292273\n",
      "Trained batch 137 batch loss 6.76333809 epoch total loss 6.5544591\n",
      "Trained batch 138 batch loss 6.72306108 epoch total loss 6.55568075\n",
      "Trained batch 139 batch loss 6.53718138 epoch total loss 6.55554771\n",
      "Trained batch 140 batch loss 6.53798771 epoch total loss 6.55542231\n",
      "Trained batch 141 batch loss 6.53789043 epoch total loss 6.55529785\n",
      "Trained batch 142 batch loss 6.57777 epoch total loss 6.55545616\n",
      "Trained batch 143 batch loss 6.52230597 epoch total loss 6.55522394\n",
      "Trained batch 144 batch loss 6.62444353 epoch total loss 6.55570459\n",
      "Trained batch 145 batch loss 6.62010431 epoch total loss 6.55614901\n",
      "Trained batch 146 batch loss 6.89196 epoch total loss 6.55844927\n",
      "Trained batch 147 batch loss 6.23176479 epoch total loss 6.55622673\n",
      "Trained batch 148 batch loss 6.62888861 epoch total loss 6.55671787\n",
      "Trained batch 149 batch loss 6.26958704 epoch total loss 6.5547905\n",
      "Trained batch 150 batch loss 6.14177513 epoch total loss 6.55203724\n",
      "Trained batch 151 batch loss 5.85174656 epoch total loss 6.54739952\n",
      "Trained batch 152 batch loss 6.06382084 epoch total loss 6.54421854\n",
      "Trained batch 153 batch loss 6.14894342 epoch total loss 6.54163456\n",
      "Trained batch 154 batch loss 6.51550436 epoch total loss 6.54146528\n",
      "Trained batch 155 batch loss 6.21114349 epoch total loss 6.53933382\n",
      "Trained batch 156 batch loss 6.38541269 epoch total loss 6.53834724\n",
      "Trained batch 157 batch loss 6.32595778 epoch total loss 6.53699446\n",
      "Trained batch 158 batch loss 6.57341909 epoch total loss 6.53722429\n",
      "Trained batch 159 batch loss 6.75365353 epoch total loss 6.53858566\n",
      "Trained batch 160 batch loss 6.61622381 epoch total loss 6.53907108\n",
      "Trained batch 161 batch loss 6.49066639 epoch total loss 6.53877068\n",
      "Trained batch 162 batch loss 6.58730364 epoch total loss 6.53907\n",
      "Trained batch 163 batch loss 6.21412802 epoch total loss 6.53707647\n",
      "Trained batch 164 batch loss 6.41860056 epoch total loss 6.53635406\n",
      "Trained batch 165 batch loss 6.18809175 epoch total loss 6.53424311\n",
      "Trained batch 166 batch loss 6.37325096 epoch total loss 6.5332737\n",
      "Trained batch 167 batch loss 6.70155811 epoch total loss 6.53428125\n",
      "Trained batch 168 batch loss 6.44048738 epoch total loss 6.53372288\n",
      "Trained batch 169 batch loss 6.19529963 epoch total loss 6.53172\n",
      "Trained batch 170 batch loss 6.48678255 epoch total loss 6.53145599\n",
      "Trained batch 171 batch loss 6.27599192 epoch total loss 6.52996206\n",
      "Trained batch 172 batch loss 6.51446486 epoch total loss 6.52987194\n",
      "Trained batch 173 batch loss 6.5490694 epoch total loss 6.52998257\n",
      "Trained batch 174 batch loss 6.93463135 epoch total loss 6.5323081\n",
      "Trained batch 175 batch loss 7.11707067 epoch total loss 6.5356493\n",
      "Trained batch 176 batch loss 6.78109598 epoch total loss 6.53704405\n",
      "Trained batch 177 batch loss 6.74667072 epoch total loss 6.53822851\n",
      "Trained batch 178 batch loss 6.53235912 epoch total loss 6.53819561\n",
      "Trained batch 179 batch loss 6.54014826 epoch total loss 6.53820658\n",
      "Trained batch 180 batch loss 6.55024385 epoch total loss 6.53827381\n",
      "Trained batch 181 batch loss 6.67915106 epoch total loss 6.53905249\n",
      "Trained batch 182 batch loss 6.72789478 epoch total loss 6.54009\n",
      "Trained batch 183 batch loss 6.5203352 epoch total loss 6.53998232\n",
      "Trained batch 184 batch loss 6.35287952 epoch total loss 6.5389657\n",
      "Trained batch 185 batch loss 6.66927862 epoch total loss 6.53967\n",
      "Trained batch 186 batch loss 6.73835897 epoch total loss 6.54073858\n",
      "Trained batch 187 batch loss 6.67822313 epoch total loss 6.54147387\n",
      "Trained batch 188 batch loss 6.8255558 epoch total loss 6.54298496\n",
      "Trained batch 189 batch loss 6.85215569 epoch total loss 6.54462099\n",
      "Trained batch 190 batch loss 6.51968718 epoch total loss 6.54448938\n",
      "Trained batch 191 batch loss 6.69077492 epoch total loss 6.54525566\n",
      "Trained batch 192 batch loss 6.28916121 epoch total loss 6.54392195\n",
      "Trained batch 193 batch loss 6.1399622 epoch total loss 6.54182911\n",
      "Trained batch 194 batch loss 6.16930962 epoch total loss 6.53990889\n",
      "Trained batch 195 batch loss 6.67696524 epoch total loss 6.54061174\n",
      "Trained batch 196 batch loss 6.9350791 epoch total loss 6.54262447\n",
      "Trained batch 197 batch loss 6.73690557 epoch total loss 6.54361057\n",
      "Trained batch 198 batch loss 6.85813665 epoch total loss 6.54519939\n",
      "Trained batch 199 batch loss 7.05921698 epoch total loss 6.54778242\n",
      "Trained batch 200 batch loss 6.82522392 epoch total loss 6.54916954\n",
      "Trained batch 201 batch loss 6.60148335 epoch total loss 6.54942942\n",
      "Trained batch 202 batch loss 6.32151031 epoch total loss 6.54830122\n",
      "Trained batch 203 batch loss 6.72326469 epoch total loss 6.54916286\n",
      "Trained batch 204 batch loss 6.35515308 epoch total loss 6.54821157\n",
      "Trained batch 205 batch loss 6.54804325 epoch total loss 6.5482111\n",
      "Trained batch 206 batch loss 6.4180851 epoch total loss 6.54758\n",
      "Trained batch 207 batch loss 6.26305294 epoch total loss 6.54620504\n",
      "Trained batch 208 batch loss 6.52726841 epoch total loss 6.54611397\n",
      "Trained batch 209 batch loss 6.26765442 epoch total loss 6.54478168\n",
      "Trained batch 210 batch loss 6.40795755 epoch total loss 6.54413033\n",
      "Trained batch 211 batch loss 6.43136644 epoch total loss 6.54359579\n",
      "Trained batch 212 batch loss 6.19830656 epoch total loss 6.54196739\n",
      "Trained batch 213 batch loss 6.21409559 epoch total loss 6.54042816\n",
      "Trained batch 214 batch loss 6.37791061 epoch total loss 6.53966904\n",
      "Trained batch 215 batch loss 6.19522572 epoch total loss 6.53806686\n",
      "Trained batch 216 batch loss 6.14785194 epoch total loss 6.53626\n",
      "Trained batch 217 batch loss 6.05132675 epoch total loss 6.53402519\n",
      "Trained batch 218 batch loss 6.52685833 epoch total loss 6.53399229\n",
      "Trained batch 219 batch loss 6.46640205 epoch total loss 6.53368378\n",
      "Trained batch 220 batch loss 6.40916824 epoch total loss 6.53311777\n",
      "Trained batch 221 batch loss 6.62662697 epoch total loss 6.53354073\n",
      "Trained batch 222 batch loss 6.63601255 epoch total loss 6.5340023\n",
      "Trained batch 223 batch loss 6.69454336 epoch total loss 6.53472233\n",
      "Trained batch 224 batch loss 6.75448 epoch total loss 6.53570318\n",
      "Trained batch 225 batch loss 6.12591362 epoch total loss 6.53388166\n",
      "Trained batch 226 batch loss 6.44995356 epoch total loss 6.53351068\n",
      "Trained batch 227 batch loss 6.36292696 epoch total loss 6.53275871\n",
      "Trained batch 228 batch loss 6.41511393 epoch total loss 6.53224325\n",
      "Trained batch 229 batch loss 6.11949348 epoch total loss 6.53044081\n",
      "Trained batch 230 batch loss 6.34335232 epoch total loss 6.52962732\n",
      "Trained batch 231 batch loss 6.05580139 epoch total loss 6.52757645\n",
      "Trained batch 232 batch loss 6.27955246 epoch total loss 6.52650738\n",
      "Trained batch 233 batch loss 6.40860415 epoch total loss 6.52600098\n",
      "Trained batch 234 batch loss 6.91614342 epoch total loss 6.527668\n",
      "Trained batch 235 batch loss 6.62277937 epoch total loss 6.52807283\n",
      "Trained batch 236 batch loss 6.79387283 epoch total loss 6.52919912\n",
      "Trained batch 237 batch loss 6.38274765 epoch total loss 6.52858114\n",
      "Trained batch 238 batch loss 5.96131849 epoch total loss 6.52619743\n",
      "Trained batch 239 batch loss 6.32049513 epoch total loss 6.52533627\n",
      "Trained batch 240 batch loss 6.67278099 epoch total loss 6.52595043\n",
      "Trained batch 241 batch loss 6.6626792 epoch total loss 6.52651834\n",
      "Trained batch 242 batch loss 6.7790575 epoch total loss 6.52756166\n",
      "Trained batch 243 batch loss 6.58367538 epoch total loss 6.52779245\n",
      "Trained batch 244 batch loss 6.65299892 epoch total loss 6.52830553\n",
      "Trained batch 245 batch loss 6.59393024 epoch total loss 6.52857304\n",
      "Trained batch 246 batch loss 6.37511539 epoch total loss 6.52794933\n",
      "Trained batch 247 batch loss 6.49867392 epoch total loss 6.5278306\n",
      "Trained batch 248 batch loss 6.44580173 epoch total loss 6.52749968\n",
      "Trained batch 249 batch loss 5.84577084 epoch total loss 6.52476215\n",
      "Trained batch 250 batch loss 6.55365801 epoch total loss 6.52487803\n",
      "Trained batch 251 batch loss 6.57925797 epoch total loss 6.52509451\n",
      "Trained batch 252 batch loss 6.4335165 epoch total loss 6.52473068\n",
      "Trained batch 253 batch loss 6.06405354 epoch total loss 6.52291\n",
      "Trained batch 254 batch loss 6.29778814 epoch total loss 6.52202368\n",
      "Trained batch 255 batch loss 6.36246347 epoch total loss 6.52139759\n",
      "Trained batch 256 batch loss 6.18142939 epoch total loss 6.5200696\n",
      "Trained batch 257 batch loss 6.38539028 epoch total loss 6.51954556\n",
      "Trained batch 258 batch loss 6.4905777 epoch total loss 6.5194335\n",
      "Trained batch 259 batch loss 6.48849821 epoch total loss 6.51931381\n",
      "Trained batch 260 batch loss 6.4369607 epoch total loss 6.51899719\n",
      "Trained batch 261 batch loss 6.53281879 epoch total loss 6.5190506\n",
      "Trained batch 262 batch loss 6.61633921 epoch total loss 6.51942158\n",
      "Trained batch 263 batch loss 6.4663806 epoch total loss 6.51922035\n",
      "Trained batch 264 batch loss 6.62774849 epoch total loss 6.51963139\n",
      "Trained batch 265 batch loss 6.67222595 epoch total loss 6.52020741\n",
      "Trained batch 266 batch loss 6.70492649 epoch total loss 6.52090216\n",
      "Trained batch 267 batch loss 6.33166695 epoch total loss 6.5201931\n",
      "Trained batch 268 batch loss 6.32340574 epoch total loss 6.51945877\n",
      "Trained batch 269 batch loss 5.80348825 epoch total loss 6.51679707\n",
      "Trained batch 270 batch loss 6.22175121 epoch total loss 6.51570463\n",
      "Trained batch 271 batch loss 6.4914341 epoch total loss 6.51561499\n",
      "Trained batch 272 batch loss 6.44375086 epoch total loss 6.51535082\n",
      "Trained batch 273 batch loss 6.78057766 epoch total loss 6.51632214\n",
      "Trained batch 274 batch loss 6.51745415 epoch total loss 6.51632643\n",
      "Trained batch 275 batch loss 6.44434214 epoch total loss 6.51606464\n",
      "Trained batch 276 batch loss 6.41887856 epoch total loss 6.51571226\n",
      "Trained batch 277 batch loss 6.72392273 epoch total loss 6.51646376\n",
      "Trained batch 278 batch loss 6.20214605 epoch total loss 6.51533318\n",
      "Trained batch 279 batch loss 6.42087746 epoch total loss 6.51499462\n",
      "Trained batch 280 batch loss 6.32112694 epoch total loss 6.51430225\n",
      "Trained batch 281 batch loss 6.68198 epoch total loss 6.51489925\n",
      "Trained batch 282 batch loss 6.66277552 epoch total loss 6.5154233\n",
      "Trained batch 283 batch loss 6.30498314 epoch total loss 6.51467943\n",
      "Trained batch 284 batch loss 6.54312 epoch total loss 6.51477957\n",
      "Trained batch 285 batch loss 6.62277937 epoch total loss 6.51515865\n",
      "Trained batch 286 batch loss 6.61103439 epoch total loss 6.51549387\n",
      "Trained batch 287 batch loss 6.99498367 epoch total loss 6.51716471\n",
      "Trained batch 288 batch loss 6.71324968 epoch total loss 6.51784563\n",
      "Trained batch 289 batch loss 6.70328426 epoch total loss 6.51848698\n",
      "Trained batch 290 batch loss 6.70257139 epoch total loss 6.51912165\n",
      "Trained batch 291 batch loss 6.38041067 epoch total loss 6.51864481\n",
      "Trained batch 292 batch loss 6.69809437 epoch total loss 6.51925945\n",
      "Trained batch 293 batch loss 6.43429804 epoch total loss 6.51896954\n",
      "Trained batch 294 batch loss 6.18879223 epoch total loss 6.51784658\n",
      "Trained batch 295 batch loss 5.83059311 epoch total loss 6.51551723\n",
      "Trained batch 296 batch loss 6.0783782 epoch total loss 6.51404\n",
      "Trained batch 297 batch loss 6.49525309 epoch total loss 6.51397705\n",
      "Trained batch 298 batch loss 6.26337767 epoch total loss 6.51313591\n",
      "Trained batch 299 batch loss 6.78437519 epoch total loss 6.51404333\n",
      "Trained batch 300 batch loss 6.58500814 epoch total loss 6.51428\n",
      "Trained batch 301 batch loss 6.30091763 epoch total loss 6.51357079\n",
      "Trained batch 302 batch loss 5.84045029 epoch total loss 6.51134205\n",
      "Trained batch 303 batch loss 6.66133451 epoch total loss 6.51183701\n",
      "Trained batch 304 batch loss 6.64874077 epoch total loss 6.51228714\n",
      "Trained batch 305 batch loss 6.49773169 epoch total loss 6.51223946\n",
      "Trained batch 306 batch loss 6.37563848 epoch total loss 6.51179314\n",
      "Trained batch 307 batch loss 6.70133781 epoch total loss 6.51241\n",
      "Trained batch 308 batch loss 6.67183208 epoch total loss 6.51292801\n",
      "Trained batch 309 batch loss 6.64868832 epoch total loss 6.51336718\n",
      "Trained batch 310 batch loss 6.748312 epoch total loss 6.51412535\n",
      "Trained batch 311 batch loss 6.71322298 epoch total loss 6.51476526\n",
      "Trained batch 312 batch loss 6.30870628 epoch total loss 6.51410484\n",
      "Trained batch 313 batch loss 6.16119242 epoch total loss 6.51297712\n",
      "Trained batch 314 batch loss 6.36585474 epoch total loss 6.51250887\n",
      "Trained batch 315 batch loss 6.63194609 epoch total loss 6.51288748\n",
      "Trained batch 316 batch loss 6.32120609 epoch total loss 6.51228142\n",
      "Trained batch 317 batch loss 6.47684193 epoch total loss 6.51216936\n",
      "Trained batch 318 batch loss 6.89119673 epoch total loss 6.51336098\n",
      "Trained batch 319 batch loss 6.38166428 epoch total loss 6.51294804\n",
      "Trained batch 320 batch loss 6.64084244 epoch total loss 6.51334763\n",
      "Trained batch 321 batch loss 5.97841072 epoch total loss 6.51168156\n",
      "Trained batch 322 batch loss 6.25978327 epoch total loss 6.51089907\n",
      "Trained batch 323 batch loss 6.49386454 epoch total loss 6.51084661\n",
      "Trained batch 324 batch loss 6.7727952 epoch total loss 6.51165485\n",
      "Trained batch 325 batch loss 6.63612652 epoch total loss 6.51203823\n",
      "Trained batch 326 batch loss 6.08537912 epoch total loss 6.51072931\n",
      "Trained batch 327 batch loss 6.05552149 epoch total loss 6.50933695\n",
      "Trained batch 328 batch loss 6.61514854 epoch total loss 6.50966\n",
      "Trained batch 329 batch loss 6.34144 epoch total loss 6.50914907\n",
      "Trained batch 330 batch loss 6.63997841 epoch total loss 6.50954533\n",
      "Trained batch 331 batch loss 6.43299389 epoch total loss 6.50931406\n",
      "Trained batch 332 batch loss 6.58345604 epoch total loss 6.5095377\n",
      "Trained batch 333 batch loss 6.66718626 epoch total loss 6.5100112\n",
      "Trained batch 334 batch loss 6.99508 epoch total loss 6.51146364\n",
      "Trained batch 335 batch loss 6.6280756 epoch total loss 6.51181221\n",
      "Trained batch 336 batch loss 6.6364212 epoch total loss 6.51218319\n",
      "Trained batch 337 batch loss 6.55339479 epoch total loss 6.51230574\n",
      "Trained batch 338 batch loss 6.49296331 epoch total loss 6.51224804\n",
      "Trained batch 339 batch loss 6.52668619 epoch total loss 6.51229048\n",
      "Trained batch 340 batch loss 6.77680826 epoch total loss 6.51306868\n",
      "Trained batch 341 batch loss 6.73505211 epoch total loss 6.51372\n",
      "Trained batch 342 batch loss 6.90827 epoch total loss 6.5148735\n",
      "Trained batch 343 batch loss 6.5980382 epoch total loss 6.51511621\n",
      "Trained batch 344 batch loss 6.40357351 epoch total loss 6.51479197\n",
      "Trained batch 345 batch loss 5.67492151 epoch total loss 6.51235723\n",
      "Trained batch 346 batch loss 6.32487345 epoch total loss 6.51181555\n",
      "Trained batch 347 batch loss 6.88201618 epoch total loss 6.51288223\n",
      "Trained batch 348 batch loss 6.89721394 epoch total loss 6.51398706\n",
      "Trained batch 349 batch loss 6.60636425 epoch total loss 6.51425171\n",
      "Trained batch 350 batch loss 6.52733374 epoch total loss 6.51428938\n",
      "Trained batch 351 batch loss 6.47759724 epoch total loss 6.51418447\n",
      "Trained batch 352 batch loss 6.6125555 epoch total loss 6.5144639\n",
      "Trained batch 353 batch loss 6.68625927 epoch total loss 6.51495075\n",
      "Trained batch 354 batch loss 6.90566969 epoch total loss 6.51605463\n",
      "Trained batch 355 batch loss 6.83290529 epoch total loss 6.51694727\n",
      "Trained batch 356 batch loss 6.47634125 epoch total loss 6.51683331\n",
      "Trained batch 357 batch loss 6.68806505 epoch total loss 6.517313\n",
      "Trained batch 358 batch loss 7.14430428 epoch total loss 6.51906395\n",
      "Trained batch 359 batch loss 6.26784039 epoch total loss 6.51836443\n",
      "Trained batch 360 batch loss 7.01209259 epoch total loss 6.51973629\n",
      "Trained batch 361 batch loss 6.72004557 epoch total loss 6.52029085\n",
      "Trained batch 362 batch loss 6.5211587 epoch total loss 6.52029324\n",
      "Trained batch 363 batch loss 6.78861475 epoch total loss 6.52103233\n",
      "Trained batch 364 batch loss 6.80533695 epoch total loss 6.52181387\n",
      "Trained batch 365 batch loss 6.4871974 epoch total loss 6.52171898\n",
      "Trained batch 366 batch loss 6.54537821 epoch total loss 6.52178383\n",
      "Trained batch 367 batch loss 6.41702318 epoch total loss 6.5214982\n",
      "Trained batch 368 batch loss 7.02381372 epoch total loss 6.52286386\n",
      "Trained batch 369 batch loss 6.94301891 epoch total loss 6.52400255\n",
      "Trained batch 370 batch loss 6.81349754 epoch total loss 6.52478504\n",
      "Trained batch 371 batch loss 6.27093363 epoch total loss 6.52410078\n",
      "Trained batch 372 batch loss 6.33330345 epoch total loss 6.5235877\n",
      "Trained batch 373 batch loss 6.60410213 epoch total loss 6.52380323\n",
      "Trained batch 374 batch loss 6.62601185 epoch total loss 6.52407646\n",
      "Trained batch 375 batch loss 6.75424814 epoch total loss 6.52469\n",
      "Trained batch 376 batch loss 6.45841408 epoch total loss 6.5245142\n",
      "Trained batch 377 batch loss 6.46783972 epoch total loss 6.52436352\n",
      "Trained batch 378 batch loss 5.96477127 epoch total loss 6.52288342\n",
      "Trained batch 379 batch loss 6.52079391 epoch total loss 6.52287769\n",
      "Trained batch 380 batch loss 6.46937943 epoch total loss 6.52273703\n",
      "Trained batch 381 batch loss 6.85341 epoch total loss 6.52360535\n",
      "Trained batch 382 batch loss 7.04868174 epoch total loss 6.52497959\n",
      "Trained batch 383 batch loss 6.86972046 epoch total loss 6.52587938\n",
      "Trained batch 384 batch loss 7.18528795 epoch total loss 6.52759695\n",
      "Trained batch 385 batch loss 7.02523041 epoch total loss 6.52888918\n",
      "Trained batch 386 batch loss 6.31458569 epoch total loss 6.52833414\n",
      "Trained batch 387 batch loss 6.09713316 epoch total loss 6.52722025\n",
      "Trained batch 388 batch loss 5.6679697 epoch total loss 6.52500534\n",
      "Trained batch 389 batch loss 5.52282953 epoch total loss 6.52242947\n",
      "Trained batch 390 batch loss 5.75366926 epoch total loss 6.52045822\n",
      "Trained batch 391 batch loss 6.99740314 epoch total loss 6.52167797\n",
      "Trained batch 392 batch loss 7.21443272 epoch total loss 6.52344513\n",
      "Trained batch 393 batch loss 6.72901 epoch total loss 6.52396822\n",
      "Trained batch 394 batch loss 6.57313967 epoch total loss 6.52409315\n",
      "Trained batch 395 batch loss 6.5047574 epoch total loss 6.52404404\n",
      "Trained batch 396 batch loss 6.46630669 epoch total loss 6.52389812\n",
      "Trained batch 397 batch loss 6.32247925 epoch total loss 6.52339077\n",
      "Trained batch 398 batch loss 6.27484465 epoch total loss 6.52276659\n",
      "Trained batch 399 batch loss 6.12007427 epoch total loss 6.52175713\n",
      "Trained batch 400 batch loss 6.62971544 epoch total loss 6.52202702\n",
      "Trained batch 401 batch loss 6.88916969 epoch total loss 6.52294254\n",
      "Trained batch 402 batch loss 6.86238337 epoch total loss 6.52378654\n",
      "Trained batch 403 batch loss 6.79354239 epoch total loss 6.52445602\n",
      "Trained batch 404 batch loss 6.68228817 epoch total loss 6.52484655\n",
      "Trained batch 405 batch loss 6.44806337 epoch total loss 6.52465677\n",
      "Trained batch 406 batch loss 6.57601404 epoch total loss 6.52478313\n",
      "Trained batch 407 batch loss 6.82567883 epoch total loss 6.52552271\n",
      "Trained batch 408 batch loss 6.64700031 epoch total loss 6.52582026\n",
      "Trained batch 409 batch loss 6.24058199 epoch total loss 6.52512264\n",
      "Trained batch 410 batch loss 6.53916359 epoch total loss 6.5251565\n",
      "Trained batch 411 batch loss 6.463377 epoch total loss 6.52500629\n",
      "Trained batch 412 batch loss 6.17872286 epoch total loss 6.52416563\n",
      "Trained batch 413 batch loss 6.16496611 epoch total loss 6.52329636\n",
      "Trained batch 414 batch loss 6.54148769 epoch total loss 6.52334\n",
      "Trained batch 415 batch loss 6.58856297 epoch total loss 6.52349758\n",
      "Trained batch 416 batch loss 6.66530418 epoch total loss 6.52383852\n",
      "Trained batch 417 batch loss 6.87601137 epoch total loss 6.524683\n",
      "Trained batch 418 batch loss 6.67653799 epoch total loss 6.52504587\n",
      "Trained batch 419 batch loss 6.47094822 epoch total loss 6.52491713\n",
      "Trained batch 420 batch loss 6.39254093 epoch total loss 6.52460194\n",
      "Trained batch 421 batch loss 6.41346741 epoch total loss 6.52433825\n",
      "Trained batch 422 batch loss 6.3807106 epoch total loss 6.52399731\n",
      "Trained batch 423 batch loss 6.57144403 epoch total loss 6.52411\n",
      "Trained batch 424 batch loss 6.76666117 epoch total loss 6.52468204\n",
      "Trained batch 425 batch loss 6.9106245 epoch total loss 6.52559\n",
      "Trained batch 426 batch loss 7.36061573 epoch total loss 6.52755\n",
      "Trained batch 427 batch loss 6.79347658 epoch total loss 6.52817297\n",
      "Trained batch 428 batch loss 7.09954405 epoch total loss 6.52950811\n",
      "Trained batch 429 batch loss 7.00082541 epoch total loss 6.53060627\n",
      "Trained batch 430 batch loss 7.30608368 epoch total loss 6.53241\n",
      "Trained batch 431 batch loss 6.99408913 epoch total loss 6.53348112\n",
      "Trained batch 432 batch loss 6.89815235 epoch total loss 6.5343256\n",
      "Trained batch 433 batch loss 6.81732607 epoch total loss 6.53497934\n",
      "Trained batch 434 batch loss 5.65319681 epoch total loss 6.53294706\n",
      "Trained batch 435 batch loss 5.57315826 epoch total loss 6.53074074\n",
      "Trained batch 436 batch loss 6.04469681 epoch total loss 6.52962589\n",
      "Trained batch 437 batch loss 6.11346 epoch total loss 6.52867413\n",
      "Trained batch 438 batch loss 6.17039967 epoch total loss 6.52785587\n",
      "Trained batch 439 batch loss 6.61400938 epoch total loss 6.52805233\n",
      "Trained batch 440 batch loss 6.69073677 epoch total loss 6.52842188\n",
      "Trained batch 441 batch loss 6.5275979 epoch total loss 6.52842\n",
      "Trained batch 442 batch loss 6.6170516 epoch total loss 6.52862024\n",
      "Trained batch 443 batch loss 6.54696512 epoch total loss 6.52866125\n",
      "Trained batch 444 batch loss 6.33718634 epoch total loss 6.52823\n",
      "Trained batch 445 batch loss 6.64519453 epoch total loss 6.52849293\n",
      "Trained batch 446 batch loss 6.1829915 epoch total loss 6.52771854\n",
      "Trained batch 447 batch loss 6.57519197 epoch total loss 6.52782488\n",
      "Trained batch 448 batch loss 6.15775728 epoch total loss 6.526999\n",
      "Trained batch 449 batch loss 6.568995 epoch total loss 6.52709246\n",
      "Trained batch 450 batch loss 6.64907837 epoch total loss 6.52736378\n",
      "Trained batch 451 batch loss 6.95782757 epoch total loss 6.52831793\n",
      "Trained batch 452 batch loss 6.76107216 epoch total loss 6.52883291\n",
      "Trained batch 453 batch loss 6.46328354 epoch total loss 6.52868843\n",
      "Trained batch 454 batch loss 6.01720238 epoch total loss 6.52756166\n",
      "Trained batch 455 batch loss 6.09038973 epoch total loss 6.52660036\n",
      "Trained batch 456 batch loss 6.74751186 epoch total loss 6.5270853\n",
      "Trained batch 457 batch loss 6.81340694 epoch total loss 6.52771187\n",
      "Trained batch 458 batch loss 6.64926767 epoch total loss 6.52797699\n",
      "Trained batch 459 batch loss 6.61956835 epoch total loss 6.52817678\n",
      "Trained batch 460 batch loss 7.27485418 epoch total loss 6.5298\n",
      "Trained batch 461 batch loss 6.95273352 epoch total loss 6.53071737\n",
      "Trained batch 462 batch loss 6.57109213 epoch total loss 6.53080463\n",
      "Trained batch 463 batch loss 6.39133596 epoch total loss 6.53050327\n",
      "Trained batch 464 batch loss 6.13455 epoch total loss 6.52964973\n",
      "Trained batch 465 batch loss 5.59108591 epoch total loss 6.52763128\n",
      "Trained batch 466 batch loss 5.71911 epoch total loss 6.52589607\n",
      "Trained batch 467 batch loss 6.4562521 epoch total loss 6.5257473\n",
      "Trained batch 468 batch loss 5.5447731 epoch total loss 6.52365065\n",
      "Trained batch 469 batch loss 5.23822498 epoch total loss 6.52091026\n",
      "Trained batch 470 batch loss 5.3565855 epoch total loss 6.51843309\n",
      "Trained batch 471 batch loss 5.70197868 epoch total loss 6.51669931\n",
      "Trained batch 472 batch loss 6.23352814 epoch total loss 6.5161\n",
      "Trained batch 473 batch loss 6.38884068 epoch total loss 6.51583099\n",
      "Trained batch 474 batch loss 6.64066267 epoch total loss 6.51609421\n",
      "Trained batch 475 batch loss 7.02802134 epoch total loss 6.51717186\n",
      "Trained batch 476 batch loss 7.11763906 epoch total loss 6.51843357\n",
      "Trained batch 477 batch loss 6.49656296 epoch total loss 6.51838779\n",
      "Trained batch 478 batch loss 6.61101627 epoch total loss 6.51858187\n",
      "Trained batch 479 batch loss 7.08163452 epoch total loss 6.51975727\n",
      "Trained batch 480 batch loss 6.83364725 epoch total loss 6.52041101\n",
      "Trained batch 481 batch loss 6.98246861 epoch total loss 6.52137184\n",
      "Trained batch 482 batch loss 7.20280838 epoch total loss 6.52278566\n",
      "Trained batch 483 batch loss 7.47688818 epoch total loss 6.52476072\n",
      "Trained batch 484 batch loss 6.75660706 epoch total loss 6.52524\n",
      "Trained batch 485 batch loss 6.46225 epoch total loss 6.52511\n",
      "Trained batch 486 batch loss 6.61610317 epoch total loss 6.52529716\n",
      "Trained batch 487 batch loss 6.78807068 epoch total loss 6.52583694\n",
      "Trained batch 488 batch loss 6.72795057 epoch total loss 6.52625132\n",
      "Trained batch 489 batch loss 6.40736 epoch total loss 6.52600813\n",
      "Trained batch 490 batch loss 6.54502153 epoch total loss 6.52604675\n",
      "Trained batch 491 batch loss 6.55211306 epoch total loss 6.52609968\n",
      "Trained batch 492 batch loss 6.48966265 epoch total loss 6.52602577\n",
      "Trained batch 493 batch loss 6.25278378 epoch total loss 6.52547121\n",
      "Trained batch 494 batch loss 6.67622709 epoch total loss 6.52577639\n",
      "Trained batch 495 batch loss 7.18533802 epoch total loss 6.52710915\n",
      "Trained batch 496 batch loss 6.43768167 epoch total loss 6.5269289\n",
      "Trained batch 497 batch loss 7.0279 epoch total loss 6.52793646\n",
      "Trained batch 498 batch loss 7.01364708 epoch total loss 6.52891207\n",
      "Trained batch 499 batch loss 6.75106859 epoch total loss 6.52935696\n",
      "Trained batch 500 batch loss 6.53624296 epoch total loss 6.52937078\n",
      "Trained batch 501 batch loss 5.92596817 epoch total loss 6.52816629\n",
      "Trained batch 502 batch loss 6.38910294 epoch total loss 6.52788925\n",
      "Trained batch 503 batch loss 6.00976372 epoch total loss 6.52685928\n",
      "Trained batch 504 batch loss 6.08937 epoch total loss 6.52599144\n",
      "Trained batch 505 batch loss 6.78326321 epoch total loss 6.5265007\n",
      "Trained batch 506 batch loss 7.20790529 epoch total loss 6.52784729\n",
      "Trained batch 507 batch loss 7.16666889 epoch total loss 6.52910757\n",
      "Trained batch 508 batch loss 6.98339844 epoch total loss 6.53000212\n",
      "Trained batch 509 batch loss 7.09727764 epoch total loss 6.53111601\n",
      "Trained batch 510 batch loss 6.85450649 epoch total loss 6.53175\n",
      "Trained batch 511 batch loss 6.52235222 epoch total loss 6.53173208\n",
      "Trained batch 512 batch loss 6.79185295 epoch total loss 6.53224\n",
      "Trained batch 513 batch loss 6.8073 epoch total loss 6.53277636\n",
      "Trained batch 514 batch loss 6.55831766 epoch total loss 6.53282595\n",
      "Trained batch 515 batch loss 6.72663 epoch total loss 6.53320217\n",
      "Trained batch 516 batch loss 6.71215916 epoch total loss 6.53354883\n",
      "Trained batch 517 batch loss 6.6432972 epoch total loss 6.5337615\n",
      "Trained batch 518 batch loss 6.74694824 epoch total loss 6.53417301\n",
      "Trained batch 519 batch loss 6.56267405 epoch total loss 6.53422832\n",
      "Trained batch 520 batch loss 6.82549334 epoch total loss 6.53478813\n",
      "Trained batch 521 batch loss 6.38885212 epoch total loss 6.53450823\n",
      "Trained batch 522 batch loss 6.75311565 epoch total loss 6.53492689\n",
      "Trained batch 523 batch loss 6.69519758 epoch total loss 6.53523397\n",
      "Trained batch 524 batch loss 6.8903966 epoch total loss 6.53591156\n",
      "Trained batch 525 batch loss 6.65392 epoch total loss 6.53613615\n",
      "Trained batch 526 batch loss 6.74139357 epoch total loss 6.5365262\n",
      "Trained batch 527 batch loss 6.60044 epoch total loss 6.53664732\n",
      "Trained batch 528 batch loss 6.58839941 epoch total loss 6.53674555\n",
      "Trained batch 529 batch loss 6.77135944 epoch total loss 6.53718853\n",
      "Trained batch 530 batch loss 6.8401103 epoch total loss 6.53776026\n",
      "Trained batch 531 batch loss 6.86160469 epoch total loss 6.53837\n",
      "Trained batch 532 batch loss 6.47719908 epoch total loss 6.53825521\n",
      "Trained batch 533 batch loss 6.0340929 epoch total loss 6.53730965\n",
      "Trained batch 534 batch loss 6.57645178 epoch total loss 6.5373826\n",
      "Trained batch 535 batch loss 6.76718807 epoch total loss 6.53781223\n",
      "Trained batch 536 batch loss 6.46648407 epoch total loss 6.5376792\n",
      "Trained batch 537 batch loss 6.66756439 epoch total loss 6.53792095\n",
      "Trained batch 538 batch loss 6.64573765 epoch total loss 6.53812122\n",
      "Trained batch 539 batch loss 7.0179162 epoch total loss 6.53901148\n",
      "Trained batch 540 batch loss 6.73789692 epoch total loss 6.5393796\n",
      "Trained batch 541 batch loss 6.57142878 epoch total loss 6.53943872\n",
      "Trained batch 542 batch loss 6.66862059 epoch total loss 6.53967714\n",
      "Trained batch 543 batch loss 6.53902197 epoch total loss 6.53967619\n",
      "Trained batch 544 batch loss 6.60691 epoch total loss 6.53979969\n",
      "Trained batch 545 batch loss 6.35332918 epoch total loss 6.5394578\n",
      "Trained batch 546 batch loss 5.9454093 epoch total loss 6.53836966\n",
      "Trained batch 547 batch loss 5.66735125 epoch total loss 6.53677702\n",
      "Trained batch 548 batch loss 6.26984453 epoch total loss 6.53628969\n",
      "Trained batch 549 batch loss 6.12602282 epoch total loss 6.53554249\n",
      "Trained batch 550 batch loss 6.5860095 epoch total loss 6.53563404\n",
      "Trained batch 551 batch loss 6.77639103 epoch total loss 6.53607082\n",
      "Trained batch 552 batch loss 6.76226377 epoch total loss 6.53648043\n",
      "Trained batch 553 batch loss 6.70522594 epoch total loss 6.5367856\n",
      "Trained batch 554 batch loss 6.7330327 epoch total loss 6.53714037\n",
      "Trained batch 555 batch loss 6.72833109 epoch total loss 6.53748465\n",
      "Trained batch 556 batch loss 6.50310612 epoch total loss 6.53742266\n",
      "Trained batch 557 batch loss 6.474895 epoch total loss 6.5373106\n",
      "Trained batch 558 batch loss 6.54115534 epoch total loss 6.53731775\n",
      "Trained batch 559 batch loss 6.28296518 epoch total loss 6.53686285\n",
      "Trained batch 560 batch loss 6.06199503 epoch total loss 6.53601456\n",
      "Trained batch 561 batch loss 6.51151562 epoch total loss 6.53597069\n",
      "Trained batch 562 batch loss 6.12036514 epoch total loss 6.53523159\n",
      "Trained batch 563 batch loss 6.24414158 epoch total loss 6.53471422\n",
      "Trained batch 564 batch loss 6.45439 epoch total loss 6.53457165\n",
      "Trained batch 565 batch loss 6.66275263 epoch total loss 6.5347991\n",
      "Trained batch 566 batch loss 7.03637552 epoch total loss 6.53568506\n",
      "Trained batch 567 batch loss 6.73939896 epoch total loss 6.5360446\n",
      "Trained batch 568 batch loss 6.27411413 epoch total loss 6.5355835\n",
      "Trained batch 569 batch loss 6.34946728 epoch total loss 6.53525639\n",
      "Trained batch 570 batch loss 6.77179146 epoch total loss 6.53567123\n",
      "Trained batch 571 batch loss 6.76915503 epoch total loss 6.53608\n",
      "Trained batch 572 batch loss 6.62226677 epoch total loss 6.53623056\n",
      "Trained batch 573 batch loss 6.76539421 epoch total loss 6.53663063\n",
      "Trained batch 574 batch loss 6.44676113 epoch total loss 6.53647375\n",
      "Trained batch 575 batch loss 6.22083282 epoch total loss 6.53592539\n",
      "Trained batch 576 batch loss 6.65711164 epoch total loss 6.53613567\n",
      "Trained batch 577 batch loss 6.63070965 epoch total loss 6.53629971\n",
      "Trained batch 578 batch loss 6.77914286 epoch total loss 6.53671932\n",
      "Trained batch 579 batch loss 6.84780407 epoch total loss 6.53725672\n",
      "Trained batch 580 batch loss 6.67595053 epoch total loss 6.53749609\n",
      "Trained batch 581 batch loss 6.68415976 epoch total loss 6.53774834\n",
      "Trained batch 582 batch loss 6.5363245 epoch total loss 6.53774595\n",
      "Trained batch 583 batch loss 6.57893133 epoch total loss 6.53781652\n",
      "Trained batch 584 batch loss 6.55075121 epoch total loss 6.53783894\n",
      "Trained batch 585 batch loss 6.89046907 epoch total loss 6.53844166\n",
      "Trained batch 586 batch loss 6.59979296 epoch total loss 6.53854609\n",
      "Trained batch 587 batch loss 6.50625944 epoch total loss 6.53849125\n",
      "Trained batch 588 batch loss 6.62630892 epoch total loss 6.5386405\n",
      "Trained batch 589 batch loss 6.47072411 epoch total loss 6.5385251\n",
      "Trained batch 590 batch loss 6.70581722 epoch total loss 6.53880882\n",
      "Trained batch 591 batch loss 6.60628223 epoch total loss 6.53892279\n",
      "Trained batch 592 batch loss 6.54727 epoch total loss 6.53893709\n",
      "Trained batch 593 batch loss 6.53977823 epoch total loss 6.53893852\n",
      "Trained batch 594 batch loss 6.63568163 epoch total loss 6.5391016\n",
      "Trained batch 595 batch loss 6.64811134 epoch total loss 6.53928471\n",
      "Trained batch 596 batch loss 6.63764906 epoch total loss 6.53945\n",
      "Trained batch 597 batch loss 6.68618917 epoch total loss 6.53969574\n",
      "Trained batch 598 batch loss 6.2789669 epoch total loss 6.53926\n",
      "Trained batch 599 batch loss 6.58485413 epoch total loss 6.5393362\n",
      "Trained batch 600 batch loss 6.85545826 epoch total loss 6.53986311\n",
      "Trained batch 601 batch loss 6.93290234 epoch total loss 6.54051733\n",
      "Trained batch 602 batch loss 7.13114786 epoch total loss 6.54149818\n",
      "Trained batch 603 batch loss 7.5683856 epoch total loss 6.54320097\n",
      "Trained batch 604 batch loss 7.63438368 epoch total loss 6.54500771\n",
      "Trained batch 605 batch loss 7.39877892 epoch total loss 6.54641867\n",
      "Trained batch 606 batch loss 7.02172899 epoch total loss 6.54720306\n",
      "Trained batch 607 batch loss 6.74731255 epoch total loss 6.54753256\n",
      "Trained batch 608 batch loss 6.32847929 epoch total loss 6.54717207\n",
      "Trained batch 609 batch loss 6.62253141 epoch total loss 6.54729605\n",
      "Trained batch 610 batch loss 6.82865524 epoch total loss 6.54775715\n",
      "Trained batch 611 batch loss 6.79757357 epoch total loss 6.5481658\n",
      "Trained batch 612 batch loss 6.90937138 epoch total loss 6.54875612\n",
      "Trained batch 613 batch loss 7.01083708 epoch total loss 6.54951\n",
      "Trained batch 614 batch loss 6.78505898 epoch total loss 6.54989386\n",
      "Trained batch 615 batch loss 6.76606035 epoch total loss 6.55024529\n",
      "Trained batch 616 batch loss 6.8134985 epoch total loss 6.55067253\n",
      "Trained batch 617 batch loss 6.65863371 epoch total loss 6.55084753\n",
      "Trained batch 618 batch loss 6.98290443 epoch total loss 6.55154705\n",
      "Trained batch 619 batch loss 6.76487255 epoch total loss 6.55189133\n",
      "Trained batch 620 batch loss 6.74521828 epoch total loss 6.55220318\n",
      "Trained batch 621 batch loss 6.87014341 epoch total loss 6.5527153\n",
      "Trained batch 622 batch loss 6.88101482 epoch total loss 6.55324316\n",
      "Trained batch 623 batch loss 6.60427189 epoch total loss 6.5533247\n",
      "Trained batch 624 batch loss 6.82276583 epoch total loss 6.55375671\n",
      "Trained batch 625 batch loss 6.60789156 epoch total loss 6.55384302\n",
      "Trained batch 626 batch loss 6.7058115 epoch total loss 6.55408621\n",
      "Trained batch 627 batch loss 6.02435398 epoch total loss 6.55324125\n",
      "Trained batch 628 batch loss 6.02892685 epoch total loss 6.55240631\n",
      "Trained batch 629 batch loss 6.2768631 epoch total loss 6.5519681\n",
      "Trained batch 630 batch loss 6.62287045 epoch total loss 6.55208111\n",
      "Trained batch 631 batch loss 6.70921087 epoch total loss 6.55232954\n",
      "Trained batch 632 batch loss 6.41738605 epoch total loss 6.55211639\n",
      "Trained batch 633 batch loss 6.2200408 epoch total loss 6.55159187\n",
      "Trained batch 634 batch loss 6.4948988 epoch total loss 6.5515027\n",
      "Trained batch 635 batch loss 6.42547226 epoch total loss 6.55130434\n",
      "Trained batch 636 batch loss 6.23223972 epoch total loss 6.55080271\n",
      "Trained batch 637 batch loss 6.29906273 epoch total loss 6.55040741\n",
      "Trained batch 638 batch loss 6.5242939 epoch total loss 6.5503664\n",
      "Trained batch 639 batch loss 6.24580717 epoch total loss 6.54988956\n",
      "Trained batch 640 batch loss 6.72819948 epoch total loss 6.55016804\n",
      "Trained batch 641 batch loss 6.48829651 epoch total loss 6.55007124\n",
      "Trained batch 642 batch loss 6.62978411 epoch total loss 6.55019569\n",
      "Trained batch 643 batch loss 6.58526564 epoch total loss 6.55025053\n",
      "Trained batch 644 batch loss 6.82854414 epoch total loss 6.55068254\n",
      "Trained batch 645 batch loss 6.67657852 epoch total loss 6.55087805\n",
      "Trained batch 646 batch loss 6.85902929 epoch total loss 6.55135489\n",
      "Trained batch 647 batch loss 6.47522354 epoch total loss 6.55123711\n",
      "Trained batch 648 batch loss 6.6605072 epoch total loss 6.55140591\n",
      "Trained batch 649 batch loss 5.8606 epoch total loss 6.55034208\n",
      "Trained batch 650 batch loss 6.45325422 epoch total loss 6.55019236\n",
      "Trained batch 651 batch loss 6.49489307 epoch total loss 6.55010748\n",
      "Trained batch 652 batch loss 6.47642803 epoch total loss 6.54999495\n",
      "Trained batch 653 batch loss 6.23996258 epoch total loss 6.54952\n",
      "Trained batch 654 batch loss 6.11548 epoch total loss 6.54885626\n",
      "Trained batch 655 batch loss 6.5453043 epoch total loss 6.54885101\n",
      "Trained batch 656 batch loss 6.17160559 epoch total loss 6.54827595\n",
      "Trained batch 657 batch loss 6.61653137 epoch total loss 6.54838\n",
      "Trained batch 658 batch loss 6.30370092 epoch total loss 6.54800797\n",
      "Trained batch 659 batch loss 6.36561394 epoch total loss 6.5477314\n",
      "Trained batch 660 batch loss 6.66538048 epoch total loss 6.54791\n",
      "Trained batch 661 batch loss 6.78469944 epoch total loss 6.54826832\n",
      "Trained batch 662 batch loss 6.89370871 epoch total loss 6.54879\n",
      "Trained batch 663 batch loss 6.21716833 epoch total loss 6.54829\n",
      "Trained batch 664 batch loss 6.29551935 epoch total loss 6.54790878\n",
      "Trained batch 665 batch loss 6.47414923 epoch total loss 6.54779816\n",
      "Trained batch 666 batch loss 6.84164476 epoch total loss 6.54823923\n",
      "Trained batch 667 batch loss 6.65137434 epoch total loss 6.5483942\n",
      "Trained batch 668 batch loss 6.59581661 epoch total loss 6.54846478\n",
      "Trained batch 669 batch loss 6.68538666 epoch total loss 6.54867\n",
      "Trained batch 670 batch loss 6.01443815 epoch total loss 6.54787254\n",
      "Trained batch 671 batch loss 6.36230135 epoch total loss 6.54759598\n",
      "Trained batch 672 batch loss 6.37258291 epoch total loss 6.54733562\n",
      "Trained batch 673 batch loss 6.28549623 epoch total loss 6.546947\n",
      "Trained batch 674 batch loss 5.90774632 epoch total loss 6.54599857\n",
      "Trained batch 675 batch loss 6.68839264 epoch total loss 6.54620934\n",
      "Trained batch 676 batch loss 6.84175253 epoch total loss 6.5466466\n",
      "Trained batch 677 batch loss 6.44132233 epoch total loss 6.54649115\n",
      "Trained batch 678 batch loss 6.20269918 epoch total loss 6.54598427\n",
      "Trained batch 679 batch loss 6.0979681 epoch total loss 6.5453248\n",
      "Trained batch 680 batch loss 6.53177166 epoch total loss 6.54530478\n",
      "Trained batch 681 batch loss 6.01992559 epoch total loss 6.54453325\n",
      "Trained batch 682 batch loss 6.45707321 epoch total loss 6.54440498\n",
      "Trained batch 683 batch loss 6.24195814 epoch total loss 6.54396248\n",
      "Trained batch 684 batch loss 6.21130848 epoch total loss 6.5434761\n",
      "Trained batch 685 batch loss 5.97750425 epoch total loss 6.54265\n",
      "Trained batch 686 batch loss 6.2660718 epoch total loss 6.54224682\n",
      "Trained batch 687 batch loss 5.99677801 epoch total loss 6.54145288\n",
      "Trained batch 688 batch loss 6.53420734 epoch total loss 6.54144239\n",
      "Trained batch 689 batch loss 5.96161079 epoch total loss 6.5406003\n",
      "Trained batch 690 batch loss 6.35840702 epoch total loss 6.54033613\n",
      "Trained batch 691 batch loss 6.69628668 epoch total loss 6.54056215\n",
      "Trained batch 692 batch loss 6.7677393 epoch total loss 6.54089\n",
      "Trained batch 693 batch loss 6.59140062 epoch total loss 6.5409627\n",
      "Trained batch 694 batch loss 6.04882479 epoch total loss 6.54025364\n",
      "Trained batch 695 batch loss 6.04911709 epoch total loss 6.53954744\n",
      "Trained batch 696 batch loss 6.00615311 epoch total loss 6.53878117\n",
      "Trained batch 697 batch loss 6.56609 epoch total loss 6.53882027\n",
      "Trained batch 698 batch loss 6.59697866 epoch total loss 6.53890371\n",
      "Trained batch 699 batch loss 6.57671499 epoch total loss 6.5389576\n",
      "Trained batch 700 batch loss 6.69767666 epoch total loss 6.53918457\n",
      "Trained batch 701 batch loss 5.94630194 epoch total loss 6.53833866\n",
      "Trained batch 702 batch loss 5.92103529 epoch total loss 6.53745937\n",
      "Trained batch 703 batch loss 6.3770895 epoch total loss 6.53723097\n",
      "Trained batch 704 batch loss 6.87603807 epoch total loss 6.5377121\n",
      "Trained batch 705 batch loss 6.56680536 epoch total loss 6.53775358\n",
      "Trained batch 706 batch loss 6.56768465 epoch total loss 6.53779602\n",
      "Trained batch 707 batch loss 6.17795753 epoch total loss 6.53728676\n",
      "Trained batch 708 batch loss 6.51990318 epoch total loss 6.53726244\n",
      "Trained batch 709 batch loss 7.04796124 epoch total loss 6.53798246\n",
      "Trained batch 710 batch loss 6.87995863 epoch total loss 6.53846407\n",
      "Trained batch 711 batch loss 6.58769083 epoch total loss 6.53853369\n",
      "Trained batch 712 batch loss 6.66628456 epoch total loss 6.53871346\n",
      "Trained batch 713 batch loss 6.39938164 epoch total loss 6.53851795\n",
      "Trained batch 714 batch loss 6.36738348 epoch total loss 6.5382781\n",
      "Trained batch 715 batch loss 6.57183027 epoch total loss 6.53832483\n",
      "Trained batch 716 batch loss 6.42292643 epoch total loss 6.53816366\n",
      "Trained batch 717 batch loss 6.45709944 epoch total loss 6.53805065\n",
      "Trained batch 718 batch loss 6.5431881 epoch total loss 6.53805733\n",
      "Trained batch 719 batch loss 6.53363037 epoch total loss 6.53805113\n",
      "Trained batch 720 batch loss 6.58481455 epoch total loss 6.53811646\n",
      "Trained batch 721 batch loss 6.26374388 epoch total loss 6.53773594\n",
      "Trained batch 722 batch loss 6.61698818 epoch total loss 6.53784609\n",
      "Trained batch 723 batch loss 6.43493509 epoch total loss 6.53770351\n",
      "Trained batch 724 batch loss 6.71701765 epoch total loss 6.53795099\n",
      "Trained batch 725 batch loss 6.37529182 epoch total loss 6.53772688\n",
      "Trained batch 726 batch loss 6.5740881 epoch total loss 6.53777742\n",
      "Trained batch 727 batch loss 6.63213921 epoch total loss 6.53790712\n",
      "Trained batch 728 batch loss 6.51712513 epoch total loss 6.53787851\n",
      "Trained batch 729 batch loss 6.31351805 epoch total loss 6.53757095\n",
      "Trained batch 730 batch loss 6.94135761 epoch total loss 6.53812408\n",
      "Trained batch 731 batch loss 6.64179897 epoch total loss 6.53826571\n",
      "Trained batch 732 batch loss 7.02983475 epoch total loss 6.53893709\n",
      "Trained batch 733 batch loss 6.6937623 epoch total loss 6.53914833\n",
      "Trained batch 734 batch loss 6.61619091 epoch total loss 6.53925323\n",
      "Trained batch 735 batch loss 6.69383192 epoch total loss 6.53946352\n",
      "Trained batch 736 batch loss 6.65447283 epoch total loss 6.53962\n",
      "Trained batch 737 batch loss 6.69778967 epoch total loss 6.5398345\n",
      "Trained batch 738 batch loss 6.74256325 epoch total loss 6.54010916\n",
      "Trained batch 739 batch loss 6.41570568 epoch total loss 6.53994083\n",
      "Trained batch 740 batch loss 5.73452187 epoch total loss 6.53885221\n",
      "Trained batch 741 batch loss 5.62978077 epoch total loss 6.53762531\n",
      "Trained batch 742 batch loss 5.52489328 epoch total loss 6.5362606\n",
      "Trained batch 743 batch loss 6.00768089 epoch total loss 6.53554916\n",
      "Trained batch 744 batch loss 6.02478743 epoch total loss 6.534863\n",
      "Trained batch 745 batch loss 6.93350172 epoch total loss 6.53539801\n",
      "Trained batch 746 batch loss 7.12026119 epoch total loss 6.53618193\n",
      "Trained batch 747 batch loss 6.8843689 epoch total loss 6.5366478\n",
      "Trained batch 748 batch loss 6.98797369 epoch total loss 6.537251\n",
      "Trained batch 749 batch loss 7.34189415 epoch total loss 6.53832531\n",
      "Trained batch 750 batch loss 6.71835089 epoch total loss 6.53856516\n",
      "Trained batch 751 batch loss 7.02775955 epoch total loss 6.53921652\n",
      "Trained batch 752 batch loss 6.68726397 epoch total loss 6.53941393\n",
      "Trained batch 753 batch loss 6.88081503 epoch total loss 6.5398674\n",
      "Trained batch 754 batch loss 7.03453445 epoch total loss 6.54052353\n",
      "Trained batch 755 batch loss 6.57643747 epoch total loss 6.54057121\n",
      "Trained batch 756 batch loss 6.49863911 epoch total loss 6.5405159\n",
      "Trained batch 757 batch loss 6.49034548 epoch total loss 6.54044914\n",
      "Trained batch 758 batch loss 6.04438591 epoch total loss 6.53979492\n",
      "Trained batch 759 batch loss 5.74638 epoch total loss 6.53874969\n",
      "Trained batch 760 batch loss 5.34795904 epoch total loss 6.53718328\n",
      "Trained batch 761 batch loss 5.54128838 epoch total loss 6.53587484\n",
      "Trained batch 762 batch loss 6.01142311 epoch total loss 6.53518629\n",
      "Trained batch 763 batch loss 6.61842871 epoch total loss 6.53529596\n",
      "Trained batch 764 batch loss 6.63569403 epoch total loss 6.53542709\n",
      "Trained batch 765 batch loss 6.85104322 epoch total loss 6.53584\n",
      "Trained batch 766 batch loss 6.7027216 epoch total loss 6.53605747\n",
      "Trained batch 767 batch loss 5.9846158 epoch total loss 6.5353384\n",
      "Trained batch 768 batch loss 6.49714851 epoch total loss 6.53528833\n",
      "Trained batch 769 batch loss 6.73182 epoch total loss 6.5355444\n",
      "Trained batch 770 batch loss 6.83571625 epoch total loss 6.53593445\n",
      "Trained batch 771 batch loss 6.71652508 epoch total loss 6.5361681\n",
      "Trained batch 772 batch loss 6.69370461 epoch total loss 6.53637266\n",
      "Trained batch 773 batch loss 6.76964378 epoch total loss 6.53667402\n",
      "Trained batch 774 batch loss 6.57645178 epoch total loss 6.536726\n",
      "Trained batch 775 batch loss 6.9345541 epoch total loss 6.53723907\n",
      "Trained batch 776 batch loss 6.51721 epoch total loss 6.53721333\n",
      "Trained batch 777 batch loss 6.78001118 epoch total loss 6.53752518\n",
      "Trained batch 778 batch loss 6.6167593 epoch total loss 6.53762722\n",
      "Trained batch 779 batch loss 6.79788 epoch total loss 6.53796101\n",
      "Trained batch 780 batch loss 5.74219179 epoch total loss 6.53694105\n",
      "Trained batch 781 batch loss 6.12697172 epoch total loss 6.53641605\n",
      "Trained batch 782 batch loss 6.79525948 epoch total loss 6.53674746\n",
      "Trained batch 783 batch loss 6.49972057 epoch total loss 6.5367\n",
      "Trained batch 784 batch loss 6.42723656 epoch total loss 6.53656\n",
      "Trained batch 785 batch loss 6.57800484 epoch total loss 6.53661299\n",
      "Trained batch 786 batch loss 5.7987833 epoch total loss 6.53567457\n",
      "Trained batch 787 batch loss 6.3035531 epoch total loss 6.53537941\n",
      "Trained batch 788 batch loss 6.07933378 epoch total loss 6.53480053\n",
      "Trained batch 789 batch loss 6.25701141 epoch total loss 6.53444815\n",
      "Trained batch 790 batch loss 5.99050379 epoch total loss 6.53376\n",
      "Trained batch 791 batch loss 5.9203887 epoch total loss 6.53298473\n",
      "Trained batch 792 batch loss 6.2042079 epoch total loss 6.53256941\n",
      "Trained batch 793 batch loss 5.90724325 epoch total loss 6.53178072\n",
      "Trained batch 794 batch loss 5.81814289 epoch total loss 6.53088236\n",
      "Trained batch 795 batch loss 6.29687929 epoch total loss 6.53058767\n",
      "Trained batch 796 batch loss 6.36918783 epoch total loss 6.53038502\n",
      "Trained batch 797 batch loss 6.50149393 epoch total loss 6.53034878\n",
      "Trained batch 798 batch loss 6.73483 epoch total loss 6.53060484\n",
      "Trained batch 799 batch loss 6.74168921 epoch total loss 6.53086948\n",
      "Trained batch 800 batch loss 6.9374795 epoch total loss 6.53137779\n",
      "Trained batch 801 batch loss 6.83183861 epoch total loss 6.53175306\n",
      "Trained batch 802 batch loss 6.83856106 epoch total loss 6.53213501\n",
      "Trained batch 803 batch loss 6.52999449 epoch total loss 6.53213215\n",
      "Trained batch 804 batch loss 6.70013 epoch total loss 6.53234148\n",
      "Trained batch 805 batch loss 6.54867887 epoch total loss 6.53236198\n",
      "Trained batch 806 batch loss 6.03820229 epoch total loss 6.53174877\n",
      "Trained batch 807 batch loss 6.53364229 epoch total loss 6.53175116\n",
      "Trained batch 808 batch loss 6.50432873 epoch total loss 6.5317173\n",
      "Trained batch 809 batch loss 6.51848507 epoch total loss 6.53170109\n",
      "Trained batch 810 batch loss 6.32171917 epoch total loss 6.53144169\n",
      "Trained batch 811 batch loss 6.57674599 epoch total loss 6.53149748\n",
      "Trained batch 812 batch loss 6.2904439 epoch total loss 6.53120089\n",
      "Trained batch 813 batch loss 6.52013969 epoch total loss 6.53118706\n",
      "Trained batch 814 batch loss 6.57259321 epoch total loss 6.53123808\n",
      "Trained batch 815 batch loss 6.94314337 epoch total loss 6.53174353\n",
      "Trained batch 816 batch loss 7.05281591 epoch total loss 6.53238201\n",
      "Trained batch 817 batch loss 7.0130167 epoch total loss 6.53297043\n",
      "Trained batch 818 batch loss 6.69164133 epoch total loss 6.5331645\n",
      "Trained batch 819 batch loss 6.24197149 epoch total loss 6.53280926\n",
      "Trained batch 820 batch loss 5.74269533 epoch total loss 6.53184557\n",
      "Trained batch 821 batch loss 6.43934345 epoch total loss 6.53173304\n",
      "Trained batch 822 batch loss 6.51001883 epoch total loss 6.53170681\n",
      "Trained batch 823 batch loss 6.74565315 epoch total loss 6.53196669\n",
      "Trained batch 824 batch loss 6.82365131 epoch total loss 6.53232098\n",
      "Trained batch 825 batch loss 6.62758923 epoch total loss 6.53243589\n",
      "Trained batch 826 batch loss 6.23491096 epoch total loss 6.53207588\n",
      "Trained batch 827 batch loss 6.52037 epoch total loss 6.53206205\n",
      "Trained batch 828 batch loss 6.13732672 epoch total loss 6.53158474\n",
      "Trained batch 829 batch loss 5.98261499 epoch total loss 6.53092241\n",
      "Trained batch 830 batch loss 6.55916548 epoch total loss 6.53095627\n",
      "Trained batch 831 batch loss 6.06541443 epoch total loss 6.53039646\n",
      "Trained batch 832 batch loss 5.65009546 epoch total loss 6.52933788\n",
      "Trained batch 833 batch loss 5.38533974 epoch total loss 6.52796459\n",
      "Trained batch 834 batch loss 5.72578049 epoch total loss 6.52700233\n",
      "Trained batch 835 batch loss 6.11800861 epoch total loss 6.52651262\n",
      "Trained batch 836 batch loss 5.6021595 epoch total loss 6.52540684\n",
      "Trained batch 837 batch loss 6.39371824 epoch total loss 6.52524948\n",
      "Trained batch 838 batch loss 6.18272495 epoch total loss 6.52484035\n",
      "Trained batch 839 batch loss 6.34716654 epoch total loss 6.52462864\n",
      "Trained batch 840 batch loss 6.34222078 epoch total loss 6.52441168\n",
      "Trained batch 841 batch loss 5.99805212 epoch total loss 6.52378607\n",
      "Trained batch 842 batch loss 6.06810713 epoch total loss 6.52324438\n",
      "Trained batch 843 batch loss 6.49634218 epoch total loss 6.52321291\n",
      "Trained batch 844 batch loss 6.63581657 epoch total loss 6.52334595\n",
      "Trained batch 845 batch loss 6.97714233 epoch total loss 6.52388287\n",
      "Trained batch 846 batch loss 7.33668089 epoch total loss 6.52484417\n",
      "Trained batch 847 batch loss 6.77218533 epoch total loss 6.52513599\n",
      "Trained batch 848 batch loss 6.52696896 epoch total loss 6.5251379\n",
      "Trained batch 849 batch loss 6.57842731 epoch total loss 6.52520084\n",
      "Trained batch 850 batch loss 6.68742037 epoch total loss 6.52539158\n",
      "Trained batch 851 batch loss 6.55977774 epoch total loss 6.52543211\n",
      "Trained batch 852 batch loss 6.78504848 epoch total loss 6.52573681\n",
      "Trained batch 853 batch loss 6.58602953 epoch total loss 6.52580738\n",
      "Trained batch 854 batch loss 6.35880518 epoch total loss 6.52561188\n",
      "Trained batch 855 batch loss 6.27233219 epoch total loss 6.52531576\n",
      "Trained batch 856 batch loss 5.67614889 epoch total loss 6.52432394\n",
      "Trained batch 857 batch loss 5.82030725 epoch total loss 6.52350235\n",
      "Trained batch 858 batch loss 6.02499771 epoch total loss 6.52292156\n",
      "Trained batch 859 batch loss 5.6867466 epoch total loss 6.52194786\n",
      "Trained batch 860 batch loss 5.35864 epoch total loss 6.5205946\n",
      "Trained batch 861 batch loss 5.35907221 epoch total loss 6.51924562\n",
      "Trained batch 862 batch loss 5.64805031 epoch total loss 6.51823473\n",
      "Trained batch 863 batch loss 6.67822409 epoch total loss 6.51842\n",
      "Trained batch 864 batch loss 6.46459 epoch total loss 6.51835728\n",
      "Trained batch 865 batch loss 6.67117262 epoch total loss 6.51853418\n",
      "Trained batch 866 batch loss 6.1430068 epoch total loss 6.51810074\n",
      "Trained batch 867 batch loss 6.64342642 epoch total loss 6.5182457\n",
      "Trained batch 868 batch loss 5.88992 epoch total loss 6.51752186\n",
      "Trained batch 869 batch loss 6.4177146 epoch total loss 6.51740694\n",
      "Trained batch 870 batch loss 6.28401566 epoch total loss 6.51713848\n",
      "Trained batch 871 batch loss 6.55685663 epoch total loss 6.51718426\n",
      "Trained batch 872 batch loss 6.54742527 epoch total loss 6.51721859\n",
      "Trained batch 873 batch loss 6.6560421 epoch total loss 6.51737785\n",
      "Trained batch 874 batch loss 6.85511065 epoch total loss 6.51776409\n",
      "Trained batch 875 batch loss 6.33997202 epoch total loss 6.51756096\n",
      "Trained batch 876 batch loss 6.47486305 epoch total loss 6.51751232\n",
      "Trained batch 877 batch loss 6.9897542 epoch total loss 6.51805067\n",
      "Trained batch 878 batch loss 6.56407833 epoch total loss 6.51810312\n",
      "Trained batch 879 batch loss 6.7820158 epoch total loss 6.51840353\n",
      "Trained batch 880 batch loss 6.70963478 epoch total loss 6.51862049\n",
      "Trained batch 881 batch loss 5.84598875 epoch total loss 6.51785755\n",
      "Trained batch 882 batch loss 5.89999962 epoch total loss 6.5171566\n",
      "Trained batch 883 batch loss 6.05206966 epoch total loss 6.51663\n",
      "Trained batch 884 batch loss 5.74436522 epoch total loss 6.51575661\n",
      "Trained batch 885 batch loss 6.14920521 epoch total loss 6.51534271\n",
      "Trained batch 886 batch loss 6.82000065 epoch total loss 6.51568604\n",
      "Trained batch 887 batch loss 6.51379299 epoch total loss 6.51568413\n",
      "Trained batch 888 batch loss 7.10594177 epoch total loss 6.51634884\n",
      "Trained batch 889 batch loss 7.10209608 epoch total loss 6.51700735\n",
      "Trained batch 890 batch loss 6.56220531 epoch total loss 6.5170579\n",
      "Trained batch 891 batch loss 6.80433226 epoch total loss 6.51738\n",
      "Trained batch 892 batch loss 6.77744246 epoch total loss 6.51767159\n",
      "Trained batch 893 batch loss 6.2959938 epoch total loss 6.51742315\n",
      "Trained batch 894 batch loss 5.7693181 epoch total loss 6.51658678\n",
      "Trained batch 895 batch loss 6.81869316 epoch total loss 6.51692438\n",
      "Trained batch 896 batch loss 6.43324 epoch total loss 6.51683092\n",
      "Trained batch 897 batch loss 6.68537283 epoch total loss 6.51701927\n",
      "Trained batch 898 batch loss 6.56459093 epoch total loss 6.51707172\n",
      "Trained batch 899 batch loss 6.64776945 epoch total loss 6.51721764\n",
      "Trained batch 900 batch loss 6.34494925 epoch total loss 6.51702595\n",
      "Trained batch 901 batch loss 6.4726882 epoch total loss 6.51697636\n",
      "Trained batch 902 batch loss 6.30272436 epoch total loss 6.51673889\n",
      "Trained batch 903 batch loss 6.25355244 epoch total loss 6.51644754\n",
      "Trained batch 904 batch loss 6.46347904 epoch total loss 6.51638889\n",
      "Trained batch 905 batch loss 6.41766596 epoch total loss 6.51627922\n",
      "Trained batch 906 batch loss 6.80268526 epoch total loss 6.51659584\n",
      "Trained batch 907 batch loss 7.40336514 epoch total loss 6.51757336\n",
      "Trained batch 908 batch loss 7.0602746 epoch total loss 6.51817083\n",
      "Trained batch 909 batch loss 7.14717817 epoch total loss 6.51886225\n",
      "Trained batch 910 batch loss 6.73638 epoch total loss 6.51910162\n",
      "Trained batch 911 batch loss 6.50155449 epoch total loss 6.51908207\n",
      "Trained batch 912 batch loss 6.06153965 epoch total loss 6.51858044\n",
      "Trained batch 913 batch loss 5.94935369 epoch total loss 6.51795673\n",
      "Trained batch 914 batch loss 6.56196928 epoch total loss 6.51800489\n",
      "Trained batch 915 batch loss 6.44567442 epoch total loss 6.51792622\n",
      "Trained batch 916 batch loss 6.20136738 epoch total loss 6.51758\n",
      "Trained batch 917 batch loss 5.72200918 epoch total loss 6.51671267\n",
      "Trained batch 918 batch loss 6.00026512 epoch total loss 6.51615047\n",
      "Trained batch 919 batch loss 6.03078938 epoch total loss 6.51562214\n",
      "Trained batch 920 batch loss 6.33002186 epoch total loss 6.51542044\n",
      "Trained batch 921 batch loss 6.68568802 epoch total loss 6.51560545\n",
      "Trained batch 922 batch loss 6.61492157 epoch total loss 6.51571274\n",
      "Trained batch 923 batch loss 6.36145687 epoch total loss 6.51554585\n",
      "Trained batch 924 batch loss 6.23897314 epoch total loss 6.51524591\n",
      "Trained batch 925 batch loss 6.43053627 epoch total loss 6.51515484\n",
      "Trained batch 926 batch loss 6.40902376 epoch total loss 6.5150404\n",
      "Trained batch 927 batch loss 6.34699535 epoch total loss 6.5148592\n",
      "Trained batch 928 batch loss 6.49063492 epoch total loss 6.51483297\n",
      "Trained batch 929 batch loss 6.38062525 epoch total loss 6.51468897\n",
      "Trained batch 930 batch loss 6.54962921 epoch total loss 6.51472664\n",
      "Trained batch 931 batch loss 6.45383549 epoch total loss 6.51466084\n",
      "Trained batch 932 batch loss 6.56328 epoch total loss 6.51471329\n",
      "Trained batch 933 batch loss 6.3875494 epoch total loss 6.51457739\n",
      "Trained batch 934 batch loss 6.17172146 epoch total loss 6.51421\n",
      "Trained batch 935 batch loss 6.08382702 epoch total loss 6.51375\n",
      "Trained batch 936 batch loss 6.10909033 epoch total loss 6.51331758\n",
      "Trained batch 937 batch loss 6.09093094 epoch total loss 6.51286697\n",
      "Trained batch 938 batch loss 5.84209585 epoch total loss 6.51215172\n",
      "Trained batch 939 batch loss 5.9423933 epoch total loss 6.51154518\n",
      "Trained batch 940 batch loss 6.21703053 epoch total loss 6.51123142\n",
      "Trained batch 941 batch loss 6.45691442 epoch total loss 6.51117373\n",
      "Trained batch 942 batch loss 6.7396636 epoch total loss 6.51141644\n",
      "Trained batch 943 batch loss 6.90242577 epoch total loss 6.51183128\n",
      "Trained batch 944 batch loss 6.61593 epoch total loss 6.51194096\n",
      "Trained batch 945 batch loss 6.49803 epoch total loss 6.51192665\n",
      "Trained batch 946 batch loss 6.52383518 epoch total loss 6.51193905\n",
      "Trained batch 947 batch loss 6.15647173 epoch total loss 6.51156378\n",
      "Trained batch 948 batch loss 5.746912 epoch total loss 6.51075697\n",
      "Trained batch 949 batch loss 6.57780123 epoch total loss 6.51082754\n",
      "Trained batch 950 batch loss 6.64202642 epoch total loss 6.51096582\n",
      "Trained batch 951 batch loss 6.71844959 epoch total loss 6.51118374\n",
      "Trained batch 952 batch loss 6.58180857 epoch total loss 6.51125813\n",
      "Trained batch 953 batch loss 6.66320753 epoch total loss 6.51141739\n",
      "Trained batch 954 batch loss 6.56993628 epoch total loss 6.5114789\n",
      "Trained batch 955 batch loss 6.56253576 epoch total loss 6.51153231\n",
      "Trained batch 956 batch loss 6.51004 epoch total loss 6.51153088\n",
      "Trained batch 957 batch loss 6.65202713 epoch total loss 6.51167727\n",
      "Trained batch 958 batch loss 6.21545219 epoch total loss 6.51136827\n",
      "Trained batch 959 batch loss 5.98042202 epoch total loss 6.51081467\n",
      "Trained batch 960 batch loss 6.02952242 epoch total loss 6.51031303\n",
      "Trained batch 961 batch loss 6.14300966 epoch total loss 6.50993061\n",
      "Trained batch 962 batch loss 6.62542105 epoch total loss 6.51005077\n",
      "Trained batch 963 batch loss 6.83668661 epoch total loss 6.51039028\n",
      "Trained batch 964 batch loss 6.71397305 epoch total loss 6.51060152\n",
      "Trained batch 965 batch loss 6.98466682 epoch total loss 6.51109266\n",
      "Trained batch 966 batch loss 6.90625191 epoch total loss 6.51150179\n",
      "Trained batch 967 batch loss 6.8962307 epoch total loss 6.51189947\n",
      "Trained batch 968 batch loss 6.78800249 epoch total loss 6.51218462\n",
      "Trained batch 969 batch loss 6.54384613 epoch total loss 6.51221752\n",
      "Trained batch 970 batch loss 6.71606684 epoch total loss 6.51242781\n",
      "Trained batch 971 batch loss 6.46924829 epoch total loss 6.51238346\n",
      "Trained batch 972 batch loss 6.69917679 epoch total loss 6.51257563\n",
      "Trained batch 973 batch loss 6.70474434 epoch total loss 6.51277304\n",
      "Trained batch 974 batch loss 6.50414944 epoch total loss 6.51276398\n",
      "Trained batch 975 batch loss 6.70547771 epoch total loss 6.51296186\n",
      "Trained batch 976 batch loss 5.94811487 epoch total loss 6.51238298\n",
      "Trained batch 977 batch loss 5.49646 epoch total loss 6.51134348\n",
      "Trained batch 978 batch loss 6.26447678 epoch total loss 6.51109123\n",
      "Trained batch 979 batch loss 6.57891273 epoch total loss 6.51116085\n",
      "Trained batch 980 batch loss 6.58381414 epoch total loss 6.51123476\n",
      "Trained batch 981 batch loss 6.91962719 epoch total loss 6.51165104\n",
      "Trained batch 982 batch loss 6.73840332 epoch total loss 6.51188183\n",
      "Trained batch 983 batch loss 6.92934704 epoch total loss 6.51230621\n",
      "Trained batch 984 batch loss 6.81148243 epoch total loss 6.51261044\n",
      "Trained batch 985 batch loss 6.90274715 epoch total loss 6.51300669\n",
      "Trained batch 986 batch loss 7.25011492 epoch total loss 6.51375389\n",
      "Trained batch 987 batch loss 6.82551765 epoch total loss 6.51407\n",
      "Trained batch 988 batch loss 6.81425524 epoch total loss 6.51437426\n",
      "Trained batch 989 batch loss 6.39173603 epoch total loss 6.51425\n",
      "Trained batch 990 batch loss 6.6067543 epoch total loss 6.51434374\n",
      "Trained batch 991 batch loss 5.85800695 epoch total loss 6.51368141\n",
      "Trained batch 992 batch loss 5.84557104 epoch total loss 6.51300764\n",
      "Trained batch 993 batch loss 6.89254284 epoch total loss 6.51339\n",
      "Trained batch 994 batch loss 6.35309887 epoch total loss 6.51322889\n",
      "Trained batch 995 batch loss 6.63070297 epoch total loss 6.51334715\n",
      "Trained batch 996 batch loss 6.70454788 epoch total loss 6.51353884\n",
      "Trained batch 997 batch loss 6.28371382 epoch total loss 6.51330853\n",
      "Trained batch 998 batch loss 6.23249483 epoch total loss 6.51302719\n",
      "Trained batch 999 batch loss 6.32478714 epoch total loss 6.51283836\n",
      "Trained batch 1000 batch loss 6.67029142 epoch total loss 6.5129962\n",
      "Trained batch 1001 batch loss 6.68034124 epoch total loss 6.51316309\n",
      "Trained batch 1002 batch loss 6.74396372 epoch total loss 6.5133934\n",
      "Trained batch 1003 batch loss 6.40795422 epoch total loss 6.5132885\n",
      "Trained batch 1004 batch loss 6.44908667 epoch total loss 6.5132246\n",
      "Trained batch 1005 batch loss 6.27815723 epoch total loss 6.51299047\n",
      "Trained batch 1006 batch loss 6.56055689 epoch total loss 6.51303816\n",
      "Trained batch 1007 batch loss 6.12501526 epoch total loss 6.51265287\n",
      "Trained batch 1008 batch loss 6.44789553 epoch total loss 6.51258802\n",
      "Trained batch 1009 batch loss 6.74781656 epoch total loss 6.51282167\n",
      "Trained batch 1010 batch loss 6.91652107 epoch total loss 6.51322126\n",
      "Trained batch 1011 batch loss 6.73061943 epoch total loss 6.51343632\n",
      "Trained batch 1012 batch loss 6.85838413 epoch total loss 6.51377726\n",
      "Trained batch 1013 batch loss 5.98765087 epoch total loss 6.51325798\n",
      "Trained batch 1014 batch loss 6.65348577 epoch total loss 6.51339579\n",
      "Trained batch 1015 batch loss 6.61179 epoch total loss 6.51349306\n",
      "Trained batch 1016 batch loss 6.342103 epoch total loss 6.51332426\n",
      "Trained batch 1017 batch loss 6.07486916 epoch total loss 6.5128932\n",
      "Trained batch 1018 batch loss 6.44490051 epoch total loss 6.51282644\n",
      "Trained batch 1019 batch loss 6.52908 epoch total loss 6.51284266\n",
      "Trained batch 1020 batch loss 6.21164751 epoch total loss 6.51254702\n",
      "Trained batch 1021 batch loss 6.4753809 epoch total loss 6.51251078\n",
      "Trained batch 1022 batch loss 6.66821814 epoch total loss 6.51266336\n",
      "Trained batch 1023 batch loss 6.61704922 epoch total loss 6.51276541\n",
      "Trained batch 1024 batch loss 6.67838573 epoch total loss 6.51292706\n",
      "Trained batch 1025 batch loss 6.45228052 epoch total loss 6.51286793\n",
      "Trained batch 1026 batch loss 6.7232995 epoch total loss 6.51307249\n",
      "Trained batch 1027 batch loss 6.55212116 epoch total loss 6.51311064\n",
      "Trained batch 1028 batch loss 6.81573915 epoch total loss 6.51340532\n",
      "Trained batch 1029 batch loss 6.51161289 epoch total loss 6.51340389\n",
      "Trained batch 1030 batch loss 6.61338663 epoch total loss 6.51350069\n",
      "Trained batch 1031 batch loss 6.24484491 epoch total loss 6.51324\n",
      "Trained batch 1032 batch loss 5.61688662 epoch total loss 6.51237106\n",
      "Trained batch 1033 batch loss 5.75686789 epoch total loss 6.5116396\n",
      "Trained batch 1034 batch loss 6.14116478 epoch total loss 6.51128149\n",
      "Trained batch 1035 batch loss 6.65197086 epoch total loss 6.51141739\n",
      "Trained batch 1036 batch loss 6.23907614 epoch total loss 6.51115465\n",
      "Trained batch 1037 batch loss 6.07808256 epoch total loss 6.51073694\n",
      "Trained batch 1038 batch loss 5.87824106 epoch total loss 6.51012802\n",
      "Trained batch 1039 batch loss 6.29927206 epoch total loss 6.50992489\n",
      "Trained batch 1040 batch loss 5.90102291 epoch total loss 6.50933933\n",
      "Trained batch 1041 batch loss 6.50018549 epoch total loss 6.50933027\n",
      "Trained batch 1042 batch loss 6.43117285 epoch total loss 6.50925541\n",
      "Trained batch 1043 batch loss 6.78183746 epoch total loss 6.50951672\n",
      "Trained batch 1044 batch loss 6.33198404 epoch total loss 6.50934649\n",
      "Trained batch 1045 batch loss 6.66341639 epoch total loss 6.5094943\n",
      "Trained batch 1046 batch loss 6.12064552 epoch total loss 6.50912237\n",
      "Trained batch 1047 batch loss 6.15990162 epoch total loss 6.50878859\n",
      "Trained batch 1048 batch loss 6.35423231 epoch total loss 6.50864077\n",
      "Trained batch 1049 batch loss 6.30662775 epoch total loss 6.50844812\n",
      "Trained batch 1050 batch loss 6.52485132 epoch total loss 6.50846386\n",
      "Trained batch 1051 batch loss 6.40873909 epoch total loss 6.50836897\n",
      "Trained batch 1052 batch loss 6.80200815 epoch total loss 6.5086484\n",
      "Trained batch 1053 batch loss 6.67490387 epoch total loss 6.50880623\n",
      "Trained batch 1054 batch loss 6.62947083 epoch total loss 6.50892067\n",
      "Trained batch 1055 batch loss 6.86619139 epoch total loss 6.50925922\n",
      "Trained batch 1056 batch loss 6.35466576 epoch total loss 6.50911283\n",
      "Trained batch 1057 batch loss 6.27628803 epoch total loss 6.50889254\n",
      "Trained batch 1058 batch loss 6.49632359 epoch total loss 6.50888062\n",
      "Trained batch 1059 batch loss 6.43924093 epoch total loss 6.50881481\n",
      "Trained batch 1060 batch loss 6.58660364 epoch total loss 6.50888824\n",
      "Trained batch 1061 batch loss 6.90648651 epoch total loss 6.50926256\n",
      "Trained batch 1062 batch loss 6.93227386 epoch total loss 6.50966072\n",
      "Trained batch 1063 batch loss 6.85437155 epoch total loss 6.50998497\n",
      "Trained batch 1064 batch loss 6.56576347 epoch total loss 6.5100379\n",
      "Trained batch 1065 batch loss 6.41321039 epoch total loss 6.50994682\n",
      "Trained batch 1066 batch loss 6.41201735 epoch total loss 6.50985479\n",
      "Trained batch 1067 batch loss 6.4577961 epoch total loss 6.50980616\n",
      "Trained batch 1068 batch loss 6.21497536 epoch total loss 6.50953\n",
      "Trained batch 1069 batch loss 6.59935617 epoch total loss 6.50961399\n",
      "Trained batch 1070 batch loss 6.466887 epoch total loss 6.50957394\n",
      "Trained batch 1071 batch loss 6.69649029 epoch total loss 6.50974846\n",
      "Trained batch 1072 batch loss 6.93355703 epoch total loss 6.51014376\n",
      "Trained batch 1073 batch loss 7.06261635 epoch total loss 6.51065826\n",
      "Trained batch 1074 batch loss 6.53741884 epoch total loss 6.51068354\n",
      "Trained batch 1075 batch loss 6.639153 epoch total loss 6.51080322\n",
      "Trained batch 1076 batch loss 6.37642145 epoch total loss 6.51067829\n",
      "Trained batch 1077 batch loss 6.12545586 epoch total loss 6.51032066\n",
      "Trained batch 1078 batch loss 6.34776115 epoch total loss 6.51016951\n",
      "Trained batch 1079 batch loss 6.37270355 epoch total loss 6.51004219\n",
      "Trained batch 1080 batch loss 5.97116232 epoch total loss 6.50954342\n",
      "Trained batch 1081 batch loss 6.37936068 epoch total loss 6.50942278\n",
      "Trained batch 1082 batch loss 6.56417942 epoch total loss 6.50947332\n",
      "Trained batch 1083 batch loss 6.43034458 epoch total loss 6.5094\n",
      "Trained batch 1084 batch loss 6.55044651 epoch total loss 6.50943756\n",
      "Trained batch 1085 batch loss 6.42035437 epoch total loss 6.50935555\n",
      "Trained batch 1086 batch loss 6.47528362 epoch total loss 6.50932407\n",
      "Trained batch 1087 batch loss 6.3607254 epoch total loss 6.5091877\n",
      "Trained batch 1088 batch loss 6.82887602 epoch total loss 6.50948143\n",
      "Trained batch 1089 batch loss 7.04148817 epoch total loss 6.50997\n",
      "Trained batch 1090 batch loss 6.93506384 epoch total loss 6.51036024\n",
      "Trained batch 1091 batch loss 7.17110205 epoch total loss 6.51096535\n",
      "Trained batch 1092 batch loss 6.37418127 epoch total loss 6.51084\n",
      "Trained batch 1093 batch loss 5.95675516 epoch total loss 6.51033306\n",
      "Trained batch 1094 batch loss 6.41876364 epoch total loss 6.51024961\n",
      "Trained batch 1095 batch loss 6.51329279 epoch total loss 6.510252\n",
      "Trained batch 1096 batch loss 6.43032837 epoch total loss 6.51017904\n",
      "Trained batch 1097 batch loss 6.62558556 epoch total loss 6.51028395\n",
      "Trained batch 1098 batch loss 6.63372231 epoch total loss 6.51039648\n",
      "Trained batch 1099 batch loss 6.56968594 epoch total loss 6.51045084\n",
      "Trained batch 1100 batch loss 6.67722273 epoch total loss 6.51060247\n",
      "Trained batch 1101 batch loss 6.69707346 epoch total loss 6.51077175\n",
      "Trained batch 1102 batch loss 6.58792496 epoch total loss 6.51084185\n",
      "Trained batch 1103 batch loss 6.54993963 epoch total loss 6.51087713\n",
      "Trained batch 1104 batch loss 6.6217494 epoch total loss 6.51097727\n",
      "Trained batch 1105 batch loss 6.59413433 epoch total loss 6.51105261\n",
      "Trained batch 1106 batch loss 6.49746895 epoch total loss 6.51104069\n",
      "Trained batch 1107 batch loss 6.55021667 epoch total loss 6.51107597\n",
      "Trained batch 1108 batch loss 6.61003828 epoch total loss 6.51116514\n",
      "Trained batch 1109 batch loss 6.39578295 epoch total loss 6.51106119\n",
      "Trained batch 1110 batch loss 6.65873528 epoch total loss 6.51119423\n",
      "Trained batch 1111 batch loss 6.4834547 epoch total loss 6.51116943\n",
      "Trained batch 1112 batch loss 6.20557976 epoch total loss 6.51089478\n",
      "Trained batch 1113 batch loss 6.59512472 epoch total loss 6.51097\n",
      "Trained batch 1114 batch loss 6.20065641 epoch total loss 6.51069164\n",
      "Trained batch 1115 batch loss 6.40593767 epoch total loss 6.51059771\n",
      "Trained batch 1116 batch loss 7.11282587 epoch total loss 6.51113749\n",
      "Trained batch 1117 batch loss 7.07150745 epoch total loss 6.51163864\n",
      "Trained batch 1118 batch loss 6.755126 epoch total loss 6.51185656\n",
      "Trained batch 1119 batch loss 6.70797729 epoch total loss 6.51203156\n",
      "Trained batch 1120 batch loss 6.88409376 epoch total loss 6.51236391\n",
      "Trained batch 1121 batch loss 6.92634869 epoch total loss 6.51273298\n",
      "Trained batch 1122 batch loss 6.58481026 epoch total loss 6.51279736\n",
      "Trained batch 1123 batch loss 6.94500732 epoch total loss 6.51318216\n",
      "Trained batch 1124 batch loss 6.77217865 epoch total loss 6.51341248\n",
      "Trained batch 1125 batch loss 6.9544425 epoch total loss 6.51380491\n",
      "Trained batch 1126 batch loss 6.85768795 epoch total loss 6.51411\n",
      "Trained batch 1127 batch loss 6.77306 epoch total loss 6.51434\n",
      "Trained batch 1128 batch loss 6.68081045 epoch total loss 6.51448727\n",
      "Trained batch 1129 batch loss 6.8097 epoch total loss 6.51474857\n",
      "Trained batch 1130 batch loss 6.9677844 epoch total loss 6.51514959\n",
      "Trained batch 1131 batch loss 7.06336117 epoch total loss 6.51563454\n",
      "Trained batch 1132 batch loss 7.52419281 epoch total loss 6.51652575\n",
      "Trained batch 1133 batch loss 7.01665783 epoch total loss 6.51696682\n",
      "Trained batch 1134 batch loss 6.69565964 epoch total loss 6.51712465\n",
      "Trained batch 1135 batch loss 6.46854925 epoch total loss 6.51708221\n",
      "Trained batch 1136 batch loss 6.53836823 epoch total loss 6.51710081\n",
      "Trained batch 1137 batch loss 6.86318 epoch total loss 6.51740551\n",
      "Trained batch 1138 batch loss 7.04685783 epoch total loss 6.5178709\n",
      "Trained batch 1139 batch loss 6.92044973 epoch total loss 6.51822424\n",
      "Trained batch 1140 batch loss 6.7325449 epoch total loss 6.51841211\n",
      "Trained batch 1141 batch loss 6.60726261 epoch total loss 6.51849\n",
      "Trained batch 1142 batch loss 6.58006382 epoch total loss 6.5185442\n",
      "Trained batch 1143 batch loss 6.87523699 epoch total loss 6.51885605\n",
      "Trained batch 1144 batch loss 7.10753632 epoch total loss 6.51937056\n",
      "Trained batch 1145 batch loss 7.18742 epoch total loss 6.51995373\n",
      "Trained batch 1146 batch loss 6.78781271 epoch total loss 6.52018738\n",
      "Trained batch 1147 batch loss 6.84373617 epoch total loss 6.52046967\n",
      "Trained batch 1148 batch loss 6.84874439 epoch total loss 6.52075529\n",
      "Trained batch 1149 batch loss 6.91252232 epoch total loss 6.52109623\n",
      "Trained batch 1150 batch loss 6.71266365 epoch total loss 6.52126312\n",
      "Trained batch 1151 batch loss 6.28227234 epoch total loss 6.5210557\n",
      "Trained batch 1152 batch loss 6.48917389 epoch total loss 6.52102804\n",
      "Trained batch 1153 batch loss 6.38028955 epoch total loss 6.52090597\n",
      "Trained batch 1154 batch loss 6.85139894 epoch total loss 6.52119255\n",
      "Trained batch 1155 batch loss 6.66297626 epoch total loss 6.5213151\n",
      "Trained batch 1156 batch loss 6.88086081 epoch total loss 6.52162647\n",
      "Trained batch 1157 batch loss 6.44819117 epoch total loss 6.52156305\n",
      "Trained batch 1158 batch loss 6.38964462 epoch total loss 6.52144909\n",
      "Trained batch 1159 batch loss 6.2771349 epoch total loss 6.52123833\n",
      "Trained batch 1160 batch loss 6.46623182 epoch total loss 6.52119112\n",
      "Trained batch 1161 batch loss 6.64983702 epoch total loss 6.52130175\n",
      "Trained batch 1162 batch loss 6.60725546 epoch total loss 6.52137613\n",
      "Trained batch 1163 batch loss 6.81207037 epoch total loss 6.521626\n",
      "Trained batch 1164 batch loss 6.93685 epoch total loss 6.52198267\n",
      "Trained batch 1165 batch loss 7.00182343 epoch total loss 6.52239466\n",
      "Trained batch 1166 batch loss 7.13983965 epoch total loss 6.52292395\n",
      "Trained batch 1167 batch loss 6.6330471 epoch total loss 6.52301836\n",
      "Trained batch 1168 batch loss 6.71674871 epoch total loss 6.5231843\n",
      "Trained batch 1169 batch loss 6.58677 epoch total loss 6.52323866\n",
      "Trained batch 1170 batch loss 6.45395231 epoch total loss 6.52317953\n",
      "Trained batch 1171 batch loss 6.54356813 epoch total loss 6.5231967\n",
      "Trained batch 1172 batch loss 6.39075279 epoch total loss 6.52308369\n",
      "Trained batch 1173 batch loss 6.2824192 epoch total loss 6.52287865\n",
      "Trained batch 1174 batch loss 6.34762383 epoch total loss 6.5227294\n",
      "Trained batch 1175 batch loss 6.50297546 epoch total loss 6.52271223\n",
      "Trained batch 1176 batch loss 6.4838171 epoch total loss 6.52267933\n",
      "Trained batch 1177 batch loss 6.15083265 epoch total loss 6.52236366\n",
      "Trained batch 1178 batch loss 6.31324291 epoch total loss 6.52218628\n",
      "Trained batch 1179 batch loss 6.3125639 epoch total loss 6.52200842\n",
      "Trained batch 1180 batch loss 6.41163874 epoch total loss 6.52191496\n",
      "Trained batch 1181 batch loss 6.65470076 epoch total loss 6.52202702\n",
      "Trained batch 1182 batch loss 6.8650775 epoch total loss 6.52231741\n",
      "Trained batch 1183 batch loss 6.55411243 epoch total loss 6.52234459\n",
      "Trained batch 1184 batch loss 6.26739883 epoch total loss 6.52212954\n",
      "Trained batch 1185 batch loss 6.42916059 epoch total loss 6.52205086\n",
      "Trained batch 1186 batch loss 6.8056488 epoch total loss 6.52229\n",
      "Trained batch 1187 batch loss 6.75631428 epoch total loss 6.52248716\n",
      "Trained batch 1188 batch loss 6.75889635 epoch total loss 6.522686\n",
      "Trained batch 1189 batch loss 6.24700356 epoch total loss 6.52245426\n",
      "Trained batch 1190 batch loss 6.39118814 epoch total loss 6.52234411\n",
      "Trained batch 1191 batch loss 6.39269876 epoch total loss 6.52223492\n",
      "Trained batch 1192 batch loss 6.14146852 epoch total loss 6.52191591\n",
      "Trained batch 1193 batch loss 6.37729549 epoch total loss 6.5217948\n",
      "Trained batch 1194 batch loss 6.40153503 epoch total loss 6.52169371\n",
      "Trained batch 1195 batch loss 6.28685045 epoch total loss 6.52149725\n",
      "Trained batch 1196 batch loss 6.29787683 epoch total loss 6.52131\n",
      "Trained batch 1197 batch loss 6.52047777 epoch total loss 6.52130938\n",
      "Trained batch 1198 batch loss 6.56798 epoch total loss 6.52134848\n",
      "Trained batch 1199 batch loss 6.74182892 epoch total loss 6.52153206\n",
      "Trained batch 1200 batch loss 6.16430902 epoch total loss 6.52123451\n",
      "Trained batch 1201 batch loss 5.69591856 epoch total loss 6.52054739\n",
      "Trained batch 1202 batch loss 5.24068975 epoch total loss 6.51948261\n",
      "Trained batch 1203 batch loss 5.4318347 epoch total loss 6.51857805\n",
      "Trained batch 1204 batch loss 6.22364 epoch total loss 6.51833344\n",
      "Trained batch 1205 batch loss 6.87542915 epoch total loss 6.51862955\n",
      "Trained batch 1206 batch loss 6.83586836 epoch total loss 6.51889277\n",
      "Trained batch 1207 batch loss 5.96329784 epoch total loss 6.51843262\n",
      "Trained batch 1208 batch loss 6.71934605 epoch total loss 6.51859856\n",
      "Trained batch 1209 batch loss 6.48292 epoch total loss 6.51856899\n",
      "Trained batch 1210 batch loss 6.11493063 epoch total loss 6.51823568\n",
      "Trained batch 1211 batch loss 5.89253235 epoch total loss 6.51771879\n",
      "Trained batch 1212 batch loss 6.38605595 epoch total loss 6.51761055\n",
      "Trained batch 1213 batch loss 6.10486126 epoch total loss 6.51727\n",
      "Trained batch 1214 batch loss 6.21336746 epoch total loss 6.51701975\n",
      "Trained batch 1215 batch loss 6.41461563 epoch total loss 6.51693535\n",
      "Trained batch 1216 batch loss 6.28483725 epoch total loss 6.51674461\n",
      "Trained batch 1217 batch loss 6.61279964 epoch total loss 6.51682329\n",
      "Trained batch 1218 batch loss 6.68501854 epoch total loss 6.51696157\n",
      "Trained batch 1219 batch loss 6.75903368 epoch total loss 6.51716042\n",
      "Trained batch 1220 batch loss 6.55923462 epoch total loss 6.51719475\n",
      "Trained batch 1221 batch loss 6.47187138 epoch total loss 6.51715755\n",
      "Trained batch 1222 batch loss 6.82844877 epoch total loss 6.51741219\n",
      "Trained batch 1223 batch loss 6.58084488 epoch total loss 6.51746416\n",
      "Trained batch 1224 batch loss 6.66932917 epoch total loss 6.51758862\n",
      "Trained batch 1225 batch loss 6.86924553 epoch total loss 6.51787567\n",
      "Trained batch 1226 batch loss 7.08348465 epoch total loss 6.51833677\n",
      "Trained batch 1227 batch loss 6.90065813 epoch total loss 6.51864862\n",
      "Trained batch 1228 batch loss 6.85748 epoch total loss 6.51892471\n",
      "Trained batch 1229 batch loss 7.06830788 epoch total loss 6.51937151\n",
      "Trained batch 1230 batch loss 6.91577673 epoch total loss 6.51969385\n",
      "Trained batch 1231 batch loss 6.88795424 epoch total loss 6.51999331\n",
      "Trained batch 1232 batch loss 6.75150871 epoch total loss 6.52018118\n",
      "Trained batch 1233 batch loss 6.67135572 epoch total loss 6.52030373\n",
      "Trained batch 1234 batch loss 6.80658245 epoch total loss 6.52053595\n",
      "Trained batch 1235 batch loss 6.76272058 epoch total loss 6.52073193\n",
      "Trained batch 1236 batch loss 6.74761438 epoch total loss 6.52091551\n",
      "Trained batch 1237 batch loss 6.73976946 epoch total loss 6.52109241\n",
      "Trained batch 1238 batch loss 6.49992609 epoch total loss 6.52107525\n",
      "Trained batch 1239 batch loss 5.8328495 epoch total loss 6.52052\n",
      "Trained batch 1240 batch loss 5.5751 epoch total loss 6.51975775\n",
      "Trained batch 1241 batch loss 6.30994892 epoch total loss 6.51958847\n",
      "Trained batch 1242 batch loss 6.29565334 epoch total loss 6.51940823\n",
      "Trained batch 1243 batch loss 6.76051521 epoch total loss 6.5196023\n",
      "Trained batch 1244 batch loss 6.46429825 epoch total loss 6.51955795\n",
      "Trained batch 1245 batch loss 6.40430212 epoch total loss 6.51946545\n",
      "Trained batch 1246 batch loss 6.71853447 epoch total loss 6.51962519\n",
      "Trained batch 1247 batch loss 6.68810558 epoch total loss 6.51976\n",
      "Trained batch 1248 batch loss 6.53922462 epoch total loss 6.51977587\n",
      "Trained batch 1249 batch loss 6.313169 epoch total loss 6.5196104\n",
      "Trained batch 1250 batch loss 6.15567636 epoch total loss 6.51931906\n",
      "Trained batch 1251 batch loss 6.43629169 epoch total loss 6.51925278\n",
      "Trained batch 1252 batch loss 6.39787817 epoch total loss 6.51915598\n",
      "Trained batch 1253 batch loss 6.63966799 epoch total loss 6.5192523\n",
      "Trained batch 1254 batch loss 6.47801876 epoch total loss 6.5192194\n",
      "Trained batch 1255 batch loss 6.53953552 epoch total loss 6.51923561\n",
      "Trained batch 1256 batch loss 6.43267298 epoch total loss 6.51916647\n",
      "Trained batch 1257 batch loss 6.70364952 epoch total loss 6.51931381\n",
      "Trained batch 1258 batch loss 6.37554693 epoch total loss 6.5192\n",
      "Trained batch 1259 batch loss 6.63832 epoch total loss 6.51929474\n",
      "Trained batch 1260 batch loss 6.37258673 epoch total loss 6.51917839\n",
      "Trained batch 1261 batch loss 6.37733316 epoch total loss 6.51906586\n",
      "Trained batch 1262 batch loss 6.84043312 epoch total loss 6.51932096\n",
      "Trained batch 1263 batch loss 6.82206678 epoch total loss 6.51956081\n",
      "Trained batch 1264 batch loss 6.69990063 epoch total loss 6.51970339\n",
      "Trained batch 1265 batch loss 6.62853956 epoch total loss 6.5197897\n",
      "Trained batch 1266 batch loss 6.50144577 epoch total loss 6.51977491\n",
      "Trained batch 1267 batch loss 6.58315945 epoch total loss 6.51982498\n",
      "Trained batch 1268 batch loss 6.77529526 epoch total loss 6.52002668\n",
      "Trained batch 1269 batch loss 6.71242905 epoch total loss 6.52017832\n",
      "Trained batch 1270 batch loss 6.60621881 epoch total loss 6.52024651\n",
      "Trained batch 1271 batch loss 6.58757639 epoch total loss 6.52029943\n",
      "Trained batch 1272 batch loss 6.42507553 epoch total loss 6.52022457\n",
      "Trained batch 1273 batch loss 5.85439539 epoch total loss 6.51970148\n",
      "Trained batch 1274 batch loss 6.02695894 epoch total loss 6.51931524\n",
      "Trained batch 1275 batch loss 6.57574272 epoch total loss 6.51935959\n",
      "Trained batch 1276 batch loss 6.57005501 epoch total loss 6.51939964\n",
      "Trained batch 1277 batch loss 6.62354088 epoch total loss 6.51948166\n",
      "Trained batch 1278 batch loss 6.472651 epoch total loss 6.51944494\n",
      "Trained batch 1279 batch loss 6.12380219 epoch total loss 6.51913595\n",
      "Trained batch 1280 batch loss 6.20437336 epoch total loss 6.51888943\n",
      "Trained batch 1281 batch loss 6.03192234 epoch total loss 6.51851\n",
      "Trained batch 1282 batch loss 6.36942244 epoch total loss 6.51839304\n",
      "Trained batch 1283 batch loss 6.33965063 epoch total loss 6.5182538\n",
      "Trained batch 1284 batch loss 6.6688633 epoch total loss 6.51837158\n",
      "Trained batch 1285 batch loss 6.53480053 epoch total loss 6.51838446\n",
      "Trained batch 1286 batch loss 6.61807299 epoch total loss 6.51846218\n",
      "Trained batch 1287 batch loss 6.56262827 epoch total loss 6.51849604\n",
      "Trained batch 1288 batch loss 6.68472481 epoch total loss 6.51862526\n",
      "Trained batch 1289 batch loss 6.72419357 epoch total loss 6.518785\n",
      "Trained batch 1290 batch loss 6.77179766 epoch total loss 6.51898098\n",
      "Trained batch 1291 batch loss 6.56681871 epoch total loss 6.5190177\n",
      "Trained batch 1292 batch loss 6.54041481 epoch total loss 6.51903391\n",
      "Trained batch 1293 batch loss 6.57753944 epoch total loss 6.51907873\n",
      "Trained batch 1294 batch loss 7.12542 epoch total loss 6.51954699\n",
      "Trained batch 1295 batch loss 6.68418264 epoch total loss 6.5196743\n",
      "Trained batch 1296 batch loss 6.7793889 epoch total loss 6.51987505\n",
      "Trained batch 1297 batch loss 6.48988819 epoch total loss 6.51985216\n",
      "Trained batch 1298 batch loss 6.5386548 epoch total loss 6.51986694\n",
      "Trained batch 1299 batch loss 6.74392223 epoch total loss 6.52003956\n",
      "Trained batch 1300 batch loss 6.69722366 epoch total loss 6.52017593\n",
      "Trained batch 1301 batch loss 6.79897594 epoch total loss 6.52039\n",
      "Trained batch 1302 batch loss 6.4696126 epoch total loss 6.52035093\n",
      "Trained batch 1303 batch loss 6.73107862 epoch total loss 6.52051306\n",
      "Trained batch 1304 batch loss 6.66464663 epoch total loss 6.52062368\n",
      "Trained batch 1305 batch loss 6.66435432 epoch total loss 6.52073383\n",
      "Trained batch 1306 batch loss 6.74585772 epoch total loss 6.52090645\n",
      "Trained batch 1307 batch loss 6.68281746 epoch total loss 6.52103\n",
      "Trained batch 1308 batch loss 6.68406773 epoch total loss 6.5211544\n",
      "Trained batch 1309 batch loss 6.84338474 epoch total loss 6.52140093\n",
      "Trained batch 1310 batch loss 6.86137772 epoch total loss 6.52166033\n",
      "Trained batch 1311 batch loss 6.65924 epoch total loss 6.52176523\n",
      "Trained batch 1312 batch loss 6.69983053 epoch total loss 6.52190113\n",
      "Trained batch 1313 batch loss 6.6967454 epoch total loss 6.52203417\n",
      "Trained batch 1314 batch loss 6.57910442 epoch total loss 6.52207756\n",
      "Trained batch 1315 batch loss 6.67751455 epoch total loss 6.52219582\n",
      "Trained batch 1316 batch loss 6.57901812 epoch total loss 6.52223921\n",
      "Trained batch 1317 batch loss 6.23289108 epoch total loss 6.52201891\n",
      "Trained batch 1318 batch loss 6.37566042 epoch total loss 6.52190828\n",
      "Trained batch 1319 batch loss 6.47571087 epoch total loss 6.521873\n",
      "Trained batch 1320 batch loss 6.34446335 epoch total loss 6.52173901\n",
      "Trained batch 1321 batch loss 6.77451849 epoch total loss 6.52193\n",
      "Trained batch 1322 batch loss 6.6188755 epoch total loss 6.52200365\n",
      "Trained batch 1323 batch loss 6.50662899 epoch total loss 6.52199221\n",
      "Trained batch 1324 batch loss 6.02361631 epoch total loss 6.52161551\n",
      "Trained batch 1325 batch loss 6.56013536 epoch total loss 6.52164507\n",
      "Trained batch 1326 batch loss 6.2084446 epoch total loss 6.52140856\n",
      "Trained batch 1327 batch loss 6.54678679 epoch total loss 6.52142763\n",
      "Trained batch 1328 batch loss 6.20605659 epoch total loss 6.52119\n",
      "Trained batch 1329 batch loss 6.39954 epoch total loss 6.52109861\n",
      "Trained batch 1330 batch loss 6.40419722 epoch total loss 6.52101088\n",
      "Trained batch 1331 batch loss 6.76292038 epoch total loss 6.52119255\n",
      "Trained batch 1332 batch loss 6.31677198 epoch total loss 6.52103853\n",
      "Trained batch 1333 batch loss 5.90735245 epoch total loss 6.52057791\n",
      "Trained batch 1334 batch loss 5.73133135 epoch total loss 6.51998663\n",
      "Trained batch 1335 batch loss 6.02637911 epoch total loss 6.5196166\n",
      "Trained batch 1336 batch loss 6.2181778 epoch total loss 6.51939106\n",
      "Trained batch 1337 batch loss 6.09127903 epoch total loss 6.51907\n",
      "Trained batch 1338 batch loss 6.3581214 epoch total loss 6.51895046\n",
      "Trained batch 1339 batch loss 6.55168772 epoch total loss 6.51897478\n",
      "Trained batch 1340 batch loss 6.41067505 epoch total loss 6.5188942\n",
      "Trained batch 1341 batch loss 6.63536072 epoch total loss 6.51898146\n",
      "Trained batch 1342 batch loss 6.36204529 epoch total loss 6.51886463\n",
      "Trained batch 1343 batch loss 7.46983099 epoch total loss 6.51957273\n",
      "Trained batch 1344 batch loss 7.32260799 epoch total loss 6.52016973\n",
      "Trained batch 1345 batch loss 6.50334024 epoch total loss 6.52015734\n",
      "Trained batch 1346 batch loss 6.40761662 epoch total loss 6.52007341\n",
      "Trained batch 1347 batch loss 6.14116096 epoch total loss 6.51979208\n",
      "Trained batch 1348 batch loss 6.96564913 epoch total loss 6.520123\n",
      "Trained batch 1349 batch loss 6.43849039 epoch total loss 6.52006245\n",
      "Trained batch 1350 batch loss 6.46543455 epoch total loss 6.52002239\n",
      "Trained batch 1351 batch loss 6.12502098 epoch total loss 6.51973\n",
      "Trained batch 1352 batch loss 6.43973637 epoch total loss 6.51967049\n",
      "Trained batch 1353 batch loss 6.62434721 epoch total loss 6.51974773\n",
      "Trained batch 1354 batch loss 6.66616917 epoch total loss 6.51985598\n",
      "Trained batch 1355 batch loss 6.73471069 epoch total loss 6.52001429\n",
      "Trained batch 1356 batch loss 6.54801035 epoch total loss 6.52003479\n",
      "Trained batch 1357 batch loss 6.643435 epoch total loss 6.52012587\n",
      "Trained batch 1358 batch loss 6.49302769 epoch total loss 6.52010584\n",
      "Trained batch 1359 batch loss 6.2754612 epoch total loss 6.51992559\n",
      "Trained batch 1360 batch loss 6.46338558 epoch total loss 6.51988459\n",
      "Trained batch 1361 batch loss 6.21069193 epoch total loss 6.51965761\n",
      "Trained batch 1362 batch loss 6.28672647 epoch total loss 6.5194869\n",
      "Trained batch 1363 batch loss 6.70230246 epoch total loss 6.5196209\n",
      "Trained batch 1364 batch loss 6.65606356 epoch total loss 6.51972103\n",
      "Trained batch 1365 batch loss 6.46552944 epoch total loss 6.51968145\n",
      "Trained batch 1366 batch loss 6.4082756 epoch total loss 6.5196\n",
      "Trained batch 1367 batch loss 5.88416 epoch total loss 6.519135\n",
      "Trained batch 1368 batch loss 6.76108837 epoch total loss 6.51931143\n",
      "Trained batch 1369 batch loss 6.7448349 epoch total loss 6.51947641\n",
      "Trained batch 1370 batch loss 7.43244505 epoch total loss 6.52014303\n",
      "Trained batch 1371 batch loss 7.38804531 epoch total loss 6.52077579\n",
      "Trained batch 1372 batch loss 7.42381144 epoch total loss 6.52143383\n",
      "Trained batch 1373 batch loss 7.27852821 epoch total loss 6.52198505\n",
      "Trained batch 1374 batch loss 7.01884747 epoch total loss 6.5223465\n",
      "Trained batch 1375 batch loss 6.35665894 epoch total loss 6.52222586\n",
      "Trained batch 1376 batch loss 6.09606171 epoch total loss 6.52191591\n",
      "Trained batch 1377 batch loss 6.6117382 epoch total loss 6.52198076\n",
      "Trained batch 1378 batch loss 6.73818588 epoch total loss 6.52213764\n",
      "Trained batch 1379 batch loss 7.15216208 epoch total loss 6.52259493\n",
      "Trained batch 1380 batch loss 6.25351954 epoch total loss 6.5224\n",
      "Trained batch 1381 batch loss 6.15833807 epoch total loss 6.52213621\n",
      "Trained batch 1382 batch loss 6.63827324 epoch total loss 6.52222061\n",
      "Trained batch 1383 batch loss 6.47876072 epoch total loss 6.52218914\n",
      "Trained batch 1384 batch loss 6.00430441 epoch total loss 6.52181482\n",
      "Trained batch 1385 batch loss 6.42001677 epoch total loss 6.52174091\n",
      "Trained batch 1386 batch loss 6.39086103 epoch total loss 6.5216465\n",
      "Trained batch 1387 batch loss 6.60513496 epoch total loss 6.52170706\n",
      "Trained batch 1388 batch loss 6.74712372 epoch total loss 6.52186918\n",
      "Epoch 8 train loss 6.52186918258667\n",
      "Validated batch 1 batch loss 6.96523285\n",
      "Validated batch 2 batch loss 7.85147667\n",
      "Validated batch 3 batch loss 7.59920549\n",
      "Validated batch 4 batch loss 7.52693748\n",
      "Validated batch 5 batch loss 7.4806447\n",
      "Validated batch 6 batch loss 7.83208418\n",
      "Validated batch 7 batch loss 7.41089058\n",
      "Validated batch 8 batch loss 7.85566664\n",
      "Validated batch 9 batch loss 7.16866302\n",
      "Validated batch 10 batch loss 7.44031334\n",
      "Validated batch 11 batch loss 7.28024578\n",
      "Validated batch 12 batch loss 6.91873884\n",
      "Validated batch 13 batch loss 7.06413078\n",
      "Validated batch 14 batch loss 7.70274305\n",
      "Validated batch 15 batch loss 7.64318085\n",
      "Validated batch 16 batch loss 7.15133858\n",
      "Validated batch 17 batch loss 7.68669891\n",
      "Validated batch 18 batch loss 7.62779951\n",
      "Validated batch 19 batch loss 7.63452148\n",
      "Validated batch 20 batch loss 7.85289049\n",
      "Validated batch 21 batch loss 7.72789955\n",
      "Validated batch 22 batch loss 7.35795593\n",
      "Validated batch 23 batch loss 7.12247181\n",
      "Validated batch 24 batch loss 7.31824303\n",
      "Validated batch 25 batch loss 7.57772779\n",
      "Validated batch 26 batch loss 7.33274126\n",
      "Validated batch 27 batch loss 7.96745253\n",
      "Validated batch 28 batch loss 7.86815691\n",
      "Validated batch 29 batch loss 7.80119896\n",
      "Validated batch 30 batch loss 7.59229851\n",
      "Validated batch 31 batch loss 7.5069561\n",
      "Validated batch 32 batch loss 7.60099745\n",
      "Validated batch 33 batch loss 7.79489708\n",
      "Validated batch 34 batch loss 7.95107317\n",
      "Validated batch 35 batch loss 7.56690264\n",
      "Validated batch 36 batch loss 7.28210402\n",
      "Validated batch 37 batch loss 7.67074537\n",
      "Validated batch 38 batch loss 7.39852953\n",
      "Validated batch 39 batch loss 7.30909681\n",
      "Validated batch 40 batch loss 7.65521431\n",
      "Validated batch 41 batch loss 6.77117109\n",
      "Validated batch 42 batch loss 7.33710289\n",
      "Validated batch 43 batch loss 7.69346571\n",
      "Validated batch 44 batch loss 7.38328791\n",
      "Validated batch 45 batch loss 7.8253417\n",
      "Validated batch 46 batch loss 7.54438734\n",
      "Validated batch 47 batch loss 7.90790367\n",
      "Validated batch 48 batch loss 7.58084297\n",
      "Validated batch 49 batch loss 7.57860374\n",
      "Validated batch 50 batch loss 7.49387312\n",
      "Validated batch 51 batch loss 7.67249489\n",
      "Validated batch 52 batch loss 7.73906136\n",
      "Validated batch 53 batch loss 7.56037951\n",
      "Validated batch 54 batch loss 7.17388868\n",
      "Validated batch 55 batch loss 7.4981389\n",
      "Validated batch 56 batch loss 7.41438866\n",
      "Validated batch 57 batch loss 7.64123678\n",
      "Validated batch 58 batch loss 7.30181313\n",
      "Validated batch 59 batch loss 7.28821611\n",
      "Validated batch 60 batch loss 7.11635637\n",
      "Validated batch 61 batch loss 7.41446114\n",
      "Validated batch 62 batch loss 7.65177202\n",
      "Validated batch 63 batch loss 7.82780552\n",
      "Validated batch 64 batch loss 7.51666212\n",
      "Validated batch 65 batch loss 7.96494865\n",
      "Validated batch 66 batch loss 8.07655239\n",
      "Validated batch 67 batch loss 7.82658339\n",
      "Validated batch 68 batch loss 7.37567186\n",
      "Validated batch 69 batch loss 7.08503723\n",
      "Validated batch 70 batch loss 7.56587267\n",
      "Validated batch 71 batch loss 7.90394545\n",
      "Validated batch 72 batch loss 7.38583326\n",
      "Validated batch 73 batch loss 6.85958\n",
      "Validated batch 74 batch loss 7.51448774\n",
      "Validated batch 75 batch loss 7.17191267\n",
      "Validated batch 76 batch loss 7.01614332\n",
      "Validated batch 77 batch loss 7.32636213\n",
      "Validated batch 78 batch loss 6.99550581\n",
      "Validated batch 79 batch loss 7.71372414\n",
      "Validated batch 80 batch loss 7.06366\n",
      "Validated batch 81 batch loss 7.03952026\n",
      "Validated batch 82 batch loss 7.38539076\n",
      "Validated batch 83 batch loss 7.48909569\n",
      "Validated batch 84 batch loss 7.32995\n",
      "Validated batch 85 batch loss 7.41503906\n",
      "Validated batch 86 batch loss 7.26034594\n",
      "Validated batch 87 batch loss 7.58404493\n",
      "Validated batch 88 batch loss 7.27056742\n",
      "Validated batch 89 batch loss 7.2478261\n",
      "Validated batch 90 batch loss 7.69798326\n",
      "Validated batch 91 batch loss 6.52366877\n",
      "Validated batch 92 batch loss 7.81160879\n",
      "Validated batch 93 batch loss 7.31587696\n",
      "Validated batch 94 batch loss 7.14396715\n",
      "Validated batch 95 batch loss 7.49143791\n",
      "Validated batch 96 batch loss 7.58199358\n",
      "Validated batch 97 batch loss 7.13033104\n",
      "Validated batch 98 batch loss 7.65769243\n",
      "Validated batch 99 batch loss 7.52338457\n",
      "Validated batch 100 batch loss 7.54269457\n",
      "Validated batch 101 batch loss 7.54662371\n",
      "Validated batch 102 batch loss 7.55059099\n",
      "Validated batch 103 batch loss 7.3430934\n",
      "Validated batch 104 batch loss 7.40484571\n",
      "Validated batch 105 batch loss 7.30852556\n",
      "Validated batch 106 batch loss 7.6732707\n",
      "Validated batch 107 batch loss 7.63441515\n",
      "Validated batch 108 batch loss 7.39997864\n",
      "Validated batch 109 batch loss 7.44919538\n",
      "Validated batch 110 batch loss 6.96312\n",
      "Validated batch 111 batch loss 7.7164073\n",
      "Validated batch 112 batch loss 7.83262587\n",
      "Validated batch 113 batch loss 7.54583168\n",
      "Validated batch 114 batch loss 7.62558031\n",
      "Validated batch 115 batch loss 7.34278297\n",
      "Validated batch 116 batch loss 7.47434187\n",
      "Validated batch 117 batch loss 7.37472582\n",
      "Validated batch 118 batch loss 7.20484972\n",
      "Validated batch 119 batch loss 6.76462603\n",
      "Validated batch 120 batch loss 7.23521233\n",
      "Validated batch 121 batch loss 7.74899864\n",
      "Validated batch 122 batch loss 7.07585621\n",
      "Validated batch 123 batch loss 7.61058092\n",
      "Validated batch 124 batch loss 7.65184975\n",
      "Validated batch 125 batch loss 7.54763\n",
      "Validated batch 126 batch loss 7.76743174\n",
      "Validated batch 127 batch loss 7.45803881\n",
      "Validated batch 128 batch loss 7.39001465\n",
      "Validated batch 129 batch loss 7.38016319\n",
      "Validated batch 130 batch loss 7.4740696\n",
      "Validated batch 131 batch loss 7.93802834\n",
      "Validated batch 132 batch loss 7.37873125\n",
      "Validated batch 133 batch loss 7.65839338\n",
      "Validated batch 134 batch loss 7.68455553\n",
      "Validated batch 135 batch loss 7.38015795\n",
      "Validated batch 136 batch loss 7.59137917\n",
      "Validated batch 137 batch loss 7.66625\n",
      "Validated batch 138 batch loss 7.6998539\n",
      "Validated batch 139 batch loss 7.68063498\n",
      "Validated batch 140 batch loss 7.33419847\n",
      "Validated batch 141 batch loss 7.21310472\n",
      "Validated batch 142 batch loss 7.22915697\n",
      "Validated batch 143 batch loss 7.35930824\n",
      "Validated batch 144 batch loss 7.60390663\n",
      "Validated batch 145 batch loss 7.29429\n",
      "Validated batch 146 batch loss 7.66630602\n",
      "Validated batch 147 batch loss 7.43685722\n",
      "Validated batch 148 batch loss 7.56482744\n",
      "Validated batch 149 batch loss 7.94356585\n",
      "Validated batch 150 batch loss 7.71491385\n",
      "Validated batch 151 batch loss 7.33167315\n",
      "Validated batch 152 batch loss 7.42032814\n",
      "Validated batch 153 batch loss 7.84441328\n",
      "Validated batch 154 batch loss 7.1740551\n",
      "Validated batch 155 batch loss 7.69492388\n",
      "Validated batch 156 batch loss 7.20193529\n",
      "Validated batch 157 batch loss 7.74691\n",
      "Validated batch 158 batch loss 7.15516472\n",
      "Validated batch 159 batch loss 7.30585432\n",
      "Validated batch 160 batch loss 7.46228743\n",
      "Validated batch 161 batch loss 7.64028931\n",
      "Validated batch 162 batch loss 7.82274675\n",
      "Validated batch 163 batch loss 7.51030684\n",
      "Validated batch 164 batch loss 7.45948648\n",
      "Validated batch 165 batch loss 7.34059906\n",
      "Validated batch 166 batch loss 7.14444447\n",
      "Validated batch 167 batch loss 7.41714478\n",
      "Validated batch 168 batch loss 7.21989489\n",
      "Validated batch 169 batch loss 7.47736359\n",
      "Validated batch 170 batch loss 7.63370562\n",
      "Validated batch 171 batch loss 7.60109\n",
      "Validated batch 172 batch loss 7.32441902\n",
      "Validated batch 173 batch loss 7.37406874\n",
      "Validated batch 174 batch loss 7.05943489\n",
      "Validated batch 175 batch loss 7.75296974\n",
      "Validated batch 176 batch loss 7.45983744\n",
      "Validated batch 177 batch loss 7.52142477\n",
      "Validated batch 178 batch loss 7.45948744\n",
      "Validated batch 179 batch loss 7.06936455\n",
      "Validated batch 180 batch loss 7.06979418\n",
      "Validated batch 181 batch loss 7.4809494\n",
      "Validated batch 182 batch loss 7.43612432\n",
      "Validated batch 183 batch loss 6.92134142\n",
      "Validated batch 184 batch loss 7.50434685\n",
      "Validated batch 185 batch loss 3.83295202\n",
      "Epoch 8 val loss 7.447145462036133\n",
      "Epoch 8 completed in 765.89 seconds\n",
      "Start epoch 9 with learning rate 0.0007\n",
      "Trained batch 1 batch loss 6.11739111 epoch total loss 6.11739111\n",
      "Trained batch 2 batch loss 7.0710454 epoch total loss 6.59421825\n",
      "Trained batch 3 batch loss 7.19172621 epoch total loss 6.79338789\n",
      "Trained batch 4 batch loss 7.05131578 epoch total loss 6.85786963\n",
      "Trained batch 5 batch loss 7.11932755 epoch total loss 6.91016102\n",
      "Trained batch 6 batch loss 7.20875645 epoch total loss 6.95992661\n",
      "Trained batch 7 batch loss 6.97593069 epoch total loss 6.96221256\n",
      "Trained batch 8 batch loss 6.45321226 epoch total loss 6.8985877\n",
      "Trained batch 9 batch loss 6.45398283 epoch total loss 6.84918737\n",
      "Trained batch 10 batch loss 7.06023884 epoch total loss 6.87029266\n",
      "Trained batch 11 batch loss 6.39821577 epoch total loss 6.82737684\n",
      "Trained batch 12 batch loss 6.31963873 epoch total loss 6.78506517\n",
      "Trained batch 13 batch loss 6.81548548 epoch total loss 6.78740501\n",
      "Trained batch 14 batch loss 6.69572544 epoch total loss 6.78085661\n",
      "Trained batch 15 batch loss 6.45741034 epoch total loss 6.75929356\n",
      "Trained batch 16 batch loss 6.30817604 epoch total loss 6.73109865\n",
      "Trained batch 17 batch loss 6.69824409 epoch total loss 6.72916603\n",
      "Trained batch 18 batch loss 6.85210133 epoch total loss 6.73599577\n",
      "Trained batch 19 batch loss 6.79584074 epoch total loss 6.73914528\n",
      "Trained batch 20 batch loss 6.96695948 epoch total loss 6.75053644\n",
      "Trained batch 21 batch loss 6.89548302 epoch total loss 6.75743818\n",
      "Trained batch 22 batch loss 6.74858904 epoch total loss 6.75703621\n",
      "Trained batch 23 batch loss 6.44780493 epoch total loss 6.74359131\n",
      "Trained batch 24 batch loss 6.68318224 epoch total loss 6.74107409\n",
      "Trained batch 25 batch loss 6.71191311 epoch total loss 6.73990774\n",
      "Trained batch 26 batch loss 6.93094 epoch total loss 6.74725533\n",
      "Trained batch 27 batch loss 7.02609921 epoch total loss 6.75758266\n",
      "Trained batch 28 batch loss 6.96480894 epoch total loss 6.76498365\n",
      "Trained batch 29 batch loss 6.48674774 epoch total loss 6.75538921\n",
      "Trained batch 30 batch loss 6.90471125 epoch total loss 6.76036644\n",
      "Trained batch 31 batch loss 6.85568857 epoch total loss 6.76344109\n",
      "Trained batch 32 batch loss 6.8409338 epoch total loss 6.76586246\n",
      "Trained batch 33 batch loss 7.01474047 epoch total loss 6.77340412\n",
      "Trained batch 34 batch loss 6.70704126 epoch total loss 6.77145243\n",
      "Trained batch 35 batch loss 6.79907322 epoch total loss 6.77224159\n",
      "Trained batch 36 batch loss 6.79122925 epoch total loss 6.77276897\n",
      "Trained batch 37 batch loss 6.5660429 epoch total loss 6.76718187\n",
      "Trained batch 38 batch loss 6.67779064 epoch total loss 6.76482916\n",
      "Trained batch 39 batch loss 6.55572462 epoch total loss 6.7594676\n",
      "Trained batch 40 batch loss 6.83989811 epoch total loss 6.76147842\n",
      "Trained batch 41 batch loss 6.75288486 epoch total loss 6.76126909\n",
      "Trained batch 42 batch loss 5.90184307 epoch total loss 6.74080706\n",
      "Trained batch 43 batch loss 5.88830757 epoch total loss 6.72098112\n",
      "Trained batch 44 batch loss 6.2403965 epoch total loss 6.71005869\n",
      "Trained batch 45 batch loss 6.51850176 epoch total loss 6.70580196\n",
      "Trained batch 46 batch loss 6.12749386 epoch total loss 6.69323\n",
      "Trained batch 47 batch loss 6.6447854 epoch total loss 6.69219923\n",
      "Trained batch 48 batch loss 6.63888836 epoch total loss 6.6910882\n",
      "Trained batch 49 batch loss 6.93211746 epoch total loss 6.69600773\n",
      "Trained batch 50 batch loss 6.63883305 epoch total loss 6.6948638\n",
      "Trained batch 51 batch loss 6.61258507 epoch total loss 6.69325066\n",
      "Trained batch 52 batch loss 6.34689283 epoch total loss 6.68658972\n",
      "Trained batch 53 batch loss 6.21343565 epoch total loss 6.67766237\n",
      "Trained batch 54 batch loss 6.42296 epoch total loss 6.67294598\n",
      "Trained batch 55 batch loss 6.58240843 epoch total loss 6.67129946\n",
      "Trained batch 56 batch loss 6.22193432 epoch total loss 6.66327524\n",
      "Trained batch 57 batch loss 6.42408133 epoch total loss 6.6590786\n",
      "Trained batch 58 batch loss 6.67557096 epoch total loss 6.65936279\n",
      "Trained batch 59 batch loss 6.59145546 epoch total loss 6.65821171\n",
      "Trained batch 60 batch loss 6.42954826 epoch total loss 6.65440083\n",
      "Trained batch 61 batch loss 5.96948481 epoch total loss 6.64317226\n",
      "Trained batch 62 batch loss 6.57877445 epoch total loss 6.64213371\n",
      "Trained batch 63 batch loss 6.66088724 epoch total loss 6.64243126\n",
      "Trained batch 64 batch loss 6.73460293 epoch total loss 6.64387131\n",
      "Trained batch 65 batch loss 6.69122744 epoch total loss 6.6446\n",
      "Trained batch 66 batch loss 6.71166325 epoch total loss 6.64561605\n",
      "Trained batch 67 batch loss 6.70992804 epoch total loss 6.64657593\n",
      "Trained batch 68 batch loss 6.08378506 epoch total loss 6.63829947\n",
      "Trained batch 69 batch loss 6.49382401 epoch total loss 6.63620567\n",
      "Trained batch 70 batch loss 6.84907293 epoch total loss 6.63924646\n",
      "Trained batch 71 batch loss 6.5148735 epoch total loss 6.63749456\n",
      "Trained batch 72 batch loss 6.67349434 epoch total loss 6.63799477\n",
      "Trained batch 73 batch loss 6.40567303 epoch total loss 6.63481188\n",
      "Trained batch 74 batch loss 5.99399281 epoch total loss 6.62615204\n",
      "Trained batch 75 batch loss 6.29548168 epoch total loss 6.6217432\n",
      "Trained batch 76 batch loss 6.69024 epoch total loss 6.62264442\n",
      "Trained batch 77 batch loss 6.79380083 epoch total loss 6.62486744\n",
      "Trained batch 78 batch loss 6.76094532 epoch total loss 6.62661219\n",
      "Trained batch 79 batch loss 6.98082352 epoch total loss 6.63109589\n",
      "Trained batch 80 batch loss 7.16205072 epoch total loss 6.63773251\n",
      "Trained batch 81 batch loss 7.23753452 epoch total loss 6.64513779\n",
      "Trained batch 82 batch loss 6.74788189 epoch total loss 6.64639044\n",
      "Trained batch 83 batch loss 6.00266075 epoch total loss 6.63863516\n",
      "Trained batch 84 batch loss 5.7888732 epoch total loss 6.62851906\n",
      "Trained batch 85 batch loss 5.50053692 epoch total loss 6.61524868\n",
      "Trained batch 86 batch loss 5.53161287 epoch total loss 6.60264826\n",
      "Trained batch 87 batch loss 5.65816545 epoch total loss 6.59179211\n",
      "Trained batch 88 batch loss 5.60238123 epoch total loss 6.58054829\n",
      "Trained batch 89 batch loss 6.60625839 epoch total loss 6.58083725\n",
      "Trained batch 90 batch loss 6.34474564 epoch total loss 6.57821369\n",
      "Trained batch 91 batch loss 6.46546268 epoch total loss 6.57697487\n",
      "Trained batch 92 batch loss 6.46540308 epoch total loss 6.5757618\n",
      "Trained batch 93 batch loss 6.6459918 epoch total loss 6.57651711\n",
      "Trained batch 94 batch loss 6.63203287 epoch total loss 6.57710743\n",
      "Trained batch 95 batch loss 6.71154356 epoch total loss 6.57852268\n",
      "Trained batch 96 batch loss 6.85740232 epoch total loss 6.58142805\n",
      "Trained batch 97 batch loss 6.84170818 epoch total loss 6.58411169\n",
      "Trained batch 98 batch loss 6.76825905 epoch total loss 6.58599043\n",
      "Trained batch 99 batch loss 6.79599571 epoch total loss 6.58811188\n",
      "Trained batch 100 batch loss 6.48490477 epoch total loss 6.58708\n",
      "Trained batch 101 batch loss 6.21957207 epoch total loss 6.58344126\n",
      "Trained batch 102 batch loss 6.84343815 epoch total loss 6.58599\n",
      "Trained batch 103 batch loss 6.74561787 epoch total loss 6.58753967\n",
      "Trained batch 104 batch loss 6.78227186 epoch total loss 6.58941221\n",
      "Trained batch 105 batch loss 6.2214694 epoch total loss 6.58590841\n",
      "Trained batch 106 batch loss 6.33442593 epoch total loss 6.58353567\n",
      "Trained batch 107 batch loss 6.18486834 epoch total loss 6.57981\n",
      "Trained batch 108 batch loss 6.53251839 epoch total loss 6.57937241\n",
      "Trained batch 109 batch loss 6.59990644 epoch total loss 6.57956076\n",
      "Trained batch 110 batch loss 6.30705261 epoch total loss 6.57708359\n",
      "Trained batch 111 batch loss 6.37907219 epoch total loss 6.57529974\n",
      "Trained batch 112 batch loss 6.61745119 epoch total loss 6.57567596\n",
      "Trained batch 113 batch loss 6.99332094 epoch total loss 6.57937241\n",
      "Trained batch 114 batch loss 6.96625948 epoch total loss 6.58276558\n",
      "Trained batch 115 batch loss 7.11260128 epoch total loss 6.58737326\n",
      "Trained batch 116 batch loss 6.70597553 epoch total loss 6.5883956\n",
      "Trained batch 117 batch loss 5.7037816 epoch total loss 6.58083487\n",
      "Trained batch 118 batch loss 6.68225527 epoch total loss 6.5816946\n",
      "Trained batch 119 batch loss 6.36913776 epoch total loss 6.57990837\n",
      "Trained batch 120 batch loss 6.31909847 epoch total loss 6.57773495\n",
      "Trained batch 121 batch loss 6.64771032 epoch total loss 6.57831335\n",
      "Trained batch 122 batch loss 6.60298538 epoch total loss 6.57851505\n",
      "Trained batch 123 batch loss 6.68570185 epoch total loss 6.57938671\n",
      "Trained batch 124 batch loss 6.66216946 epoch total loss 6.58005428\n",
      "Trained batch 125 batch loss 6.62562561 epoch total loss 6.58041906\n",
      "Trained batch 126 batch loss 6.66797209 epoch total loss 6.58111382\n",
      "Trained batch 127 batch loss 6.49057055 epoch total loss 6.58040094\n",
      "Trained batch 128 batch loss 6.60171652 epoch total loss 6.58056784\n",
      "Trained batch 129 batch loss 6.64146948 epoch total loss 6.58104\n",
      "Trained batch 130 batch loss 6.47393513 epoch total loss 6.58021593\n",
      "Trained batch 131 batch loss 6.62024 epoch total loss 6.58052158\n",
      "Trained batch 132 batch loss 6.46089458 epoch total loss 6.57961512\n",
      "Trained batch 133 batch loss 6.53573275 epoch total loss 6.57928514\n",
      "Trained batch 134 batch loss 6.47563314 epoch total loss 6.57851171\n",
      "Trained batch 135 batch loss 6.51954794 epoch total loss 6.57807493\n",
      "Trained batch 136 batch loss 6.5784626 epoch total loss 6.57807779\n",
      "Trained batch 137 batch loss 6.98395061 epoch total loss 6.58104038\n",
      "Trained batch 138 batch loss 6.93508673 epoch total loss 6.58360577\n",
      "Trained batch 139 batch loss 6.49161577 epoch total loss 6.58294392\n",
      "Trained batch 140 batch loss 6.74230814 epoch total loss 6.5840826\n",
      "Trained batch 141 batch loss 6.76453066 epoch total loss 6.58536243\n",
      "Trained batch 142 batch loss 6.54956341 epoch total loss 6.58511\n",
      "Trained batch 143 batch loss 6.85221767 epoch total loss 6.58697796\n",
      "Trained batch 144 batch loss 6.60817337 epoch total loss 6.5871253\n",
      "Trained batch 145 batch loss 6.52216101 epoch total loss 6.58667707\n",
      "Trained batch 146 batch loss 6.89576292 epoch total loss 6.58879423\n",
      "Trained batch 147 batch loss 6.93167591 epoch total loss 6.59112692\n",
      "Trained batch 148 batch loss 6.61047554 epoch total loss 6.59125757\n",
      "Trained batch 149 batch loss 6.49851036 epoch total loss 6.5906353\n",
      "Trained batch 150 batch loss 6.34528 epoch total loss 6.58899927\n",
      "Trained batch 151 batch loss 7.20963907 epoch total loss 6.59310961\n",
      "Trained batch 152 batch loss 7.07313967 epoch total loss 6.5962677\n",
      "Trained batch 153 batch loss 6.98110199 epoch total loss 6.59878302\n",
      "Trained batch 154 batch loss 7.01571178 epoch total loss 6.60149\n",
      "Trained batch 155 batch loss 6.8827095 epoch total loss 6.60330439\n",
      "Trained batch 156 batch loss 6.770504 epoch total loss 6.60437536\n",
      "Trained batch 157 batch loss 6.44117069 epoch total loss 6.60333586\n",
      "Trained batch 158 batch loss 6.46763659 epoch total loss 6.60247707\n",
      "Trained batch 159 batch loss 6.48573 epoch total loss 6.60174274\n",
      "Trained batch 160 batch loss 6.68171453 epoch total loss 6.60224295\n",
      "Trained batch 161 batch loss 6.55030298 epoch total loss 6.6019206\n",
      "Trained batch 162 batch loss 6.65057945 epoch total loss 6.60222101\n",
      "Trained batch 163 batch loss 6.47179937 epoch total loss 6.60142088\n",
      "Trained batch 164 batch loss 6.52439642 epoch total loss 6.60095119\n",
      "Trained batch 165 batch loss 6.59638119 epoch total loss 6.60092402\n",
      "Trained batch 166 batch loss 6.67223072 epoch total loss 6.60135365\n",
      "Trained batch 167 batch loss 6.64068699 epoch total loss 6.60158968\n",
      "Trained batch 168 batch loss 6.49331284 epoch total loss 6.600945\n",
      "Trained batch 169 batch loss 6.76387024 epoch total loss 6.60190916\n",
      "Trained batch 170 batch loss 6.33565712 epoch total loss 6.60034323\n",
      "Trained batch 171 batch loss 6.35891533 epoch total loss 6.59893131\n",
      "Trained batch 172 batch loss 5.83358288 epoch total loss 6.59448195\n",
      "Trained batch 173 batch loss 5.69774914 epoch total loss 6.58929825\n",
      "Trained batch 174 batch loss 6.24992657 epoch total loss 6.58734751\n",
      "Trained batch 175 batch loss 6.43801117 epoch total loss 6.58649397\n",
      "Trained batch 176 batch loss 6.47163153 epoch total loss 6.58584166\n",
      "Trained batch 177 batch loss 6.40561533 epoch total loss 6.58482361\n",
      "Trained batch 178 batch loss 6.58538 epoch total loss 6.58482647\n",
      "Trained batch 179 batch loss 6.34556389 epoch total loss 6.58349\n",
      "Trained batch 180 batch loss 6.56161785 epoch total loss 6.58336878\n",
      "Trained batch 181 batch loss 5.88587904 epoch total loss 6.57951498\n",
      "Trained batch 182 batch loss 5.79087448 epoch total loss 6.57518196\n",
      "Trained batch 183 batch loss 5.41828632 epoch total loss 6.56886053\n",
      "Trained batch 184 batch loss 5.36261368 epoch total loss 6.56230497\n",
      "Trained batch 185 batch loss 6.63585472 epoch total loss 6.56270266\n",
      "Trained batch 186 batch loss 6.78012228 epoch total loss 6.56387186\n",
      "Trained batch 187 batch loss 6.68795538 epoch total loss 6.56453514\n",
      "Trained batch 188 batch loss 6.81165075 epoch total loss 6.56585\n",
      "Trained batch 189 batch loss 6.30040693 epoch total loss 6.5644455\n",
      "Trained batch 190 batch loss 6.1634016 epoch total loss 6.56233501\n",
      "Trained batch 191 batch loss 5.8984623 epoch total loss 6.55885887\n",
      "Trained batch 192 batch loss 5.9801259 epoch total loss 6.55584478\n",
      "Trained batch 193 batch loss 6.14238596 epoch total loss 6.55370188\n",
      "Trained batch 194 batch loss 6.2094841 epoch total loss 6.55192757\n",
      "Trained batch 195 batch loss 6.53598404 epoch total loss 6.55184603\n",
      "Trained batch 196 batch loss 6.04852867 epoch total loss 6.54927826\n",
      "Trained batch 197 batch loss 6.21665335 epoch total loss 6.54759026\n",
      "Trained batch 198 batch loss 6.3418045 epoch total loss 6.54655075\n",
      "Trained batch 199 batch loss 6.02729845 epoch total loss 6.5439415\n",
      "Trained batch 200 batch loss 5.97648716 epoch total loss 6.54110432\n",
      "Trained batch 201 batch loss 6.01891088 epoch total loss 6.53850603\n",
      "Trained batch 202 batch loss 5.8900938 epoch total loss 6.53529644\n",
      "Trained batch 203 batch loss 6.28681183 epoch total loss 6.53407288\n",
      "Trained batch 204 batch loss 6.75220585 epoch total loss 6.53514194\n",
      "Trained batch 205 batch loss 6.79602957 epoch total loss 6.53641462\n",
      "Trained batch 206 batch loss 6.74417925 epoch total loss 6.53742266\n",
      "Trained batch 207 batch loss 6.56208038 epoch total loss 6.53754234\n",
      "Trained batch 208 batch loss 6.5314 epoch total loss 6.53751278\n",
      "Trained batch 209 batch loss 6.41019392 epoch total loss 6.53690338\n",
      "Trained batch 210 batch loss 5.9213891 epoch total loss 6.53397226\n",
      "Trained batch 211 batch loss 6.20044661 epoch total loss 6.53239155\n",
      "Trained batch 212 batch loss 6.35465908 epoch total loss 6.53155279\n",
      "Trained batch 213 batch loss 6.71877623 epoch total loss 6.5324316\n",
      "Trained batch 214 batch loss 6.75154686 epoch total loss 6.53345585\n",
      "Trained batch 215 batch loss 6.63545418 epoch total loss 6.5339303\n",
      "Trained batch 216 batch loss 6.64574242 epoch total loss 6.53444815\n",
      "Trained batch 217 batch loss 6.68968773 epoch total loss 6.5351634\n",
      "Trained batch 218 batch loss 6.57932043 epoch total loss 6.53536606\n",
      "Trained batch 219 batch loss 6.35349083 epoch total loss 6.53453588\n",
      "Trained batch 220 batch loss 6.47835588 epoch total loss 6.53428078\n",
      "Trained batch 221 batch loss 6.31167555 epoch total loss 6.53327322\n",
      "Trained batch 222 batch loss 6.19601107 epoch total loss 6.53175402\n",
      "Trained batch 223 batch loss 6.30884933 epoch total loss 6.53075457\n",
      "Trained batch 224 batch loss 6.23912239 epoch total loss 6.5294528\n",
      "Trained batch 225 batch loss 6.38843918 epoch total loss 6.52882576\n",
      "Trained batch 226 batch loss 6.66409111 epoch total loss 6.52942419\n",
      "Trained batch 227 batch loss 6.89596033 epoch total loss 6.53103924\n",
      "Trained batch 228 batch loss 6.78094673 epoch total loss 6.53213549\n",
      "Trained batch 229 batch loss 6.38293171 epoch total loss 6.53148413\n",
      "Trained batch 230 batch loss 6.28559732 epoch total loss 6.53041506\n",
      "Trained batch 231 batch loss 6.30841732 epoch total loss 6.52945423\n",
      "Trained batch 232 batch loss 6.78731346 epoch total loss 6.53056622\n",
      "Trained batch 233 batch loss 6.69553804 epoch total loss 6.53127432\n",
      "Trained batch 234 batch loss 6.76921749 epoch total loss 6.53229094\n",
      "Trained batch 235 batch loss 6.66083908 epoch total loss 6.53283787\n",
      "Trained batch 236 batch loss 6.18271875 epoch total loss 6.53135443\n",
      "Trained batch 237 batch loss 6.26760912 epoch total loss 6.53024149\n",
      "Trained batch 238 batch loss 6.06132698 epoch total loss 6.5282712\n",
      "Trained batch 239 batch loss 6.36016178 epoch total loss 6.52756739\n",
      "Trained batch 240 batch loss 5.83080339 epoch total loss 6.5246644\n",
      "Trained batch 241 batch loss 6.26044178 epoch total loss 6.52356815\n",
      "Trained batch 242 batch loss 6.6686759 epoch total loss 6.52416801\n",
      "Trained batch 243 batch loss 6.67533541 epoch total loss 6.52479\n",
      "Trained batch 244 batch loss 6.75158405 epoch total loss 6.52571917\n",
      "Trained batch 245 batch loss 6.16442966 epoch total loss 6.52424479\n",
      "Trained batch 246 batch loss 5.98568201 epoch total loss 6.52205563\n",
      "Trained batch 247 batch loss 6.10133696 epoch total loss 6.52035236\n",
      "Trained batch 248 batch loss 6.71854258 epoch total loss 6.52115107\n",
      "Trained batch 249 batch loss 6.42009211 epoch total loss 6.52074528\n",
      "Trained batch 250 batch loss 6.56491184 epoch total loss 6.52092171\n",
      "Trained batch 251 batch loss 6.39534521 epoch total loss 6.5204215\n",
      "Trained batch 252 batch loss 6.61618328 epoch total loss 6.52080202\n",
      "Trained batch 253 batch loss 6.51169252 epoch total loss 6.52076578\n",
      "Trained batch 254 batch loss 6.86152124 epoch total loss 6.5221076\n",
      "Trained batch 255 batch loss 6.8596139 epoch total loss 6.5234313\n",
      "Trained batch 256 batch loss 6.53804111 epoch total loss 6.52348852\n",
      "Trained batch 257 batch loss 6.58004522 epoch total loss 6.52370882\n",
      "Trained batch 258 batch loss 6.48021698 epoch total loss 6.52354\n",
      "Trained batch 259 batch loss 6.39656305 epoch total loss 6.52305031\n",
      "Trained batch 260 batch loss 6.17280149 epoch total loss 6.52170324\n",
      "Trained batch 261 batch loss 5.55719519 epoch total loss 6.51800776\n",
      "Trained batch 262 batch loss 5.18397951 epoch total loss 6.51291609\n",
      "Trained batch 263 batch loss 6.11205339 epoch total loss 6.51139212\n",
      "Trained batch 264 batch loss 6.28018236 epoch total loss 6.51051617\n",
      "Trained batch 265 batch loss 7.06965446 epoch total loss 6.51262617\n",
      "Trained batch 266 batch loss 6.89911938 epoch total loss 6.51407957\n",
      "Trained batch 267 batch loss 6.81233931 epoch total loss 6.5151968\n",
      "Trained batch 268 batch loss 6.06211519 epoch total loss 6.51350594\n",
      "Trained batch 269 batch loss 6.43208313 epoch total loss 6.51320362\n",
      "Trained batch 270 batch loss 6.56165266 epoch total loss 6.51338291\n",
      "Trained batch 271 batch loss 6.76247454 epoch total loss 6.51430225\n",
      "Trained batch 272 batch loss 6.61836863 epoch total loss 6.51468468\n",
      "Trained batch 273 batch loss 6.67664289 epoch total loss 6.51527786\n",
      "Trained batch 274 batch loss 6.8080864 epoch total loss 6.51634693\n",
      "Trained batch 275 batch loss 6.70389938 epoch total loss 6.51702881\n",
      "Trained batch 276 batch loss 6.70403337 epoch total loss 6.51770592\n",
      "Trained batch 277 batch loss 6.5352273 epoch total loss 6.51776934\n",
      "Trained batch 278 batch loss 6.54416847 epoch total loss 6.51786423\n",
      "Trained batch 279 batch loss 6.75471973 epoch total loss 6.51871347\n",
      "Trained batch 280 batch loss 6.82206 epoch total loss 6.51979685\n",
      "Trained batch 281 batch loss 6.31097 epoch total loss 6.51905346\n",
      "Trained batch 282 batch loss 6.05071497 epoch total loss 6.51739264\n",
      "Trained batch 283 batch loss 6.28934479 epoch total loss 6.5165863\n",
      "Trained batch 284 batch loss 6.63254118 epoch total loss 6.51699495\n",
      "Trained batch 285 batch loss 6.45082664 epoch total loss 6.51676273\n",
      "Trained batch 286 batch loss 6.62055779 epoch total loss 6.51712561\n",
      "Trained batch 287 batch loss 6.47857571 epoch total loss 6.51699114\n",
      "Trained batch 288 batch loss 6.67600441 epoch total loss 6.51754332\n",
      "Trained batch 289 batch loss 6.27929401 epoch total loss 6.51671886\n",
      "Trained batch 290 batch loss 6.17656708 epoch total loss 6.51554585\n",
      "Trained batch 291 batch loss 6.19860458 epoch total loss 6.51445675\n",
      "Trained batch 292 batch loss 6.0577445 epoch total loss 6.51289272\n",
      "Trained batch 293 batch loss 6.37639666 epoch total loss 6.51242638\n",
      "Trained batch 294 batch loss 6.68849134 epoch total loss 6.51302528\n",
      "Trained batch 295 batch loss 6.42334 epoch total loss 6.51272154\n",
      "Trained batch 296 batch loss 6.62069654 epoch total loss 6.51308632\n",
      "Trained batch 297 batch loss 6.51562786 epoch total loss 6.5130949\n",
      "Trained batch 298 batch loss 6.45346212 epoch total loss 6.51289463\n",
      "Trained batch 299 batch loss 6.69062138 epoch total loss 6.51348925\n",
      "Trained batch 300 batch loss 6.43211746 epoch total loss 6.5132184\n",
      "Trained batch 301 batch loss 6.32897425 epoch total loss 6.51260614\n",
      "Trained batch 302 batch loss 6.3404274 epoch total loss 6.51203585\n",
      "Trained batch 303 batch loss 6.82799244 epoch total loss 6.51307869\n",
      "Trained batch 304 batch loss 6.93915558 epoch total loss 6.51448059\n",
      "Trained batch 305 batch loss 6.69346952 epoch total loss 6.51506758\n",
      "Trained batch 306 batch loss 7.16248894 epoch total loss 6.5171833\n",
      "Trained batch 307 batch loss 7.25241613 epoch total loss 6.51957798\n",
      "Trained batch 308 batch loss 7.50098 epoch total loss 6.52276468\n",
      "Trained batch 309 batch loss 6.82666397 epoch total loss 6.52374792\n",
      "Trained batch 310 batch loss 6.60401821 epoch total loss 6.52400684\n",
      "Trained batch 311 batch loss 6.36697912 epoch total loss 6.52350187\n",
      "Trained batch 312 batch loss 6.50126171 epoch total loss 6.52343035\n",
      "Trained batch 313 batch loss 6.88328743 epoch total loss 6.52458\n",
      "Trained batch 314 batch loss 6.85532 epoch total loss 6.52563334\n",
      "Trained batch 315 batch loss 7.20542 epoch total loss 6.52779102\n",
      "Trained batch 316 batch loss 6.51150656 epoch total loss 6.52773952\n",
      "Trained batch 317 batch loss 6.83095789 epoch total loss 6.52869606\n",
      "Trained batch 318 batch loss 6.77626419 epoch total loss 6.52947521\n",
      "Trained batch 319 batch loss 6.74993658 epoch total loss 6.53016615\n",
      "Trained batch 320 batch loss 7.20055199 epoch total loss 6.53226089\n",
      "Trained batch 321 batch loss 7.15412807 epoch total loss 6.53419781\n",
      "Trained batch 322 batch loss 7.00989437 epoch total loss 6.53567553\n",
      "Trained batch 323 batch loss 6.37302446 epoch total loss 6.53517199\n",
      "Trained batch 324 batch loss 6.9746151 epoch total loss 6.53652859\n",
      "Trained batch 325 batch loss 7.05227804 epoch total loss 6.53811502\n",
      "Trained batch 326 batch loss 6.53098869 epoch total loss 6.53809357\n",
      "Trained batch 327 batch loss 6.6145792 epoch total loss 6.53832722\n",
      "Trained batch 328 batch loss 6.66238594 epoch total loss 6.53870535\n",
      "Trained batch 329 batch loss 6.67482281 epoch total loss 6.53911877\n",
      "Trained batch 330 batch loss 6.27980185 epoch total loss 6.53833294\n",
      "Trained batch 331 batch loss 5.99147892 epoch total loss 6.5366807\n",
      "Trained batch 332 batch loss 6.55918026 epoch total loss 6.53674841\n",
      "Trained batch 333 batch loss 6.32747126 epoch total loss 6.53611946\n",
      "Trained batch 334 batch loss 6.64178562 epoch total loss 6.53643608\n",
      "Trained batch 335 batch loss 6.78954792 epoch total loss 6.53719187\n",
      "Trained batch 336 batch loss 6.64385462 epoch total loss 6.53750896\n",
      "Trained batch 337 batch loss 6.54648304 epoch total loss 6.53753519\n",
      "Trained batch 338 batch loss 6.55064917 epoch total loss 6.53757381\n",
      "Trained batch 339 batch loss 6.64688826 epoch total loss 6.53789663\n",
      "Trained batch 340 batch loss 6.28964329 epoch total loss 6.53716612\n",
      "Trained batch 341 batch loss 6.26687336 epoch total loss 6.53637314\n",
      "Trained batch 342 batch loss 6.41048145 epoch total loss 6.53600502\n",
      "Trained batch 343 batch loss 6.48606491 epoch total loss 6.53585958\n",
      "Trained batch 344 batch loss 6.45789385 epoch total loss 6.53563309\n",
      "Trained batch 345 batch loss 6.63102388 epoch total loss 6.53591\n",
      "Trained batch 346 batch loss 6.27887487 epoch total loss 6.53516674\n",
      "Trained batch 347 batch loss 6.41636848 epoch total loss 6.53482437\n",
      "Trained batch 348 batch loss 6.33948612 epoch total loss 6.53426313\n",
      "Trained batch 349 batch loss 6.56876135 epoch total loss 6.53436232\n",
      "Trained batch 350 batch loss 6.7087245 epoch total loss 6.53486061\n",
      "Trained batch 351 batch loss 6.3480792 epoch total loss 6.53432846\n",
      "Trained batch 352 batch loss 5.97986412 epoch total loss 6.53275347\n",
      "Trained batch 353 batch loss 6.36967707 epoch total loss 6.53229141\n",
      "Trained batch 354 batch loss 6.57737875 epoch total loss 6.5324192\n",
      "Trained batch 355 batch loss 6.79314 epoch total loss 6.53315353\n",
      "Trained batch 356 batch loss 6.8191781 epoch total loss 6.533957\n",
      "Trained batch 357 batch loss 6.54633427 epoch total loss 6.53399181\n",
      "Trained batch 358 batch loss 6.26527309 epoch total loss 6.53324127\n",
      "Trained batch 359 batch loss 6.46163225 epoch total loss 6.53304195\n",
      "Trained batch 360 batch loss 6.37887526 epoch total loss 6.53261375\n",
      "Trained batch 361 batch loss 6.3088665 epoch total loss 6.53199387\n",
      "Trained batch 362 batch loss 6.47661829 epoch total loss 6.5318408\n",
      "Trained batch 363 batch loss 6.54103231 epoch total loss 6.53186607\n",
      "Trained batch 364 batch loss 7.23054075 epoch total loss 6.53378534\n",
      "Trained batch 365 batch loss 7.11225462 epoch total loss 6.53537035\n",
      "Trained batch 366 batch loss 7.11220169 epoch total loss 6.53694677\n",
      "Trained batch 367 batch loss 6.98086 epoch total loss 6.53815651\n",
      "Trained batch 368 batch loss 6.53685331 epoch total loss 6.53815317\n",
      "Trained batch 369 batch loss 5.91142797 epoch total loss 6.5364542\n",
      "Trained batch 370 batch loss 6.27351713 epoch total loss 6.53574371\n",
      "Trained batch 371 batch loss 6.51077032 epoch total loss 6.535676\n",
      "Trained batch 372 batch loss 6.46470165 epoch total loss 6.53548527\n",
      "Trained batch 373 batch loss 6.77573681 epoch total loss 6.536129\n",
      "Trained batch 374 batch loss 6.80086279 epoch total loss 6.53683662\n",
      "Trained batch 375 batch loss 6.45107508 epoch total loss 6.53660822\n",
      "Trained batch 376 batch loss 6.61998606 epoch total loss 6.53682947\n",
      "Trained batch 377 batch loss 6.56928158 epoch total loss 6.53691578\n",
      "Trained batch 378 batch loss 6.62236547 epoch total loss 6.5371418\n",
      "Trained batch 379 batch loss 6.53049946 epoch total loss 6.53712416\n",
      "Trained batch 380 batch loss 6.66386127 epoch total loss 6.53745747\n",
      "Trained batch 381 batch loss 6.90830803 epoch total loss 6.53843069\n",
      "Trained batch 382 batch loss 6.18914938 epoch total loss 6.53751659\n",
      "Trained batch 383 batch loss 6.08948469 epoch total loss 6.53634691\n",
      "Trained batch 384 batch loss 5.81884 epoch total loss 6.53447866\n",
      "Trained batch 385 batch loss 5.54666519 epoch total loss 6.5319128\n",
      "Trained batch 386 batch loss 6.22324944 epoch total loss 6.53111267\n",
      "Trained batch 387 batch loss 6.6607151 epoch total loss 6.53144741\n",
      "Trained batch 388 batch loss 6.73497 epoch total loss 6.53197193\n",
      "Trained batch 389 batch loss 7.023417 epoch total loss 6.53323507\n",
      "Trained batch 390 batch loss 6.81962061 epoch total loss 6.5339694\n",
      "Trained batch 391 batch loss 6.6673007 epoch total loss 6.53431034\n",
      "Trained batch 392 batch loss 6.70577526 epoch total loss 6.5347476\n",
      "Trained batch 393 batch loss 6.3883419 epoch total loss 6.53437519\n",
      "Trained batch 394 batch loss 6.77400255 epoch total loss 6.53498316\n",
      "Trained batch 395 batch loss 6.58824 epoch total loss 6.5351181\n",
      "Trained batch 396 batch loss 6.54536486 epoch total loss 6.53514385\n",
      "Trained batch 397 batch loss 6.38062382 epoch total loss 6.53475475\n",
      "Trained batch 398 batch loss 6.45332336 epoch total loss 6.53455\n",
      "Trained batch 399 batch loss 6.27863312 epoch total loss 6.53390884\n",
      "Trained batch 400 batch loss 6.10812426 epoch total loss 6.53284407\n",
      "Trained batch 401 batch loss 6.60124302 epoch total loss 6.53301477\n",
      "Trained batch 402 batch loss 6.96389484 epoch total loss 6.5340867\n",
      "Trained batch 403 batch loss 6.47846031 epoch total loss 6.5339489\n",
      "Trained batch 404 batch loss 6.8625555 epoch total loss 6.53476238\n",
      "Trained batch 405 batch loss 6.59901762 epoch total loss 6.53492117\n",
      "Trained batch 406 batch loss 6.73449373 epoch total loss 6.53541231\n",
      "Trained batch 407 batch loss 6.38077927 epoch total loss 6.53503275\n",
      "Trained batch 408 batch loss 6.4339695 epoch total loss 6.53478527\n",
      "Trained batch 409 batch loss 6.41270971 epoch total loss 6.53448629\n",
      "Trained batch 410 batch loss 6.48045588 epoch total loss 6.53435469\n",
      "Trained batch 411 batch loss 6.32964 epoch total loss 6.53385639\n",
      "Trained batch 412 batch loss 6.49617195 epoch total loss 6.53376484\n",
      "Trained batch 413 batch loss 5.92054319 epoch total loss 6.53228045\n",
      "Trained batch 414 batch loss 6.54943562 epoch total loss 6.53232145\n",
      "Trained batch 415 batch loss 6.32728624 epoch total loss 6.53182793\n",
      "Trained batch 416 batch loss 6.3365984 epoch total loss 6.53135872\n",
      "Trained batch 417 batch loss 6.02531767 epoch total loss 6.53014517\n",
      "Trained batch 418 batch loss 6.45431328 epoch total loss 6.52996397\n",
      "Trained batch 419 batch loss 6.36747837 epoch total loss 6.52957582\n",
      "Trained batch 420 batch loss 6.5748105 epoch total loss 6.52968359\n",
      "Trained batch 421 batch loss 6.38417673 epoch total loss 6.52933788\n",
      "Trained batch 422 batch loss 5.65350723 epoch total loss 6.52726269\n",
      "Trained batch 423 batch loss 5.68174362 epoch total loss 6.52526379\n",
      "Trained batch 424 batch loss 5.8241477 epoch total loss 6.52361\n",
      "Trained batch 425 batch loss 6.14201117 epoch total loss 6.52271271\n",
      "Trained batch 426 batch loss 6.16542292 epoch total loss 6.52187395\n",
      "Trained batch 427 batch loss 6.32443905 epoch total loss 6.5214119\n",
      "Trained batch 428 batch loss 6.57018137 epoch total loss 6.52152538\n",
      "Trained batch 429 batch loss 6.37157345 epoch total loss 6.52117586\n",
      "Trained batch 430 batch loss 6.61102629 epoch total loss 6.52138519\n",
      "Trained batch 431 batch loss 6.55541611 epoch total loss 6.52146387\n",
      "Trained batch 432 batch loss 7.11971807 epoch total loss 6.52284861\n",
      "Trained batch 433 batch loss 7.42559814 epoch total loss 6.52493334\n",
      "Trained batch 434 batch loss 6.55356312 epoch total loss 6.52499914\n",
      "Trained batch 435 batch loss 6.41475248 epoch total loss 6.52474594\n",
      "Trained batch 436 batch loss 6.61414242 epoch total loss 6.52495098\n",
      "Trained batch 437 batch loss 6.50126028 epoch total loss 6.52489662\n",
      "Trained batch 438 batch loss 6.92999792 epoch total loss 6.52582169\n",
      "Trained batch 439 batch loss 6.71952105 epoch total loss 6.52626276\n",
      "Trained batch 440 batch loss 6.66144466 epoch total loss 6.52657\n",
      "Trained batch 441 batch loss 6.57757616 epoch total loss 6.52668571\n",
      "Trained batch 442 batch loss 6.45703268 epoch total loss 6.52652788\n",
      "Trained batch 443 batch loss 6.46776056 epoch total loss 6.52639532\n",
      "Trained batch 444 batch loss 6.82339334 epoch total loss 6.52706432\n",
      "Trained batch 445 batch loss 6.75833 epoch total loss 6.52758408\n",
      "Trained batch 446 batch loss 6.42307663 epoch total loss 6.52735\n",
      "Trained batch 447 batch loss 6.47860527 epoch total loss 6.52724075\n",
      "Trained batch 448 batch loss 6.39654922 epoch total loss 6.52694845\n",
      "Trained batch 449 batch loss 5.94960833 epoch total loss 6.5256629\n",
      "Trained batch 450 batch loss 6.37872505 epoch total loss 6.52533627\n",
      "Trained batch 451 batch loss 6.46198654 epoch total loss 6.5251956\n",
      "Trained batch 452 batch loss 6.695086 epoch total loss 6.52557135\n",
      "Trained batch 453 batch loss 6.53959084 epoch total loss 6.52560234\n",
      "Trained batch 454 batch loss 6.53174496 epoch total loss 6.52561617\n",
      "Trained batch 455 batch loss 6.27810287 epoch total loss 6.5250721\n",
      "Trained batch 456 batch loss 6.24350166 epoch total loss 6.52445412\n",
      "Trained batch 457 batch loss 6.31778383 epoch total loss 6.52400208\n",
      "Trained batch 458 batch loss 6.06417036 epoch total loss 6.52299833\n",
      "Trained batch 459 batch loss 6.43832874 epoch total loss 6.5228138\n",
      "Trained batch 460 batch loss 6.61617 epoch total loss 6.52301645\n",
      "Trained batch 461 batch loss 5.93269396 epoch total loss 6.52173615\n",
      "Trained batch 462 batch loss 5.98866558 epoch total loss 6.5205822\n",
      "Trained batch 463 batch loss 5.9209938 epoch total loss 6.51928711\n",
      "Trained batch 464 batch loss 6.2574296 epoch total loss 6.51872253\n",
      "Trained batch 465 batch loss 6.35674763 epoch total loss 6.51837397\n",
      "Trained batch 466 batch loss 6.70274162 epoch total loss 6.51876926\n",
      "Trained batch 467 batch loss 6.73737431 epoch total loss 6.51923752\n",
      "Trained batch 468 batch loss 6.17320728 epoch total loss 6.51849794\n",
      "Trained batch 469 batch loss 6.06962585 epoch total loss 6.51754045\n",
      "Trained batch 470 batch loss 6.60448217 epoch total loss 6.51772547\n",
      "Trained batch 471 batch loss 6.50155592 epoch total loss 6.51769114\n",
      "Trained batch 472 batch loss 6.54052305 epoch total loss 6.5177393\n",
      "Trained batch 473 batch loss 6.40458107 epoch total loss 6.5175004\n",
      "Trained batch 474 batch loss 6.28169107 epoch total loss 6.51700258\n",
      "Trained batch 475 batch loss 6.38538599 epoch total loss 6.51672602\n",
      "Trained batch 476 batch loss 6.42874527 epoch total loss 6.516541\n",
      "Trained batch 477 batch loss 6.58335829 epoch total loss 6.51668072\n",
      "Trained batch 478 batch loss 6.78943872 epoch total loss 6.51725197\n",
      "Trained batch 479 batch loss 6.60348701 epoch total loss 6.51743174\n",
      "Trained batch 480 batch loss 6.50917292 epoch total loss 6.51741505\n",
      "Trained batch 481 batch loss 6.37189054 epoch total loss 6.51711226\n",
      "Trained batch 482 batch loss 6.5599823 epoch total loss 6.51720142\n",
      "Trained batch 483 batch loss 6.44984341 epoch total loss 6.51706219\n",
      "Trained batch 484 batch loss 6.1016674 epoch total loss 6.5162034\n",
      "Trained batch 485 batch loss 6.08592653 epoch total loss 6.51531649\n",
      "Trained batch 486 batch loss 6.4834795 epoch total loss 6.51525068\n",
      "Trained batch 487 batch loss 6.43162966 epoch total loss 6.51507902\n",
      "Trained batch 488 batch loss 6.48055363 epoch total loss 6.51500797\n",
      "Trained batch 489 batch loss 6.44251251 epoch total loss 6.51486\n",
      "Trained batch 490 batch loss 6.78724146 epoch total loss 6.51541615\n",
      "Trained batch 491 batch loss 6.72368479 epoch total loss 6.51584053\n",
      "Trained batch 492 batch loss 6.56271601 epoch total loss 6.51593542\n",
      "Trained batch 493 batch loss 6.72716618 epoch total loss 6.5163641\n",
      "Trained batch 494 batch loss 6.81597614 epoch total loss 6.51697\n",
      "Trained batch 495 batch loss 6.82632923 epoch total loss 6.51759529\n",
      "Trained batch 496 batch loss 6.53573561 epoch total loss 6.51763201\n",
      "Trained batch 497 batch loss 6.13246679 epoch total loss 6.51685715\n",
      "Trained batch 498 batch loss 6.31914663 epoch total loss 6.51646\n",
      "Trained batch 499 batch loss 6.1090703 epoch total loss 6.5156436\n",
      "Trained batch 500 batch loss 6.43805695 epoch total loss 6.51548815\n",
      "Trained batch 501 batch loss 6.21828747 epoch total loss 6.51489496\n",
      "Trained batch 502 batch loss 6.72397709 epoch total loss 6.51531124\n",
      "Trained batch 503 batch loss 6.81611824 epoch total loss 6.51590919\n",
      "Trained batch 504 batch loss 6.6867404 epoch total loss 6.51624823\n",
      "Trained batch 505 batch loss 6.75040388 epoch total loss 6.51671219\n",
      "Trained batch 506 batch loss 6.52989578 epoch total loss 6.51673794\n",
      "Trained batch 507 batch loss 6.14320374 epoch total loss 6.5160017\n",
      "Trained batch 508 batch loss 6.327425 epoch total loss 6.51563025\n",
      "Trained batch 509 batch loss 6.44960499 epoch total loss 6.51550055\n",
      "Trained batch 510 batch loss 6.70665264 epoch total loss 6.51587534\n",
      "Trained batch 511 batch loss 6.67132473 epoch total loss 6.51617956\n",
      "Trained batch 512 batch loss 7.0340476 epoch total loss 6.51719093\n",
      "Trained batch 513 batch loss 7.29265976 epoch total loss 6.51870251\n",
      "Trained batch 514 batch loss 6.90219259 epoch total loss 6.51944876\n",
      "Trained batch 515 batch loss 7.06843328 epoch total loss 6.52051449\n",
      "Trained batch 516 batch loss 7.29000664 epoch total loss 6.52200556\n",
      "Trained batch 517 batch loss 7.04233456 epoch total loss 6.52301216\n",
      "Trained batch 518 batch loss 6.9686985 epoch total loss 6.52387238\n",
      "Trained batch 519 batch loss 6.8150897 epoch total loss 6.52443361\n",
      "Trained batch 520 batch loss 6.82753515 epoch total loss 6.52501678\n",
      "Trained batch 521 batch loss 6.46152496 epoch total loss 6.52489471\n",
      "Trained batch 522 batch loss 6.67711496 epoch total loss 6.52518606\n",
      "Trained batch 523 batch loss 6.39454365 epoch total loss 6.5249362\n",
      "Trained batch 524 batch loss 6.51471519 epoch total loss 6.52491665\n",
      "Trained batch 525 batch loss 6.74890423 epoch total loss 6.52534389\n",
      "Trained batch 526 batch loss 6.20093155 epoch total loss 6.52472687\n",
      "Trained batch 527 batch loss 6.16358328 epoch total loss 6.52404165\n",
      "Trained batch 528 batch loss 6.45008183 epoch total loss 6.52390194\n",
      "Trained batch 529 batch loss 6.2615881 epoch total loss 6.52340555\n",
      "Trained batch 530 batch loss 6.4809165 epoch total loss 6.52332544\n",
      "Trained batch 531 batch loss 6.43794489 epoch total loss 6.52316475\n",
      "Trained batch 532 batch loss 6.57264614 epoch total loss 6.52325821\n",
      "Trained batch 533 batch loss 6.87989092 epoch total loss 6.52392721\n",
      "Trained batch 534 batch loss 6.62610722 epoch total loss 6.5241189\n",
      "Trained batch 535 batch loss 6.92985392 epoch total loss 6.52487707\n",
      "Trained batch 536 batch loss 6.10557127 epoch total loss 6.52409458\n",
      "Trained batch 537 batch loss 6.75696421 epoch total loss 6.5245285\n",
      "Trained batch 538 batch loss 6.38277388 epoch total loss 6.52426529\n",
      "Trained batch 539 batch loss 6.59570408 epoch total loss 6.52439785\n",
      "Trained batch 540 batch loss 6.71230602 epoch total loss 6.52474594\n",
      "Trained batch 541 batch loss 6.92576408 epoch total loss 6.52548742\n",
      "Trained batch 542 batch loss 6.80418253 epoch total loss 6.52600145\n",
      "Trained batch 543 batch loss 6.65686417 epoch total loss 6.52624273\n",
      "Trained batch 544 batch loss 6.49837112 epoch total loss 6.52619123\n",
      "Trained batch 545 batch loss 6.68909836 epoch total loss 6.52649\n",
      "Trained batch 546 batch loss 6.72456312 epoch total loss 6.52685308\n",
      "Trained batch 547 batch loss 6.54798508 epoch total loss 6.52689219\n",
      "Trained batch 548 batch loss 6.52870703 epoch total loss 6.52689552\n",
      "Trained batch 549 batch loss 6.71055222 epoch total loss 6.52723\n",
      "Trained batch 550 batch loss 6.72696495 epoch total loss 6.52759314\n",
      "Trained batch 551 batch loss 6.68472433 epoch total loss 6.52787876\n",
      "Trained batch 552 batch loss 6.78153801 epoch total loss 6.52833796\n",
      "Trained batch 553 batch loss 6.90885735 epoch total loss 6.52902603\n",
      "Trained batch 554 batch loss 7.0244236 epoch total loss 6.52992058\n",
      "Trained batch 555 batch loss 6.88955593 epoch total loss 6.5305686\n",
      "Trained batch 556 batch loss 7.04079819 epoch total loss 6.53148603\n",
      "Trained batch 557 batch loss 6.89226103 epoch total loss 6.53213406\n",
      "Trained batch 558 batch loss 6.7139039 epoch total loss 6.53245974\n",
      "Trained batch 559 batch loss 6.91025352 epoch total loss 6.53313541\n",
      "Trained batch 560 batch loss 6.66647148 epoch total loss 6.53337336\n",
      "Trained batch 561 batch loss 6.75333452 epoch total loss 6.53376579\n",
      "Trained batch 562 batch loss 6.87301159 epoch total loss 6.53436947\n",
      "Trained batch 563 batch loss 6.56773901 epoch total loss 6.5344286\n",
      "Trained batch 564 batch loss 6.61051559 epoch total loss 6.53456354\n",
      "Trained batch 565 batch loss 6.73320675 epoch total loss 6.53491497\n",
      "Trained batch 566 batch loss 6.76338673 epoch total loss 6.53531885\n",
      "Trained batch 567 batch loss 6.60731125 epoch total loss 6.53544617\n",
      "Trained batch 568 batch loss 6.9564805 epoch total loss 6.53618717\n",
      "Trained batch 569 batch loss 7.16740227 epoch total loss 6.53729677\n",
      "Trained batch 570 batch loss 6.80442476 epoch total loss 6.5377655\n",
      "Trained batch 571 batch loss 6.09699678 epoch total loss 6.5369935\n",
      "Trained batch 572 batch loss 6.65149212 epoch total loss 6.53719378\n",
      "Trained batch 573 batch loss 5.66686583 epoch total loss 6.53567457\n",
      "Trained batch 574 batch loss 6.02185106 epoch total loss 6.53478\n",
      "Trained batch 575 batch loss 6.24491405 epoch total loss 6.53427553\n",
      "Trained batch 576 batch loss 5.88372374 epoch total loss 6.53314638\n",
      "Trained batch 577 batch loss 5.34983 epoch total loss 6.5310955\n",
      "Trained batch 578 batch loss 5.45509481 epoch total loss 6.52923393\n",
      "Trained batch 579 batch loss 5.62782288 epoch total loss 6.52767706\n",
      "Trained batch 580 batch loss 5.88717556 epoch total loss 6.52657318\n",
      "Trained batch 581 batch loss 6.25119495 epoch total loss 6.52609921\n",
      "Trained batch 582 batch loss 6.70626926 epoch total loss 6.52640867\n",
      "Trained batch 583 batch loss 6.94355106 epoch total loss 6.5271244\n",
      "Trained batch 584 batch loss 6.90625429 epoch total loss 6.52777338\n",
      "Trained batch 585 batch loss 7.04795074 epoch total loss 6.52866268\n",
      "Trained batch 586 batch loss 6.67555094 epoch total loss 6.52891302\n",
      "Trained batch 587 batch loss 6.47611094 epoch total loss 6.52882338\n",
      "Trained batch 588 batch loss 5.94340611 epoch total loss 6.52782726\n",
      "Trained batch 589 batch loss 6.57158089 epoch total loss 6.52790165\n",
      "Trained batch 590 batch loss 6.46362352 epoch total loss 6.52779293\n",
      "Trained batch 591 batch loss 6.22085047 epoch total loss 6.52727365\n",
      "Trained batch 592 batch loss 6.0311718 epoch total loss 6.52643585\n",
      "Trained batch 593 batch loss 6.3737483 epoch total loss 6.52617836\n",
      "Trained batch 594 batch loss 6.47024727 epoch total loss 6.52608395\n",
      "Trained batch 595 batch loss 6.46053696 epoch total loss 6.5259738\n",
      "Trained batch 596 batch loss 6.77926731 epoch total loss 6.52639866\n",
      "Trained batch 597 batch loss 6.76525831 epoch total loss 6.52679873\n",
      "Trained batch 598 batch loss 6.11816788 epoch total loss 6.52611542\n",
      "Trained batch 599 batch loss 6.38162756 epoch total loss 6.52587414\n",
      "Trained batch 600 batch loss 6.7518034 epoch total loss 6.52625036\n",
      "Trained batch 601 batch loss 6.83082199 epoch total loss 6.52675724\n",
      "Trained batch 602 batch loss 6.77540874 epoch total loss 6.52717\n",
      "Trained batch 603 batch loss 6.54519844 epoch total loss 6.5272\n",
      "Trained batch 604 batch loss 6.28223276 epoch total loss 6.52679443\n",
      "Trained batch 605 batch loss 6.48319769 epoch total loss 6.52672243\n",
      "Trained batch 606 batch loss 6.26224709 epoch total loss 6.52628565\n",
      "Trained batch 607 batch loss 6.06104279 epoch total loss 6.52551937\n",
      "Trained batch 608 batch loss 6.04562664 epoch total loss 6.52473\n",
      "Trained batch 609 batch loss 6.32110119 epoch total loss 6.52439547\n",
      "Trained batch 610 batch loss 6.55567551 epoch total loss 6.52444696\n",
      "Trained batch 611 batch loss 6.33806038 epoch total loss 6.52414179\n",
      "Trained batch 612 batch loss 6.15183783 epoch total loss 6.52353382\n",
      "Trained batch 613 batch loss 6.0502553 epoch total loss 6.52276182\n",
      "Trained batch 614 batch loss 6.20790195 epoch total loss 6.52224922\n",
      "Trained batch 615 batch loss 6.37236 epoch total loss 6.52200508\n",
      "Trained batch 616 batch loss 6.24585104 epoch total loss 6.52155685\n",
      "Trained batch 617 batch loss 6.32873726 epoch total loss 6.52124453\n",
      "Trained batch 618 batch loss 6.29009151 epoch total loss 6.52087069\n",
      "Trained batch 619 batch loss 6.57185078 epoch total loss 6.5209527\n",
      "Trained batch 620 batch loss 6.56179237 epoch total loss 6.52101851\n",
      "Trained batch 621 batch loss 6.40292168 epoch total loss 6.52082825\n",
      "Trained batch 622 batch loss 6.45440626 epoch total loss 6.52072144\n",
      "Trained batch 623 batch loss 6.12464046 epoch total loss 6.52008581\n",
      "Trained batch 624 batch loss 6.60818291 epoch total loss 6.52022696\n",
      "Trained batch 625 batch loss 6.54154062 epoch total loss 6.52026081\n",
      "Trained batch 626 batch loss 6.31182289 epoch total loss 6.51992798\n",
      "Trained batch 627 batch loss 6.50846767 epoch total loss 6.51991\n",
      "Trained batch 628 batch loss 6.3175683 epoch total loss 6.51958752\n",
      "Trained batch 629 batch loss 6.62410545 epoch total loss 6.51975393\n",
      "Trained batch 630 batch loss 6.50835228 epoch total loss 6.51973581\n",
      "Trained batch 631 batch loss 6.58906937 epoch total loss 6.51984549\n",
      "Trained batch 632 batch loss 6.56662512 epoch total loss 6.51991892\n",
      "Trained batch 633 batch loss 6.79671383 epoch total loss 6.52035666\n",
      "Trained batch 634 batch loss 6.84131575 epoch total loss 6.52086306\n",
      "Trained batch 635 batch loss 6.5992837 epoch total loss 6.52098608\n",
      "Trained batch 636 batch loss 6.03951311 epoch total loss 6.52022934\n",
      "Trained batch 637 batch loss 6.8504262 epoch total loss 6.52074766\n",
      "Trained batch 638 batch loss 6.59358406 epoch total loss 6.5208621\n",
      "Trained batch 639 batch loss 6.81040907 epoch total loss 6.52131557\n",
      "Trained batch 640 batch loss 6.51123762 epoch total loss 6.5213\n",
      "Trained batch 641 batch loss 6.27836752 epoch total loss 6.52092075\n",
      "Trained batch 642 batch loss 6.06818724 epoch total loss 6.52021599\n",
      "Trained batch 643 batch loss 6.54281569 epoch total loss 6.52025127\n",
      "Trained batch 644 batch loss 6.47259 epoch total loss 6.52017736\n",
      "Trained batch 645 batch loss 6.49915457 epoch total loss 6.52014446\n",
      "Trained batch 646 batch loss 6.50068855 epoch total loss 6.52011395\n",
      "Trained batch 647 batch loss 6.40295649 epoch total loss 6.51993275\n",
      "Trained batch 648 batch loss 6.75613642 epoch total loss 6.52029753\n",
      "Trained batch 649 batch loss 6.69206858 epoch total loss 6.52056217\n",
      "Trained batch 650 batch loss 6.25859451 epoch total loss 6.52015924\n",
      "Trained batch 651 batch loss 6.71792316 epoch total loss 6.52046299\n",
      "Trained batch 652 batch loss 7.06492662 epoch total loss 6.52129793\n",
      "Trained batch 653 batch loss 6.76007748 epoch total loss 6.52166367\n",
      "Trained batch 654 batch loss 6.82529259 epoch total loss 6.52212811\n",
      "Trained batch 655 batch loss 7.10463619 epoch total loss 6.52301693\n",
      "Trained batch 656 batch loss 6.58889055 epoch total loss 6.52311754\n",
      "Trained batch 657 batch loss 6.68731356 epoch total loss 6.5233674\n",
      "Trained batch 658 batch loss 6.46952 epoch total loss 6.52328634\n",
      "Trained batch 659 batch loss 6.43411922 epoch total loss 6.52315092\n",
      "Trained batch 660 batch loss 6.32813883 epoch total loss 6.52285528\n",
      "Trained batch 661 batch loss 6.23763 epoch total loss 6.52242422\n",
      "Trained batch 662 batch loss 6.34766865 epoch total loss 6.52216\n",
      "Trained batch 663 batch loss 6.42038584 epoch total loss 6.52200651\n",
      "Trained batch 664 batch loss 6.30801868 epoch total loss 6.52168417\n",
      "Trained batch 665 batch loss 6.31454897 epoch total loss 6.5213728\n",
      "Trained batch 666 batch loss 6.55843592 epoch total loss 6.52142859\n",
      "Trained batch 667 batch loss 6.32403278 epoch total loss 6.52113295\n",
      "Trained batch 668 batch loss 6.26336908 epoch total loss 6.52074671\n",
      "Trained batch 669 batch loss 6.30240059 epoch total loss 6.52042\n",
      "Trained batch 670 batch loss 6.27705097 epoch total loss 6.52005672\n",
      "Trained batch 671 batch loss 6.05285072 epoch total loss 6.51936\n",
      "Trained batch 672 batch loss 6.32214165 epoch total loss 6.51906681\n",
      "Trained batch 673 batch loss 6.1693387 epoch total loss 6.51854753\n",
      "Trained batch 674 batch loss 6.56197548 epoch total loss 6.51861191\n",
      "Trained batch 675 batch loss 6.25460052 epoch total loss 6.51822042\n",
      "Trained batch 676 batch loss 6.71118546 epoch total loss 6.51850653\n",
      "Trained batch 677 batch loss 5.80740118 epoch total loss 6.51745605\n",
      "Trained batch 678 batch loss 6.59578943 epoch total loss 6.51757145\n",
      "Trained batch 679 batch loss 6.60354757 epoch total loss 6.51769829\n",
      "Trained batch 680 batch loss 6.51624918 epoch total loss 6.5176959\n",
      "Trained batch 681 batch loss 6.37888861 epoch total loss 6.51749229\n",
      "Trained batch 682 batch loss 6.47282314 epoch total loss 6.51742649\n",
      "Trained batch 683 batch loss 6.52500772 epoch total loss 6.51743746\n",
      "Trained batch 684 batch loss 6.34199286 epoch total loss 6.51718044\n",
      "Trained batch 685 batch loss 6.17074156 epoch total loss 6.516675\n",
      "Trained batch 686 batch loss 6.0998292 epoch total loss 6.51606703\n",
      "Trained batch 687 batch loss 6.71661234 epoch total loss 6.51635933\n",
      "Trained batch 688 batch loss 6.32726717 epoch total loss 6.51608419\n",
      "Trained batch 689 batch loss 6.43149614 epoch total loss 6.51596165\n",
      "Trained batch 690 batch loss 6.42409658 epoch total loss 6.51582861\n",
      "Trained batch 691 batch loss 6.41093874 epoch total loss 6.51567745\n",
      "Trained batch 692 batch loss 6.43467283 epoch total loss 6.51556\n",
      "Trained batch 693 batch loss 6.66094351 epoch total loss 6.51577\n",
      "Trained batch 694 batch loss 6.02356911 epoch total loss 6.5150609\n",
      "Trained batch 695 batch loss 6.46541166 epoch total loss 6.51498938\n",
      "Trained batch 696 batch loss 6.4296279 epoch total loss 6.51486683\n",
      "Trained batch 697 batch loss 6.33208466 epoch total loss 6.51460409\n",
      "Trained batch 698 batch loss 6.69356537 epoch total loss 6.51486063\n",
      "Trained batch 699 batch loss 6.68372583 epoch total loss 6.51510191\n",
      "Trained batch 700 batch loss 6.63551331 epoch total loss 6.51527405\n",
      "Trained batch 701 batch loss 6.6663785 epoch total loss 6.51549\n",
      "Trained batch 702 batch loss 6.48123837 epoch total loss 6.51544142\n",
      "Trained batch 703 batch loss 5.9157033 epoch total loss 6.51458788\n",
      "Trained batch 704 batch loss 6.39268875 epoch total loss 6.51441479\n",
      "Trained batch 705 batch loss 6.52117443 epoch total loss 6.51442385\n",
      "Trained batch 706 batch loss 6.3173275 epoch total loss 6.5141449\n",
      "Trained batch 707 batch loss 6.08936071 epoch total loss 6.51354408\n",
      "Trained batch 708 batch loss 6.11972094 epoch total loss 6.51298761\n",
      "Trained batch 709 batch loss 6.38854933 epoch total loss 6.51281261\n",
      "Trained batch 710 batch loss 6.43625546 epoch total loss 6.51270437\n",
      "Trained batch 711 batch loss 6.72823143 epoch total loss 6.51300716\n",
      "Trained batch 712 batch loss 6.76475286 epoch total loss 6.5133605\n",
      "Trained batch 713 batch loss 6.86880684 epoch total loss 6.5138588\n",
      "Trained batch 714 batch loss 6.49855471 epoch total loss 6.51383734\n",
      "Trained batch 715 batch loss 6.08046055 epoch total loss 6.51323128\n",
      "Trained batch 716 batch loss 6.25416136 epoch total loss 6.51287\n",
      "Trained batch 717 batch loss 6.8557477 epoch total loss 6.51334858\n",
      "Trained batch 718 batch loss 6.54238272 epoch total loss 6.51338911\n",
      "Trained batch 719 batch loss 6.41222858 epoch total loss 6.51324797\n",
      "Trained batch 720 batch loss 6.67501974 epoch total loss 6.51347256\n",
      "Trained batch 721 batch loss 6.60566854 epoch total loss 6.5136\n",
      "Trained batch 722 batch loss 6.60615969 epoch total loss 6.51372814\n",
      "Trained batch 723 batch loss 6.562222 epoch total loss 6.5137949\n",
      "Trained batch 724 batch loss 6.20459 epoch total loss 6.51336765\n",
      "Trained batch 725 batch loss 6.1981349 epoch total loss 6.51293325\n",
      "Trained batch 726 batch loss 5.96950102 epoch total loss 6.51218462\n",
      "Trained batch 727 batch loss 5.89584637 epoch total loss 6.51133728\n",
      "Trained batch 728 batch loss 6.03051233 epoch total loss 6.51067638\n",
      "Trained batch 729 batch loss 5.59094286 epoch total loss 6.50941467\n",
      "Trained batch 730 batch loss 5.26687336 epoch total loss 6.50771284\n",
      "Trained batch 731 batch loss 5.29515457 epoch total loss 6.50605392\n",
      "Trained batch 732 batch loss 5.86464 epoch total loss 6.5051775\n",
      "Trained batch 733 batch loss 6.41258621 epoch total loss 6.50505114\n",
      "Trained batch 734 batch loss 6.94811678 epoch total loss 6.50565529\n",
      "Trained batch 735 batch loss 6.51155 epoch total loss 6.50566339\n",
      "Trained batch 736 batch loss 6.57833529 epoch total loss 6.50576162\n",
      "Trained batch 737 batch loss 6.80845261 epoch total loss 6.50617266\n",
      "Trained batch 738 batch loss 6.39193296 epoch total loss 6.50601816\n",
      "Trained batch 739 batch loss 6.4084487 epoch total loss 6.50588655\n",
      "Trained batch 740 batch loss 6.23880672 epoch total loss 6.50552559\n",
      "Trained batch 741 batch loss 6.13851738 epoch total loss 6.50503063\n",
      "Trained batch 742 batch loss 6.42902613 epoch total loss 6.50492811\n",
      "Trained batch 743 batch loss 7.06817722 epoch total loss 6.50568676\n",
      "Trained batch 744 batch loss 7.29467058 epoch total loss 6.50674677\n",
      "Trained batch 745 batch loss 6.795753 epoch total loss 6.50713491\n",
      "Trained batch 746 batch loss 7.11042643 epoch total loss 6.50794363\n",
      "Trained batch 747 batch loss 6.64123 epoch total loss 6.50812149\n",
      "Trained batch 748 batch loss 6.93500185 epoch total loss 6.50869226\n",
      "Trained batch 749 batch loss 6.40948725 epoch total loss 6.50856\n",
      "Trained batch 750 batch loss 6.6170578 epoch total loss 6.50870514\n",
      "Trained batch 751 batch loss 6.57403183 epoch total loss 6.5087924\n",
      "Trained batch 752 batch loss 6.50150871 epoch total loss 6.50878239\n",
      "Trained batch 753 batch loss 6.95049191 epoch total loss 6.50936937\n",
      "Trained batch 754 batch loss 7.16886806 epoch total loss 6.51024437\n",
      "Trained batch 755 batch loss 6.7515707 epoch total loss 6.51056385\n",
      "Trained batch 756 batch loss 6.47056341 epoch total loss 6.51051092\n",
      "Trained batch 757 batch loss 6.34697199 epoch total loss 6.51029539\n",
      "Trained batch 758 batch loss 6.5114975 epoch total loss 6.5102973\n",
      "Trained batch 759 batch loss 6.4570117 epoch total loss 6.51022673\n",
      "Trained batch 760 batch loss 6.31903601 epoch total loss 6.50997496\n",
      "Trained batch 761 batch loss 6.52436209 epoch total loss 6.50999403\n",
      "Trained batch 762 batch loss 6.32580662 epoch total loss 6.50975227\n",
      "Trained batch 763 batch loss 6.71122169 epoch total loss 6.51001644\n",
      "Trained batch 764 batch loss 6.85184526 epoch total loss 6.51046419\n",
      "Trained batch 765 batch loss 6.70786905 epoch total loss 6.51072264\n",
      "Trained batch 766 batch loss 6.57997084 epoch total loss 6.51081276\n",
      "Trained batch 767 batch loss 6.65415287 epoch total loss 6.511\n",
      "Trained batch 768 batch loss 6.66661787 epoch total loss 6.51120234\n",
      "Trained batch 769 batch loss 6.53251 epoch total loss 6.51123047\n",
      "Trained batch 770 batch loss 6.66233 epoch total loss 6.51142645\n",
      "Trained batch 771 batch loss 6.72339535 epoch total loss 6.51170158\n",
      "Trained batch 772 batch loss 6.52494574 epoch total loss 6.51171875\n",
      "Trained batch 773 batch loss 6.77076769 epoch total loss 6.51205397\n",
      "Trained batch 1010 batch loss 6.49012136 epoch total loss 6.51824045\n",
      "Trained batch 1011 batch loss 6.3695364 epoch total loss 6.51809359\n",
      "Trained batch 1012 batch loss 7.20247507 epoch total loss 6.51876974\n",
      "Trained batch 1013 batch loss 6.20854521 epoch total loss 6.51846361\n",
      "Trained batch 1014 batch loss 6.63691759 epoch total loss 6.51858044\n",
      "Trained batch 1015 batch loss 6.58210135 epoch total loss 6.5186429\n",
      "Trained batch 1016 batch loss 6.09092188 epoch total loss 6.51822186\n",
      "Trained batch 1017 batch loss 6.92540741 epoch total loss 6.51862192\n",
      "Trained batch 1018 batch loss 6.75702858 epoch total loss 6.51885605\n",
      "Trained batch 1019 batch loss 6.31936264 epoch total loss 6.51866\n",
      "Trained batch 1020 batch loss 6.71153355 epoch total loss 6.5188489\n",
      "Trained batch 1021 batch loss 6.33795881 epoch total loss 6.51867199\n",
      "Trained batch 1022 batch loss 6.40403366 epoch total loss 6.51855946\n",
      "Trained batch 1023 batch loss 6.30353069 epoch total loss 6.51834965\n",
      "Trained batch 1024 batch loss 6.11484623 epoch total loss 6.5179553\n",
      "Trained batch 1025 batch loss 5.78720808 epoch total loss 6.51724243\n",
      "Trained batch 1026 batch loss 5.98923445 epoch total loss 6.51672745\n",
      "Trained batch 1027 batch loss 6.81645918 epoch total loss 6.51701927\n",
      "Trained batch 1028 batch loss 6.92999649 epoch total loss 6.51742125\n",
      "Trained batch 1029 batch loss 6.8883996 epoch total loss 6.51778173\n",
      "Trained batch 1030 batch loss 6.89616823 epoch total loss 6.5181489\n",
      "Trained batch 1031 batch loss 6.87401676 epoch total loss 6.51849413\n",
      "Trained batch 1032 batch loss 6.66734 epoch total loss 6.51863861\n",
      "Trained batch 1033 batch loss 7.10497665 epoch total loss 6.51920605\n",
      "Trained batch 1034 batch loss 6.66286087 epoch total loss 6.51934528\n",
      "Trained batch 1035 batch loss 6.55594969 epoch total loss 6.51938057\n",
      "Trained batch 1036 batch loss 6.64268637 epoch total loss 6.5195\n",
      "Trained batch 1037 batch loss 6.60994101 epoch total loss 6.51958704\n",
      "Trained batch 1038 batch loss 6.77633095 epoch total loss 6.51983404\n",
      "Trained batch 1039 batch loss 6.61093426 epoch total loss 6.51992178\n",
      "Trained batch 1040 batch loss 6.37669754 epoch total loss 6.51978397\n",
      "Trained batch 1041 batch loss 6.47672415 epoch total loss 6.51974249\n",
      "Trained batch 1042 batch loss 6.69479513 epoch total loss 6.51991034\n",
      "Trained batch 1043 batch loss 6.5731473 epoch total loss 6.51996136\n",
      "Trained batch 1044 batch loss 5.89532042 epoch total loss 6.5193634\n",
      "Trained batch 1045 batch loss 5.63306236 epoch total loss 6.51851559\n",
      "Trained batch 1046 batch loss 6.30208445 epoch total loss 6.51830864\n",
      "Trained batch 1047 batch loss 6.05877495 epoch total loss 6.51786947\n",
      "Trained batch 1048 batch loss 6.60265064 epoch total loss 6.51795053\n",
      "Trained batch 1049 batch loss 6.24765301 epoch total loss 6.51769257\n",
      "Trained batch 1050 batch loss 6.35509 epoch total loss 6.51753759\n",
      "Trained batch 1051 batch loss 6.29232121 epoch total loss 6.51732349\n",
      "Trained batch 1052 batch loss 6.51741362 epoch total loss 6.51732397\n",
      "Trained batch 1053 batch loss 6.56201267 epoch total loss 6.51736641\n",
      "Trained batch 1054 batch loss 6.47077703 epoch total loss 6.51732206\n",
      "Trained batch 1055 batch loss 6.86068678 epoch total loss 6.51764774\n",
      "Trained batch 1056 batch loss 5.50245047 epoch total loss 6.51668596\n",
      "Trained batch 1057 batch loss 5.03388166 epoch total loss 6.51528311\n",
      "Trained batch 1058 batch loss 5.54103565 epoch total loss 6.51436234\n",
      "Trained batch 1059 batch loss 6.75444365 epoch total loss 6.51458883\n",
      "Trained batch 1060 batch loss 7.18094921 epoch total loss 6.51521778\n",
      "Trained batch 1061 batch loss 7.4256587 epoch total loss 6.51607609\n",
      "Trained batch 1062 batch loss 7.12471867 epoch total loss 6.51664877\n",
      "Trained batch 1063 batch loss 6.79730082 epoch total loss 6.51691294\n",
      "Trained batch 1064 batch loss 6.75100279 epoch total loss 6.51713276\n",
      "Trained batch 1065 batch loss 6.83863735 epoch total loss 6.51743507\n",
      "Trained batch 1066 batch loss 6.91686916 epoch total loss 6.51781\n",
      "Trained batch 1067 batch loss 6.52697277 epoch total loss 6.51781845\n",
      "Trained batch 1068 batch loss 6.65260839 epoch total loss 6.51794481\n",
      "Trained batch 1069 batch loss 6.79714394 epoch total loss 6.51820612\n",
      "Trained batch 1070 batch loss 6.24770546 epoch total loss 6.5179534\n",
      "Trained batch 1071 batch loss 6.41706467 epoch total loss 6.51785898\n",
      "Trained batch 1072 batch loss 6.41759825 epoch total loss 6.51776552\n",
      "Trained batch 1073 batch loss 6.32226944 epoch total loss 6.51758289\n",
      "Trained batch 1074 batch loss 6.20221567 epoch total loss 6.51728964\n",
      "Trained batch 1075 batch loss 6.47439289 epoch total loss 6.51724958\n",
      "Trained batch 1076 batch loss 6.4952836 epoch total loss 6.51722908\n",
      "Trained batch 1077 batch loss 6.29296684 epoch total loss 6.5170207\n",
      "Trained batch 1078 batch loss 6.32898617 epoch total loss 6.51684666\n",
      "Trained batch 1079 batch loss 6.36397028 epoch total loss 6.51670456\n",
      "Trained batch 1080 batch loss 6.09706545 epoch total loss 6.51631641\n",
      "Trained batch 1081 batch loss 6.27869463 epoch total loss 6.51609659\n",
      "Trained batch 1082 batch loss 6.16376877 epoch total loss 6.51577091\n",
      "Trained batch 1083 batch loss 6.63473 epoch total loss 6.51588058\n",
      "Trained batch 1084 batch loss 6.79034281 epoch total loss 6.51613379\n",
      "Trained batch 1085 batch loss 6.62650442 epoch total loss 6.51623583\n",
      "Trained batch 1086 batch loss 6.32985973 epoch total loss 6.51606417\n",
      "Trained batch 1087 batch loss 6.79916763 epoch total loss 6.516325\n",
      "Trained batch 1088 batch loss 6.05629301 epoch total loss 6.51590204\n",
      "Trained batch 1089 batch loss 7.13251781 epoch total loss 6.51646805\n",
      "Trained batch 1090 batch loss 6.58889723 epoch total loss 6.51653433\n",
      "Trained batch 1091 batch loss 6.88104677 epoch total loss 6.51686811\n",
      "Trained batch 1092 batch loss 6.47601175 epoch total loss 6.51683092\n",
      "Trained batch 1093 batch loss 6.87417221 epoch total loss 6.51715755\n",
      "Trained batch 1094 batch loss 6.34883881 epoch total loss 6.51700354\n",
      "Trained batch 1095 batch loss 6.64096975 epoch total loss 6.51711702\n",
      "Trained batch 1096 batch loss 6.18127775 epoch total loss 6.51681042\n",
      "Trained batch 1097 batch loss 6.27334785 epoch total loss 6.51658869\n",
      "Trained batch 1098 batch loss 5.73292494 epoch total loss 6.51587486\n",
      "Trained batch 1099 batch loss 6.27927256 epoch total loss 6.51566\n",
      "Trained batch 1100 batch loss 6.37728 epoch total loss 6.51553392\n",
      "Trained batch 1101 batch loss 6.51630211 epoch total loss 6.5155344\n",
      "Trained batch 1102 batch loss 6.54418 epoch total loss 6.51556\n",
      "Trained batch 1103 batch loss 6.38968563 epoch total loss 6.51544619\n",
      "Trained batch 1104 batch loss 6.15584 epoch total loss 6.51512051\n",
      "Trained batch 1105 batch loss 6.51635742 epoch total loss 6.51512146\n",
      "Trained batch 1106 batch loss 6.42784548 epoch total loss 6.5150423\n",
      "Trained batch 1107 batch loss 6.50579882 epoch total loss 6.51503372\n",
      "Trained batch 1108 batch loss 6.80383873 epoch total loss 6.51529455\n",
      "Trained batch 1109 batch loss 6.63659096 epoch total loss 6.51540375\n",
      "Trained batch 1110 batch loss 6.80163145 epoch total loss 6.51566172\n",
      "Trained batch 1111 batch loss 6.88259411 epoch total loss 6.51599264\n",
      "Trained batch 1112 batch loss 6.68314314 epoch total loss 6.51614285\n",
      "Trained batch 1113 batch loss 6.45499897 epoch total loss 6.51608801\n",
      "Trained batch 1114 batch loss 6.77839756 epoch total loss 6.51632309\n",
      "Trained batch 1115 batch loss 6.819911 epoch total loss 6.51659536\n",
      "Trained batch 1116 batch loss 6.70258474 epoch total loss 6.51676226\n",
      "Trained batch 1117 batch loss 6.47793436 epoch total loss 6.51672745\n",
      "Trained batch 1118 batch loss 6.44560623 epoch total loss 6.51666403\n",
      "Trained batch 1119 batch loss 6.22925854 epoch total loss 6.51640749\n",
      "Trained batch 1120 batch loss 6.60021973 epoch total loss 6.51648188\n",
      "Trained batch 1121 batch loss 6.58837938 epoch total loss 6.51654625\n",
      "Trained batch 1122 batch loss 6.56951332 epoch total loss 6.51659346\n",
      "Trained batch 1123 batch loss 6.46774817 epoch total loss 6.51654959\n",
      "Trained batch 1124 batch loss 6.88406897 epoch total loss 6.51687717\n",
      "Trained batch 1125 batch loss 6.90737534 epoch total loss 6.51722383\n",
      "Trained batch 1126 batch loss 6.7118659 epoch total loss 6.51739693\n",
      "Trained batch 1127 batch loss 6.66668081 epoch total loss 6.51752901\n",
      "Trained batch 1128 batch loss 6.52546263 epoch total loss 6.51753616\n",
      "Trained batch 1129 batch loss 6.56435299 epoch total loss 6.51757765\n",
      "Trained batch 1130 batch loss 6.34676 epoch total loss 6.51742649\n",
      "Trained batch 1131 batch loss 6.60912132 epoch total loss 6.51750755\n",
      "Trained batch 1132 batch loss 6.78113413 epoch total loss 6.51774025\n",
      "Trained batch 1133 batch loss 6.39986038 epoch total loss 6.5176363\n",
      "Trained batch 1134 batch loss 6.65791416 epoch total loss 6.51776\n",
      "Trained batch 1135 batch loss 6.32259655 epoch total loss 6.51758814\n",
      "Trained batch 1136 batch loss 6.57928276 epoch total loss 6.51764202\n",
      "Trained batch 1137 batch loss 6.59223461 epoch total loss 6.51770782\n",
      "Trained batch 1138 batch loss 6.53945637 epoch total loss 6.5177269\n",
      "Trained batch 1139 batch loss 6.57453156 epoch total loss 6.51777697\n",
      "Trained batch 1140 batch loss 6.60447884 epoch total loss 6.51785326\n",
      "Trained batch 1141 batch loss 6.49178267 epoch total loss 6.51783037\n",
      "Trained batch 1142 batch loss 6.78981066 epoch total loss 6.51806831\n",
      "Trained batch 1143 batch loss 6.79415417 epoch total loss 6.51831\n",
      "Trained batch 1144 batch loss 6.91484118 epoch total loss 6.51865673\n",
      "Trained batch 1145 batch loss 6.69954586 epoch total loss 6.51881504\n",
      "Trained batch 1146 batch loss 6.61987448 epoch total loss 6.51890326\n",
      "Trained batch 1147 batch loss 6.6166091 epoch total loss 6.51898861\n",
      "Trained batch 1148 batch loss 6.67561388 epoch total loss 6.51912498\n",
      "Trained batch 1149 batch loss 6.8323288 epoch total loss 6.51939774\n",
      "Trained batch 1150 batch loss 6.62330055 epoch total loss 6.51948833\n",
      "Trained batch 1151 batch loss 6.73062563 epoch total loss 6.51967144\n",
      "Trained batch 1152 batch loss 6.47133636 epoch total loss 6.51962948\n",
      "Trained batch 1153 batch loss 6.46843 epoch total loss 6.51958513\n",
      "Trained batch 1154 batch loss 6.43651867 epoch total loss 6.51951313\n",
      "Trained batch 1155 batch loss 6.57717514 epoch total loss 6.51956272\n",
      "Trained batch 1156 batch loss 6.5504241 epoch total loss 6.51958942\n",
      "Trained batch 1157 batch loss 6.32149553 epoch total loss 6.51941824\n",
      "Trained batch 1158 batch loss 6.38711929 epoch total loss 6.5193038\n",
      "Trained batch 1159 batch loss 6.77169466 epoch total loss 6.51952171\n",
      "Trained batch 1160 batch loss 7.00998449 epoch total loss 6.51994419\n",
      "Trained batch 1161 batch loss 6.63071346 epoch total loss 6.52003956\n",
      "Trained batch 1162 batch loss 6.78354406 epoch total loss 6.52026653\n",
      "Trained batch 1163 batch loss 6.86209631 epoch total loss 6.52056074\n",
      "Trained batch 1164 batch loss 6.88000679 epoch total loss 6.52086926\n",
      "Trained batch 1165 batch loss 6.883008 epoch total loss 6.52118\n",
      "Trained batch 1166 batch loss 6.35985184 epoch total loss 6.52104187\n",
      "Trained batch 1167 batch loss 6.91779375 epoch total loss 6.52138186\n",
      "Trained batch 1168 batch loss 7.0161705 epoch total loss 6.52180529\n",
      "Trained batch 1169 batch loss 6.94129658 epoch total loss 6.52216434\n",
      "Trained batch 1170 batch loss 6.65115 epoch total loss 6.52227497\n",
      "Trained batch 1171 batch loss 6.69238949 epoch total loss 6.52242\n",
      "Trained batch 1172 batch loss 6.66385078 epoch total loss 6.52254105\n",
      "Trained batch 1173 batch loss 6.5824337 epoch total loss 6.52259207\n",
      "Trained batch 1174 batch loss 6.30317831 epoch total loss 6.52240515\n",
      "Trained batch 1175 batch loss 5.82755804 epoch total loss 6.52181387\n",
      "Trained batch 1176 batch loss 5.98121214 epoch total loss 6.5213542\n",
      "Trained batch 1177 batch loss 6.48390293 epoch total loss 6.52132273\n",
      "Trained batch 1178 batch loss 6.14133596 epoch total loss 6.521\n",
      "Trained batch 1179 batch loss 6.69508076 epoch total loss 6.52114773\n",
      "Trained batch 1180 batch loss 6.68264389 epoch total loss 6.52128458\n",
      "Trained batch 1181 batch loss 6.56244516 epoch total loss 6.52131939\n",
      "Trained batch 1182 batch loss 6.70943117 epoch total loss 6.52147865\n",
      "Trained batch 1183 batch loss 6.61972809 epoch total loss 6.52156162\n",
      "Trained batch 1184 batch loss 6.78831 epoch total loss 6.52178669\n",
      "Trained batch 1185 batch loss 6.49003315 epoch total loss 6.52176\n",
      "Trained batch 1186 batch loss 6.45601273 epoch total loss 6.52170467\n",
      "Trained batch 1187 batch loss 6.38477182 epoch total loss 6.52158928\n",
      "Trained batch 1188 batch loss 6.44204712 epoch total loss 6.52152205\n",
      "Trained batch 1189 batch loss 6.27750635 epoch total loss 6.52131701\n",
      "Trained batch 1190 batch loss 6.16386032 epoch total loss 6.5210166\n",
      "Trained batch 1191 batch loss 6.13962 epoch total loss 6.52069616\n",
      "Trained batch 1192 batch loss 6.0272851 epoch total loss 6.52028275\n",
      "Trained batch 1193 batch loss 6.91519451 epoch total loss 6.52061319\n",
      "Trained batch 1194 batch loss 7.01947737 epoch total loss 6.52103138\n",
      "Trained batch 1195 batch loss 6.6625967 epoch total loss 6.52114964\n",
      "Trained batch 1196 batch loss 6.58282185 epoch total loss 6.52120161\n",
      "Trained batch 1197 batch loss 6.90262413 epoch total loss 6.52152\n",
      "Trained batch 1198 batch loss 6.82309246 epoch total loss 6.52177191\n",
      "Trained batch 1199 batch loss 6.53050137 epoch total loss 6.52177906\n",
      "Trained batch 1200 batch loss 6.46554756 epoch total loss 6.52173233\n",
      "Trained batch 1201 batch loss 6.52528429 epoch total loss 6.52173519\n",
      "Trained batch 1202 batch loss 6.45375538 epoch total loss 6.52167845\n",
      "Trained batch 1203 batch loss 6.45070267 epoch total loss 6.52161932\n",
      "Trained batch 1204 batch loss 6.85594654 epoch total loss 6.52189732\n",
      "Trained batch 1205 batch loss 6.73671532 epoch total loss 6.52207565\n",
      "Trained batch 1206 batch loss 6.72433853 epoch total loss 6.52224302\n",
      "Trained batch 1207 batch loss 6.71563244 epoch total loss 6.52240372\n",
      "Trained batch 1208 batch loss 6.85003 epoch total loss 6.52267456\n",
      "Trained batch 1209 batch loss 6.64633751 epoch total loss 6.52277708\n",
      "Trained batch 1210 batch loss 6.68847179 epoch total loss 6.52291393\n",
      "Trained batch 1211 batch loss 5.98808193 epoch total loss 6.52247286\n",
      "Trained batch 1212 batch loss 6.30036116 epoch total loss 6.52228928\n",
      "Trained batch 1213 batch loss 6.29897 epoch total loss 6.52210522\n",
      "Trained batch 1214 batch loss 6.51759815 epoch total loss 6.5221014\n",
      "Trained batch 1215 batch loss 6.80984068 epoch total loss 6.52233839\n",
      "Trained batch 1216 batch loss 6.272645 epoch total loss 6.52213287\n",
      "Trained batch 1217 batch loss 5.79713917 epoch total loss 6.5215373\n",
      "Trained batch 1218 batch loss 6.07505465 epoch total loss 6.52117109\n",
      "Trained batch 1219 batch loss 6.04165268 epoch total loss 6.52077723\n",
      "Trained batch 1220 batch loss 6.47073936 epoch total loss 6.52073622\n",
      "Trained batch 1221 batch loss 7.38206482 epoch total loss 6.52144146\n",
      "Trained batch 1222 batch loss 6.95581341 epoch total loss 6.52179718\n",
      "Trained batch 1223 batch loss 6.95559931 epoch total loss 6.52215195\n",
      "Trained batch 1224 batch loss 6.9034934 epoch total loss 6.52246332\n",
      "Trained batch 1225 batch loss 7.12821293 epoch total loss 6.5229578\n",
      "Trained batch 1226 batch loss 6.98885489 epoch total loss 6.52333784\n",
      "Trained batch 1227 batch loss 6.89335299 epoch total loss 6.52363968\n",
      "Trained batch 1228 batch loss 6.24224138 epoch total loss 6.52341032\n",
      "Trained batch 1229 batch loss 6.64926767 epoch total loss 6.52351284\n",
      "Trained batch 1230 batch loss 6.51652098 epoch total loss 6.5235076\n",
      "Trained batch 1231 batch loss 6.47380543 epoch total loss 6.52346706\n",
      "Trained batch 1232 batch loss 6.80834818 epoch total loss 6.52369785\n",
      "Trained batch 1233 batch loss 6.48326397 epoch total loss 6.52366543\n",
      "Trained batch 1234 batch loss 6.71314573 epoch total loss 6.52381897\n",
      "Trained batch 1235 batch loss 6.72905445 epoch total loss 6.52398491\n",
      "Trained batch 1236 batch loss 6.85405302 epoch total loss 6.52425194\n",
      "Trained batch 1237 batch loss 7.3758378 epoch total loss 6.52494049\n",
      "Trained batch 1238 batch loss 7.10760975 epoch total loss 6.52541113\n",
      "Trained batch 1239 batch loss 6.4619751 epoch total loss 6.52536\n",
      "Trained batch 1240 batch loss 6.90499449 epoch total loss 6.52566576\n",
      "Trained batch 1241 batch loss 6.34994316 epoch total loss 6.52552462\n",
      "Trained batch 1242 batch loss 6.64463282 epoch total loss 6.52562\n",
      "Trained batch 1243 batch loss 6.08863926 epoch total loss 6.52526903\n",
      "Trained batch 1244 batch loss 6.36949921 epoch total loss 6.52514362\n",
      "Trained batch 1245 batch loss 6.21183109 epoch total loss 6.52489233\n",
      "Trained batch 1246 batch loss 6.68899059 epoch total loss 6.52502394\n",
      "Trained batch 1247 batch loss 6.75055122 epoch total loss 6.52520466\n",
      "Trained batch 1248 batch loss 6.77968884 epoch total loss 6.52540874\n",
      "Trained batch 1249 batch loss 6.65218115 epoch total loss 6.52551031\n",
      "Trained batch 1250 batch loss 6.59070826 epoch total loss 6.52556229\n",
      "Trained batch 1251 batch loss 6.28351879 epoch total loss 6.52536917\n",
      "Trained batch 1252 batch loss 6.64004087 epoch total loss 6.52546072\n",
      "Trained batch 1253 batch loss 6.56683636 epoch total loss 6.5254941\n",
      "Trained batch 1254 batch loss 6.61842442 epoch total loss 6.52556801\n",
      "Trained batch 1255 batch loss 6.61710739 epoch total loss 6.52564096\n",
      "Trained batch 1256 batch loss 6.50743294 epoch total loss 6.52562714\n",
      "Trained batch 1257 batch loss 6.15813971 epoch total loss 6.52533484\n",
      "Trained batch 1258 batch loss 6.50296783 epoch total loss 6.52531672\n",
      "Trained batch 1259 batch loss 6.58225918 epoch total loss 6.52536201\n",
      "Trained batch 1260 batch loss 6.5547986 epoch total loss 6.52538538\n",
      "Trained batch 1261 batch loss 6.20952177 epoch total loss 6.52513504\n",
      "Trained batch 1262 batch loss 6.46076822 epoch total loss 6.52508402\n",
      "Trained batch 1263 batch loss 6.30862522 epoch total loss 6.52491283\n",
      "Trained batch 1264 batch loss 6.49988317 epoch total loss 6.52489328\n",
      "Trained batch 1265 batch loss 6.41299343 epoch total loss 6.52480459\n",
      "Trained batch 1266 batch loss 6.2748518 epoch total loss 6.5246067\n",
      "Trained batch 1267 batch loss 6.71754169 epoch total loss 6.52475929\n",
      "Trained batch 1268 batch loss 6.56009912 epoch total loss 6.52478743\n",
      "Trained batch 1269 batch loss 6.22713041 epoch total loss 6.5245533\n",
      "Trained batch 1270 batch loss 6.71423817 epoch total loss 6.52470255\n",
      "Trained batch 1271 batch loss 6.92330885 epoch total loss 6.52501583\n",
      "Trained batch 1272 batch loss 7.09859943 epoch total loss 6.52546644\n",
      "Trained batch 1273 batch loss 6.87597895 epoch total loss 6.52574205\n",
      "Trained batch 1274 batch loss 6.41798306 epoch total loss 6.52565718\n",
      "Trained batch 1275 batch loss 6.10953951 epoch total loss 6.52533102\n",
      "Trained batch 1276 batch loss 5.57580948 epoch total loss 6.52458715\n",
      "Trained batch 1277 batch loss 6.61858082 epoch total loss 6.52466\n",
      "Trained batch 1278 batch loss 6.61592531 epoch total loss 6.52473211\n",
      "Trained batch 1279 batch loss 6.60276413 epoch total loss 6.52479267\n",
      "Trained batch 1280 batch loss 6.39385 epoch total loss 6.52469\n",
      "Trained batch 1281 batch loss 6.40025282 epoch total loss 6.52459335\n",
      "Trained batch 1282 batch loss 6.33202696 epoch total loss 6.52444315\n",
      "Trained batch 1283 batch loss 6.53877687 epoch total loss 6.52445459\n",
      "Trained batch 1284 batch loss 6.67581701 epoch total loss 6.52457237\n",
      "Trained batch 1285 batch loss 6.47063971 epoch total loss 6.52453041\n",
      "Trained batch 1286 batch loss 6.0913744 epoch total loss 6.52419376\n",
      "Trained batch 1287 batch loss 6.2811451 epoch total loss 6.52400494\n",
      "Trained batch 1288 batch loss 6.16519976 epoch total loss 6.52372646\n",
      "Trained batch 1289 batch loss 6.24610043 epoch total loss 6.52351093\n",
      "Trained batch 1290 batch loss 6.19061852 epoch total loss 6.52325296\n",
      "Trained batch 1291 batch loss 6.39408588 epoch total loss 6.52315331\n",
      "Trained batch 1292 batch loss 5.9334054 epoch total loss 6.52269697\n",
      "Trained batch 1293 batch loss 6.23697281 epoch total loss 6.5224762\n",
      "Trained batch 1294 batch loss 5.84153748 epoch total loss 6.52195024\n",
      "Trained batch 1295 batch loss 6.44298029 epoch total loss 6.52188921\n",
      "Trained batch 1296 batch loss 6.27416849 epoch total loss 6.52169847\n",
      "Trained batch 1297 batch loss 6.45002127 epoch total loss 6.52164316\n",
      "Trained batch 1298 batch loss 6.86565876 epoch total loss 6.52190781\n",
      "Trained batch 1299 batch loss 6.50853825 epoch total loss 6.52189779\n",
      "Trained batch 1300 batch loss 6.57908583 epoch total loss 6.52194166\n",
      "Trained batch 1301 batch loss 6.38243389 epoch total loss 6.52183485\n",
      "Trained batch 1302 batch loss 6.67425442 epoch total loss 6.52195168\n",
      "Trained batch 1303 batch loss 6.2472434 epoch total loss 6.52174091\n",
      "Trained batch 1304 batch loss 6.84782171 epoch total loss 6.52199078\n",
      "Trained batch 1305 batch loss 6.09353209 epoch total loss 6.52166271\n",
      "Trained batch 1306 batch loss 5.90909767 epoch total loss 6.5211935\n",
      "Trained batch 1307 batch loss 6.4034791 epoch total loss 6.52110338\n",
      "Trained batch 1308 batch loss 5.92139387 epoch total loss 6.52064514\n",
      "Trained batch 1309 batch loss 5.55699301 epoch total loss 6.51990891\n",
      "Trained batch 1310 batch loss 5.79552746 epoch total loss 6.51935625\n",
      "Trained batch 1311 batch loss 5.68490505 epoch total loss 6.5187192\n",
      "Trained batch 1312 batch loss 6.12250519 epoch total loss 6.51841688\n",
      "Trained batch 1313 batch loss 5.66133308 epoch total loss 6.51776409\n",
      "Trained batch 1314 batch loss 6.00573826 epoch total loss 6.51737452\n",
      "Trained batch 1315 batch loss 6.36387253 epoch total loss 6.51725817\n",
      "Trained batch 1316 batch loss 6.12390184 epoch total loss 6.51695919\n",
      "Trained batch 1317 batch loss 6.11420488 epoch total loss 6.51665354\n",
      "Trained batch 1318 batch loss 5.87691832 epoch total loss 6.51616812\n",
      "Trained batch 1319 batch loss 6.29089689 epoch total loss 6.51599741\n",
      "Trained batch 1320 batch loss 6.52228069 epoch total loss 6.51600218\n",
      "Trained batch 1321 batch loss 6.71527863 epoch total loss 6.51615286\n",
      "Trained batch 1322 batch loss 6.90678263 epoch total loss 6.5164485\n",
      "Trained batch 1323 batch loss 6.90664244 epoch total loss 6.51674318\n",
      "Trained batch 1324 batch loss 7.00773096 epoch total loss 6.51711416\n",
      "Trained batch 1325 batch loss 7.00388336 epoch total loss 6.5174818\n",
      "Trained batch 1326 batch loss 6.81321096 epoch total loss 6.51770496\n",
      "Trained batch 1327 batch loss 6.54604244 epoch total loss 6.51772594\n",
      "Trained batch 1328 batch loss 6.68054676 epoch total loss 6.51784897\n",
      "Trained batch 1329 batch loss 6.65026855 epoch total loss 6.51794863\n",
      "Trained batch 1330 batch loss 6.79515839 epoch total loss 6.51815653\n",
      "Trained batch 1331 batch loss 6.80036 epoch total loss 6.5183692\n",
      "Trained batch 1332 batch loss 7.17009592 epoch total loss 6.51885843\n",
      "Trained batch 1333 batch loss 7.05575323 epoch total loss 6.51926088\n",
      "Trained batch 1334 batch loss 6.89427328 epoch total loss 6.51954222\n",
      "Trained batch 1335 batch loss 6.80722141 epoch total loss 6.51975822\n",
      "Trained batch 1336 batch loss 6.84238768 epoch total loss 6.52\n",
      "Trained batch 1337 batch loss 6.65887833 epoch total loss 6.52010393\n",
      "Trained batch 1338 batch loss 6.26051188 epoch total loss 6.51991\n",
      "Trained batch 1339 batch loss 6.81167603 epoch total loss 6.52012777\n",
      "Trained batch 1340 batch loss 6.70815802 epoch total loss 6.52026796\n",
      "Trained batch 1341 batch loss 6.76917791 epoch total loss 6.52045393\n",
      "Trained batch 1342 batch loss 6.7080946 epoch total loss 6.52059364\n",
      "Trained batch 1343 batch loss 6.51102161 epoch total loss 6.52058649\n",
      "Trained batch 1344 batch loss 6.51731539 epoch total loss 6.52058411\n",
      "Trained batch 1345 batch loss 6.66858339 epoch total loss 6.52069426\n",
      "Trained batch 1346 batch loss 6.57806921 epoch total loss 6.52073717\n",
      "Trained batch 1347 batch loss 6.7959013 epoch total loss 6.52094126\n",
      "Trained batch 1348 batch loss 6.73513556 epoch total loss 6.52110052\n",
      "Trained batch 1349 batch loss 6.75999451 epoch total loss 6.52127743\n",
      "Trained batch 1350 batch loss 6.67800379 epoch total loss 6.5213933\n",
      "Trained batch 1351 batch loss 6.79268265 epoch total loss 6.52159405\n",
      "Trained batch 1352 batch loss 6.71455574 epoch total loss 6.5217371\n",
      "Trained batch 1353 batch loss 6.82868958 epoch total loss 6.52196455\n",
      "Trained batch 1354 batch loss 6.70500755 epoch total loss 6.52209949\n",
      "Trained batch 1355 batch loss 6.6628046 epoch total loss 6.52220345\n",
      "Trained batch 1356 batch loss 6.79001045 epoch total loss 6.52240133\n",
      "Trained batch 1357 batch loss 6.5428133 epoch total loss 6.52241611\n",
      "Trained batch 1358 batch loss 6.67431116 epoch total loss 6.52252769\n",
      "Trained batch 1359 batch loss 6.57088184 epoch total loss 6.52256346\n",
      "Trained batch 1360 batch loss 6.39857054 epoch total loss 6.52247238\n",
      "Trained batch 1361 batch loss 6.36588097 epoch total loss 6.52235746\n",
      "Trained batch 1362 batch loss 6.5826869 epoch total loss 6.52240229\n",
      "Trained batch 1363 batch loss 6.59915066 epoch total loss 6.52245855\n",
      "Trained batch 1364 batch loss 6.58868694 epoch total loss 6.52250767\n",
      "Trained batch 1365 batch loss 6.2028451 epoch total loss 6.52227354\n",
      "Trained batch 1366 batch loss 6.58231735 epoch total loss 6.52231741\n",
      "Trained batch 1367 batch loss 6.36019897 epoch total loss 6.52219868\n",
      "Trained batch 1368 batch loss 6.59557199 epoch total loss 6.52225256\n",
      "Trained batch 1369 batch loss 6.57722569 epoch total loss 6.52229261\n",
      "Trained batch 1370 batch loss 6.66047478 epoch total loss 6.52239323\n",
      "Trained batch 1371 batch loss 6.50418663 epoch total loss 6.52238\n",
      "Trained batch 1372 batch loss 6.56292725 epoch total loss 6.52240896\n",
      "Trained batch 1373 batch loss 6.45458746 epoch total loss 6.52235937\n",
      "Trained batch 1374 batch loss 6.60093307 epoch total loss 6.52241611\n",
      "Trained batch 1375 batch loss 6.6888032 epoch total loss 6.52253675\n",
      "Trained batch 1376 batch loss 6.64164162 epoch total loss 6.52262354\n",
      "Trained batch 1377 batch loss 6.72879124 epoch total loss 6.52277279\n",
      "Trained batch 1378 batch loss 6.14566755 epoch total loss 6.52249908\n",
      "Trained batch 1379 batch loss 5.94345045 epoch total loss 6.52207947\n",
      "Trained batch 1380 batch loss 5.95569658 epoch total loss 6.52166891\n",
      "Trained batch 1381 batch loss 5.82413626 epoch total loss 6.52116394\n",
      "Trained batch 1382 batch loss 6.42886305 epoch total loss 6.52109718\n",
      "Trained batch 1383 batch loss 6.13824034 epoch total loss 6.52082062\n",
      "Trained batch 1384 batch loss 6.22647 epoch total loss 6.52060795\n",
      "Trained batch 1385 batch loss 6.01228809 epoch total loss 6.52024126\n",
      "Trained batch 1386 batch loss 6.29667568 epoch total loss 6.52008\n",
      "Trained batch 1387 batch loss 6.13301563 epoch total loss 6.51980114\n",
      "Trained batch 1388 batch loss 5.95480299 epoch total loss 6.51939392\n",
      "Epoch 9 train loss 6.5193939208984375\n",
      "Validated batch 1 batch loss 6.46838427\n",
      "Validated batch 2 batch loss 5.98403215\n",
      "Validated batch 3 batch loss 6.69871521\n",
      "Validated batch 4 batch loss 6.70245409\n",
      "Validated batch 5 batch loss 6.47421789\n",
      "Validated batch 6 batch loss 6.85382843\n",
      "Validated batch 7 batch loss 6.70221663\n",
      "Validated batch 8 batch loss 6.82584572\n",
      "Validated batch 9 batch loss 6.73535204\n",
      "Validated batch 10 batch loss 6.64823198\n",
      "Validated batch 11 batch loss 6.35229445\n",
      "Validated batch 12 batch loss 6.55611467\n",
      "Validated batch 13 batch loss 6.48821211\n",
      "Validated batch 14 batch loss 7.08926868\n",
      "Validated batch 15 batch loss 6.72440529\n",
      "Validated batch 16 batch loss 6.46188307\n",
      "Validated batch 17 batch loss 6.7454586\n",
      "Validated batch 18 batch loss 6.01546431\n",
      "Validated batch 19 batch loss 6.63923454\n",
      "Validated batch 20 batch loss 6.74579906\n",
      "Validated batch 21 batch loss 6.66266441\n",
      "Validated batch 22 batch loss 6.91672707\n",
      "Validated batch 23 batch loss 6.3143363\n",
      "Validated batch 24 batch loss 6.20696783\n",
      "Validated batch 25 batch loss 6.97506142\n",
      "Validated batch 26 batch loss 6.64637566\n",
      "Validated batch 27 batch loss 6.84538269\n",
      "Validated batch 28 batch loss 6.72234297\n",
      "Validated batch 29 batch loss 6.9514637\n",
      "Validated batch 30 batch loss 6.55404615\n",
      "Validated batch 31 batch loss 7.02636766\n",
      "Validated batch 32 batch loss 6.31726313\n",
      "Validated batch 33 batch loss 6.652\n",
      "Validated batch 34 batch loss 6.71911621\n",
      "Validated batch 35 batch loss 5.95281744\n",
      "Validated batch 36 batch loss 6.07903\n",
      "Validated batch 37 batch loss 6.98114061\n",
      "Validated batch 38 batch loss 6.79342604\n",
      "Validated batch 39 batch loss 6.25320482\n",
      "Validated batch 40 batch loss 6.86370659\n",
      "Validated batch 41 batch loss 6.93182182\n",
      "Validated batch 42 batch loss 6.83642387\n",
      "Validated batch 43 batch loss 6.97028351\n",
      "Validated batch 44 batch loss 6.96020317\n",
      "Validated batch 45 batch loss 6.43604326\n",
      "Validated batch 46 batch loss 6.23884583\n",
      "Validated batch 47 batch loss 6.75337791\n",
      "Validated batch 48 batch loss 6.58191633\n",
      "Validated batch 49 batch loss 6.39213514\n",
      "Validated batch 50 batch loss 6.29438734\n",
      "Validated batch 51 batch loss 6.36675215\n",
      "Validated batch 52 batch loss 6.75239515\n",
      "Validated batch 53 batch loss 6.48847246\n",
      "Validated batch 54 batch loss 6.68364716\n",
      "Validated batch 55 batch loss 6.85128736\n",
      "Validated batch 56 batch loss 6.61645031\n",
      "Validated batch 57 batch loss 6.4284997\n",
      "Validated batch 58 batch loss 6.07840824\n",
      "Validated batch 59 batch loss 6.87482405\n",
      "Validated batch 60 batch loss 6.54037523\n",
      "Validated batch 61 batch loss 6.69430351\n",
      "Validated batch 62 batch loss 6.50924921\n",
      "Validated batch 63 batch loss 6.58830833\n",
      "Validated batch 64 batch loss 5.87445116\n",
      "Validated batch 65 batch loss 6.24477386\n",
      "Validated batch 66 batch loss 6.65612841\n",
      "Validated batch 67 batch loss 6.20137739\n",
      "Validated batch 68 batch loss 6.38639641\n",
      "Validated batch 69 batch loss 6.50641394\n",
      "Validated batch 70 batch loss 6.79759789\n",
      "Validated batch 71 batch loss 7.1024251\n",
      "Validated batch 72 batch loss 6.39981031\n",
      "Validated batch 73 batch loss 5.87522936\n",
      "Validated batch 74 batch loss 6.88420916\n",
      "Validated batch 75 batch loss 6.3127923\n",
      "Validated batch 76 batch loss 6.09555292\n",
      "Validated batch 77 batch loss 6.41875696\n",
      "Validated batch 78 batch loss 6.14524841\n",
      "Validated batch 79 batch loss 6.93844414\n",
      "Validated batch 80 batch loss 6.17967129\n",
      "Validated batch 81 batch loss 6.08929253\n",
      "Validated batch 82 batch loss 6.49221897\n",
      "Validated batch 83 batch loss 6.6234355\n",
      "Validated batch 84 batch loss 6.26911974\n",
      "Validated batch 85 batch loss 6.61677742\n",
      "Validated batch 86 batch loss 6.43284655\n",
      "Validated batch 87 batch loss 6.86502886\n",
      "Validated batch 88 batch loss 6.42908382\n",
      "Validated batch 89 batch loss 6.47405958\n",
      "Validated batch 90 batch loss 6.83074236\n",
      "Validated batch 91 batch loss 5.55076504\n",
      "Validated batch 92 batch loss 6.90997458\n",
      "Validated batch 93 batch loss 6.65285444\n",
      "Validated batch 94 batch loss 6.58428192\n",
      "Validated batch 95 batch loss 6.47018242\n",
      "Validated batch 96 batch loss 5.77289343\n",
      "Validated batch 97 batch loss 6.25626326\n",
      "Validated batch 98 batch loss 7.08898115\n",
      "Validated batch 99 batch loss 6.27973413\n",
      "Validated batch 100 batch loss 6.60770226\n",
      "Validated batch 101 batch loss 6.7145772\n",
      "Validated batch 102 batch loss 6.82698774\n",
      "Validated batch 103 batch loss 6.91834831\n",
      "Validated batch 104 batch loss 6.48025751\n",
      "Validated batch 105 batch loss 6.58793592\n",
      "Validated batch 106 batch loss 6.5930562\n",
      "Validated batch 107 batch loss 6.7874074\n",
      "Validated batch 108 batch loss 7.17363453\n",
      "Validated batch 109 batch loss 6.41828108\n",
      "Validated batch 110 batch loss 7.02976274\n",
      "Validated batch 111 batch loss 6.8805027\n",
      "Validated batch 112 batch loss 6.43114424\n",
      "Validated batch 113 batch loss 6.61105299\n",
      "Validated batch 114 batch loss 6.70720482\n",
      "Validated batch 115 batch loss 6.89980125\n",
      "Validated batch 116 batch loss 6.9119854\n",
      "Validated batch 117 batch loss 6.51916\n",
      "Validated batch 118 batch loss 6.42982435\n",
      "Validated batch 119 batch loss 6.31795549\n",
      "Validated batch 120 batch loss 6.55217457\n",
      "Validated batch 121 batch loss 6.61431122\n",
      "Validated batch 122 batch loss 6.62157726\n",
      "Validated batch 123 batch loss 6.745399\n",
      "Validated batch 124 batch loss 6.41192722\n",
      "Validated batch 125 batch loss 6.76246452\n",
      "Validated batch 126 batch loss 7.11192131\n",
      "Validated batch 127 batch loss 6.8689785\n",
      "Validated batch 128 batch loss 6.53223372\n",
      "Validated batch 129 batch loss 6.53386927\n",
      "Validated batch 130 batch loss 6.69010353\n",
      "Validated batch 131 batch loss 6.52085114\n",
      "Validated batch 132 batch loss 6.92199755\n",
      "Validated batch 133 batch loss 6.37348223\n",
      "Validated batch 134 batch loss 6.84745312\n",
      "Validated batch 135 batch loss 6.59132433\n",
      "Validated batch 136 batch loss 6.35783339\n",
      "Validated batch 137 batch loss 6.76071215\n",
      "Validated batch 138 batch loss 6.66636086\n",
      "Validated batch 139 batch loss 7.06629467\n",
      "Validated batch 140 batch loss 6.73028517\n",
      "Validated batch 141 batch loss 6.76167297\n",
      "Validated batch 142 batch loss 6.6209178\n",
      "Validated batch 143 batch loss 6.6503849\n",
      "Validated batch 144 batch loss 7.0460453\n",
      "Validated batch 145 batch loss 6.62204695\n",
      "Validated batch 146 batch loss 6.20786476\n",
      "Validated batch 147 batch loss 6.33347607\n",
      "Validated batch 148 batch loss 6.56018877\n",
      "Validated batch 149 batch loss 6.66515541\n",
      "Validated batch 150 batch loss 6.59544373\n",
      "Validated batch 151 batch loss 6.37791204\n",
      "Validated batch 152 batch loss 6.26538086\n",
      "Validated batch 153 batch loss 6.53579807\n",
      "Validated batch 154 batch loss 6.64004469\n",
      "Validated batch 155 batch loss 6.83178139\n",
      "Validated batch 156 batch loss 6.86504412\n",
      "Validated batch 157 batch loss 7.17907858\n",
      "Validated batch 158 batch loss 7.53560638\n",
      "Validated batch 159 batch loss 7.26255083\n",
      "Validated batch 160 batch loss 6.63024187\n",
      "Validated batch 161 batch loss 6.16859627\n",
      "Validated batch 162 batch loss 6.38680363\n",
      "Validated batch 163 batch loss 6.56939888\n",
      "Validated batch 164 batch loss 6.65362597\n",
      "Validated batch 165 batch loss 6.67936\n",
      "Validated batch 166 batch loss 7.09235334\n",
      "Validated batch 167 batch loss 6.78163099\n",
      "Validated batch 168 batch loss 6.82049799\n",
      "Validated batch 169 batch loss 6.71598\n",
      "Validated batch 170 batch loss 6.91332817\n",
      "Validated batch 171 batch loss 6.78528643\n",
      "Validated batch 172 batch loss 7.10399675\n",
      "Validated batch 173 batch loss 7.20316601\n",
      "Validated batch 174 batch loss 6.1916604\n",
      "Validated batch 175 batch loss 6.89071465\n",
      "Validated batch 176 batch loss 6.74416924\n",
      "Validated batch 177 batch loss 6.55103874\n",
      "Validated batch 178 batch loss 6.67767525\n",
      "Validated batch 179 batch loss 6.20306301\n",
      "Validated batch 180 batch loss 6.14321184\n",
      "Validated batch 181 batch loss 6.87001848\n",
      "Validated batch 182 batch loss 6.64549398\n",
      "Validated batch 183 batch loss 7.12050438\n",
      "Validated batch 184 batch loss 6.53606653\n",
      "Validated batch 185 batch loss 3.48463607\n",
      "Epoch 9 val loss 6.595504283905029\n",
      "Epoch 9 completed in 766.07 seconds\n",
      "Start epoch 10 with learning rate 0.0007\n",
      "Trained batch 1 batch loss 6.70370674 epoch total loss 6.70370674\n",
      "Trained batch 2 batch loss 6.41483 epoch total loss 6.55926847\n",
      "Trained batch 3 batch loss 6.53085899 epoch total loss 6.54979849\n",
      "Trained batch 4 batch loss 6.51754761 epoch total loss 6.54173565\n",
      "Trained batch 5 batch loss 6.48401976 epoch total loss 6.53019238\n",
      "Trained batch 6 batch loss 6.80023861 epoch total loss 6.57520056\n",
      "Trained batch 7 batch loss 6.4725585 epoch total loss 6.56053686\n",
      "Trained batch 8 batch loss 6.29808092 epoch total loss 6.52773\n",
      "Trained batch 9 batch loss 6.31155968 epoch total loss 6.50371122\n",
      "Trained batch 10 batch loss 6.41650105 epoch total loss 6.49499035\n",
      "Trained batch 11 batch loss 6.85329151 epoch total loss 6.52756357\n",
      "Trained batch 12 batch loss 6.58837414 epoch total loss 6.53263092\n",
      "Trained batch 13 batch loss 6.6571188 epoch total loss 6.54220676\n",
      "Trained batch 14 batch loss 6.65773582 epoch total loss 6.55045938\n",
      "Trained batch 15 batch loss 6.50942564 epoch total loss 6.54772329\n",
      "Trained batch 16 batch loss 6.39881 epoch total loss 6.53841639\n",
      "Trained batch 17 batch loss 6.50571823 epoch total loss 6.53649282\n",
      "Trained batch 18 batch loss 6.42407513 epoch total loss 6.53024721\n",
      "Trained batch 19 batch loss 6.48914576 epoch total loss 6.5280838\n",
      "Trained batch 20 batch loss 6.09646416 epoch total loss 6.50650263\n",
      "Trained batch 21 batch loss 6.76733828 epoch total loss 6.51892328\n",
      "Trained batch 22 batch loss 6.79364586 epoch total loss 6.53141\n",
      "Trained batch 23 batch loss 6.96835423 epoch total loss 6.55040789\n",
      "Trained batch 24 batch loss 6.835042 epoch total loss 6.5622673\n",
      "Trained batch 25 batch loss 6.94359541 epoch total loss 6.57752037\n",
      "Trained batch 26 batch loss 6.70431614 epoch total loss 6.58239698\n",
      "Trained batch 27 batch loss 6.34222364 epoch total loss 6.57350159\n",
      "Trained batch 28 batch loss 6.56604815 epoch total loss 6.57323551\n",
      "Trained batch 29 batch loss 6.62012386 epoch total loss 6.57485247\n",
      "Trained batch 30 batch loss 6.60005713 epoch total loss 6.57569218\n",
      "Trained batch 31 batch loss 6.56904221 epoch total loss 6.57547808\n",
      "Trained batch 32 batch loss 6.53862143 epoch total loss 6.57432604\n",
      "Trained batch 33 batch loss 6.57115793 epoch total loss 6.57422972\n",
      "Trained batch 34 batch loss 6.58398628 epoch total loss 6.57451677\n",
      "Trained batch 35 batch loss 6.36818266 epoch total loss 6.56862116\n",
      "Trained batch 36 batch loss 6.81919575 epoch total loss 6.57558203\n",
      "Trained batch 37 batch loss 6.66290569 epoch total loss 6.57794189\n",
      "Trained batch 38 batch loss 6.37582 epoch total loss 6.57262278\n",
      "Trained batch 39 batch loss 6.37910318 epoch total loss 6.56766081\n",
      "Trained batch 40 batch loss 6.60064459 epoch total loss 6.56848526\n",
      "Trained batch 41 batch loss 6.35958576 epoch total loss 6.56339025\n",
      "Trained batch 42 batch loss 5.84711885 epoch total loss 6.5463357\n",
      "Trained batch 43 batch loss 5.86605072 epoch total loss 6.53051567\n",
      "Trained batch 44 batch loss 6.42536926 epoch total loss 6.52812624\n",
      "Trained batch 45 batch loss 6.17158031 epoch total loss 6.52020264\n",
      "Trained batch 46 batch loss 6.25675058 epoch total loss 6.51447535\n",
      "Trained batch 47 batch loss 6.59935236 epoch total loss 6.5162816\n",
      "Trained batch 48 batch loss 6.61334085 epoch total loss 6.51830339\n",
      "Trained batch 49 batch loss 6.80214214 epoch total loss 6.52409649\n",
      "Trained batch 50 batch loss 6.64332867 epoch total loss 6.52648115\n",
      "Trained batch 51 batch loss 6.60141087 epoch total loss 6.52795029\n",
      "Trained batch 52 batch loss 6.3984766 epoch total loss 6.52546024\n",
      "Trained batch 53 batch loss 6.12206125 epoch total loss 6.51784945\n",
      "Trained batch 54 batch loss 6.3341465 epoch total loss 6.51444721\n",
      "Trained batch 55 batch loss 6.51058722 epoch total loss 6.51437712\n",
      "Trained batch 56 batch loss 6.66565895 epoch total loss 6.5170784\n",
      "Trained batch 57 batch loss 6.29767895 epoch total loss 6.51322889\n",
      "Trained batch 58 batch loss 6.59175587 epoch total loss 6.51458311\n",
      "Trained batch 59 batch loss 6.78467226 epoch total loss 6.51916075\n",
      "Trained batch 60 batch loss 6.16200399 epoch total loss 6.51320839\n",
      "Trained batch 61 batch loss 5.93140316 epoch total loss 6.50367069\n",
      "Trained batch 62 batch loss 6.42960215 epoch total loss 6.50247574\n",
      "Trained batch 63 batch loss 6.77880764 epoch total loss 6.50686216\n",
      "Trained batch 64 batch loss 6.73444891 epoch total loss 6.51041794\n",
      "Trained batch 65 batch loss 6.28374577 epoch total loss 6.50693083\n",
      "Trained batch 66 batch loss 5.72653818 epoch total loss 6.4951067\n",
      "Trained batch 67 batch loss 6.17509937 epoch total loss 6.4903307\n",
      "Trained batch 68 batch loss 6.57920599 epoch total loss 6.49163723\n",
      "Trained batch 69 batch loss 6.61419487 epoch total loss 6.49341345\n",
      "Trained batch 70 batch loss 6.66531849 epoch total loss 6.49586916\n",
      "Trained batch 71 batch loss 6.6097312 epoch total loss 6.49747324\n",
      "Trained batch 72 batch loss 6.91549826 epoch total loss 6.50327873\n",
      "Trained batch 73 batch loss 7.23506927 epoch total loss 6.51330376\n",
      "Trained batch 74 batch loss 6.78076363 epoch total loss 6.51691771\n",
      "Trained batch 75 batch loss 6.84960365 epoch total loss 6.52135372\n",
      "Trained batch 76 batch loss 6.93020582 epoch total loss 6.5267334\n",
      "Trained batch 77 batch loss 6.86572266 epoch total loss 6.53113604\n",
      "Trained batch 78 batch loss 6.6152277 epoch total loss 6.53221416\n",
      "Trained batch 79 batch loss 6.1676 epoch total loss 6.52759886\n",
      "Trained batch 80 batch loss 5.810956 epoch total loss 6.518641\n",
      "Trained batch 81 batch loss 6.17522573 epoch total loss 6.51440144\n",
      "Trained batch 82 batch loss 6.54981375 epoch total loss 6.51483297\n",
      "Trained batch 83 batch loss 6.76982212 epoch total loss 6.51790524\n",
      "Trained batch 84 batch loss 6.50617218 epoch total loss 6.51776552\n",
      "Trained batch 85 batch loss 6.76656103 epoch total loss 6.52069235\n",
      "Trained batch 86 batch loss 6.30739784 epoch total loss 6.51821184\n",
      "Trained batch 87 batch loss 6.37774277 epoch total loss 6.51659727\n",
      "Trained batch 88 batch loss 6.30933905 epoch total loss 6.51424217\n",
      "Trained batch 89 batch loss 6.40435266 epoch total loss 6.51300716\n",
      "Trained batch 90 batch loss 6.24931622 epoch total loss 6.51007748\n",
      "Trained batch 91 batch loss 5.67650175 epoch total loss 6.50091743\n",
      "Trained batch 92 batch loss 6.53058481 epoch total loss 6.50124\n",
      "Trained batch 93 batch loss 6.36018324 epoch total loss 6.49972296\n",
      "Trained batch 94 batch loss 5.87398529 epoch total loss 6.49306583\n",
      "Trained batch 95 batch loss 5.52758789 epoch total loss 6.482903\n",
      "Trained batch 96 batch loss 5.38503 epoch total loss 6.47146654\n",
      "Trained batch 97 batch loss 5.86406612 epoch total loss 6.46520472\n",
      "Trained batch 98 batch loss 6.2057209 epoch total loss 6.46255732\n",
      "Trained batch 99 batch loss 5.61731625 epoch total loss 6.45401955\n",
      "Trained batch 100 batch loss 6.21325922 epoch total loss 6.451612\n",
      "Trained batch 101 batch loss 6.34352684 epoch total loss 6.4505415\n",
      "Trained batch 102 batch loss 6.15771389 epoch total loss 6.44767094\n",
      "Trained batch 103 batch loss 6.07065344 epoch total loss 6.44401073\n",
      "Trained batch 104 batch loss 6.05038738 epoch total loss 6.44022608\n",
      "Trained batch 105 batch loss 6.46404362 epoch total loss 6.44045305\n",
      "Trained batch 106 batch loss 6.49521542 epoch total loss 6.44097\n",
      "Trained batch 107 batch loss 6.82105923 epoch total loss 6.4445219\n",
      "Trained batch 108 batch loss 6.84035873 epoch total loss 6.44818687\n",
      "Trained batch 109 batch loss 6.97402859 epoch total loss 6.45301056\n",
      "Trained batch 110 batch loss 6.59843111 epoch total loss 6.45433283\n",
      "Trained batch 111 batch loss 6.62580919 epoch total loss 6.45587778\n",
      "Trained batch 112 batch loss 6.46530628 epoch total loss 6.45596218\n",
      "Trained batch 113 batch loss 6.71066523 epoch total loss 6.45821619\n",
      "Trained batch 114 batch loss 6.2264924 epoch total loss 6.45618391\n",
      "Trained batch 115 batch loss 6.751966 epoch total loss 6.45875549\n",
      "Trained batch 116 batch loss 6.22372866 epoch total loss 6.45673\n",
      "Trained batch 117 batch loss 6.39258909 epoch total loss 6.45618153\n",
      "Trained batch 118 batch loss 6.8525486 epoch total loss 6.45954037\n",
      "Trained batch 119 batch loss 6.88996363 epoch total loss 6.46315718\n",
      "Trained batch 120 batch loss 6.80669975 epoch total loss 6.46602\n",
      "Trained batch 121 batch loss 6.77060938 epoch total loss 6.46853781\n",
      "Trained batch 122 batch loss 6.91554689 epoch total loss 6.47220135\n",
      "Trained batch 123 batch loss 6.87896156 epoch total loss 6.47550869\n",
      "Trained batch 124 batch loss 6.6787653 epoch total loss 6.47714758\n",
      "Trained batch 125 batch loss 6.76431 epoch total loss 6.47944498\n",
      "Trained batch 126 batch loss 6.97857904 epoch total loss 6.48340607\n",
      "Trained batch 127 batch loss 6.91415167 epoch total loss 6.48679781\n",
      "Trained batch 128 batch loss 6.80576515 epoch total loss 6.48928976\n",
      "Trained batch 129 batch loss 6.63573551 epoch total loss 6.49042511\n",
      "Trained batch 130 batch loss 6.60725927 epoch total loss 6.49132347\n",
      "Trained batch 131 batch loss 6.69024944 epoch total loss 6.4928422\n",
      "Trained batch 132 batch loss 6.67885208 epoch total loss 6.49425125\n",
      "Trained batch 133 batch loss 6.1824789 epoch total loss 6.49190712\n",
      "Trained batch 134 batch loss 6.65708303 epoch total loss 6.49313974\n",
      "Trained batch 135 batch loss 6.01694059 epoch total loss 6.48961258\n",
      "Trained batch 136 batch loss 6.02656364 epoch total loss 6.48620796\n",
      "Trained batch 137 batch loss 6.52320385 epoch total loss 6.48647785\n",
      "Trained batch 138 batch loss 6.56869698 epoch total loss 6.4870739\n",
      "Trained batch 139 batch loss 6.73588848 epoch total loss 6.48886395\n",
      "Trained batch 140 batch loss 6.62772226 epoch total loss 6.48985577\n",
      "Trained batch 141 batch loss 6.60217381 epoch total loss 6.49065256\n",
      "Trained batch 142 batch loss 6.55613756 epoch total loss 6.49111366\n",
      "Trained batch 143 batch loss 6.62026596 epoch total loss 6.49201679\n",
      "Trained batch 144 batch loss 6.51493359 epoch total loss 6.49217606\n",
      "Trained batch 145 batch loss 6.11597967 epoch total loss 6.48958158\n",
      "Trained batch 146 batch loss 6.47559118 epoch total loss 6.48948574\n",
      "Trained batch 147 batch loss 6.32505083 epoch total loss 6.48836708\n",
      "Trained batch 148 batch loss 6.43541241 epoch total loss 6.48800945\n",
      "Trained batch 149 batch loss 6.42575312 epoch total loss 6.48759174\n",
      "Trained batch 150 batch loss 6.33966446 epoch total loss 6.48660564\n",
      "Trained batch 151 batch loss 6.47179461 epoch total loss 6.48650742\n",
      "Trained batch 152 batch loss 6.47848082 epoch total loss 6.48645449\n",
      "Trained batch 153 batch loss 6.64436245 epoch total loss 6.48748636\n",
      "Trained batch 154 batch loss 6.8265357 epoch total loss 6.4896884\n",
      "Trained batch 155 batch loss 6.58565235 epoch total loss 6.49030733\n",
      "Trained batch 156 batch loss 6.93588543 epoch total loss 6.49316359\n",
      "Trained batch 157 batch loss 7.220222 epoch total loss 6.49779463\n",
      "Trained batch 158 batch loss 6.20744419 epoch total loss 6.4959569\n",
      "Trained batch 159 batch loss 6.54208136 epoch total loss 6.49624729\n",
      "Trained batch 160 batch loss 6.34910488 epoch total loss 6.49532795\n",
      "Trained batch 161 batch loss 5.60417175 epoch total loss 6.48979235\n",
      "Trained batch 162 batch loss 6.12268257 epoch total loss 6.48752642\n",
      "Trained batch 163 batch loss 6.29004383 epoch total loss 6.48631477\n",
      "Trained batch 164 batch loss 5.75073099 epoch total loss 6.48182917\n",
      "Trained batch 165 batch loss 5.29005337 epoch total loss 6.47460651\n",
      "Trained batch 166 batch loss 5.32431221 epoch total loss 6.46767712\n",
      "Trained batch 167 batch loss 5.50577831 epoch total loss 6.46191692\n",
      "Trained batch 168 batch loss 5.8296957 epoch total loss 6.45815372\n",
      "Trained batch 169 batch loss 6.35830736 epoch total loss 6.45756292\n",
      "Trained batch 170 batch loss 6.65109 epoch total loss 6.45870161\n",
      "Trained batch 171 batch loss 6.82161427 epoch total loss 6.46082401\n",
      "Trained batch 172 batch loss 7.24574614 epoch total loss 6.46538734\n",
      "Trained batch 173 batch loss 6.98574 epoch total loss 6.46839523\n",
      "Trained batch 174 batch loss 6.63839436 epoch total loss 6.46937227\n",
      "Trained batch 175 batch loss 6.75577 epoch total loss 6.47100878\n",
      "Trained batch 176 batch loss 6.70622635 epoch total loss 6.47234488\n",
      "Trained batch 177 batch loss 6.65855169 epoch total loss 6.47339678\n",
      "Trained batch 178 batch loss 6.67452335 epoch total loss 6.47452688\n",
      "Trained batch 179 batch loss 6.52619123 epoch total loss 6.47481585\n",
      "Trained batch 180 batch loss 6.54036283 epoch total loss 6.47518063\n",
      "Trained batch 181 batch loss 6.60093832 epoch total loss 6.47587538\n",
      "Trained batch 182 batch loss 5.80046558 epoch total loss 6.47216415\n",
      "Trained batch 183 batch loss 6.00627279 epoch total loss 6.46961784\n",
      "Trained batch 184 batch loss 6.4388566 epoch total loss 6.46945047\n",
      "Trained batch 185 batch loss 6.29102612 epoch total loss 6.46848583\n",
      "Trained batch 186 batch loss 6.82198524 epoch total loss 6.47038698\n",
      "Trained batch 187 batch loss 6.67091227 epoch total loss 6.47145891\n",
      "Trained batch 188 batch loss 6.67832327 epoch total loss 6.47255945\n",
      "Trained batch 189 batch loss 6.55675316 epoch total loss 6.47300482\n",
      "Trained batch 190 batch loss 6.58894634 epoch total loss 6.47361565\n",
      "Trained batch 191 batch loss 6.34800196 epoch total loss 6.47295809\n",
      "Trained batch 192 batch loss 6.3273859 epoch total loss 6.4722\n",
      "Trained batch 193 batch loss 6.11981106 epoch total loss 6.47037363\n",
      "Trained batch 194 batch loss 6.47580719 epoch total loss 6.47040176\n",
      "Trained batch 195 batch loss 6.8135767 epoch total loss 6.47216177\n",
      "Trained batch 196 batch loss 6.80186844 epoch total loss 6.47384405\n",
      "Trained batch 197 batch loss 6.67026472 epoch total loss 6.47484112\n",
      "Trained batch 198 batch loss 6.58369446 epoch total loss 6.47539091\n",
      "Trained batch 199 batch loss 6.41453171 epoch total loss 6.47508526\n",
      "Trained batch 200 batch loss 6.73775578 epoch total loss 6.47639894\n",
      "Trained batch 201 batch loss 6.62555838 epoch total loss 6.47714138\n",
      "Trained batch 202 batch loss 6.31394434 epoch total loss 6.47633362\n",
      "Trained batch 203 batch loss 6.45841169 epoch total loss 6.47624493\n",
      "Trained batch 204 batch loss 6.49229765 epoch total loss 6.4763236\n",
      "Trained batch 205 batch loss 6.58530521 epoch total loss 6.47685528\n",
      "Trained batch 206 batch loss 6.74285603 epoch total loss 6.47814655\n",
      "Trained batch 207 batch loss 6.63084555 epoch total loss 6.47888422\n",
      "Trained batch 208 batch loss 6.5850029 epoch total loss 6.47939396\n",
      "Trained batch 209 batch loss 6.66512632 epoch total loss 6.48028278\n",
      "Trained batch 210 batch loss 6.64835405 epoch total loss 6.48108339\n",
      "Trained batch 211 batch loss 6.81162596 epoch total loss 6.48265\n",
      "Trained batch 212 batch loss 6.89328957 epoch total loss 6.48458672\n",
      "Trained batch 213 batch loss 6.61933279 epoch total loss 6.48522\n",
      "Trained batch 214 batch loss 6.4604888 epoch total loss 6.48510408\n",
      "Trained batch 215 batch loss 6.6791544 epoch total loss 6.48600674\n",
      "Trained batch 216 batch loss 6.78606129 epoch total loss 6.48739576\n",
      "Trained batch 217 batch loss 7.22565 epoch total loss 6.490798\n",
      "Trained batch 218 batch loss 7.23721695 epoch total loss 6.49422169\n",
      "Trained batch 219 batch loss 7.20344973 epoch total loss 6.49746037\n",
      "Trained batch 220 batch loss 6.82127905 epoch total loss 6.49893236\n",
      "Trained batch 221 batch loss 6.57610941 epoch total loss 6.49928141\n",
      "Trained batch 222 batch loss 6.59841061 epoch total loss 6.49972773\n",
      "Trained batch 223 batch loss 6.4024086 epoch total loss 6.4992919\n",
      "Trained batch 224 batch loss 6.26259661 epoch total loss 6.49823475\n",
      "Trained batch 225 batch loss 6.22657681 epoch total loss 6.4970274\n",
      "Trained batch 226 batch loss 6.55863285 epoch total loss 6.49729967\n",
      "Trained batch 227 batch loss 6.66657066 epoch total loss 6.49804592\n",
      "Trained batch 228 batch loss 6.71747112 epoch total loss 6.49900866\n",
      "Trained batch 229 batch loss 6.727139 epoch total loss 6.50000477\n",
      "Trained batch 230 batch loss 6.63820457 epoch total loss 6.50060558\n",
      "Trained batch 231 batch loss 6.47582293 epoch total loss 6.50049829\n",
      "Trained batch 232 batch loss 6.67640924 epoch total loss 6.50125647\n",
      "Trained batch 233 batch loss 6.46390963 epoch total loss 6.50109625\n",
      "Trained batch 234 batch loss 6.6231637 epoch total loss 6.50161791\n",
      "Trained batch 235 batch loss 6.39653492 epoch total loss 6.50117\n",
      "Trained batch 236 batch loss 6.51406288 epoch total loss 6.50122499\n",
      "Trained batch 237 batch loss 6.41563225 epoch total loss 6.50086355\n",
      "Trained batch 238 batch loss 6.19309282 epoch total loss 6.49957085\n",
      "Trained batch 239 batch loss 5.91725206 epoch total loss 6.49713421\n",
      "Trained batch 240 batch loss 6.21308517 epoch total loss 6.4959507\n",
      "Trained batch 241 batch loss 6.34881067 epoch total loss 6.49534\n",
      "Trained batch 242 batch loss 6.56586838 epoch total loss 6.49563169\n",
      "Trained batch 243 batch loss 6.41984701 epoch total loss 6.49532\n",
      "Trained batch 244 batch loss 6.34938192 epoch total loss 6.49472141\n",
      "Trained batch 245 batch loss 6.5211544 epoch total loss 6.49482918\n",
      "Trained batch 246 batch loss 6.65619755 epoch total loss 6.49548531\n",
      "Trained batch 247 batch loss 6.85200214 epoch total loss 6.49692917\n",
      "Trained batch 248 batch loss 6.91643715 epoch total loss 6.49862051\n",
      "Trained batch 249 batch loss 6.83680916 epoch total loss 6.49997854\n",
      "Trained batch 250 batch loss 6.46404552 epoch total loss 6.49983454\n",
      "Trained batch 251 batch loss 6.78032064 epoch total loss 6.50095177\n",
      "Trained batch 252 batch loss 6.33001709 epoch total loss 6.50027323\n",
      "Trained batch 253 batch loss 6.48815775 epoch total loss 6.50022554\n",
      "Trained batch 254 batch loss 6.58206558 epoch total loss 6.50054741\n",
      "Trained batch 255 batch loss 7.02563906 epoch total loss 6.50260639\n",
      "Trained batch 256 batch loss 6.73885393 epoch total loss 6.50352955\n",
      "Trained batch 257 batch loss 6.01769 epoch total loss 6.50163937\n",
      "Trained batch 258 batch loss 6.33780956 epoch total loss 6.50100422\n",
      "Trained batch 259 batch loss 6.27921343 epoch total loss 6.50014734\n",
      "Trained batch 260 batch loss 6.67845297 epoch total loss 6.50083351\n",
      "Trained batch 261 batch loss 6.39267731 epoch total loss 6.50041914\n",
      "Trained batch 262 batch loss 6.72820377 epoch total loss 6.50128841\n",
      "Trained batch 263 batch loss 6.95787716 epoch total loss 6.50302458\n",
      "Trained batch 264 batch loss 6.99197 epoch total loss 6.50487614\n",
      "Trained batch 265 batch loss 6.76345873 epoch total loss 6.50585222\n",
      "Trained batch 266 batch loss 6.63931036 epoch total loss 6.50635386\n",
      "Trained batch 267 batch loss 6.49524212 epoch total loss 6.50631189\n",
      "Trained batch 268 batch loss 6.76515627 epoch total loss 6.50727797\n",
      "Trained batch 269 batch loss 6.63952541 epoch total loss 6.50776958\n",
      "Trained batch 270 batch loss 6.86608124 epoch total loss 6.50909662\n",
      "Trained batch 271 batch loss 6.70412779 epoch total loss 6.50981617\n",
      "Trained batch 272 batch loss 6.09509373 epoch total loss 6.50829124\n",
      "Trained batch 273 batch loss 6.30210304 epoch total loss 6.50753593\n",
      "Trained batch 274 batch loss 6.60902452 epoch total loss 6.50790644\n",
      "Trained batch 275 batch loss 6.75914049 epoch total loss 6.50882\n",
      "Trained batch 276 batch loss 6.52098846 epoch total loss 6.5088644\n",
      "Trained batch 277 batch loss 6.63160658 epoch total loss 6.50930738\n",
      "Trained batch 278 batch loss 6.57777166 epoch total loss 6.50955343\n",
      "Trained batch 279 batch loss 6.86145639 epoch total loss 6.51081467\n",
      "Trained batch 280 batch loss 6.83275366 epoch total loss 6.5119648\n",
      "Trained batch 281 batch loss 6.7387557 epoch total loss 6.51277161\n",
      "Trained batch 282 batch loss 6.56590176 epoch total loss 6.51296043\n",
      "Trained batch 283 batch loss 6.34420681 epoch total loss 6.51236391\n",
      "Trained batch 284 batch loss 6.27897835 epoch total loss 6.51154232\n",
      "Trained batch 285 batch loss 6.47808743 epoch total loss 6.51142454\n",
      "Trained batch 286 batch loss 6.29619932 epoch total loss 6.51067162\n",
      "Trained batch 287 batch loss 6.16676617 epoch total loss 6.50947332\n",
      "Trained batch 288 batch loss 7.33556843 epoch total loss 6.51234198\n",
      "Trained batch 289 batch loss 7.1423583 epoch total loss 6.5145216\n",
      "Trained batch 290 batch loss 7.24005699 epoch total loss 6.51702356\n",
      "Trained batch 291 batch loss 6.6038456 epoch total loss 6.51732206\n",
      "Trained batch 292 batch loss 6.53665257 epoch total loss 6.51738834\n",
      "Trained batch 293 batch loss 6.67903 epoch total loss 6.51794\n",
      "Trained batch 294 batch loss 6.71838808 epoch total loss 6.51862192\n",
      "Trained batch 295 batch loss 6.44987869 epoch total loss 6.51838875\n",
      "Trained batch 296 batch loss 6.50574064 epoch total loss 6.51834583\n",
      "Trained batch 297 batch loss 6.66830587 epoch total loss 6.5188508\n",
      "Trained batch 298 batch loss 6.76387739 epoch total loss 6.51967335\n",
      "Trained batch 299 batch loss 7.04396105 epoch total loss 6.52142668\n",
      "Trained batch 300 batch loss 6.62738466 epoch total loss 6.52178\n",
      "Trained batch 301 batch loss 6.24601603 epoch total loss 6.52086401\n",
      "Trained batch 302 batch loss 6.54426146 epoch total loss 6.52094173\n",
      "Trained batch 303 batch loss 6.25451374 epoch total loss 6.52006245\n",
      "Trained batch 304 batch loss 5.51654148 epoch total loss 6.5167613\n",
      "Trained batch 305 batch loss 5.52436495 epoch total loss 6.51350784\n",
      "Trained batch 306 batch loss 5.97568369 epoch total loss 6.51175\n",
      "Trained batch 307 batch loss 5.92529106 epoch total loss 6.50984\n",
      "Trained batch 308 batch loss 6.31874275 epoch total loss 6.50921965\n",
      "Trained batch 309 batch loss 6.71415091 epoch total loss 6.50988245\n",
      "Trained batch 310 batch loss 6.67020226 epoch total loss 6.5104\n",
      "Trained batch 311 batch loss 6.59595156 epoch total loss 6.51067448\n",
      "Trained batch 312 batch loss 6.50023508 epoch total loss 6.5106411\n",
      "Trained batch 313 batch loss 6.56788683 epoch total loss 6.5108242\n",
      "Trained batch 314 batch loss 6.54421473 epoch total loss 6.51093054\n",
      "Trained batch 315 batch loss 6.42906094 epoch total loss 6.51067\n",
      "Trained batch 316 batch loss 6.34880161 epoch total loss 6.51015806\n",
      "Trained batch 317 batch loss 6.50591278 epoch total loss 6.51014471\n",
      "Trained batch 318 batch loss 6.01828718 epoch total loss 6.50859785\n",
      "Trained batch 319 batch loss 6.69598436 epoch total loss 6.50918531\n",
      "Trained batch 320 batch loss 6.54874563 epoch total loss 6.50930929\n",
      "Trained batch 321 batch loss 6.99150038 epoch total loss 6.51081133\n",
      "Trained batch 322 batch loss 6.92179537 epoch total loss 6.51208782\n",
      "Trained batch 323 batch loss 6.46212387 epoch total loss 6.51193333\n",
      "Trained batch 324 batch loss 5.89809847 epoch total loss 6.51003933\n",
      "Trained batch 325 batch loss 6.22464514 epoch total loss 6.509161\n",
      "Trained batch 326 batch loss 6.47724438 epoch total loss 6.50906324\n",
      "Trained batch 327 batch loss 6.3204155 epoch total loss 6.50848579\n",
      "Trained batch 328 batch loss 6.22619867 epoch total loss 6.50762558\n",
      "Trained batch 329 batch loss 6.93400621 epoch total loss 6.5089221\n",
      "Trained batch 330 batch loss 6.98130703 epoch total loss 6.51035309\n",
      "Trained batch 331 batch loss 6.96587181 epoch total loss 6.51172924\n",
      "Trained batch 332 batch loss 7.20210695 epoch total loss 6.51380873\n",
      "Trained batch 333 batch loss 6.60156584 epoch total loss 6.51407194\n",
      "Trained batch 334 batch loss 6.02979612 epoch total loss 6.51262236\n",
      "Trained batch 335 batch loss 6.3486352 epoch total loss 6.51213264\n",
      "Trained batch 336 batch loss 6.47994184 epoch total loss 6.5120368\n",
      "Trained batch 337 batch loss 6.38520098 epoch total loss 6.51166058\n",
      "Trained batch 338 batch loss 6.46764946 epoch total loss 6.5115304\n",
      "Trained batch 339 batch loss 6.70087147 epoch total loss 6.51208878\n",
      "Trained batch 340 batch loss 6.69724083 epoch total loss 6.51263332\n",
      "Trained batch 341 batch loss 6.63479 epoch total loss 6.51299191\n",
      "Trained batch 342 batch loss 6.519 epoch total loss 6.51300955\n",
      "Trained batch 343 batch loss 6.67254829 epoch total loss 6.51347494\n",
      "Trained batch 344 batch loss 6.58943129 epoch total loss 6.51369524\n",
      "Trained batch 345 batch loss 6.55608892 epoch total loss 6.51381826\n",
      "Trained batch 346 batch loss 6.52928495 epoch total loss 6.51386309\n",
      "Trained batch 347 batch loss 6.6014185 epoch total loss 6.51411533\n",
      "Trained batch 348 batch loss 6.64874649 epoch total loss 6.51450205\n",
      "Trained batch 349 batch loss 6.65783501 epoch total loss 6.51491213\n",
      "Trained batch 350 batch loss 6.23883963 epoch total loss 6.51412296\n",
      "Trained batch 351 batch loss 6.32286072 epoch total loss 6.51357794\n",
      "Trained batch 352 batch loss 6.46895123 epoch total loss 6.5134511\n",
      "Trained batch 353 batch loss 6.44509792 epoch total loss 6.5132575\n",
      "Trained batch 354 batch loss 6.30901766 epoch total loss 6.51268101\n",
      "Trained batch 355 batch loss 6.37314081 epoch total loss 6.51228762\n",
      "Trained batch 356 batch loss 6.58515644 epoch total loss 6.51249218\n",
      "Trained batch 357 batch loss 6.17086601 epoch total loss 6.51153564\n",
      "Trained batch 358 batch loss 6.63533115 epoch total loss 6.51188087\n",
      "Trained batch 359 batch loss 6.84657049 epoch total loss 6.51281357\n",
      "Trained batch 360 batch loss 6.93943167 epoch total loss 6.51399899\n",
      "Trained batch 361 batch loss 7.12209892 epoch total loss 6.51568317\n",
      "Trained batch 362 batch loss 6.72583389 epoch total loss 6.51626348\n",
      "Trained batch 363 batch loss 6.57637501 epoch total loss 6.51642942\n",
      "Trained batch 364 batch loss 5.97586489 epoch total loss 6.51494408\n",
      "Trained batch 365 batch loss 6.18577099 epoch total loss 6.51404238\n",
      "Trained batch 366 batch loss 6.21296215 epoch total loss 6.51322\n",
      "Trained batch 367 batch loss 6.81595469 epoch total loss 6.51404428\n",
      "Trained batch 368 batch loss 6.53292274 epoch total loss 6.51409578\n",
      "Trained batch 369 batch loss 6.42265177 epoch total loss 6.51384783\n",
      "Trained batch 370 batch loss 6.41116762 epoch total loss 6.51357031\n",
      "Trained batch 371 batch loss 6.58589268 epoch total loss 6.51376534\n",
      "Trained batch 372 batch loss 6.65075779 epoch total loss 6.51413393\n",
      "Trained batch 373 batch loss 6.65705395 epoch total loss 6.51451683\n",
      "Trained batch 374 batch loss 6.87168646 epoch total loss 6.51547146\n",
      "Trained batch 375 batch loss 6.7725606 epoch total loss 6.51615667\n",
      "Trained batch 376 batch loss 6.46896505 epoch total loss 6.51603127\n",
      "Trained batch 377 batch loss 6.11785078 epoch total loss 6.51497555\n",
      "Trained batch 378 batch loss 6.4734683 epoch total loss 6.5148654\n",
      "Trained batch 379 batch loss 6.40953112 epoch total loss 6.5145874\n",
      "Trained batch 380 batch loss 6.7785573 epoch total loss 6.51528215\n",
      "Trained batch 381 batch loss 6.69261932 epoch total loss 6.51574755\n",
      "Trained batch 382 batch loss 7.06925869 epoch total loss 6.51719666\n",
      "Trained batch 383 batch loss 6.95025921 epoch total loss 6.51832724\n",
      "Trained batch 384 batch loss 7.06274366 epoch total loss 6.51974487\n",
      "Trained batch 385 batch loss 7.15572739 epoch total loss 6.52139664\n",
      "Trained batch 386 batch loss 7.17079639 epoch total loss 6.5230794\n",
      "Trained batch 387 batch loss 7.2508707 epoch total loss 6.52496052\n",
      "Trained batch 388 batch loss 6.97728252 epoch total loss 6.52612638\n",
      "Trained batch 389 batch loss 6.82911873 epoch total loss 6.52690506\n",
      "Trained batch 390 batch loss 6.47490168 epoch total loss 6.52677155\n",
      "Trained batch 391 batch loss 6.60804939 epoch total loss 6.52698\n",
      "Trained batch 392 batch loss 6.46053123 epoch total loss 6.52681\n",
      "Trained batch 393 batch loss 5.76032639 epoch total loss 6.52485943\n",
      "Trained batch 394 batch loss 6.07723427 epoch total loss 6.52372313\n",
      "Trained batch 395 batch loss 5.84621382 epoch total loss 6.52200794\n",
      "Trained batch 396 batch loss 6.72777033 epoch total loss 6.52252769\n",
      "Trained batch 397 batch loss 7.17689896 epoch total loss 6.52417612\n",
      "Trained batch 398 batch loss 7.13721704 epoch total loss 6.5257163\n",
      "Trained batch 399 batch loss 6.88070297 epoch total loss 6.52660561\n",
      "Trained batch 400 batch loss 7.05766487 epoch total loss 6.52793312\n",
      "Trained batch 401 batch loss 6.88635063 epoch total loss 6.52882671\n",
      "Trained batch 402 batch loss 6.95093203 epoch total loss 6.52987671\n",
      "Trained batch 403 batch loss 6.81001282 epoch total loss 6.53057194\n",
      "Trained batch 404 batch loss 6.2751112 epoch total loss 6.52993965\n",
      "Trained batch 405 batch loss 6.67562151 epoch total loss 6.53029919\n",
      "Trained batch 406 batch loss 6.62086964 epoch total loss 6.53052235\n",
      "Trained batch 407 batch loss 6.56999683 epoch total loss 6.53061962\n",
      "Trained batch 408 batch loss 6.76588535 epoch total loss 6.53119612\n",
      "Trained batch 409 batch loss 6.52711201 epoch total loss 6.5311861\n",
      "Trained batch 410 batch loss 6.80756569 epoch total loss 6.53186035\n",
      "Trained batch 411 batch loss 6.7000246 epoch total loss 6.53226948\n",
      "Trained batch 412 batch loss 6.5452776 epoch total loss 6.53230047\n",
      "Trained batch 413 batch loss 6.37255669 epoch total loss 6.53191376\n",
      "Trained batch 414 batch loss 6.6081 epoch total loss 6.53209782\n",
      "Trained batch 415 batch loss 6.42100191 epoch total loss 6.53183\n",
      "Trained batch 416 batch loss 6.31710148 epoch total loss 6.5313139\n",
      "Trained batch 417 batch loss 6.50246334 epoch total loss 6.53124475\n",
      "Trained batch 418 batch loss 6.55368328 epoch total loss 6.53129864\n",
      "Trained batch 419 batch loss 6.57271767 epoch total loss 6.53139734\n",
      "Trained batch 420 batch loss 6.38166094 epoch total loss 6.53104067\n",
      "Trained batch 421 batch loss 6.46594572 epoch total loss 6.53088617\n",
      "Trained batch 422 batch loss 6.52758026 epoch total loss 6.53087854\n",
      "Trained batch 423 batch loss 6.3198123 epoch total loss 6.53038\n",
      "Trained batch 424 batch loss 6.38421 epoch total loss 6.53003502\n",
      "Trained batch 425 batch loss 6.20423317 epoch total loss 6.52926874\n",
      "Trained batch 426 batch loss 6.18229723 epoch total loss 6.5284543\n",
      "Trained batch 427 batch loss 6.61511517 epoch total loss 6.52865744\n",
      "Trained batch 428 batch loss 6.32193756 epoch total loss 6.52817488\n",
      "Trained batch 429 batch loss 6.60922909 epoch total loss 6.5283637\n",
      "Trained batch 430 batch loss 6.35247183 epoch total loss 6.52795458\n",
      "Trained batch 431 batch loss 6.63805342 epoch total loss 6.52820969\n",
      "Trained batch 432 batch loss 6.46586609 epoch total loss 6.5280652\n",
      "Trained batch 433 batch loss 6.22792482 epoch total loss 6.52737236\n",
      "Trained batch 434 batch loss 6.31017065 epoch total loss 6.52687168\n",
      "Trained batch 435 batch loss 6.45308208 epoch total loss 6.5267024\n",
      "Trained batch 436 batch loss 6.61279202 epoch total loss 6.5269\n",
      "Trained batch 437 batch loss 6.56385 epoch total loss 6.52698469\n",
      "Trained batch 438 batch loss 6.61468315 epoch total loss 6.52718496\n",
      "Trained batch 439 batch loss 6.67399693 epoch total loss 6.5275197\n",
      "Trained batch 440 batch loss 6.66542816 epoch total loss 6.52783298\n",
      "Trained batch 441 batch loss 6.52318382 epoch total loss 6.52782249\n",
      "Trained batch 442 batch loss 6.58850813 epoch total loss 6.5279603\n",
      "Trained batch 443 batch loss 6.53758907 epoch total loss 6.52798176\n",
      "Trained batch 444 batch loss 6.65493536 epoch total loss 6.52826786\n",
      "Trained batch 445 batch loss 6.59964561 epoch total loss 6.52842855\n",
      "Trained batch 446 batch loss 6.660851 epoch total loss 6.52872562\n",
      "Trained batch 447 batch loss 6.637043 epoch total loss 6.52896738\n",
      "Trained batch 448 batch loss 6.43785286 epoch total loss 6.52876377\n",
      "Trained batch 449 batch loss 6.87670374 epoch total loss 6.52953863\n",
      "Trained batch 450 batch loss 7.12873507 epoch total loss 6.53087044\n",
      "Trained batch 451 batch loss 7.30978966 epoch total loss 6.53259754\n",
      "Trained batch 452 batch loss 7.70823336 epoch total loss 6.53519821\n",
      "Trained batch 453 batch loss 7.49057531 epoch total loss 6.53730726\n",
      "Trained batch 454 batch loss 7.08402681 epoch total loss 6.53851128\n",
      "Trained batch 455 batch loss 6.7870574 epoch total loss 6.53905773\n",
      "Trained batch 456 batch loss 7.31902695 epoch total loss 6.54076815\n",
      "Trained batch 457 batch loss 7.06363392 epoch total loss 6.54191256\n",
      "Trained batch 458 batch loss 6.509 epoch total loss 6.54184103\n",
      "Trained batch 459 batch loss 6.7656312 epoch total loss 6.54232836\n",
      "Trained batch 460 batch loss 6.81013 epoch total loss 6.54291058\n",
      "Trained batch 461 batch loss 6.63975573 epoch total loss 6.54312038\n",
      "Trained batch 462 batch loss 6.50199032 epoch total loss 6.54303122\n",
      "Trained batch 463 batch loss 6.50822306 epoch total loss 6.54295588\n",
      "Trained batch 464 batch loss 6.39989519 epoch total loss 6.54264784\n",
      "Trained batch 465 batch loss 6.84212351 epoch total loss 6.54329157\n",
      "Trained batch 466 batch loss 6.75818157 epoch total loss 6.54375315\n",
      "Trained batch 467 batch loss 6.59728193 epoch total loss 6.54386759\n",
      "Trained batch 468 batch loss 6.63742495 epoch total loss 6.54406738\n",
      "Trained batch 469 batch loss 6.7453928 epoch total loss 6.54449654\n",
      "Trained batch 470 batch loss 6.6527133 epoch total loss 6.54472685\n",
      "Trained batch 471 batch loss 6.8075366 epoch total loss 6.54528522\n",
      "Trained batch 472 batch loss 6.71414518 epoch total loss 6.54564285\n",
      "Trained batch 473 batch loss 6.52555084 epoch total loss 6.54560041\n",
      "Trained batch 474 batch loss 6.05423069 epoch total loss 6.54456377\n",
      "Trained batch 475 batch loss 6.3895607 epoch total loss 6.54423761\n",
      "Trained batch 476 batch loss 6.36447144 epoch total loss 6.54386\n",
      "Trained batch 477 batch loss 6.50882769 epoch total loss 6.54378653\n",
      "Trained batch 478 batch loss 6.59928226 epoch total loss 6.54390287\n",
      "Trained batch 479 batch loss 6.48324156 epoch total loss 6.54377604\n",
      "Trained batch 480 batch loss 6.76943779 epoch total loss 6.5442462\n",
      "Trained batch 481 batch loss 6.54916096 epoch total loss 6.54425669\n",
      "Trained batch 482 batch loss 6.83888149 epoch total loss 6.54486752\n",
      "Trained batch 483 batch loss 6.61805773 epoch total loss 6.54501963\n",
      "Trained batch 484 batch loss 6.14490938 epoch total loss 6.54419279\n",
      "Trained batch 485 batch loss 6.61551857 epoch total loss 6.54434\n",
      "Trained batch 486 batch loss 6.87622213 epoch total loss 6.54502296\n",
      "Trained batch 487 batch loss 6.5472641 epoch total loss 6.54502773\n",
      "Trained batch 488 batch loss 6.45129538 epoch total loss 6.54483604\n",
      "Trained batch 489 batch loss 6.26353788 epoch total loss 6.5442605\n",
      "Trained batch 490 batch loss 6.20993471 epoch total loss 6.54357815\n",
      "Trained batch 491 batch loss 6.39494896 epoch total loss 6.54327536\n",
      "Trained batch 492 batch loss 6.74660158 epoch total loss 6.54368877\n",
      "Trained batch 493 batch loss 6.4295826 epoch total loss 6.54345751\n",
      "Trained batch 494 batch loss 6.74693775 epoch total loss 6.54386902\n",
      "Trained batch 495 batch loss 6.19895315 epoch total loss 6.54317236\n",
      "Trained batch 496 batch loss 6.6895771 epoch total loss 6.543468\n",
      "Trained batch 497 batch loss 6.53725863 epoch total loss 6.5434556\n",
      "Trained batch 498 batch loss 6.3379755 epoch total loss 6.54304266\n",
      "Trained batch 499 batch loss 6.34356165 epoch total loss 6.54264307\n",
      "Trained batch 500 batch loss 6.07168531 epoch total loss 6.54170132\n",
      "Trained batch 501 batch loss 6.37189436 epoch total loss 6.54136229\n",
      "Trained batch 502 batch loss 6.63956547 epoch total loss 6.54155779\n",
      "Trained batch 503 batch loss 6.58350706 epoch total loss 6.54164124\n",
      "Trained batch 504 batch loss 5.78448486 epoch total loss 6.54013872\n",
      "Trained batch 505 batch loss 5.86897802 epoch total loss 6.53881\n",
      "Trained batch 506 batch loss 5.9099865 epoch total loss 6.53756666\n",
      "Trained batch 507 batch loss 6.39869881 epoch total loss 6.53729296\n",
      "Trained batch 508 batch loss 6.19842863 epoch total loss 6.53662586\n",
      "Trained batch 509 batch loss 6.58957767 epoch total loss 6.53673\n",
      "Trained batch 510 batch loss 6.62134314 epoch total loss 6.53689575\n",
      "Trained batch 511 batch loss 6.67608452 epoch total loss 6.53716803\n",
      "Trained batch 512 batch loss 6.23896742 epoch total loss 6.53658581\n",
      "Trained batch 513 batch loss 6.2015276 epoch total loss 6.53593254\n",
      "Trained batch 514 batch loss 6.53332 epoch total loss 6.5359273\n",
      "Trained batch 515 batch loss 6.21272039 epoch total loss 6.5352993\n",
      "Trained batch 516 batch loss 6.59912443 epoch total loss 6.53542328\n",
      "Trained batch 517 batch loss 6.13500357 epoch total loss 6.53464842\n",
      "Trained batch 518 batch loss 6.59142923 epoch total loss 6.53475809\n",
      "Trained batch 519 batch loss 6.3087759 epoch total loss 6.53432274\n",
      "Trained batch 520 batch loss 6.4571104 epoch total loss 6.53417397\n",
      "Trained batch 521 batch loss 6.62244749 epoch total loss 6.53434372\n",
      "Trained batch 522 batch loss 6.79410601 epoch total loss 6.53484154\n",
      "Trained batch 523 batch loss 6.63156319 epoch total loss 6.53502655\n",
      "Trained batch 524 batch loss 6.70557547 epoch total loss 6.53535175\n",
      "Trained batch 525 batch loss 6.70158291 epoch total loss 6.53566885\n",
      "Trained batch 526 batch loss 6.67647743 epoch total loss 6.53593636\n",
      "Trained batch 527 batch loss 6.78129721 epoch total loss 6.53640175\n",
      "Trained batch 528 batch loss 6.7607131 epoch total loss 6.53682709\n",
      "Trained batch 529 batch loss 6.77415 epoch total loss 6.53727531\n",
      "Trained batch 530 batch loss 6.68739223 epoch total loss 6.53755903\n",
      "Trained batch 531 batch loss 6.72648382 epoch total loss 6.53791475\n",
      "Trained batch 532 batch loss 6.61396885 epoch total loss 6.5380578\n",
      "Trained batch 533 batch loss 6.69777632 epoch total loss 6.53835773\n",
      "Trained batch 534 batch loss 6.55625057 epoch total loss 6.53839111\n",
      "Trained batch 535 batch loss 6.58220673 epoch total loss 6.53847313\n",
      "Trained batch 536 batch loss 6.47232342 epoch total loss 6.53834963\n",
      "Trained batch 537 batch loss 6.48031473 epoch total loss 6.53824139\n",
      "Trained batch 538 batch loss 6.56343126 epoch total loss 6.53828812\n",
      "Trained batch 539 batch loss 6.6194191 epoch total loss 6.5384388\n",
      "Trained batch 540 batch loss 6.45484877 epoch total loss 6.53828382\n",
      "Trained batch 541 batch loss 6.64811754 epoch total loss 6.53848696\n",
      "Trained batch 542 batch loss 6.4361496 epoch total loss 6.53829813\n",
      "Trained batch 543 batch loss 7.02232647 epoch total loss 6.53918934\n",
      "Trained batch 544 batch loss 6.98750877 epoch total loss 6.54001331\n",
      "Trained batch 545 batch loss 6.86229229 epoch total loss 6.54060507\n",
      "Trained batch 546 batch loss 6.41332674 epoch total loss 6.54037189\n",
      "Trained batch 547 batch loss 6.32234192 epoch total loss 6.53997278\n",
      "Trained batch 548 batch loss 6.49911308 epoch total loss 6.5398984\n",
      "Trained batch 549 batch loss 6.62608528 epoch total loss 6.54005527\n",
      "Trained batch 550 batch loss 6.82178 epoch total loss 6.5405674\n",
      "Trained batch 551 batch loss 6.48089218 epoch total loss 6.54045916\n",
      "Trained batch 552 batch loss 6.77170658 epoch total loss 6.5408783\n",
      "Trained batch 553 batch loss 6.27389479 epoch total loss 6.54039526\n",
      "Trained batch 554 batch loss 6.37371635 epoch total loss 6.54009438\n",
      "Trained batch 555 batch loss 6.95084572 epoch total loss 6.5408349\n",
      "Trained batch 556 batch loss 6.82212353 epoch total loss 6.54134035\n",
      "Trained batch 557 batch loss 6.75208807 epoch total loss 6.54171896\n",
      "Trained batch 558 batch loss 6.85876274 epoch total loss 6.54228687\n",
      "Trained batch 559 batch loss 6.63561 epoch total loss 6.54245377\n",
      "Trained batch 560 batch loss 6.47182894 epoch total loss 6.54232788\n",
      "Trained batch 561 batch loss 6.51806784 epoch total loss 6.54228449\n",
      "Trained batch 562 batch loss 6.42923498 epoch total loss 6.54208326\n",
      "Trained batch 563 batch loss 6.83288622 epoch total loss 6.5426\n",
      "Trained batch 564 batch loss 6.9089489 epoch total loss 6.54324961\n",
      "Trained batch 565 batch loss 5.95838118 epoch total loss 6.54221487\n",
      "Trained batch 566 batch loss 6.26036263 epoch total loss 6.54171658\n",
      "Trained batch 567 batch loss 6.5252223 epoch total loss 6.54168749\n",
      "Trained batch 568 batch loss 6.48778486 epoch total loss 6.5415926\n",
      "Trained batch 569 batch loss 6.22194958 epoch total loss 6.54103041\n",
      "Trained batch 570 batch loss 6.53379536 epoch total loss 6.54101753\n",
      "Trained batch 571 batch loss 6.44117785 epoch total loss 6.54084301\n",
      "Trained batch 572 batch loss 6.57998228 epoch total loss 6.54091167\n",
      "Trained batch 573 batch loss 6.06583071 epoch total loss 6.54008245\n",
      "Trained batch 574 batch loss 5.65405655 epoch total loss 6.53853893\n",
      "Trained batch 575 batch loss 5.61680031 epoch total loss 6.53693581\n",
      "Trained batch 576 batch loss 6.1793313 epoch total loss 6.53631496\n",
      "Trained batch 577 batch loss 6.22912598 epoch total loss 6.53578234\n",
      "Trained batch 578 batch loss 6.3870039 epoch total loss 6.53552485\n",
      "Trained batch 579 batch loss 6.3305397 epoch total loss 6.53517103\n",
      "Trained batch 580 batch loss 6.44638634 epoch total loss 6.53501797\n",
      "Trained batch 581 batch loss 6.22018957 epoch total loss 6.5344758\n",
      "Trained batch 582 batch loss 6.49842119 epoch total loss 6.53441429\n",
      "Trained batch 583 batch loss 6.6001811 epoch total loss 6.53452682\n",
      "Trained batch 584 batch loss 7.41374874 epoch total loss 6.53603268\n",
      "Trained batch 585 batch loss 6.95419788 epoch total loss 6.53674698\n",
      "Trained batch 586 batch loss 7.13055944 epoch total loss 6.53776073\n",
      "Trained batch 587 batch loss 6.62968159 epoch total loss 6.53791714\n",
      "Trained batch 588 batch loss 7.31487703 epoch total loss 6.53923845\n",
      "Trained batch 589 batch loss 6.64679909 epoch total loss 6.53942108\n",
      "Trained batch 590 batch loss 7.04420137 epoch total loss 6.54027653\n",
      "Trained batch 591 batch loss 7.14013433 epoch total loss 6.54129171\n",
      "Trained batch 592 batch loss 7.42593813 epoch total loss 6.54278612\n",
      "Trained batch 593 batch loss 6.55612516 epoch total loss 6.54280853\n",
      "Trained batch 594 batch loss 6.30854082 epoch total loss 6.54241419\n",
      "Trained batch 595 batch loss 6.6991334 epoch total loss 6.54267788\n",
      "Trained batch 596 batch loss 6.70002127 epoch total loss 6.54294157\n",
      "Trained batch 597 batch loss 6.47537613 epoch total loss 6.54282856\n",
      "Trained batch 598 batch loss 6.44986486 epoch total loss 6.54267311\n",
      "Trained batch 599 batch loss 6.84927225 epoch total loss 6.54318523\n",
      "Trained batch 600 batch loss 6.61725855 epoch total loss 6.54330873\n",
      "Trained batch 601 batch loss 6.14973831 epoch total loss 6.54265356\n",
      "Trained batch 602 batch loss 6.44810772 epoch total loss 6.5424962\n",
      "Trained batch 603 batch loss 7.02946234 epoch total loss 6.54330397\n",
      "Trained batch 604 batch loss 6.93507099 epoch total loss 6.54395247\n",
      "Trained batch 605 batch loss 6.77450323 epoch total loss 6.54433346\n",
      "Trained batch 606 batch loss 6.69409132 epoch total loss 6.54458046\n",
      "Trained batch 607 batch loss 6.71976137 epoch total loss 6.54486942\n",
      "Trained batch 608 batch loss 6.31038761 epoch total loss 6.54448318\n",
      "Trained batch 609 batch loss 6.22802448 epoch total loss 6.54396391\n",
      "Trained batch 610 batch loss 6.59523535 epoch total loss 6.54404783\n",
      "Trained batch 611 batch loss 6.43317795 epoch total loss 6.54386616\n",
      "Trained batch 612 batch loss 6.65247202 epoch total loss 6.54404402\n",
      "Trained batch 613 batch loss 6.59257746 epoch total loss 6.54412317\n",
      "Trained batch 614 batch loss 5.94463921 epoch total loss 6.54314661\n",
      "Trained batch 615 batch loss 6.20799875 epoch total loss 6.54260159\n",
      "Trained batch 616 batch loss 6.32658529 epoch total loss 6.54225111\n",
      "Trained batch 617 batch loss 6.29478645 epoch total loss 6.54184961\n",
      "Trained batch 618 batch loss 6.4260006 epoch total loss 6.54166222\n",
      "Trained batch 619 batch loss 6.02016354 epoch total loss 6.54082\n",
      "Trained batch 620 batch loss 5.88489056 epoch total loss 6.53976202\n",
      "Trained batch 621 batch loss 6.07611465 epoch total loss 6.53901577\n",
      "Trained batch 622 batch loss 6.32058525 epoch total loss 6.53866434\n",
      "Trained batch 623 batch loss 6.34063339 epoch total loss 6.53834677\n",
      "Trained batch 624 batch loss 6.17291975 epoch total loss 6.53776073\n",
      "Trained batch 625 batch loss 6.45057 epoch total loss 6.5376215\n",
      "Trained batch 626 batch loss 6.80493927 epoch total loss 6.53804827\n",
      "Trained batch 627 batch loss 6.80006361 epoch total loss 6.53846693\n",
      "Trained batch 628 batch loss 6.69684172 epoch total loss 6.5387187\n",
      "Trained batch 629 batch loss 6.75818872 epoch total loss 6.53906775\n",
      "Trained batch 630 batch loss 7.2557168 epoch total loss 6.54020548\n",
      "Trained batch 631 batch loss 6.76559401 epoch total loss 6.54056311\n",
      "Trained batch 632 batch loss 6.82184601 epoch total loss 6.541008\n",
      "Trained batch 633 batch loss 6.94736671 epoch total loss 6.54165\n",
      "Trained batch 634 batch loss 6.64432669 epoch total loss 6.54181194\n",
      "Trained batch 635 batch loss 6.42307329 epoch total loss 6.54162455\n",
      "Trained batch 636 batch loss 6.92614555 epoch total loss 6.54222965\n",
      "Trained batch 637 batch loss 6.54036236 epoch total loss 6.54222679\n",
      "Trained batch 638 batch loss 6.26191854 epoch total loss 6.54178715\n",
      "Trained batch 639 batch loss 6.56220722 epoch total loss 6.54181862\n",
      "Trained batch 640 batch loss 6.2971797 epoch total loss 6.54143667\n",
      "Trained batch 641 batch loss 6.29984713 epoch total loss 6.54106\n",
      "Trained batch 642 batch loss 6.50548315 epoch total loss 6.54100418\n",
      "Trained batch 643 batch loss 6.01634026 epoch total loss 6.54018784\n",
      "Trained batch 644 batch loss 6.54342318 epoch total loss 6.54019308\n",
      "Trained batch 645 batch loss 6.45596743 epoch total loss 6.54006243\n",
      "Trained batch 646 batch loss 6.30415869 epoch total loss 6.53969765\n",
      "Trained batch 647 batch loss 6.3985424 epoch total loss 6.53947926\n",
      "Trained batch 648 batch loss 6.11979532 epoch total loss 6.53883123\n",
      "Trained batch 649 batch loss 6.08277 epoch total loss 6.53812885\n",
      "Trained batch 650 batch loss 6.36507225 epoch total loss 6.53786278\n",
      "Trained batch 651 batch loss 6.03114605 epoch total loss 6.53708458\n",
      "Trained batch 652 batch loss 6.40207052 epoch total loss 6.53687716\n",
      "Trained batch 653 batch loss 6.79517412 epoch total loss 6.53727293\n",
      "Trained batch 654 batch loss 7.0202837 epoch total loss 6.53801203\n",
      "Trained batch 655 batch loss 7.58220148 epoch total loss 6.53960609\n",
      "Trained batch 656 batch loss 7.38009119 epoch total loss 6.54088688\n",
      "Trained batch 657 batch loss 7.29605103 epoch total loss 6.54203606\n",
      "Trained batch 658 batch loss 6.98183584 epoch total loss 6.54270458\n",
      "Trained batch 659 batch loss 6.9318924 epoch total loss 6.54329538\n",
      "Trained batch 660 batch loss 5.85416317 epoch total loss 6.54225111\n",
      "Trained batch 661 batch loss 6.24832344 epoch total loss 6.5418067\n",
      "Trained batch 662 batch loss 6.5918088 epoch total loss 6.54188251\n",
      "Trained batch 663 batch loss 6.84683418 epoch total loss 6.54234219\n",
      "Trained batch 664 batch loss 6.65908813 epoch total loss 6.54251814\n",
      "Trained batch 665 batch loss 6.16234303 epoch total loss 6.54194593\n",
      "Trained batch 666 batch loss 6.43534136 epoch total loss 6.54178619\n",
      "Trained batch 667 batch loss 6.3743 epoch total loss 6.54153538\n",
      "Trained batch 668 batch loss 6.48242855 epoch total loss 6.54144669\n",
      "Trained batch 669 batch loss 6.24188423 epoch total loss 6.54099894\n",
      "Trained batch 670 batch loss 6.42474461 epoch total loss 6.54082537\n",
      "Trained batch 671 batch loss 6.3546524 epoch total loss 6.54054785\n",
      "Trained batch 672 batch loss 6.66393328 epoch total loss 6.54073143\n",
      "Trained batch 673 batch loss 6.54666 epoch total loss 6.54074049\n",
      "Trained batch 674 batch loss 6.53037739 epoch total loss 6.54072523\n",
      "Trained batch 675 batch loss 6.47467899 epoch total loss 6.540627\n",
      "Trained batch 676 batch loss 6.69265032 epoch total loss 6.54085255\n",
      "Trained batch 677 batch loss 6.19038677 epoch total loss 6.5403347\n",
      "Trained batch 678 batch loss 6.17917252 epoch total loss 6.53980207\n",
      "Trained batch 679 batch loss 5.9428153 epoch total loss 6.53892279\n",
      "Trained batch 680 batch loss 6.16987 epoch total loss 6.53838\n",
      "Trained batch 681 batch loss 6.46887 epoch total loss 6.5382781\n",
      "Trained batch 682 batch loss 6.17053604 epoch total loss 6.5377388\n",
      "Trained batch 683 batch loss 5.91590357 epoch total loss 6.53682852\n",
      "Trained batch 684 batch loss 5.66296196 epoch total loss 6.53555107\n",
      "Trained batch 685 batch loss 6.10479164 epoch total loss 6.5349226\n",
      "Trained batch 686 batch loss 6.22649765 epoch total loss 6.53447294\n",
      "Trained batch 687 batch loss 6.4737196 epoch total loss 6.53438425\n",
      "Trained batch 688 batch loss 6.37285233 epoch total loss 6.53414965\n",
      "Trained batch 689 batch loss 6.53022528 epoch total loss 6.5341444\n",
      "Trained batch 690 batch loss 6.59705973 epoch total loss 6.53423548\n",
      "Trained batch 691 batch loss 6.7840519 epoch total loss 6.5345974\n",
      "Trained batch 692 batch loss 6.82988834 epoch total loss 6.53502417\n",
      "Trained batch 693 batch loss 6.7057662 epoch total loss 6.53527\n",
      "Trained batch 694 batch loss 6.58403873 epoch total loss 6.53534079\n",
      "Trained batch 695 batch loss 5.77251244 epoch total loss 6.53424311\n",
      "Trained batch 696 batch loss 6.53657532 epoch total loss 6.53424644\n",
      "Trained batch 697 batch loss 6.53643036 epoch total loss 6.53425\n",
      "Trained batch 698 batch loss 6.75575209 epoch total loss 6.53456736\n",
      "Trained batch 699 batch loss 6.63363886 epoch total loss 6.53470898\n",
      "Trained batch 700 batch loss 6.66611195 epoch total loss 6.53489685\n",
      "Trained batch 701 batch loss 6.4480567 epoch total loss 6.53477335\n",
      "Trained batch 702 batch loss 6.84013367 epoch total loss 6.53520823\n",
      "Trained batch 703 batch loss 6.41983271 epoch total loss 6.53504419\n",
      "Trained batch 704 batch loss 6.51902771 epoch total loss 6.53502178\n",
      "Trained batch 705 batch loss 6.49651 epoch total loss 6.53496695\n",
      "Trained batch 706 batch loss 6.75058031 epoch total loss 6.5352726\n",
      "Trained batch 707 batch loss 6.27889824 epoch total loss 6.53490973\n",
      "Trained batch 708 batch loss 6.22933 epoch total loss 6.53447819\n",
      "Trained batch 709 batch loss 6.11670876 epoch total loss 6.53388929\n",
      "Trained batch 710 batch loss 5.84029627 epoch total loss 6.53291225\n",
      "Trained batch 711 batch loss 5.90124655 epoch total loss 6.53202391\n",
      "Trained batch 712 batch loss 6.49704933 epoch total loss 6.53197479\n",
      "Trained batch 713 batch loss 6.52144957 epoch total loss 6.53196\n",
      "Trained batch 714 batch loss 7.0372858 epoch total loss 6.53266764\n",
      "Trained batch 715 batch loss 7.0635829 epoch total loss 6.53341\n",
      "Trained batch 716 batch loss 7.00132799 epoch total loss 6.53406382\n",
      "Trained batch 717 batch loss 6.51320553 epoch total loss 6.53403473\n",
      "Trained batch 718 batch loss 6.42783403 epoch total loss 6.53388643\n",
      "Trained batch 719 batch loss 6.60521793 epoch total loss 6.53398561\n",
      "Trained batch 720 batch loss 6.18546391 epoch total loss 6.53350163\n",
      "Trained batch 721 batch loss 6.1689477 epoch total loss 6.5329957\n",
      "Trained batch 722 batch loss 6.5344696 epoch total loss 6.53299809\n",
      "Trained batch 723 batch loss 6.77387285 epoch total loss 6.53333139\n",
      "Trained batch 724 batch loss 6.751791 epoch total loss 6.53363323\n",
      "Trained batch 725 batch loss 6.6084156 epoch total loss 6.53373671\n",
      "Trained batch 726 batch loss 6.43386078 epoch total loss 6.53359938\n",
      "Trained batch 727 batch loss 6.4077611 epoch total loss 6.53342628\n",
      "Trained batch 728 batch loss 6.38751936 epoch total loss 6.53322601\n",
      "Trained batch 729 batch loss 6.06220961 epoch total loss 6.53257942\n",
      "Trained batch 730 batch loss 6.41699886 epoch total loss 6.53242111\n",
      "Trained batch 731 batch loss 6.46358252 epoch total loss 6.5323267\n",
      "Trained batch 732 batch loss 6.49516582 epoch total loss 6.53227615\n",
      "Trained batch 733 batch loss 6.88413191 epoch total loss 6.53275633\n",
      "Trained batch 734 batch loss 7.47591257 epoch total loss 6.5340414\n",
      "Trained batch 735 batch loss 7.08325911 epoch total loss 6.53478909\n",
      "Trained batch 736 batch loss 6.95499229 epoch total loss 6.53536\n",
      "Trained batch 737 batch loss 6.38598204 epoch total loss 6.53515673\n",
      "Trained batch 738 batch loss 6.25638771 epoch total loss 6.53477907\n",
      "Trained batch 739 batch loss 6.17766428 epoch total loss 6.53429604\n",
      "Trained batch 740 batch loss 6.36950684 epoch total loss 6.53407335\n",
      "Trained batch 741 batch loss 6.53732061 epoch total loss 6.53407764\n",
      "Trained batch 742 batch loss 6.37279749 epoch total loss 6.53385973\n",
      "Trained batch 743 batch loss 6.89746475 epoch total loss 6.53434944\n",
      "Trained batch 744 batch loss 6.78094959 epoch total loss 6.53468037\n",
      "Trained batch 745 batch loss 6.44950867 epoch total loss 6.5345664\n",
      "Trained batch 746 batch loss 6.36363554 epoch total loss 6.53433752\n",
      "Trained batch 747 batch loss 6.21322918 epoch total loss 6.53390789\n",
      "Trained batch 748 batch loss 6.45549679 epoch total loss 6.53380299\n",
      "Trained batch 749 batch loss 6.68640757 epoch total loss 6.53400707\n",
      "Trained batch 750 batch loss 6.75438261 epoch total loss 6.5343008\n",
      "Trained batch 751 batch loss 6.59310389 epoch total loss 6.53437948\n",
      "Trained batch 752 batch loss 6.8803544 epoch total loss 6.53483915\n",
      "Trained batch 753 batch loss 6.90079 epoch total loss 6.53532553\n",
      "Trained batch 754 batch loss 7.11599922 epoch total loss 6.5360961\n",
      "Trained batch 755 batch loss 6.9502387 epoch total loss 6.53664446\n",
      "Trained batch 756 batch loss 6.45774603 epoch total loss 6.53653955\n",
      "Trained batch 757 batch loss 6.54906797 epoch total loss 6.53655577\n",
      "Trained batch 758 batch loss 6.54516125 epoch total loss 6.53656721\n",
      "Trained batch 759 batch loss 6.8125124 epoch total loss 6.53693056\n",
      "Trained batch 760 batch loss 6.71628952 epoch total loss 6.5371666\n",
      "Trained batch 761 batch loss 6.89929581 epoch total loss 6.53764248\n",
      "Trained batch 762 batch loss 7.36264181 epoch total loss 6.53872538\n",
      "Trained batch 763 batch loss 7.2500453 epoch total loss 6.53965759\n",
      "Trained batch 764 batch loss 7.19177485 epoch total loss 6.54051113\n",
      "Trained batch 765 batch loss 6.81149101 epoch total loss 6.54086542\n",
      "Trained batch 766 batch loss 6.40233 epoch total loss 6.5406847\n",
      "Trained batch 767 batch loss 6.46678495 epoch total loss 6.54058838\n",
      "Trained batch 768 batch loss 6.80346489 epoch total loss 6.54093027\n",
      "Trained batch 769 batch loss 6.80484915 epoch total loss 6.54127359\n",
      "Trained batch 770 batch loss 7.22567368 epoch total loss 6.54216194\n",
      "Trained batch 771 batch loss 6.57826519 epoch total loss 6.54220867\n",
      "Trained batch 772 batch loss 6.57688761 epoch total loss 6.54225349\n",
      "Trained batch 773 batch loss 6.756464 epoch total loss 6.54253054\n",
      "Trained batch 774 batch loss 6.832798 epoch total loss 6.54290581\n",
      "Trained batch 775 batch loss 6.98067951 epoch total loss 6.54347038\n",
      "Trained batch 776 batch loss 7.37579107 epoch total loss 6.54454327\n",
      "Trained batch 777 batch loss 7.03188181 epoch total loss 6.54517031\n",
      "Trained batch 778 batch loss 6.63738346 epoch total loss 6.54528856\n",
      "Trained batch 779 batch loss 6.71308899 epoch total loss 6.54550362\n",
      "Trained batch 780 batch loss 6.82309818 epoch total loss 6.54586\n",
      "Trained batch 781 batch loss 7.00739717 epoch total loss 6.54645061\n",
      "Trained batch 782 batch loss 7.07901144 epoch total loss 6.54713154\n",
      "Trained batch 783 batch loss 6.96238327 epoch total loss 6.54766178\n",
      "Trained batch 784 batch loss 6.91424656 epoch total loss 6.54812956\n",
      "Trained batch 785 batch loss 6.74440336 epoch total loss 6.54837942\n",
      "Trained batch 786 batch loss 6.54703856 epoch total loss 6.54837751\n",
      "Trained batch 787 batch loss 6.54846716 epoch total loss 6.54837751\n",
      "Trained batch 788 batch loss 6.74926376 epoch total loss 6.54863214\n",
      "Trained batch 789 batch loss 6.83130169 epoch total loss 6.54899073\n",
      "Trained batch 790 batch loss 7.08198166 epoch total loss 6.54966545\n",
      "Trained batch 791 batch loss 7.01399612 epoch total loss 6.55025291\n",
      "Trained batch 792 batch loss 6.70881 epoch total loss 6.55045319\n",
      "Trained batch 793 batch loss 6.811306 epoch total loss 6.55078268\n",
      "Trained batch 794 batch loss 6.71053743 epoch total loss 6.55098343\n",
      "Trained batch 795 batch loss 6.74594355 epoch total loss 6.551229\n",
      "Trained batch 796 batch loss 6.41164446 epoch total loss 6.55105352\n",
      "Trained batch 797 batch loss 6.60638523 epoch total loss 6.55112314\n",
      "Trained batch 798 batch loss 6.72140837 epoch total loss 6.55133629\n",
      "Trained batch 799 batch loss 6.68777 epoch total loss 6.55150747\n",
      "Trained batch 800 batch loss 6.92386055 epoch total loss 6.55197287\n",
      "Trained batch 801 batch loss 6.66254568 epoch total loss 6.55211067\n",
      "Trained batch 802 batch loss 6.48215103 epoch total loss 6.55202341\n",
      "Trained batch 803 batch loss 6.6331811 epoch total loss 6.5521245\n",
      "Trained batch 804 batch loss 6.71835566 epoch total loss 6.55233097\n",
      "Trained batch 805 batch loss 6.90159082 epoch total loss 6.55276489\n",
      "Trained batch 806 batch loss 6.16670799 epoch total loss 6.55228567\n",
      "Trained batch 807 batch loss 6.40157 epoch total loss 6.55209827\n",
      "Trained batch 808 batch loss 6.44753838 epoch total loss 6.55196953\n",
      "Trained batch 809 batch loss 6.23798084 epoch total loss 6.55158091\n",
      "Trained batch 810 batch loss 6.33042669 epoch total loss 6.55130816\n",
      "Trained batch 811 batch loss 6.24821663 epoch total loss 6.55093431\n",
      "Trained batch 812 batch loss 6.68770361 epoch total loss 6.55110216\n",
      "Trained batch 813 batch loss 6.50409555 epoch total loss 6.55104446\n",
      "Trained batch 814 batch loss 6.34451771 epoch total loss 6.55079079\n",
      "Trained batch 815 batch loss 6.74692535 epoch total loss 6.55103159\n",
      "Trained batch 816 batch loss 6.64327288 epoch total loss 6.5511446\n",
      "Trained batch 817 batch loss 6.66431 epoch total loss 6.55128336\n",
      "Trained batch 818 batch loss 6.74714518 epoch total loss 6.55152273\n",
      "Trained batch 819 batch loss 6.92152071 epoch total loss 6.5519743\n",
      "Trained batch 820 batch loss 6.58480883 epoch total loss 6.55201435\n",
      "Trained batch 821 batch loss 6.7236886 epoch total loss 6.55222368\n",
      "Trained batch 822 batch loss 6.25242758 epoch total loss 6.5518589\n",
      "Trained batch 823 batch loss 5.84214401 epoch total loss 6.55099678\n",
      "Trained batch 824 batch loss 6.44840956 epoch total loss 6.55087185\n",
      "Trained batch 825 batch loss 6.81087637 epoch total loss 6.55118704\n",
      "Trained batch 826 batch loss 6.14011335 epoch total loss 6.5506897\n",
      "Trained batch 827 batch loss 6.35893059 epoch total loss 6.55045748\n",
      "Trained batch 828 batch loss 6.58663893 epoch total loss 6.55050135\n",
      "Trained batch 829 batch loss 6.30759096 epoch total loss 6.55020809\n",
      "Trained batch 830 batch loss 6.61133385 epoch total loss 6.550282\n",
      "Trained batch 831 batch loss 6.46603107 epoch total loss 6.55018\n",
      "Trained batch 832 batch loss 6.60976076 epoch total loss 6.55025196\n",
      "Trained batch 833 batch loss 6.7256546 epoch total loss 6.55046225\n",
      "Trained batch 834 batch loss 6.61287975 epoch total loss 6.55053711\n",
      "Trained batch 835 batch loss 6.9240694 epoch total loss 6.55098438\n",
      "Trained batch 836 batch loss 6.54800367 epoch total loss 6.55098057\n",
      "Trained batch 837 batch loss 6.53147793 epoch total loss 6.55095673\n",
      "Trained batch 838 batch loss 6.36330605 epoch total loss 6.55073309\n",
      "Trained batch 839 batch loss 6.36315632 epoch total loss 6.55050945\n",
      "Trained batch 840 batch loss 6.24984026 epoch total loss 6.55015182\n",
      "Trained batch 841 batch loss 6.05450439 epoch total loss 6.54956245\n",
      "Trained batch 842 batch loss 6.3718276 epoch total loss 6.54935169\n",
      "Trained batch 843 batch loss 6.49873495 epoch total loss 6.54929161\n",
      "Trained batch 844 batch loss 6.33448792 epoch total loss 6.54903698\n",
      "Trained batch 845 batch loss 6.49892664 epoch total loss 6.54897785\n",
      "Trained batch 846 batch loss 6.66788912 epoch total loss 6.54911852\n",
      "Trained batch 847 batch loss 6.21447325 epoch total loss 6.54872322\n",
      "Trained batch 848 batch loss 6.33217239 epoch total loss 6.54846764\n",
      "Trained batch 849 batch loss 6.50342751 epoch total loss 6.54841471\n",
      "Trained batch 850 batch loss 6.60650444 epoch total loss 6.54848289\n",
      "Trained batch 851 batch loss 6.74758339 epoch total loss 6.54871702\n",
      "Trained batch 852 batch loss 6.76235247 epoch total loss 6.54896736\n",
      "Trained batch 853 batch loss 6.61388 epoch total loss 6.54904318\n",
      "Trained batch 854 batch loss 6.62700748 epoch total loss 6.54913473\n",
      "Trained batch 855 batch loss 6.66440916 epoch total loss 6.54926968\n",
      "Trained batch 856 batch loss 6.66348553 epoch total loss 6.54940319\n",
      "Trained batch 857 batch loss 6.45852041 epoch total loss 6.54929686\n",
      "Trained batch 858 batch loss 6.18643141 epoch total loss 6.54887438\n",
      "Trained batch 859 batch loss 5.99687147 epoch total loss 6.54823208\n",
      "Trained batch 860 batch loss 5.95126438 epoch total loss 6.5475378\n",
      "Trained batch 861 batch loss 6.35979176 epoch total loss 6.54731941\n",
      "Trained batch 862 batch loss 6.31869316 epoch total loss 6.54705477\n",
      "Trained batch 863 batch loss 6.74500036 epoch total loss 6.54728413\n",
      "Trained batch 864 batch loss 6.47318268 epoch total loss 6.5471983\n",
      "Trained batch 865 batch loss 6.47592974 epoch total loss 6.5471158\n",
      "Trained batch 866 batch loss 6.05067301 epoch total loss 6.54654312\n",
      "Trained batch 867 batch loss 6.49268627 epoch total loss 6.54648066\n",
      "Trained batch 868 batch loss 6.10086966 epoch total loss 6.54596758\n",
      "Trained batch 869 batch loss 5.81424379 epoch total loss 6.54512596\n",
      "Trained batch 870 batch loss 6.29218149 epoch total loss 6.54483509\n",
      "Trained batch 871 batch loss 6.71262693 epoch total loss 6.54502726\n",
      "Trained batch 872 batch loss 6.37996626 epoch total loss 6.54483795\n",
      "Trained batch 873 batch loss 6.58771276 epoch total loss 6.54488707\n",
      "Trained batch 874 batch loss 6.69528246 epoch total loss 6.5450592\n",
      "Trained batch 875 batch loss 6.52805519 epoch total loss 6.54503965\n",
      "Trained batch 876 batch loss 6.66313505 epoch total loss 6.5451746\n",
      "Trained batch 877 batch loss 6.78799343 epoch total loss 6.54545116\n",
      "Trained batch 878 batch loss 6.77763891 epoch total loss 6.54571581\n",
      "Trained batch 879 batch loss 6.51014376 epoch total loss 6.54567575\n",
      "Trained batch 880 batch loss 6.6012516 epoch total loss 6.5457387\n",
      "Trained batch 881 batch loss 6.69851303 epoch total loss 6.54591227\n",
      "Trained batch 882 batch loss 6.96087933 epoch total loss 6.5463829\n",
      "Trained batch 883 batch loss 6.71757078 epoch total loss 6.54657698\n",
      "Trained batch 884 batch loss 6.55782747 epoch total loss 6.54658937\n",
      "Trained batch 885 batch loss 6.60211945 epoch total loss 6.54665232\n",
      "Trained batch 886 batch loss 6.6328187 epoch total loss 6.54674911\n",
      "Trained batch 887 batch loss 6.75530529 epoch total loss 6.54698467\n",
      "Trained batch 888 batch loss 6.66118431 epoch total loss 6.54711294\n",
      "Trained batch 889 batch loss 6.82282591 epoch total loss 6.54742336\n",
      "Trained batch 890 batch loss 6.65940666 epoch total loss 6.54754877\n",
      "Trained batch 891 batch loss 6.61849213 epoch total loss 6.5476284\n",
      "Trained batch 892 batch loss 6.24929237 epoch total loss 6.54729414\n",
      "Trained batch 893 batch loss 5.97070646 epoch total loss 6.5466485\n",
      "Trained batch 894 batch loss 5.77431345 epoch total loss 6.54578495\n",
      "Trained batch 895 batch loss 6.56221771 epoch total loss 6.54580307\n",
      "Trained batch 896 batch loss 6.34115314 epoch total loss 6.54557467\n",
      "Trained batch 897 batch loss 6.68425226 epoch total loss 6.54572916\n",
      "Trained batch 898 batch loss 6.50520134 epoch total loss 6.54568434\n",
      "Trained batch 899 batch loss 6.4953618 epoch total loss 6.54562855\n",
      "Trained batch 900 batch loss 6.86031771 epoch total loss 6.54597807\n",
      "Trained batch 901 batch loss 6.65944862 epoch total loss 6.54610443\n",
      "Trained batch 902 batch loss 6.80086899 epoch total loss 6.54638672\n",
      "Trained batch 903 batch loss 6.34603691 epoch total loss 6.54616499\n",
      "Trained batch 904 batch loss 6.54109383 epoch total loss 6.54615927\n",
      "Trained batch 905 batch loss 6.37399483 epoch total loss 6.54596901\n",
      "Trained batch 906 batch loss 6.32525444 epoch total loss 6.54572535\n",
      "Trained batch 907 batch loss 6.11877537 epoch total loss 6.54525471\n",
      "Trained batch 908 batch loss 6.41691065 epoch total loss 6.54511309\n",
      "Trained batch 909 batch loss 6.41794586 epoch total loss 6.54497337\n",
      "Trained batch 910 batch loss 5.93964672 epoch total loss 6.54430819\n",
      "Trained batch 911 batch loss 6.35045671 epoch total loss 6.54409552\n",
      "Trained batch 912 batch loss 6.62205267 epoch total loss 6.54418087\n",
      "Trained batch 913 batch loss 6.19953632 epoch total loss 6.54380369\n",
      "Trained batch 914 batch loss 6.36411762 epoch total loss 6.54360723\n",
      "Trained batch 915 batch loss 6.68060064 epoch total loss 6.54375696\n",
      "Trained batch 916 batch loss 6.62594748 epoch total loss 6.54384661\n",
      "Trained batch 917 batch loss 6.49922657 epoch total loss 6.54379797\n",
      "Trained batch 918 batch loss 6.62991619 epoch total loss 6.54389143\n",
      "Trained batch 919 batch loss 6.31452322 epoch total loss 6.54364204\n",
      "Trained batch 920 batch loss 6.17695522 epoch total loss 6.54324293\n",
      "Trained batch 921 batch loss 6.3633275 epoch total loss 6.5430479\n",
      "Trained batch 922 batch loss 6.59730959 epoch total loss 6.54310656\n",
      "Trained batch 923 batch loss 6.75045681 epoch total loss 6.54333115\n",
      "Trained batch 924 batch loss 6.98828697 epoch total loss 6.54381275\n",
      "Trained batch 925 batch loss 7.02390385 epoch total loss 6.54433155\n",
      "Trained batch 926 batch loss 6.98745537 epoch total loss 6.5448103\n",
      "Trained batch 927 batch loss 7.26341343 epoch total loss 6.54558516\n",
      "Trained batch 928 batch loss 6.50417662 epoch total loss 6.54554081\n",
      "Trained batch 929 batch loss 5.8795948 epoch total loss 6.54482365\n",
      "Trained batch 930 batch loss 5.71595526 epoch total loss 6.54393196\n",
      "Trained batch 931 batch loss 5.54527664 epoch total loss 6.54285955\n",
      "Trained batch 932 batch loss 5.58667755 epoch total loss 6.54183388\n",
      "Trained batch 933 batch loss 5.92223644 epoch total loss 6.54117\n",
      "Trained batch 934 batch loss 6.29561806 epoch total loss 6.54090691\n",
      "Trained batch 935 batch loss 6.56304169 epoch total loss 6.54093027\n",
      "Trained batch 936 batch loss 6.64028168 epoch total loss 6.54103661\n",
      "Trained batch 937 batch loss 6.74421358 epoch total loss 6.54125309\n",
      "Trained batch 938 batch loss 6.42037058 epoch total loss 6.54112434\n",
      "Trained batch 939 batch loss 6.37993383 epoch total loss 6.54095268\n",
      "Trained batch 940 batch loss 6.61812 epoch total loss 6.5410347\n",
      "Trained batch 941 batch loss 6.80438423 epoch total loss 6.5413146\n",
      "Trained batch 942 batch loss 6.74429321 epoch total loss 6.54152966\n",
      "Trained batch 943 batch loss 6.64028 epoch total loss 6.54163456\n",
      "Trained batch 944 batch loss 6.43103838 epoch total loss 6.54151726\n",
      "Trained batch 945 batch loss 5.6266408 epoch total loss 6.54054928\n",
      "Trained batch 946 batch loss 5.78228951 epoch total loss 6.53974724\n",
      "Trained batch 947 batch loss 6.13314295 epoch total loss 6.53931808\n",
      "Trained batch 948 batch loss 6.27634239 epoch total loss 6.53904104\n",
      "Trained batch 949 batch loss 6.13598824 epoch total loss 6.53861618\n",
      "Trained batch 950 batch loss 6.32726526 epoch total loss 6.53839397\n",
      "Trained batch 951 batch loss 5.88496923 epoch total loss 6.53770638\n",
      "Trained batch 952 batch loss 6.23254108 epoch total loss 6.53738594\n",
      "Trained batch 953 batch loss 6.11831617 epoch total loss 6.53694582\n",
      "Trained batch 954 batch loss 5.91551065 epoch total loss 6.53629446\n",
      "Trained batch 955 batch loss 6.77001429 epoch total loss 6.53653908\n",
      "Trained batch 956 batch loss 6.44484282 epoch total loss 6.53644323\n",
      "Trained batch 957 batch loss 5.81583 epoch total loss 6.53569031\n",
      "Trained batch 958 batch loss 5.46928024 epoch total loss 6.53457737\n",
      "Trained batch 959 batch loss 6.17050314 epoch total loss 6.53419733\n",
      "Trained batch 960 batch loss 6.78997421 epoch total loss 6.53446388\n",
      "Trained batch 961 batch loss 6.43096352 epoch total loss 6.53435659\n",
      "Trained batch 962 batch loss 6.34484529 epoch total loss 6.53415918\n",
      "Trained batch 963 batch loss 6.55667305 epoch total loss 6.53418255\n",
      "Trained batch 964 batch loss 6.87007332 epoch total loss 6.53453112\n",
      "Trained batch 965 batch loss 6.55819798 epoch total loss 6.53455544\n",
      "Trained batch 966 batch loss 6.47417068 epoch total loss 6.53449297\n",
      "Trained batch 967 batch loss 6.20638371 epoch total loss 6.53415394\n",
      "Trained batch 968 batch loss 6.38187742 epoch total loss 6.53399658\n",
      "Trained batch 969 batch loss 6.33754873 epoch total loss 6.53379393\n",
      "Trained batch 970 batch loss 6.16278458 epoch total loss 6.53341103\n",
      "Trained batch 971 batch loss 6.44029474 epoch total loss 6.53331518\n",
      "Trained batch 972 batch loss 6.64450169 epoch total loss 6.53342962\n",
      "Trained batch 973 batch loss 6.41607141 epoch total loss 6.53330898\n",
      "Trained batch 974 batch loss 6.59811974 epoch total loss 6.53337574\n",
      "Trained batch 975 batch loss 6.63261509 epoch total loss 6.53347778\n",
      "Trained batch 976 batch loss 6.10556507 epoch total loss 6.53303909\n",
      "Trained batch 977 batch loss 6.2895937 epoch total loss 6.53278971\n",
      "Trained batch 978 batch loss 7.01431656 epoch total loss 6.5332818\n",
      "Trained batch 979 batch loss 6.57287598 epoch total loss 6.53332233\n",
      "Trained batch 980 batch loss 6.68873835 epoch total loss 6.53348112\n",
      "Trained batch 981 batch loss 6.91711807 epoch total loss 6.53387213\n",
      "Trained batch 982 batch loss 6.80832338 epoch total loss 6.53415155\n",
      "Trained batch 983 batch loss 6.62359428 epoch total loss 6.53424215\n",
      "Trained batch 984 batch loss 6.54628658 epoch total loss 6.53425455\n",
      "Trained batch 985 batch loss 6.53789 epoch total loss 6.53425837\n",
      "Trained batch 986 batch loss 6.40086794 epoch total loss 6.53412342\n",
      "Trained batch 987 batch loss 5.95151615 epoch total loss 6.5335331\n",
      "Trained batch 988 batch loss 6.41878414 epoch total loss 6.53341722\n",
      "Trained batch 989 batch loss 6.51134968 epoch total loss 6.53339481\n",
      "Trained batch 990 batch loss 6.79190254 epoch total loss 6.53365612\n",
      "Trained batch 991 batch loss 6.70337343 epoch total loss 6.5338273\n",
      "Trained batch 992 batch loss 6.69495153 epoch total loss 6.53399\n",
      "Trained batch 993 batch loss 6.41001797 epoch total loss 6.53386497\n",
      "Trained batch 994 batch loss 6.37731838 epoch total loss 6.53370762\n",
      "Trained batch 995 batch loss 6.63597631 epoch total loss 6.53381\n",
      "Trained batch 996 batch loss 6.86715174 epoch total loss 6.53414488\n",
      "Trained batch 997 batch loss 6.7724719 epoch total loss 6.53438377\n",
      "Trained batch 998 batch loss 6.07653284 epoch total loss 6.53392506\n",
      "Trained batch 999 batch loss 6.5304637 epoch total loss 6.53392172\n",
      "Trained batch 1000 batch loss 6.01330614 epoch total loss 6.53340101\n",
      "Trained batch 1001 batch loss 6.60178518 epoch total loss 6.5334692\n",
      "Trained batch 1002 batch loss 6.36542 epoch total loss 6.53330088\n",
      "Trained batch 1003 batch loss 6.40972 epoch total loss 6.53317785\n",
      "Trained batch 1004 batch loss 6.3057 epoch total loss 6.53295135\n",
      "Trained batch 1005 batch loss 6.32488537 epoch total loss 6.53274393\n",
      "Trained batch 1006 batch loss 6.42745256 epoch total loss 6.53263903\n",
      "Trained batch 1007 batch loss 6.39319181 epoch total loss 6.53250074\n",
      "Trained batch 1008 batch loss 6.90534401 epoch total loss 6.53287029\n",
      "Trained batch 1009 batch loss 6.59193373 epoch total loss 6.53292894\n",
      "Trained batch 1010 batch loss 6.04414082 epoch total loss 6.53244448\n",
      "Trained batch 1011 batch loss 6.62485933 epoch total loss 6.53253603\n",
      "Trained batch 1012 batch loss 6.93949699 epoch total loss 6.532938\n",
      "Trained batch 1013 batch loss 6.64306688 epoch total loss 6.53304672\n",
      "Trained batch 1014 batch loss 6.80330753 epoch total loss 6.53331327\n",
      "Trained batch 1015 batch loss 6.5896287 epoch total loss 6.53336906\n",
      "Trained batch 1016 batch loss 6.31986094 epoch total loss 6.53315878\n",
      "Trained batch 1017 batch loss 6.34938812 epoch total loss 6.53297853\n",
      "Trained batch 1018 batch loss 6.27203846 epoch total loss 6.532722\n",
      "Trained batch 1019 batch loss 6.08112431 epoch total loss 6.53227901\n",
      "Trained batch 1020 batch loss 6.8912096 epoch total loss 6.53263044\n",
      "Trained batch 1021 batch loss 6.50745153 epoch total loss 6.53260565\n",
      "Trained batch 1022 batch loss 6.38988924 epoch total loss 6.53246593\n",
      "Trained batch 1023 batch loss 6.57360744 epoch total loss 6.53250647\n",
      "Trained batch 1024 batch loss 6.68630838 epoch total loss 6.53265667\n",
      "Trained batch 1025 batch loss 6.68145084 epoch total loss 6.5328021\n",
      "Trained batch 1026 batch loss 6.59801579 epoch total loss 6.53286552\n",
      "Trained batch 1027 batch loss 6.4772768 epoch total loss 6.53281116\n",
      "Trained batch 1028 batch loss 6.31956863 epoch total loss 6.53260374\n",
      "Trained batch 1029 batch loss 6.02393866 epoch total loss 6.53210926\n",
      "Trained batch 1030 batch loss 5.74781275 epoch total loss 6.53134823\n",
      "Trained batch 1031 batch loss 5.81010818 epoch total loss 6.53064871\n",
      "Trained batch 1032 batch loss 5.63886166 epoch total loss 6.5297842\n",
      "Trained batch 1033 batch loss 5.90114975 epoch total loss 6.52917576\n",
      "Trained batch 1034 batch loss 5.38603878 epoch total loss 6.52807045\n",
      "Trained batch 1035 batch loss 5.25149918 epoch total loss 6.52683687\n",
      "Trained batch 1036 batch loss 6.20903206 epoch total loss 6.52653027\n",
      "Trained batch 1037 batch loss 6.58452 epoch total loss 6.52658606\n",
      "Trained batch 1038 batch loss 6.52087688 epoch total loss 6.52658081\n",
      "Trained batch 1039 batch loss 6.54063034 epoch total loss 6.52659416\n",
      "Trained batch 1040 batch loss 6.62162685 epoch total loss 6.52668571\n",
      "Trained batch 1041 batch loss 6.74713326 epoch total loss 6.52689743\n",
      "Trained batch 1042 batch loss 6.6808691 epoch total loss 6.52704477\n",
      "Trained batch 1043 batch loss 6.29170752 epoch total loss 6.52681875\n",
      "Trained batch 1044 batch loss 5.78334141 epoch total loss 6.52610683\n",
      "Trained batch 1045 batch loss 5.72149801 epoch total loss 6.52533674\n",
      "Trained batch 1046 batch loss 6.16196632 epoch total loss 6.5249896\n",
      "Trained batch 1047 batch loss 6.36851788 epoch total loss 6.52484035\n",
      "Trained batch 1048 batch loss 6.12963676 epoch total loss 6.52446318\n",
      "Trained batch 1049 batch loss 6.09919071 epoch total loss 6.52405739\n",
      "Trained batch 1050 batch loss 6.37564325 epoch total loss 6.52391624\n",
      "Trained batch 1051 batch loss 6.35266924 epoch total loss 6.52375317\n",
      "Trained batch 1052 batch loss 6.57602501 epoch total loss 6.52380276\n",
      "Trained batch 1053 batch loss 6.51900434 epoch total loss 6.52379847\n",
      "Trained batch 1054 batch loss 6.6754961 epoch total loss 6.52394199\n",
      "Trained batch 1055 batch loss 6.63663244 epoch total loss 6.52404881\n",
      "Trained batch 1056 batch loss 6.24232531 epoch total loss 6.52378178\n",
      "Trained batch 1057 batch loss 4.66751814 epoch total loss 6.52202559\n",
      "Trained batch 1058 batch loss 5.2734704 epoch total loss 6.52084541\n",
      "Trained batch 1059 batch loss 6.77428532 epoch total loss 6.52108526\n",
      "Trained batch 1060 batch loss 7.18706179 epoch total loss 6.52171326\n",
      "Trained batch 1061 batch loss 7.41423607 epoch total loss 6.5225544\n",
      "Trained batch 1062 batch loss 6.95484924 epoch total loss 6.52296162\n",
      "Trained batch 1063 batch loss 6.5909071 epoch total loss 6.52302551\n",
      "Trained batch 1064 batch loss 6.42182779 epoch total loss 6.52293062\n",
      "Trained batch 1065 batch loss 6.46801138 epoch total loss 6.52287865\n",
      "Trained batch 1066 batch loss 6.40394449 epoch total loss 6.52276707\n",
      "Trained batch 1067 batch loss 5.84850597 epoch total loss 6.52213526\n",
      "Trained batch 1068 batch loss 5.8315053 epoch total loss 6.52148867\n",
      "Trained batch 1069 batch loss 5.79957294 epoch total loss 6.52081347\n",
      "Trained batch 1070 batch loss 6.17831087 epoch total loss 6.52049303\n",
      "Trained batch 1071 batch loss 6.56227493 epoch total loss 6.52053261\n",
      "Trained batch 1072 batch loss 6.75684166 epoch total loss 6.52075291\n",
      "Trained batch 1073 batch loss 6.81300497 epoch total loss 6.52102518\n",
      "Trained batch 1074 batch loss 6.62962532 epoch total loss 6.52112627\n",
      "Trained batch 1075 batch loss 6.58884096 epoch total loss 6.52118921\n",
      "Trained batch 1076 batch loss 6.66998768 epoch total loss 6.5213275\n",
      "Trained batch 1077 batch loss 6.1114254 epoch total loss 6.5209465\n",
      "Trained batch 1078 batch loss 5.79813576 epoch total loss 6.52027655\n",
      "Trained batch 1079 batch loss 6.39311934 epoch total loss 6.52015829\n",
      "Trained batch 1080 batch loss 6.6494627 epoch total loss 6.52027798\n",
      "Trained batch 1081 batch loss 6.54131794 epoch total loss 6.520298\n",
      "Trained batch 1082 batch loss 6.69198561 epoch total loss 6.52045631\n",
      "Trained batch 1083 batch loss 6.81741047 epoch total loss 6.5207305\n",
      "Trained batch 1084 batch loss 6.43695 epoch total loss 6.52065325\n",
      "Trained batch 1085 batch loss 6.42621469 epoch total loss 6.52056646\n",
      "Trained batch 1086 batch loss 6.54346943 epoch total loss 6.52058744\n",
      "Trained batch 1087 batch loss 6.4826169 epoch total loss 6.52055216\n",
      "Trained batch 1088 batch loss 6.59238243 epoch total loss 6.52061844\n",
      "Trained batch 1089 batch loss 6.84035683 epoch total loss 6.52091169\n",
      "Trained batch 1090 batch loss 6.74039793 epoch total loss 6.52111292\n",
      "Trained batch 1091 batch loss 6.74982691 epoch total loss 6.52132273\n",
      "Trained batch 1092 batch loss 6.61319113 epoch total loss 6.52140713\n",
      "Trained batch 1093 batch loss 6.47709084 epoch total loss 6.5213666\n",
      "Trained batch 1094 batch loss 6.73318863 epoch total loss 6.52156\n",
      "Trained batch 1095 batch loss 6.84794903 epoch total loss 6.52185869\n",
      "Trained batch 1096 batch loss 6.33290625 epoch total loss 6.52168608\n",
      "Trained batch 1097 batch loss 6.6073451 epoch total loss 6.52176428\n",
      "Trained batch 1098 batch loss 6.62305641 epoch total loss 6.52185678\n",
      "Trained batch 1099 batch loss 6.09094715 epoch total loss 6.52146435\n",
      "Trained batch 1100 batch loss 5.85468578 epoch total loss 6.52085781\n",
      "Trained batch 1101 batch loss 6.59777355 epoch total loss 6.52092791\n",
      "Trained batch 1102 batch loss 6.67223167 epoch total loss 6.52106524\n",
      "Trained batch 1103 batch loss 6.58779049 epoch total loss 6.52112579\n",
      "Trained batch 1104 batch loss 6.38869572 epoch total loss 6.52100563\n",
      "Trained batch 1105 batch loss 6.27446508 epoch total loss 6.52078247\n",
      "Trained batch 1106 batch loss 6.51295376 epoch total loss 6.52077579\n",
      "Trained batch 1107 batch loss 6.53713226 epoch total loss 6.52079058\n",
      "Trained batch 1108 batch loss 6.45404625 epoch total loss 6.5207305\n",
      "Trained batch 1109 batch loss 6.42093468 epoch total loss 6.52064037\n",
      "Trained batch 1110 batch loss 6.49798918 epoch total loss 6.52062\n",
      "Trained batch 1111 batch loss 6.5728035 epoch total loss 6.52066708\n",
      "Trained batch 1112 batch loss 6.61481762 epoch total loss 6.52075148\n",
      "Trained batch 1113 batch loss 6.48137808 epoch total loss 6.52071619\n",
      "Trained batch 1114 batch loss 6.50608349 epoch total loss 6.52070284\n",
      "Trained batch 1115 batch loss 6.22835064 epoch total loss 6.52044058\n",
      "Trained batch 1116 batch loss 6.33233166 epoch total loss 6.52027225\n",
      "Trained batch 1117 batch loss 6.38265753 epoch total loss 6.52014923\n",
      "Trained batch 1118 batch loss 6.33763838 epoch total loss 6.51998568\n",
      "Trained batch 1119 batch loss 6.22734118 epoch total loss 6.51972437\n",
      "Trained batch 1120 batch loss 5.96943474 epoch total loss 6.51923323\n",
      "Trained batch 1121 batch loss 6.2888031 epoch total loss 6.51902723\n",
      "Trained batch 1122 batch loss 6.55815458 epoch total loss 6.51906204\n",
      "Trained batch 1123 batch loss 6.76088619 epoch total loss 6.5192771\n",
      "Trained batch 1124 batch loss 6.82704163 epoch total loss 6.51955128\n",
      "Trained batch 1125 batch loss 6.71378374 epoch total loss 6.51972389\n",
      "Trained batch 1126 batch loss 6.61266279 epoch total loss 6.51980639\n",
      "Trained batch 1127 batch loss 6.18258381 epoch total loss 6.51950741\n",
      "Trained batch 1128 batch loss 5.5307045 epoch total loss 6.51863098\n",
      "Trained batch 1129 batch loss 5.8123188 epoch total loss 6.51800537\n",
      "Trained batch 1130 batch loss 6.38409138 epoch total loss 6.51788712\n",
      "Trained batch 1131 batch loss 6.70568848 epoch total loss 6.51805305\n",
      "Trained batch 1132 batch loss 6.42804813 epoch total loss 6.5179739\n",
      "Trained batch 1133 batch loss 6.51953077 epoch total loss 6.51797485\n",
      "Trained batch 1134 batch loss 6.37334442 epoch total loss 6.51784754\n",
      "Trained batch 1135 batch loss 6.04834414 epoch total loss 6.51743412\n",
      "Trained batch 1136 batch loss 6.08237123 epoch total loss 6.51705122\n",
      "Trained batch 1137 batch loss 6.57180119 epoch total loss 6.51709938\n",
      "Trained batch 1138 batch loss 6.81036377 epoch total loss 6.51735735\n",
      "Trained batch 1139 batch loss 6.73440456 epoch total loss 6.51754761\n",
      "Trained batch 1140 batch loss 6.23586035 epoch total loss 6.51730061\n",
      "Trained batch 1141 batch loss 6.40527773 epoch total loss 6.51720238\n",
      "Trained batch 1142 batch loss 6.54356718 epoch total loss 6.51722527\n",
      "Trained batch 1143 batch loss 6.88197851 epoch total loss 6.51754427\n",
      "Trained batch 1144 batch loss 6.85414314 epoch total loss 6.51783848\n",
      "Trained batch 1145 batch loss 6.87133121 epoch total loss 6.51814699\n",
      "Trained batch 1146 batch loss 6.75158787 epoch total loss 6.5183506\n",
      "Trained batch 1147 batch loss 6.31319571 epoch total loss 6.51817179\n",
      "Trained batch 1148 batch loss 6.35480881 epoch total loss 6.51802921\n",
      "Trained batch 1149 batch loss 6.3976841 epoch total loss 6.51792431\n",
      "Trained batch 1150 batch loss 6.60833836 epoch total loss 6.51800299\n",
      "Trained batch 1151 batch loss 6.66449928 epoch total loss 6.5181303\n",
      "Trained batch 1152 batch loss 6.61288357 epoch total loss 6.5182128\n",
      "Trained batch 1153 batch loss 6.47514105 epoch total loss 6.51817513\n",
      "Trained batch 1154 batch loss 6.55455923 epoch total loss 6.51820707\n",
      "Trained batch 1155 batch loss 6.5318861 epoch total loss 6.51821852\n",
      "Trained batch 1156 batch loss 7.01602697 epoch total loss 6.5186491\n",
      "Trained batch 1157 batch loss 6.7104969 epoch total loss 6.51881504\n",
      "Trained batch 1158 batch loss 6.94292259 epoch total loss 6.51918125\n",
      "Trained batch 1159 batch loss 6.54035854 epoch total loss 6.5192\n",
      "Trained batch 1160 batch loss 6.98931456 epoch total loss 6.51960468\n",
      "Trained batch 1161 batch loss 6.44995451 epoch total loss 6.51954508\n",
      "Trained batch 1162 batch loss 6.56930494 epoch total loss 6.51958799\n",
      "Trained batch 1163 batch loss 6.8611517 epoch total loss 6.51988173\n",
      "Trained batch 1164 batch loss 6.65950203 epoch total loss 6.52000189\n",
      "Trained batch 1165 batch loss 6.6757822 epoch total loss 6.5201354\n",
      "Trained batch 1166 batch loss 6.98381233 epoch total loss 6.52053356\n",
      "Trained batch 1167 batch loss 6.26045132 epoch total loss 6.5203104\n",
      "Trained batch 1168 batch loss 6.66038847 epoch total loss 6.52043\n",
      "Trained batch 1169 batch loss 6.51303768 epoch total loss 6.52042389\n",
      "Trained batch 1170 batch loss 6.83509445 epoch total loss 6.52069283\n",
      "Trained batch 1171 batch loss 6.84678411 epoch total loss 6.5209713\n",
      "Trained batch 1172 batch loss 6.37376499 epoch total loss 6.52084541\n",
      "Trained batch 1173 batch loss 6.54033089 epoch total loss 6.5208621\n",
      "Trained batch 1174 batch loss 6.59201527 epoch total loss 6.52092266\n",
      "Trained batch 1175 batch loss 6.60753918 epoch total loss 6.52099609\n",
      "Trained batch 1176 batch loss 6.49454927 epoch total loss 6.52097368\n",
      "Trained batch 1177 batch loss 6.37205362 epoch total loss 6.52084732\n",
      "Trained batch 1178 batch loss 6.72436857 epoch total loss 6.52102\n",
      "Trained batch 1179 batch loss 5.43288 epoch total loss 6.52009726\n",
      "Trained batch 1180 batch loss 5.85105419 epoch total loss 6.5195303\n",
      "Trained batch 1181 batch loss 5.35085869 epoch total loss 6.51854086\n",
      "Trained batch 1182 batch loss 5.87064028 epoch total loss 6.51799297\n",
      "Trained batch 1183 batch loss 6.67177153 epoch total loss 6.51812315\n",
      "Trained batch 1184 batch loss 6.7362051 epoch total loss 6.51830721\n",
      "Trained batch 1185 batch loss 6.19803286 epoch total loss 6.51803732\n",
      "Trained batch 1186 batch loss 6.56440926 epoch total loss 6.51807642\n",
      "Trained batch 1187 batch loss 6.44298124 epoch total loss 6.518013\n",
      "Trained batch 1188 batch loss 6.41275263 epoch total loss 6.51792431\n",
      "Trained batch 1189 batch loss 6.1801672 epoch total loss 6.51764\n",
      "Trained batch 1190 batch loss 6.35083723 epoch total loss 6.5175004\n",
      "Trained batch 1191 batch loss 5.66168928 epoch total loss 6.51678133\n",
      "Trained batch 1192 batch loss 5.97614813 epoch total loss 6.51632786\n",
      "Trained batch 1193 batch loss 6.3361392 epoch total loss 6.5161767\n",
      "Trained batch 1194 batch loss 6.51215601 epoch total loss 6.51617336\n",
      "Trained batch 1195 batch loss 5.99325609 epoch total loss 6.51573563\n",
      "Trained batch 1196 batch loss 5.54996586 epoch total loss 6.51492786\n",
      "Trained batch 1197 batch loss 5.6829958 epoch total loss 6.51423311\n",
      "Trained batch 1198 batch loss 5.73212719 epoch total loss 6.51358032\n",
      "Trained batch 1199 batch loss 6.04206657 epoch total loss 6.51318693\n",
      "Trained batch 1200 batch loss 6.47734 epoch total loss 6.51315737\n",
      "Trained batch 1371 batch loss 6.50024509 epoch total loss 6.51862669\n",
      "Trained batch 1372 batch loss 6.6408534 epoch total loss 6.51871538\n",
      "Trained batch 1373 batch loss 6.5958662 epoch total loss 6.51877165\n",
      "Trained batch 1374 batch loss 6.1602416 epoch total loss 6.51851082\n",
      "Trained batch 1375 batch loss 6.49958897 epoch total loss 6.51849699\n",
      "Trained batch 1376 batch loss 6.40123892 epoch total loss 6.51841211\n",
      "Trained batch 1377 batch loss 6.07064629 epoch total loss 6.51808643\n",
      "Trained batch 1378 batch loss 6.57813406 epoch total loss 6.5181303\n",
      "Trained batch 1379 batch loss 6.3098278 epoch total loss 6.51797915\n",
      "Trained batch 1380 batch loss 6.32045 epoch total loss 6.51783562\n",
      "Trained batch 1381 batch loss 6.53918409 epoch total loss 6.51785088\n",
      "Trained batch 1382 batch loss 6.19496107 epoch total loss 6.5176177\n",
      "Trained batch 1383 batch loss 6.11561775 epoch total loss 6.51732683\n",
      "Trained batch 1384 batch loss 6.41985178 epoch total loss 6.51725626\n",
      "Trained batch 1385 batch loss 6.05848026 epoch total loss 6.51692533\n",
      "Trained batch 1386 batch loss 6.59397459 epoch total loss 6.51698065\n",
      "Trained batch 1387 batch loss 6.71603203 epoch total loss 6.51712418\n",
      "Trained batch 1388 batch loss 6.35630751 epoch total loss 6.5170083\n",
      "Epoch 10 train loss 6.517008304595947\n",
      "Validated batch 1 batch loss 6.78710938\n",
      "Validated batch 2 batch loss 6.55088902\n",
      "Validated batch 3 batch loss 6.39906025\n",
      "Validated batch 4 batch loss 6.18774\n",
      "Validated batch 5 batch loss 6.46644306\n",
      "Validated batch 6 batch loss 6.50349426\n",
      "Validated batch 7 batch loss 6.49283886\n",
      "Validated batch 8 batch loss 6.64400101\n",
      "Validated batch 9 batch loss 6.75133371\n",
      "Validated batch 10 batch loss 6.418715\n",
      "Validated batch 11 batch loss 6.52924109\n",
      "Validated batch 12 batch loss 5.9865222\n",
      "Validated batch 13 batch loss 6.95578527\n",
      "Validated batch 14 batch loss 6.36371374\n",
      "Validated batch 15 batch loss 6.50899839\n",
      "Validated batch 16 batch loss 6.71931076\n",
      "Validated batch 17 batch loss 6.53369141\n",
      "Validated batch 18 batch loss 5.87865496\n",
      "Validated batch 19 batch loss 6.26083708\n",
      "Validated batch 20 batch loss 6.60071945\n",
      "Validated batch 21 batch loss 6.28794861\n",
      "Validated batch 22 batch loss 6.49443579\n",
      "Validated batch 23 batch loss 6.54472971\n",
      "Validated batch 24 batch loss 6.27943182\n",
      "Validated batch 25 batch loss 6.8707962\n",
      "Validated batch 26 batch loss 6.59310436\n",
      "Validated batch 27 batch loss 6.7731986\n",
      "Validated batch 28 batch loss 6.70056248\n",
      "Validated batch 29 batch loss 6.95650339\n",
      "Validated batch 30 batch loss 6.53999949\n",
      "Validated batch 31 batch loss 6.87820244\n",
      "Validated batch 32 batch loss 6.3422761\n",
      "Validated batch 33 batch loss 6.65046549\n",
      "Validated batch 34 batch loss 6.61385155\n",
      "Validated batch 35 batch loss 6.01055956\n",
      "Validated batch 36 batch loss 6.03886938\n",
      "Validated batch 37 batch loss 6.86874771\n",
      "Validated batch 38 batch loss 6.73852921\n",
      "Validated batch 39 batch loss 6.26219034\n",
      "Validated batch 40 batch loss 6.74646378\n",
      "Validated batch 41 batch loss 6.78970194\n",
      "Validated batch 42 batch loss 6.73257542\n",
      "Validated batch 43 batch loss 6.84916782\n",
      "Validated batch 44 batch loss 6.77126169\n",
      "Validated batch 45 batch loss 6.46020222\n",
      "Validated batch 46 batch loss 6.21270514\n",
      "Validated batch 47 batch loss 6.44066238\n",
      "Validated batch 48 batch loss 6.55883026\n",
      "Validated batch 49 batch loss 6.37876\n",
      "Validated batch 50 batch loss 7.10880136\n",
      "Validated batch 51 batch loss 6.79260111\n",
      "Validated batch 52 batch loss 6.72899246\n",
      "Validated batch 53 batch loss 6.71079063\n",
      "Validated batch 54 batch loss 6.67571878\n",
      "Validated batch 55 batch loss 6.77491522\n",
      "Validated batch 56 batch loss 6.82334\n",
      "Validated batch 57 batch loss 7.13982391\n",
      "Validated batch 58 batch loss 6.75951147\n",
      "Validated batch 59 batch loss 6.37842894\n",
      "Validated batch 60 batch loss 6.80733347\n",
      "Validated batch 61 batch loss 6.46529913\n",
      "Validated batch 62 batch loss 6.62388229\n",
      "Validated batch 63 batch loss 6.94113541\n",
      "Validated batch 64 batch loss 5.79362822\n",
      "Validated batch 65 batch loss 6.59262705\n",
      "Validated batch 66 batch loss 6.70145178\n",
      "Validated batch 67 batch loss 6.51688147\n",
      "Validated batch 68 batch loss 7.14861822\n",
      "Validated batch 69 batch loss 6.45256567\n",
      "Validated batch 70 batch loss 6.63724279\n",
      "Validated batch 71 batch loss 6.54212952\n",
      "Validated batch 72 batch loss 6.31646\n",
      "Validated batch 73 batch loss 6.29706144\n",
      "Validated batch 74 batch loss 6.65155554\n",
      "Validated batch 75 batch loss 6.57933044\n",
      "Validated batch 76 batch loss 6.63027716\n",
      "Validated batch 77 batch loss 6.53295898\n",
      "Validated batch 78 batch loss 6.39110518\n",
      "Validated batch 79 batch loss 6.84792089\n",
      "Validated batch 80 batch loss 6.95620489\n",
      "Validated batch 81 batch loss 6.66611242\n",
      "Validated batch 82 batch loss 6.6367774\n",
      "Validated batch 83 batch loss 6.46606\n",
      "Validated batch 84 batch loss 6.64222336\n",
      "Validated batch 85 batch loss 6.49541187\n",
      "Validated batch 86 batch loss 6.87484598\n",
      "Validated batch 87 batch loss 6.42753935\n",
      "Validated batch 88 batch loss 6.48034191\n",
      "Validated batch 89 batch loss 6.67799139\n",
      "Validated batch 90 batch loss 6.29924631\n",
      "Validated batch 91 batch loss 6.81532669\n",
      "Validated batch 92 batch loss 6.78619\n",
      "Validated batch 93 batch loss 6.54494333\n",
      "Validated batch 94 batch loss 6.57179356\n",
      "Validated batch 95 batch loss 6.39252281\n",
      "Validated batch 96 batch loss 5.78484201\n",
      "Validated batch 97 batch loss 6.25006723\n",
      "Validated batch 98 batch loss 6.99489975\n",
      "Validated batch 99 batch loss 6.25842047\n",
      "Validated batch 100 batch loss 6.60769415\n",
      "Validated batch 101 batch loss 6.65264654\n",
      "Validated batch 102 batch loss 6.73193884\n",
      "Validated batch 103 batch loss 6.85159683\n",
      "Validated batch 104 batch loss 6.47535515\n",
      "Validated batch 105 batch loss 6.55527353\n",
      "Validated batch 106 batch loss 6.50201941\n",
      "Validated batch 107 batch loss 6.68318272\n",
      "Validated batch 108 batch loss 7.06697607\n",
      "Validated batch 109 batch loss 6.36092758\n",
      "Validated batch 110 batch loss 6.89834452\n",
      "Validated batch 111 batch loss 6.79214239\n",
      "Validated batch 112 batch loss 6.31001902\n",
      "Validated batch 113 batch loss 6.56370926\n",
      "Validated batch 114 batch loss 6.61636877\n",
      "Validated batch 115 batch loss 6.90096092\n",
      "Validated batch 116 batch loss 6.73851538\n",
      "Validated batch 117 batch loss 7.18390465\n",
      "Validated batch 118 batch loss 6.56844854\n",
      "Validated batch 119 batch loss 5.88647223\n",
      "Validated batch 120 batch loss 6.60175323\n",
      "Validated batch 121 batch loss 6.41110563\n",
      "Validated batch 122 batch loss 6.15917492\n",
      "Validated batch 123 batch loss 6.27159643\n",
      "Validated batch 124 batch loss 6.06341076\n",
      "Validated batch 125 batch loss 6.81390238\n",
      "Validated batch 126 batch loss 6.26853\n",
      "Validated batch 127 batch loss 6.34696245\n",
      "Validated batch 128 batch loss 6.06249571\n",
      "Validated batch 129 batch loss 6.75841284\n",
      "Validated batch 130 batch loss 6.23122\n",
      "Validated batch 131 batch loss 6.55930328\n",
      "Validated batch 132 batch loss 6.42869663\n",
      "Validated batch 133 batch loss 6.77420568\n",
      "Validated batch 134 batch loss 6.3866396\n",
      "Validated batch 135 batch loss 6.62723303\n",
      "Validated batch 136 batch loss 6.74035645\n",
      "Validated batch 137 batch loss 5.70261431\n",
      "Validated batch 138 batch loss 6.67982864\n",
      "Validated batch 139 batch loss 6.56584263\n",
      "Validated batch 140 batch loss 6.33583546\n",
      "Validated batch 141 batch loss 6.55104685\n",
      "Validated batch 142 batch loss 6.55968285\n",
      "Validated batch 143 batch loss 6.18688965\n",
      "Validated batch 144 batch loss 6.92000961\n",
      "Validated batch 145 batch loss 6.67203903\n",
      "Validated batch 146 batch loss 6.52754307\n",
      "Validated batch 147 batch loss 6.69576645\n",
      "Validated batch 148 batch loss 6.83240366\n",
      "Validated batch 149 batch loss 6.52377176\n",
      "Validated batch 150 batch loss 6.53068399\n",
      "Validated batch 151 batch loss 6.48748302\n",
      "Validated batch 152 batch loss 6.73051262\n",
      "Validated batch 153 batch loss 6.88628244\n",
      "Validated batch 154 batch loss 6.63989782\n",
      "Validated batch 155 batch loss 6.3640604\n",
      "Validated batch 156 batch loss 6.1906724\n",
      "Validated batch 157 batch loss 6.56890106\n",
      "Validated batch 158 batch loss 6.63757181\n",
      "Validated batch 159 batch loss 6.63403177\n",
      "Validated batch 160 batch loss 6.71987677\n",
      "Validated batch 161 batch loss 6.44019175\n",
      "Validated batch 162 batch loss 6.98367071\n",
      "Validated batch 163 batch loss 6.69091558\n",
      "Validated batch 164 batch loss 6.77868652\n",
      "Validated batch 165 batch loss 6.58240891\n",
      "Validated batch 166 batch loss 6.6174736\n",
      "Validated batch 167 batch loss 6.90405893\n",
      "Validated batch 168 batch loss 6.65839243\n",
      "Validated batch 169 batch loss 6.27267\n",
      "Validated batch 170 batch loss 6.26306963\n",
      "Validated batch 171 batch loss 6.52731848\n",
      "Validated batch 172 batch loss 6.61211395\n",
      "Validated batch 173 batch loss 6.56817627\n",
      "Validated batch 174 batch loss 6.34563208\n",
      "Validated batch 175 batch loss 6.26771545\n",
      "Validated batch 176 batch loss 6.51661301\n",
      "Validated batch 177 batch loss 6.43410397\n",
      "Validated batch 178 batch loss 6.75685644\n",
      "Validated batch 179 batch loss 6.80525827\n",
      "Validated batch 180 batch loss 7.04004526\n",
      "Validated batch 181 batch loss 7.47790384\n",
      "Validated batch 182 batch loss 7.23755\n",
      "Validated batch 183 batch loss 6.58373117\n",
      "Validated batch 184 batch loss 6.15560818\n",
      "Validated batch 185 batch loss 3.29788733\n",
      "Epoch 10 val loss 6.555246353149414\n",
      "Epoch 10 completed in 765.25 seconds\n",
      "Training time: 126.16 minutes\n"
     ]
    }
   ],
   "source": [
    "train_tfrecords = os.path.join(TFRECORD_PATH, 'train*')\n",
    "val_tfrecords = os.path.join(TFRECORD_PATH, 'val*')\n",
    "epochs = 10\n",
    "batch_size = 16\n",
    "num_heatmap = 16\n",
    "learning_rate = 0.0007\n",
    "\n",
    "start_time = time.time()\n",
    "best_model_file = train(epochs, learning_rate, num_heatmap, batch_size, train_tfrecords, val_tfrecords)\n",
    "\n",
    "# 종료 시간 기록 및 경과 시간 계산\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "# 학습 소요 시간 출력 (분 단위로 변환)\n",
    "elapsed_minutes = elapsed_time / 60\n",
    "print(f\"Training time: {elapsed_minutes:.2f} minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770ed5aa",
   "metadata": {},
   "source": [
    "\n",
    "Epoch 1 completed in 765.84 seconds 대략 1epoch 당 10분 발생하는 것을 확인할 수 있습니다.\n",
    "\n",
    "에러 해결 부분 resnet 과 up_conv 를 전역변수로 사용하여 scope 에러가 발생한것  resnet과 up_conv 를 simplebaseline model 안에 넣어놈\n",
    "\n",
    "stack 영역에 변수를 할당 했어야했는데 전역변수로 선언하여 데이터 영역에 변수를 할당하여 scope 에러가 발생했다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e1ac4d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "WEIGHTS_PATH = './model_simplebase-epoch-7-loss-6.4812.h5'\n",
    "\n",
    "model = Simplebaseline(input_shape=(256, 256, 3), num_heatmap=16)\n",
    "model.load_weights(WEIGHTS_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c317d59c",
   "metadata": {},
   "source": [
    "## stackhourglass 결과"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "72974707",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_image = os.path.join(PROJECT_PATH, 'test_image.jpg')\n",
    "\n",
    "image, keypoints = predict(model, test_image)\n",
    "draw_keypoints_on_image(image, keypoints)\n",
    "draw_skeleton_on_image(image, keypoints)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011488f6",
   "metadata": {},
   "source": [
    "사람에 대한 object 위치는 알맞게 들어가 있다.\n",
    "\n",
    "top-down을 수행하는 decoder 부분에서 이미지의 해상도를 interpolation 하는 작업은 어느정도 정상적으로 학습이 된 것을 볼 수 있다.\n",
    "\n",
    "\n",
    "key keypoint 의 위치는 오른쪽 팔과 왼쪽 어깨 오른쪽 발 왼쪽 발를 찾았지만 오른쪽 발과 왼쪽 발을 연결 시킨것을 보아 \n",
    "\n",
    "bottom-up을 수행하는 encoder 부분에서 이미지의 해상도를 낮추어  keypo관계를 잘 학습하지 못한것으로 보인다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d171d26",
   "metadata": {},
   "source": [
    "## simple base model 결과"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "76ba6e43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_image = os.path.join(PROJECT_PATH, 'test_image.jpg')\n",
    "\n",
    "image, keypoints = predict(model, test_image)\n",
    "draw_keypoints_on_image(image, keypoints)\n",
    "draw_skeleton_on_image(image, keypoints)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d4052f",
   "metadata": {},
   "source": [
    "simple base model keypoint 간의 관계성을 잘 찾아냈지만 사람이라는 object 부분은 잘 찾지 못하는 것을 확인할 수 있다.\n",
    "bottom up 부분인 decoder에서 keypoint간의 관계성와 세밀하게 어느 위치에 있는지 잘 훈련이 된거 같지만 top-down 으로 찾아내는 encoder 부분에서는 잘 훈련이 된거 같지 않다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4927fa0",
   "metadata": {},
   "source": [
    "## 두 모델의 차이점"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be6a4dc",
   "metadata": {},
   "source": [
    "stakhourglass model은 objectd의 위치를 잘 찾아내지만 keypoint 처럼 좁은 이미지와 keypoint 간의 연관성을 잘 찾아내지 못한다. 그럼 반면 simplebaseline model은 keypoint간의 관계성과 좁은 이미지들을 잘 찾아내지만 전체 큰 이미지의 object 위치를 잘 찾아내지 못한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb87e14",
   "metadata": {},
   "source": [
    "## 회고\n",
    "\n",
    "\n",
    "\n",
    "모델의 전역 변수를 줄이는 습관을 가져야겠다.\n",
    "\n",
    "flops로 모델의 평가를 해보는것도 나중에 도움이 될거 같다. \n",
    "\n",
    "hr-net이 해상도별로 다양하게 보는 모델인거 같아서 이 모델을 backbone으로 넣어서 한번 해봐야겠다.\n",
    "\n",
    "\n",
    "현재는 gpu 가 1개여서 분산처리가 되지 않은것 같다 google colab의 gpu를 원격으로 가져와서 gpu를 두개 쓰는 starge를 써봐야겠다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
